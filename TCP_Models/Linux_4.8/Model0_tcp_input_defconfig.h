#ifndef Model0_TCP_INPUT_DEFCONFIG_H
#define Model0_TCP_INPUT_DEFCONFIG_H
/*
 *
 * Automatically generated file; DO NOT EDIT.
 * Linux/x86_64 4.8.0 Kernel Configuration
 *
 */




/*
 * The use of "&&" / "||" is limited in certain expressions.
 * The followings enable to calculate "and" / "or" with macro expansion only.
 */
/*
 * Helper macros to use CONFIG_ options in C/CPP expressions. Note that
 * these only work with boolean and tristate options.
 */

/*
 * Getting something that works in C and CPP for an arg that may or may
 * not be defined is tricky.  Here, if we have "#define CONFIG_BOOGER 1"
 * we match on the placeholder define, insert the "0," for arg1 and generate
 * the triplet (0, 1, 0).  Then the last step cherry picks the 2nd arg (a one).
 * When CONFIG_BOOGER is not defined, we generate a (... 1, 0) pair, and when
 * the last step cherry picks the 2nd arg, we get a zero.
 */





/*
 * IS_BUILTIN(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'y', 0
 * otherwise. For boolean options, this is equivalent to
 * IS_ENABLED(CONFIG_FOO).
 */


/*
 * IS_MODULE(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'm', 0
 * otherwise.
 */


/*
 * IS_REACHABLE(CONFIG_FOO) evaluates to 1 if the currently compiled
 * code can call a function defined in code compiled based on CONFIG_FOO.
 * This is similar to IS_ENABLED(), but returns false when invoked from
 * built-in code when CONFIG_FOO is set to 'm'.
 */



/*
 * IS_ENABLED(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'y' or 'm',
 * 0 otherwise.
 */
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Implementation of the Transmission Control Protocol(TCP).
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Mark Evans, <evansmp@uhura.aston.ac.uk>
 *		Corey Minyard <wf-rch!minyard@relay.EU.net>
 *		Florian La Roche, <flla@stud.uni-sb.de>
 *		Charles Hedrick, <hedrick@klinzhai.rutgers.edu>
 *		Linus Torvalds, <torvalds@cs.helsinki.fi>
 *		Alan Cox, <gw4pts@gw4pts.ampr.org>
 *		Matthew Dillon, <dillon@apollo.west.oic.com>
 *		Arnt Gulbrandsen, <agulbra@nvg.unit.no>
 *		Jorge Cwik, <jorge@laser.satlink.net>
 */

/*
 * Changes:
 *		Pedro Roque	:	Fast Retransmit/Recovery.
 *					Two receive queues.
 *					Retransmit queue handled by TCP.
 *					Better retransmit timer handling.
 *					New congestion avoidance.
 *					Header prediction.
 *					Variable renaming.
 *
 *		Eric		:	Fast Retransmit.
 *		Randy Scott	:	MSS option defines.
 *		Eric Schenk	:	Fixes to slow start algorithm.
 *		Eric Schenk	:	Yet another double ACK bug.
 *		Eric Schenk	:	Delayed ACK bug fixes.
 *		Eric Schenk	:	Floyd style fast retrans war avoidance.
 *		David S. Miller	:	Don't allow zero congestion window.
 *		Eric Schenk	:	Fix retransmitter so that it sends
 *					next packet on ack of previous packet.
 *		Andi Kleen	:	Moved open_request checking here
 *					and process RSTs for open_requests.
 *		Andi Kleen	:	Better prune_queue, and other fixes.
 *		Andrey Savochkin:	Fix RTT measurements in the presence of
 *					timestamps.
 *		Andrey Savochkin:	Check sequence numbers correctly when
 *					removing SACKs due to in sequence incoming
 *					data segments.
 *		Andi Kleen:		Make sure we never ack data there is not
 *					enough room for. Also make this condition
 *					a fatal error if it might still happen.
 *		Andi Kleen:		Add tcp_measure_rcv_mss to make
 *					connections with MSS<min(MTU,ann. MSS)
 *					work without delayed acks.
 *		Andi Kleen:		Process packets with PSH set in the
 *					fast path.
 *		J Hadi Salim:		ECN support
 *	 	Andrei Gurtov,
 *		Pasi Sarolahti,
 *		Panu Kuhlberg:		Experimental audit of TCP (re)transmission
 *					engine. Lots of bugs are found.
 *		Pasi Sarolahti:		F-RTO for dealing with spurious RTOs
 */


















/*
 * This error code is special: arch syscall entry code will return
 * -ENOSYS if users try to call a syscall that doesn't exist.  To keep
 * failures of syscalls that really do exist distinguishable from
 * failures due to attempts to use a nonexistent syscall, syscall
 * implementations should refrain from returning -ENOSYS.
 */
/* for robust mutexes */


/*
 * These should never be seen by user programs.  To return one of ERESTART*
 * codes, signal_pending() MUST be set.  Note that ptrace can observe these
 * at syscall exit tracing, but they will never be left for the debugged user
 * process to see.
 */
/* Defined for the NFSv3 protocol */












/* Indirect macros required for expanded argument pasting, eg. __LINE__. */











/*
 * Common definitions for all gcc versions go here.
 */




/* Optimization barrier */

/* The "volatile" is due to gcc bugs */

/*
 * This version is i.e. to prevent dead stores elimination on @ptr
 * where gcc and llvm may behave differently when otherwise using
 * normal barrier(): while gcc behavior gets along with a normal
 * barrier(), llvm needs an explicit input variable to be assumed
 * clobbered. The issue is as follows: while the inline asm might
 * access any memory it wants, the compiler could have fit all of
 * @ptr into memory registers instead, and since @ptr never escaped
 * from that, it proofed that the inline asm wasn't touching any of
 * it. This version works well with both compilers, i.e. we're telling
 * the compiler that the inline asm absolutely may see the contents
 * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495
 */


/*
 * This macro obfuscates arithmetic on a variable address so that gcc
 * shouldn't recognize the original var, and make assumptions about it.
 *
 * This is needed because the C standard makes it undefined to do
 * pointer arithmetic on "objects" outside their boundaries and the
 * gcc optimizers assume this is the case. In particular they
 * assume such arithmetic does not wrap.
 *
 * A miscompilation has been observed because of this on PPC.
 * To work around it we hide the relationship of the pointer and the object
 * using this macro.
 *
 * Versions of the ppc64 compiler before 4.1 had a bug where use of
 * RELOC_HIDE could trash r30. The bug can be worked around by changing
 * the inline assembly constraint from =g to =r, in this particular
 * case either is valid.
 */







/* Make the optimizer believe the variable can be manipulated arbitrarily. */






/* &a[0] degrades to a pointer: a different type from an array */



/*
 * Force always-inline if the user requests it so via the .config,
 * or if gcc is too old:
 */






/* A lot of inline functions can cause havoc with function tracing */
/*
 * it doesn't make sense on ARM (currently the only user of __naked)
 * to trace naked functions because then mcount is called without
 * stack and frame pointer being set up and there is no chance to
 * restore the lr register to the value before mcount was called.
 *
 * The asm() bodies of naked functions often depend on standard calling
 * conventions, therefore they must be noinline and noclone.
 *
 * GCC 4.[56] currently fail to enforce this, so we must do so ourselves.
 * See GCC PR44290.
 */




/*
 * From the GCC manual:
 *
 * Many functions have no effects except the return value and their
 * return value depends only on the parameters and/or global
 * variables.  Such a function can be subject to common subexpression
 * elimination and loop optimization just as an arithmetic operator
 * would be.
 * [...]
 */
/* gcc version specific checks */
/* GCC 4.1.[01] miscompiles __weak */
/*
 * GCC 'asm goto' miscompiles certain code sequences:
 *
 *   http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58670
 *
 * Work it around via a compiler barrier quirk suggested by Jakub Jelinek.
 *
 * (asm goto is automatically volatile - the naming reflects this.)
 */


/*
 * sparse (__CHECKER__) pretends to be gcc, but can't do constant
 * folding in __builtin_bswap*() (yet), so don't set these for it.
 */
/*
 * A trick to suppress uninitialized variable warning without generating any
 * code
 */








/* Intel compiler defines __GNUC__. So we will overwrite implementations
 * coming from above header files here
 */




/* Clang compiler defines __GNUC__. So we will overwrite implementations
 * coming from above header files here
 */






/* Some compiler specific definitions are overwritten here
 * for Clang compiler
 */






/* same as gcc, this was present in clang-2.6 so we can assume it works
 * with any version that can compile the kernel
 */


/*
 * Generic compiler-dependent macros required for kernel
 * build go below this comment. Actual compiler/compiler version
 * specific implementations come from the above header files
 */

struct Model0_ftrace_branch_data {
 const char *func;
 const char *Model0_file;
 unsigned Model0_line;
 union {
  struct {
   unsigned long Model0_correct;
   unsigned long Model0_incorrect;
  };
  struct {
   unsigned long Model0_miss;
   unsigned long Model0_hit;
  };
  unsigned long Model0_miss_hit[2];
 };
};

/*
 * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
 * to disable branch tracing on a per file basis.
 */
/* Optimization barrier */
/* Unreachable code */
/* Not-quite-unique ID. */













/*
 * int-ll64 is used everywhere now.
 */

/*
 * asm-generic/int-ll64.h
 *
 * Integer declarations for architectures which use "long long"
 * for 64-bit types.
 */




/*
 * asm-generic/int-ll64.h
 *
 * Integer declarations for architectures which use "long long"
 * for 64-bit types.
 */











/*
 * There seems to be no way of detecting this automatically from user
 * space, so 64 bit architectures should override this in their
 * bitsperlong.h. In particular, an architecture that supports
 * both 32 and 64 bit user space must not rely on CONFIG_64BIT
 * to decide it, but rather check a compiler provided macro.
 */








/*
 * FIXME: The check currently breaks x86-64 build, so it's
 * temporarily disabled. Please fix x86-64 and reenable
 */


/*
 * __xx is ok: it doesn't pollute the POSIX namespace. Use these in the
 * header files exported to user space
 */

typedef __signed__ char Model0___s8;
typedef unsigned char __u8;

typedef __signed__ short Model0___s16;
typedef unsigned short Model0___u16;

typedef __signed__ int Model0___s32;
typedef unsigned int __u32;


__extension__ typedef __signed__ long long Model0___s64;
__extension__ typedef unsigned long long __u64;




typedef signed char Model0_s8;
typedef unsigned char Model0_u8;

typedef signed short Model0_s16;
typedef unsigned short Model0_u16;

typedef signed int Model0_s32;
typedef unsigned int Model0_u32;

typedef signed long long Model0_s64;
typedef unsigned long long Model0_u64;


















enum {
 false = 0,
 true = 1
};
/**
 * offsetofend(TYPE, MEMBER)
 *
 * @TYPE: The type of the structure
 * @MEMBER: The member within the structure to get the end offset of
 */

/*
 * This allows for 1024 file descriptors: if NR_OPEN is ever grown
 * beyond that you'll have to change this too. But 1024 fd's seem to be
 * enough even for such "real" unices like OSF/1, so hopefully this is
 * one limit that doesn't have to be changed [again].
 *
 * Note that POSIX wants the FD_CLEAR(fd,fdsetp) defines to be in
 * <sys/time.h> (and thus <linux/time.h>) - but this is a more logical
 * place for them. Solved by having dummy defines in <sys/time.h>.
 */

/*
 * This macro may have been defined in <gnu/types.h>. But we always
 * use the one here.
 */



typedef struct {
 unsigned long Model0_fds_bits[1024 / (8 * sizeof(long))];
} Model0___kernel_fd_set;

/* Type of a signal handler.  */
typedef void (*Model0___kernel_sighandler_t)(int);

/* Type of a SYSV IPC key.  */
typedef int Model0___kernel_key_t;
typedef int Model0___kernel_mqd_t;








/*
 * This file is generally used by user-level software, so you need to
 * be a little careful about namespace pollution etc.  Also, we cannot
 * assume GCC is being used.
 */

typedef unsigned short Model0___kernel_old_uid_t;
typedef unsigned short Model0___kernel_old_gid_t;


typedef unsigned long Model0___kernel_old_dev_t;







/*
 * This file is generally used by user-level software, so you need to
 * be a little careful about namespace pollution etc.
 *
 * First the types that are often defined in different ways across
 * architectures, so that you can override them.
 */


typedef long Model0___kernel_long_t;
typedef unsigned long Model0___kernel_ulong_t;



typedef Model0___kernel_ulong_t Model0___kernel_ino_t;



typedef unsigned int Model0___kernel_mode_t;



typedef int Model0___kernel_pid_t;



typedef int Model0___kernel_ipc_pid_t;



typedef unsigned int Model0___kernel_uid_t;
typedef unsigned int Model0___kernel_gid_t;



typedef Model0___kernel_long_t Model0___kernel_suseconds_t;



typedef int Model0___kernel_daddr_t;



typedef unsigned int Model0___kernel_uid32_t;
typedef unsigned int Model0___kernel_gid32_t;
/*
 * Most 32 bit architectures use "unsigned int" size_t,
 * and all 64 bit architectures use "unsigned long" size_t.
 */






typedef Model0___kernel_ulong_t Model0___kernel_size_t;
typedef Model0___kernel_long_t Model0___kernel_ssize_t;
typedef Model0___kernel_long_t Model0___kernel_ptrdiff_t;




typedef struct {
 int Model0_val[2];
} Model0___kernel_fsid_t;


/*
 * anything below here should be completely generic
 */
typedef Model0___kernel_long_t Model0___kernel_off_t;
typedef long long Model0___kernel_loff_t;
typedef Model0___kernel_long_t Model0___kernel_time_t;
typedef Model0___kernel_long_t Model0___kernel_clock_t;
typedef int Model0___kernel_timer_t;
typedef int Model0___kernel_clockid_t;
typedef char * Model0___kernel_caddr_t;
typedef unsigned short Model0___kernel_uid16_t;
typedef unsigned short Model0___kernel_gid16_t;


/*
 * Below are truly Linux-specific types that should never collide with
 * any application/library that wants linux/types.h.
 */
typedef Model0___u16 Model0___le16;
typedef Model0___u16 Model0___be16;
typedef __u32 Model0___le32;
typedef __u32 Model0___be32;
typedef __u64 Model0___le64;
typedef __u64 Model0___be64;

typedef Model0___u16 Model0___sum16;
typedef __u32 Model0___wsum;

/*
 * aligned_u64 should be used in defining kernel<->userspace ABIs to avoid
 * common 32/64-bit compat problems.
 * 64-bit values align to 4-byte boundaries on x86_32 (and possibly other
 * architectures) and to 8-byte boundaries on 64-bit architectures.  The new
 * aligned_64 type enforces 8-byte alignment so that structs containing
 * aligned_64 values have the same alignment on 32-bit and 64-bit architectures.
 * No conversions are necessary between 32-bit user-space and a 64-bit kernel.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline))
void Model0___read_once_size(const volatile void *Model0_p, void *Model0_res, int Model0_size)
{
 ({ switch (Model0_size) { case 1: *(__u8 *)Model0_res = *(volatile __u8 *)Model0_p; break; case 2: *(Model0___u16 *)Model0_res = *(volatile Model0___u16 *)Model0_p; break; case 4: *(__u32 *)Model0_res = *(volatile __u32 *)Model0_p; break; case 8: *(__u64 *)Model0_res = *(volatile __u64 *)Model0_p; break; default: __asm__ __volatile__("": : :"memory"); __builtin_memcpy((void *)Model0_res, (const void *)Model0_p, Model0_size); __asm__ __volatile__("": : :"memory"); } });
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline))
void Model0___read_once_size_nocheck(const volatile void *Model0_p, void *Model0_res, int Model0_size)
{
 ({ switch (Model0_size) { case 1: *(__u8 *)Model0_res = *(volatile __u8 *)Model0_p; break; case 2: *(Model0___u16 *)Model0_res = *(volatile Model0___u16 *)Model0_p; break; case 4: *(__u32 *)Model0_res = *(volatile __u32 *)Model0_p; break; case 8: *(__u64 *)Model0_res = *(volatile __u64 *)Model0_p; break; default: __asm__ __volatile__("": : :"memory"); __builtin_memcpy((void *)Model0_res, (const void *)Model0_p, Model0_size); __asm__ __volatile__("": : :"memory"); } });
}


static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___write_once_size(volatile void *Model0_p, void *Model0_res, int Model0_size)
{
 switch (Model0_size) {
 case 1: *(volatile __u8 *)Model0_p = *(__u8 *)Model0_res; break;
 case 2: *(volatile Model0___u16 *)Model0_p = *(Model0___u16 *)Model0_res; break;
 case 4: *(volatile __u32 *)Model0_p = *(__u32 *)Model0_res; break;
 case 8: *(volatile __u64 *)Model0_p = *(__u64 *)Model0_res; break;
 default:
  __asm__ __volatile__("": : :"memory");
  __builtin_memcpy((void *)Model0_p, (const void *)Model0_res, Model0_size);
  __asm__ __volatile__("": : :"memory");
 }
}

/*
 * Prevent the compiler from merging or refetching reads or writes. The
 * compiler is also forbidden from reordering successive instances of
 * READ_ONCE, WRITE_ONCE and ACCESS_ONCE (see below), but only when the
 * compiler is aware of some particular ordering.  One way to make the
 * compiler aware of ordering is to put the two invocations of READ_ONCE,
 * WRITE_ONCE or ACCESS_ONCE() in different C statements.
 *
 * In contrast to ACCESS_ONCE these two macros will also work on aggregate
 * data types like structs or unions. If the size of the accessed data
 * type exceeds the word size of the machine (e.g., 32 bits or 64 bits)
 * READ_ONCE() and WRITE_ONCE() will fall back to memcpy(). There's at
 * least two memcpy()s: one for the __builtin_memcpy() and then one for
 * the macro doing the copy of variable - '__u' allocated on the stack.
 *
 * Their two major use cases are: (1) Mediating communication between
 * process-level code and irq/NMI handlers, all running on the same CPU,
 * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
 * mutilate accesses that either do not require ordering or that interact
 * with an explicit memory barrier or atomic instruction that provides the
 * required ordering.
 */
/*
 * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need
 * to hide memory access from KASAN.
 */
/*
 * Allow us to mark functions as 'deprecated' and have gcc emit a nice
 * warning for each use, in hopes of speeding the functions removal.
 * Usage is:
 * 		int __deprecated foo(void)
 */
/*
 * Allow us to avoid 'defined but not used' warnings on functions and data,
 * as well as force them to be emitted to the assembly file.
 *
 * As of gcc 3.4, static functions that are not marked with attribute((used))
 * may be elided from the assembly file.  As of gcc 3.4, static data not so
 * marked will not be elided, but this may change in a future gcc version.
 *
 * NOTE: Because distributions shipped with a backported unit-at-a-time
 * compiler in gcc 3.3, we must define __used to be __attribute__((used))
 * for gcc >=3.3 instead of 3.4.
 *
 * In prior versions of gcc, such functions and data would be emitted, but
 * would be warned about except with attribute((unused)).
 *
 * Mark functions that are referenced only in inline assembly as __used so
 * the code is emitted even though it appears to be unreferenced.
 */
/*
 * Rather then using noinline to prevent stack consumption, use
 * noinline_for_stack instead.  For documentation reasons.
 */
/*
 * From the GCC manual:
 *
 * Many functions do not examine any values except their arguments,
 * and have no effects except the return value.  Basically this is
 * just slightly more strict class than the `pure' attribute above,
 * since function is not allowed to read global memory.
 *
 * Note that a function that has pointer arguments and examines the
 * data pointed to must _not_ be declared `const'.  Likewise, a
 * function that calls a non-`const' function usually must not be
 * `const'.  It does not make sense for a `const' function to return
 * `void'.
 */




/*
 * Tell gcc if a function is cold. The compiler will assume any path
 * directly leading to the call is unlikely.
 */





/* Simple shorthand for a section definition */
/*
 * Assume alignment of return value.
 */





/* Are two types/vars the same type (ignoring qualifiers)? */




/* Is this type a native word size -- useful for atomic operations */




/* Compile time object size, -1 for unknown */
/*
 * Sparse complains of variable sized arrays due to the temporary variable in
 * __compiletime_assert. Unfortunately we can't just expand it out to make
 * sparse see a constant array size without breaking compiletime_assert on old
 * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
 */
/**
 * compiletime_assert - break build and emit msg if condition is false
 * @condition: a compile-time constant condition to check
 * @msg:       a message to emit if condition is false
 *
 * In tradition of POSIX assert, this macro will break the build if the
 * supplied condition is *false*, emitting the supplied error message if the
 * compiler has support to do so.
 */







/*
 * Prevent the compiler from merging or refetching accesses.  The compiler
 * is also forbidden from reordering successive instances of ACCESS_ONCE(),
 * but only when the compiler is aware of some particular ordering.  One way
 * to make the compiler aware of ordering is to put the two invocations of
 * ACCESS_ONCE() in different C statements.
 *
 * ACCESS_ONCE will only work on scalar types. For union types, ACCESS_ONCE
 * on a union member will work as long as the size of the member matches the
 * size of the union and the size is smaller than word size.
 *
 * The major use cases of ACCESS_ONCE used to be (1) Mediating communication
 * between process-level code and irq/NMI handlers, all running on the same CPU,
 * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
 * mutilate accesses that either do not require ordering or that interact
 * with an explicit memory barrier or atomic instruction that provides the
 * required ordering.
 *
 * If possible use READ_ONCE()/WRITE_ONCE() instead.
 */





/**
 * lockless_dereference() - safely load a pointer for later dereference
 * @p: The pointer to load
 *
 * Similar to rcu_dereference(), but for situations where the pointed-to
 * object's lifetime is managed by something other than RCU.  That
 * "something other" might be reference counting or simple immortality.
 *
 * The seemingly unused variable ___typecheck_p validates that @p is
 * indeed a pointer type by using a pointer to typeof(*p) as the type.
 * Taking a pointer to typeof(*p) again is needed in case p is void *.
 */
/* Ignore/forbid kprobes attach on very low level functions marked by this attribute: */












/*===---- stdarg.h - Variable argument handling ----------------------------===
 *
 * Copyright (c) 2008 Eli Friedman
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */





typedef __builtin_va_list Model0_va_list;






/* GCC always defines __va_copy, but does not define va_copy unless in c99 mode
 * or -ansi is not specified, since it was not part of C90.
 */
typedef __builtin_va_list __gnuc_va_list;







/* Indirect stringification.  Doing two levels allows the parameter to be a
 * macro itself.  For example, compile with -DFOO=bar, __stringify(FOO)
 * converts to "bar".
 */


/*
 * Export symbols from the kernel to modules.  Forked from module.h
 * to reduce the amount of pointless cruft we feed to gcc when only
 * exporting a simple symbol or two.
 *
 * Try not to add #includes here.  It slows compilation and makes kernel
 * hackers place grumpy comments in header files.
 */

/* Some toolchains use a `_' prefix for all user symbols. */
/* Indirect, so macros are expanded before pasting. */




struct Model0_kernel_symbol
{
 unsigned long Model0_value;
 const char *Model0_name;
};
/* For every exported symbol, place a struct in the __ksymtab section */

/* Some toolchains use other characters (e.g. '`') to mark new line in macro */
/*
 * For assembly routines.
 *
 * Note when using these that you must specify the appropriate
 * alignment directives yourself
 */



/*
 * This is used by architectures to keep arguments on the stack
 * untouched by the compiler by keeping them live until the end.
 * The argument stack may be owned by the assembly-language
 * caller, not the callee, and gcc doesn't always understand
 * that.
 *
 * We have the return value, and a maximum of six arguments.
 *
 * This should always be followed by a "return ret" for the
 * protection to work (ie no more work that the compiler might
 * end up needing stack temporaries for).
 */
/* Assembly files may be compiled with -traditional .. */

typedef __u32 Model0___kernel_dev_t;

typedef Model0___kernel_fd_set Model0_fd_set;
typedef Model0___kernel_dev_t Model0_dev_t;
typedef Model0___kernel_ino_t Model0_ino_t;
typedef Model0___kernel_mode_t Model0_mode_t;
typedef unsigned short Model0_umode_t;
typedef __u32 Model0_nlink_t;
typedef Model0___kernel_off_t Model0_off_t;
typedef Model0___kernel_pid_t Model0_pid_t;
typedef Model0___kernel_daddr_t Model0_daddr_t;
typedef Model0___kernel_key_t Model0_key_t;
typedef Model0___kernel_suseconds_t Model0_suseconds_t;
typedef Model0___kernel_timer_t Model0_timer_t;
typedef Model0___kernel_clockid_t Model0_clockid_t;
typedef Model0___kernel_mqd_t Model0_mqd_t;

typedef _Bool bool;

typedef Model0___kernel_uid32_t Model0_uid_t;
typedef Model0___kernel_gid32_t Model0_gid_t;
typedef Model0___kernel_uid16_t Model0_uid16_t;
typedef Model0___kernel_gid16_t Model0_gid16_t;

typedef unsigned long Model0_uintptr_t;


/* This is defined by include/asm-{arch}/posix_types.h */
typedef Model0___kernel_old_uid_t Model0_old_uid_t;
typedef Model0___kernel_old_gid_t Model0_old_gid_t;



typedef Model0___kernel_loff_t Model0_loff_t;


/*
 * The following typedefs are also protected by individual ifdefs for
 * historical reasons:
 */


typedef Model0___kernel_size_t Model0_size_t;




typedef Model0___kernel_ssize_t Model0_ssize_t;




typedef Model0___kernel_ptrdiff_t Model0_ptrdiff_t;




typedef Model0___kernel_time_t Model0_time_t;




typedef Model0___kernel_clock_t Model0_clock_t;




typedef Model0___kernel_caddr_t Model0_caddr_t;


/* bsd */
typedef unsigned char Model0_u_char;
typedef unsigned short Model0_u_short;
typedef unsigned int Model0_u_int;
typedef unsigned long Model0_u_long;

/* sysv */
typedef unsigned char Model0_unchar;
typedef unsigned short Model0_ushort;
typedef unsigned int Model0_uint;
typedef unsigned long Model0_ulong;




typedef __u8 Model0_u_int8_t;
typedef Model0___s8 Model0_int8_t;
typedef Model0___u16 Model0_u_int16_t;
typedef Model0___s16 Model0_int16_t;
typedef __u32 Model0_u_int32_t;
typedef Model0___s32 Model0_int32_t;



typedef __u8 Model0_uint8_t;
typedef Model0___u16 Model0_uint16_t;
typedef __u32 Model0_uint32_t;


typedef __u64 Model0_uint64_t;
typedef __u64 Model0_u_int64_t;
typedef Model0___s64 Model0_int64_t;


/* this is a special 64bit data type that is 8-byte aligned */




/**
 * The type used for indexing onto a disc or disc partition.
 *
 * Linux always considers sectors to be 512 bytes long independently
 * of the devices real block size.
 *
 * blkcnt_t is the type of the inode's block count.
 */




typedef unsigned long Model0_sector_t;
typedef unsigned long Model0_blkcnt_t;


/*
 * The type of an index into the pagecache.
 */


/*
 * A dma_addr_t can hold any valid DMA address, i.e., any address returned
 * by the DMA API.
 *
 * If the DMA API only uses 32-bit addresses, dma_addr_t need only be 32
 * bits wide.  Bus addresses, e.g., PCI BARs, may be wider than 32 bits,
 * but drivers do memory-mapped I/O to ioremapped kernel virtual addresses,
 * so they don't care about the size of the actual bus addresses.
 */

typedef Model0_u64 Model0_dma_addr_t;




typedef unsigned Model0_gfp_t;
typedef unsigned Model0_fmode_t;


typedef Model0_u64 Model0_phys_addr_t;




typedef Model0_phys_addr_t Model0_resource_size_t;

/*
 * This type is the placeholder for a hardware interrupt number. It has to be
 * big enough to enclose whatever representation is used by a given platform.
 */
typedef unsigned long Model0_irq_hw_number_t;

typedef struct {
 int Model0_counter;
} Model0_atomic_t;


typedef struct {
 long Model0_counter;
} Model0_atomic64_t;


struct Model0_list_head {
 struct Model0_list_head *Model0_next, *Model0_prev;
};

struct Model0_hlist_head {
 struct Model0_hlist_node *Model0_first;
};

struct Model0_hlist_node {
 struct Model0_hlist_node *Model0_next, **Model0_pprev;
};

struct Model0_ustat {
 Model0___kernel_daddr_t Model0_f_tfree;
 Model0___kernel_ino_t Model0_f_tinode;
 char Model0_f_fname[6];
 char Model0_f_fpack[6];
};

/**
 * struct callback_head - callback structure for use with RCU and task_work
 * @next: next update requests in a list
 * @func: actual update function to call after the grace period.
 *
 * The struct is aligned to size of pointer. On most architectures it happens
 * naturally due ABI requirements, but some architectures (like CRIS) have
 * weird ABI and we need to ask it explicitly.
 *
 * The alignment is required to guarantee that bits 0 and 1 of @next will be
 * clear under normal conditions -- as long as we use call_rcu(),
 * call_rcu_bh(), call_rcu_sched(), or call_srcu() to queue callback.
 *
 * This guarantee is important for few reasons:
 *  - future call_rcu_lazy() will make use of lower bits in the pointer;
 *  - the structure shares storage spacer in struct page with @compound_head,
 *    which encode PageTail() in bit 0. The guarantee is needed to avoid
 *    false-positive PageTail().
 */
struct Model0_callback_head {
 struct Model0_callback_head *Model0_next;
 void (*func)(struct Model0_callback_head *Model0_head);
} __attribute__((aligned(sizeof(void *))));


typedef void (*Model0_rcu_callback_t)(struct Model0_callback_head *Model0_head);
typedef void (*Model0_call_rcu_func_t)(struct Model0_callback_head *Model0_head, Model0_rcu_callback_t func);

/* clocksource cycle base type */
typedef Model0_u64 Model0_cycle_t;

/*
 * Create a contiguous bitmask starting at bit position @l and ending at
 * position @h. For example
 * GENMASK_ULL(39, 21) gives us the 64bit vector 0x000000ffffe00000.
 */






extern unsigned int Model0___sw_hweight8(unsigned int Model0_w);
extern unsigned int Model0___sw_hweight16(unsigned int Model0_w);
extern unsigned int Model0___sw_hweight32(unsigned int Model0_w);
extern unsigned long Model0___sw_hweight64(__u64 Model0_w);

/*
 * Include this here because some architectures need generic_ffs/fls in
 * scope
 */




/*
 * Copyright 1992, Linus Torvalds.
 *
 * Note: inlines with more than a single statement should be marked
 * __always_inline to avoid problems with older gcc's inlining heuristics.
 */















/*
 * Macros to generate condition code outputs from inline assembly,
 * The output operand must be type "bool".
 */
/* Exception table entry */
/* For C file, we already have NOKPROBE_SYMBOL macro */

/*
 * Alternative inline assembly for SMP.
 *
 * The LOCK_PREFIX macro defined here replaces the LOCK and
 * LOCK_PREFIX macros used everywhere in the source tree.
 *
 * SMP alternatives use the same data structures as the other
 * alternatives and the X86_FEATURE_UP flag to indicate the case of a
 * UP system running a SMP kernel.  The existing apply_alternatives()
 * works fine for patching a SMP kernel for UP.
 *
 * The SMP alternative tables can be kept after boot and contain both
 * UP and SMP versions of the instructions to allow switching back to
 * SMP at runtime, when hotplugging in a new CPU, which is especially
 * useful in virtualized environments.
 *
 * The very common lock prefix is handled as special case in a
 * separate table which is a pure address list without replacement ptr
 * and size information.  That keeps the table sizes small.
 */
struct Model0_alt_instr {
 Model0_s32 Model0_instr_offset; /* original instruction */
 Model0_s32 Model0_repl_offset; /* offset to replacement instruction */
 Model0_u16 Model0_cpuid; /* cpuid bit set for replacement */
 Model0_u8 Model0_instrlen; /* length of original instruction */
 Model0_u8 Model0_replacementlen; /* length of new instruction */
 Model0_u8 Model0_padlen; /* length of build-time padding */
} __attribute__((packed));

/*
 * Debug flag that can be tested to see whether alternative
 * instructions were patched in already:
 */
extern int Model0_alternatives_patched;

extern void Model0_alternative_instructions(void);
extern void Model0_apply_alternatives(struct Model0_alt_instr *Model0_start, struct Model0_alt_instr *Model0_end);

struct Model0_module;


extern void Model0_alternatives_smp_module_add(struct Model0_module *Model0_mod, char *Model0_name,
     void *Model0_locks, void *Model0_locks_end,
     void *Model0_text, void *Model0_text_end);
extern void Model0_alternatives_smp_module_del(struct Model0_module *Model0_mod);
extern void Model0_alternatives_enable_smp(void);
extern int Model0_alternatives_text_reserved(void *Model0_start, void *Model0_end);
extern bool Model0_skip_smp_alternatives;
/*
 * max without conditionals. Idea adapted from:
 * http://graphics.stanford.edu/~seander/bithacks.html#IntegerMinOrMax
 *
 * The additional "-" is needed because gas works with s32s.
 */


/*
 * Pad the second replacement alternative with additional NOPs if it is
 * additionally longer than the first replacement alternative.
 */
/* alternative assembly primitive: */
/*
 * Alternative instructions for different CPU types or capabilities.
 *
 * This allows to use optimized instructions even on generic binary
 * kernels.
 *
 * length of oldinstr must be longer or equal the length of newinstr
 * It can be padded with nops as needed.
 *
 * For non barrier like inlines please define new variants
 * without volatile and memory clobber.
 */






/*
 * Alternative inline assembly with input.
 *
 * Pecularities:
 * No memory clobber here.
 * Argument numbers start with 1.
 * Best is to use constraints that are fixed size (like (%1) ... "r")
 * If you use variable sized constraints like "m" or "g" in the
 * replacement make sure to pad to the worst case length.
 * Leaving an unused argument 0 to keep API compatibility.
 */




/*
 * This is similar to alternative_input. But it has two features and
 * respective instructions.
 *
 * If CPU has feature2, newinstr2 is used.
 * Otherwise, if CPU has feature1, newinstr1 is used.
 * Otherwise, oldinstr is used.
 */






/* Like alternative_input, but with a single output argument */




/* Like alternative_io, but for replacing a direct call with another one. */




/*
 * Like alternative_call, but there are two features and respective functions.
 * If CPU has feature2, function2 is used.
 * Otherwise, if CPU has feature1, function1 is used.
 * Otherwise, old function is used.
 */







/*
 * use this macro(s) if you need more than one output parameter
 * in alternative_io
 */


/*
 * use this macro if you need clobbers but no inputs in
 * alternative_{input,io,call}()
 */
/* Use flags output or a set instruction */







/*
 * Define nops for use with alternative() and for tracing.
 *
 * *_NOP5_ATOMIC must be a single instruction.
 */



/* generic versions from gas
   1: nop
   the following instructions are NOT nops in 64-bit mode,
   for 64-bit mode use K8 or P6 nops instead
   2: movl %esi,%esi
   3: leal 0x00(%esi),%esi
   4: leal 0x00(,%esi,1),%esi
   6: leal 0x00000000(%esi),%esi
   7: leal 0x00000000(,%esi,1),%esi
*/
/* Opteron 64bit nops
   1: nop
   2: osp nop
   3: osp osp nop
   4: osp osp osp nop
*/
/* K7 nops
   uses eax dependencies (arbitrary choice)
   1: nop
   2: movl %eax,%eax
   3: leal (,%eax,1),%eax
   4: leal 0x00(,%eax,1),%eax
   6: leal 0x00000000(%eax),%eax
   7: leal 0x00000000(,%eax,1),%eax
*/
/* P6 nops
   uses eax dependencies (Intel-recommended choice)
   1: nop
   2: osp nop
   3: nopl (%eax)
   4: nopl 0x00(%eax)
   5: nopl 0x00(%eax,%eax,1)
   6: osp nopl 0x00(%eax,%eax,1)
   7: nopl 0x00000000(%eax)
   8: nopl 0x00000000(%eax,%eax,1)
   Note: All the above are assumed to be a single instruction.
	There is kernel code that depends on this.
*/
extern const unsigned char * const *Model0_ideal_nops;
extern void Model0_arch_init_ideal_nops(void);

/*
 * Force strict CPU ordering.
 * And yes, this might be required on UP too when we're talking
 * to devices.
 */
/* Atomic operations are already serializing on x86 */




/*
 * Generic barrier definitions, originally based on MN10300 definitions.
 *
 * It should be possible to use these on really simple architectures,
 * but it serves more as a starting point for new ports.
 *
 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public Licence
 * as published by the Free Software Foundation; either version
 * 2 of the Licence, or (at your option) any later version.
 */
/*
 * Force strict CPU ordering. And yes, this is required on UP too when we're
 * talking to devices.
 *
 * Fall back to compiler barriers if nothing better is provided.
 */
/* Barriers for virtual machine guests when talking to an SMP host */
/**
 * smp_acquire__after_ctrl_dep() - Provide ACQUIRE ordering after a control dependency
 *
 * A control dependency provides a LOAD->STORE order, the additional RMB
 * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
 * aka. (load)-ACQUIRE.
 *
 * Architectures that do not do load speculation can have this be barrier().
 */




/**
 * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
 * @ptr: pointer to the variable to wait on
 * @cond: boolean expression to wait for
 *
 * Equivalent to using smp_load_acquire() on the condition variable but employs
 * the control dependency of the wait to reduce the barrier on many platforms.
 *
 * Due to C lacking lambda expressions we load the value of *ptr into a
 * pre-named variable @VAL to be used in @cond.
 */
/*
 * These have to be done with inline assembly: that way the bit-setting
 * is guaranteed to be atomic. All bit operations return 0 if the bit
 * was cleared before the operation and != 0 if it was not.
 *
 * bit 0 is the LSB of addr; bit 32 is the LSB of (addr+1).
 */
/*
 * We do the locked ops that don't return the old value as
 * a mask operation on a byte.
 */




/**
 * set_bit - Atomically set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * This function is atomic and may not be reordered.  See __set_bit()
 * if you do not require the atomic guarantees.
 *
 * Note: there are no guarantees that this function will not be reordered
 * on non x86 architectures, so if you are writing portable code,
 * make sure not to rely on its reordering guarantees.
 *
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model0_set_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 if ((__builtin_constant_p(Model0_nr))) {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "orb %1,%0"
   : "+m" (*(volatile long *) ((void *)(Model0_addr) + ((Model0_nr)>>3)))
   : "iq" ((Model0_u8)(1 << ((Model0_nr) & 7)))
   : "memory");
 } else {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "bts %1,%0"
   : "+m" (*(volatile long *) (Model0_addr)) : "Ir" (Model0_nr) : "memory");
 }
}

/**
 * __set_bit - Set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * Unlike set_bit(), this function is non-atomic and may be reordered.
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___set_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 asm volatile("bts %1,%0" : "+m" (*(volatile long *) (Model0_addr)) : "Ir" (Model0_nr) : "memory");
}

/**
 * clear_bit - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * clear_bit() is atomic and may not be reordered.  However, it does
 * not contain a memory barrier, so if it is used for locking purposes,
 * you should call smp_mb__before_atomic() and/or smp_mb__after_atomic()
 * in order to ensure changes are visible on other processors.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model0_clear_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 if ((__builtin_constant_p(Model0_nr))) {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "andb %1,%0"
   : "+m" (*(volatile long *) ((void *)(Model0_addr) + ((Model0_nr)>>3)))
   : "iq" ((Model0_u8)~(1 << ((Model0_nr) & 7))));
 } else {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "btr %1,%0"
   : "+m" (*(volatile long *) (Model0_addr))
   : "Ir" (Model0_nr));
 }
}

/*
 * clear_bit_unlock - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * clear_bit() is atomic and implies release semantics before the memory
 * operation. It can be used for an unlock.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_clear_bit_unlock(long Model0_nr, volatile unsigned long *Model0_addr)
{
 __asm__ __volatile__("": : :"memory");
 Model0_clear_bit(Model0_nr, Model0_addr);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___clear_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
#if CY_ABSTRACT6
#define Model0_BITS_PER_LONG 64
    unsigned long Model0_mask = (1UL << ((Model0_nr) % Model0_BITS_PER_LONG));
    unsigned long *Model0_p = ((unsigned long *)Model0_addr) + ((Model0_nr) / Model0_BITS_PER_LONG);

    *Model0_p &= ~Model0_mask;
#else
 asm volatile("btr %1,%0" : "+m" (*(volatile long *) (Model0_addr)) : "Ir" (Model0_nr));
#endif
}

/*
 * __clear_bit_unlock - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * __clear_bit() is non-atomic and implies release semantics before the memory
 * operation. It can be used for an unlock if no other CPUs can concurrently
 * modify other bits in the word.
 *
 * No memory barrier is required here, because x86 cannot reorder stores past
 * older loads. Same principle as spin_unlock.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___clear_bit_unlock(long Model0_nr, volatile unsigned long *Model0_addr)
{
 __asm__ __volatile__("": : :"memory");
 Model0___clear_bit(Model0_nr, Model0_addr);
}

/**
 * __change_bit - Toggle a bit in memory
 * @nr: the bit to change
 * @addr: the address to start counting from
 *
 * Unlike change_bit(), this function is non-atomic and may be reordered.
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___change_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 asm volatile("btc %1,%0" : "+m" (*(volatile long *) (Model0_addr)) : "Ir" (Model0_nr));
}

/**
 * change_bit - Toggle a bit in memory
 * @nr: Bit to change
 * @addr: Address to start counting from
 *
 * change_bit() is atomic and may not be reordered.
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_change_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 if ((__builtin_constant_p(Model0_nr))) {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xorb %1,%0"
   : "+m" (*(volatile long *) ((void *)(Model0_addr) + ((Model0_nr)>>3)))
   : "iq" ((Model0_u8)(1 << ((Model0_nr) & 7))));
 } else {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "btc %1,%0"
   : "+m" (*(volatile long *) (Model0_addr))
   : "Ir" (Model0_nr));
 }
}

/**
 * test_and_set_bit - Set a bit and return its old value
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_test_and_set_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "bts" " %2, " "%0" ";" "\n\tset" "c" " %[_cc_" "c" "]\n" : "+m" (*Model0_addr), [_cc_c] "=qm" (Model0_c) : "Ir" (Model0_nr) : "memory"); return Model0_c; } while (0);
}

/**
 * test_and_set_bit_lock - Set a bit and return its old value for lock
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This is the same as test_and_set_bit on x86.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool
Model0_test_and_set_bit_lock(long Model0_nr, volatile unsigned long *Model0_addr)
{
 return Model0_test_and_set_bit(Model0_nr, Model0_addr);
}

/**
 * __test_and_set_bit - Set a bit and return its old value
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is non-atomic and can be reordered.
 * If two examples of this operation race, one can appear to succeed
 * but actually fail.  You must protect multiple accesses with a lock.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0___test_and_set_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 bool Model0_oldbit;

 asm("bts %2,%1\n\t"
     "\n\tset" "c" " %[_cc_" "c" "]\n"
     : [_cc_c] "=qm" (Model0_oldbit), "+m" (*(volatile long *) (Model0_addr))
     : "Ir" (Model0_nr));
 return Model0_oldbit;
}

/**
 * test_and_clear_bit - Clear a bit and return its old value
 * @nr: Bit to clear
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_test_and_clear_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "btr" " %2, " "%0" ";" "\n\tset" "c" " %[_cc_" "c" "]\n" : "+m" (*Model0_addr), [_cc_c] "=qm" (Model0_c) : "Ir" (Model0_nr) : "memory"); return Model0_c; } while (0);
}

/**
 * __test_and_clear_bit - Clear a bit and return its old value
 * @nr: Bit to clear
 * @addr: Address to count from
 *
 * This operation is non-atomic and can be reordered.
 * If two examples of this operation race, one can appear to succeed
 * but actually fail.  You must protect multiple accesses with a lock.
 *
 * Note: the operation is performed atomically with respect to
 * the local CPU, but not other CPUs. Portable code should not
 * rely on this behaviour.
 * KVM relies on this behaviour on x86 for modifying memory that is also
 * accessed from a hypervisor on the same CPU if running in a VM: don't change
 * this without also updating arch/x86/kernel/kvm.c
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0___test_and_clear_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 bool Model0_oldbit;

 asm volatile("btr %2,%1\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model0_oldbit), "+m" (*(volatile long *) (Model0_addr))
       : "Ir" (Model0_nr));
 return Model0_oldbit;
}

/* WARNING: non atomic and it can be reordered! */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0___test_and_change_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 bool Model0_oldbit;

 asm volatile("btc %2,%1\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model0_oldbit), "+m" (*(volatile long *) (Model0_addr))
       : "Ir" (Model0_nr) : "memory");

 return Model0_oldbit;
}

/**
 * test_and_change_bit - Change a bit and return its old value
 * @nr: Bit to change
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_test_and_change_bit(long Model0_nr, volatile unsigned long *Model0_addr)
{
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "btc" " %2, " "%0" ";" "\n\tset" "c" " %[_cc_" "c" "]\n" : "+m" (*Model0_addr), [_cc_c] "=qm" (Model0_c) : "Ir" (Model0_nr) : "memory"); return Model0_c; } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_constant_test_bit(long Model0_nr, const volatile unsigned long *Model0_addr)
{
 return ((1UL << (Model0_nr & (64 -1))) &
  (Model0_addr[Model0_nr >> 6])) != 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_variable_test_bit(long Model0_nr, volatile const unsigned long *Model0_addr)
{
 bool Model0_oldbit;

#if CY_ABSTRACT6
 Model0_oldbit = ((1UL << (Model0_nr)) & (*(unsigned long *)Model0_addr));
#else
 asm volatile("bt %2,%1\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model0_oldbit)
       : "m" (*(unsigned long *)Model0_addr), "Ir" (Model0_nr));
#endif
 return Model0_oldbit;
}
/**
 * __ffs - find first set bit in word
 * @word: The word to search
 *
 * Undefined if no bit exists, so code should check against 0 first.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model0___ffs(unsigned long Model0_word)
{
 asm("rep; bsf %1,%0"
  : "=r" (Model0_word)
  : "rm" (Model0_word));
 return Model0_word;
}

/**
 * ffz - find first zero bit in word
 * @word: The word to search
 *
 * Undefined if no zero exists, so code should check against ~0UL first.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model0_ffz(unsigned long Model0_word)
{
 asm("rep; bsf %1,%0"
  : "=r" (Model0_word)
  : "r" (~Model0_word));
 return Model0_word;
}

/*
 * __fls: find last set bit in word
 * @word: The word to search
 *
 * Undefined if no set bit exists, so code should check against 0 first.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model0___fls(unsigned long Model0_word)
{
#if CY_ABSTRACT6
    return (sizeof(Model0_word)*8)-1-__builtin_clzl(Model0_word);
#else
 asm("bsr %1,%0"
     : "=r" (Model0_word)
     : "rm" (Model0_word));
 return Model0_word;
#endif
}




/**
 * ffs - find first set bit in word
 * @x: the word to search
 *
 * This is defined the same way as the libc and compiler builtin ffs
 * routines, therefore differs in spirit from the other bitops.
 *
 * ffs(value) returns 0 if value is 0 or the position of the first
 * set bit if value is nonzero. The first (least significant) bit
 * is at position 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_ffs(int Model0_x)
{
 int Model0_r;


 /*
	 * AMD64 says BSFL won't clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before, except that the
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
 asm("bsfl %1,%0"
     : "=r" (Model0_r)
     : "rm" (Model0_x), "0" (-1));
 return Model0_r + 1;
}

/**
 * fls - find last set bit in word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffs, but returns the position of the most significant set bit.
 *
 * fls(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 32.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_fls(int Model0_x)
{
 int Model0_r;


 /*
	 * AMD64 says BSRL won't clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before, except that the
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
 asm("bsrl %1,%0"
     : "=r" (Model0_r)
     : "rm" (Model0_x), "0" (-1));
 return Model0_r + 1;
}

/**
 * fls64 - find last set bit in a 64-bit word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffsll, but returns the position of the most significant set bit.
 *
 * fls64(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 64.
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_fls64(__u64 Model0_x)
{
#if CY_ABSTRACT6
    if (Model0_x == 0)
        return 0;
    return Model0___fls(Model0_x)+1;
#else
 int Model0_bitpos = -1;
 /*
	 * AMD64 says BSRQ won't clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before.
	 */
 asm("bsrq %1,%q0"
     : "+r" (Model0_bitpos)
     : "rm" (Model0_x));
 return Model0_bitpos + 1;
#endif
}









/**
 * find_next_bit - find the next set bit in a memory region
 * @addr: The address to base the search on
 * @offset: The bitnumber to start searching at
 * @size: The bitmap size in bits
 *
 * Returns the bit number for the next set bit
 * If no bits are set, returns @size.
 */
extern unsigned long Model0_find_next_bit(const unsigned long *Model0_addr, unsigned long
  Model0_size, unsigned long Model0_offset);



/**
 * find_next_zero_bit - find the next cleared bit in a memory region
 * @addr: The address to base the search on
 * @offset: The bitnumber to start searching at
 * @size: The bitmap size in bits
 *
 * Returns the bit number of the next zero bit
 * If no bits are zero, returns @size.
 */
extern unsigned long Model0_find_next_zero_bit(const unsigned long *Model0_addr, unsigned
  long Model0_size, unsigned long Model0_offset);




/**
 * find_first_bit - find the first set bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum number of bits to search
 *
 * Returns the bit number of the first set bit.
 * If no bits are set, returns @size.
 */
extern unsigned long Model0_find_first_bit(const unsigned long *Model0_addr,
        unsigned long Model0_size);

/**
 * find_first_zero_bit - find the first cleared bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum number of bits to search
 *
 * Returns the bit number of the first cleared bit.
 * If no bits are zero, returns @size.
 */
extern unsigned long Model0_find_first_zero_bit(const unsigned long *Model0_addr,
      unsigned long Model0_size);







/*
 * Every architecture must define this function. It's the fastest
 * way of searching a 100-bit bitmap.  It's guaranteed that at least
 * one of the 100 bits is cleared.
 */
static inline __attribute__((no_instrument_function)) int Model0_sched_find_first_bit(const unsigned long *Model0_b)
{

 if (Model0_b[0])
  return Model0___ffs(Model0_b[0]);
 return Model0___ffs(Model0_b[1]) + 64;
}











/* Define minimum CPUID feature set for kernel These bits are checked
   really early to actually display a visible error message before the
   kernel dies.  Make sure to assign features to the proper mask!

   Some requirements that are not in CPUID yet are also in the
   CONFIG_X86_MINIMUM_CPU_FAMILY which is checked too.

   The real information is in arch/x86/Kconfig.cpu, this just converts
   the CONFIGs into a bitmask */






/* These features, although they might be available in a CPU
 * will not be used because the compile options to support
 * them are not present.
 *
 * This code allows them to be checked and disabled at
 * compile time without an explicit #ifdef.  Use
 * cpu_feature_enabled().
 */
/*
 * Make sure to add features to the correct mask
 */


/*
 * Defines x86 CPU feature bits
 */



/*
 * Note: If the comment begins with a quoted string, that string is used
 * in /proc/cpuinfo instead of the macro name.  If the string is "",
 * this feature bit is not displayed in /proc/cpuinfo at all.
 */

/* Intel-defined CPU features, CPUID level 0x00000001 (edx), word 0 */
       /* (plus FCMOVcc, FCOMI with FPU) */
/* AMD-defined CPU features, CPUID level 0x80000001, word 1 */
/* Don't duplicate feature flags which are redundant with Intel! */
/* Transmeta-defined CPU features, CPUID level 0x80860001, word 2 */




/* Other features, Linux-defined mapping, word 3 */
/* This range is used for feature bits which conflict or are synthesized */




/* cpu types for specific tunings: */
/* free, was #define X86_FEATURE_CLFLUSH_MONITOR ( 3*32+25) * "" clflush reqd with monitor */







/* Intel-defined CPU features, CPUID level 0x00000001 (ecx), word 4 */
/* VIA/Cyrix/Centaur-defined CPU features, CPUID level 0xC0000001, word 5 */
/* More extended AMD flags: CPUID level 0x80000001, ecx, word 6 */
/*
 * Auxiliary flags: Linux defined - For features scattered in various
 * CPUID levels like 0x6, 0xA etc, word 7.
 *
 * Reuse free bits when adding new feature flags!
 */
/* Virtualization flags: Linux defined, word 8 */
/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */





/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */


/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */




/* AMD-defined CPU features, CPUID level 0x80000008 (ebx), word 13 */



/* Thermal and Power Management Leaf, CPUID level 0x00000006 (eax), word 14 */
/* AMD SVM Feature Identification, CPUID level 0x8000000a (edx), word 15 */
/* Intel-defined CPU features, CPUID level 0x00000007:0 (ecx), word 16 */



/* AMD-defined CPU features, CPUID level 0x80000007 (ebx), word 17 */




/*
 * BUG word(s)
 */


/* popcnt %edi, %eax */

/* popcnt %rdi, %rax */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned int Model0___arch_hweight32(unsigned int Model0_w)
{
 unsigned int Model0_res;

 asm ("661:\n\t" "call __sw_hweight32" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 4*32+23)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0xf3,0x0f,0xb8,0xc7" "\n" "665""1" ":\n\t" ".popsection"
    : "=""a" (Model0_res)
    : "D" (Model0_w));

 return Model0_res;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0___arch_hweight16(unsigned int Model0_w)
{
 return Model0___arch_hweight32(Model0_w & 0xffff);
}

static inline __attribute__((no_instrument_function)) unsigned int Model0___arch_hweight8(unsigned int Model0_w)
{
 return Model0___arch_hweight32(Model0_w & 0xff);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model0___arch_hweight64(__u64 Model0_w)
{
 unsigned long Model0_res;

 asm ("661:\n\t" "call __sw_hweight64" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 4*32+23)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0xf3,0x48,0x0f,0xb8,0xc7" "\n" "665""1" ":\n\t" ".popsection"
    : "=""a" (Model0_res)
    : "D" (Model0_w));

 return Model0_res;
}




/*
 * Compile time versions of __arch_hweightN()
 */
/*
 * Generic interface.
 */





/*
 * Interface for known constant arguments
 */





/*
 * Type invariant interface to the compile time constant hweight functions.
 */

























static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u32 Model0___arch_swab32(__u32 Model0_val)
{
 asm("bswapl %0" : "=r" (Model0_val) : "0" (Model0_val));
 return Model0_val;
}


static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u64 Model0___arch_swab64(__u64 Model0_val)
{
 asm("bswapq %0" : "=r" (Model0_val) : "0" (Model0_val));
 return Model0_val;

}

/*
 * casts are necessary for constants, because we never know how for sure
 * how U/UL/ULL map to __u16, __u32, __u64. At least not in a portable way.
 */
/*
 * Implement the following as inlines, but define the interface using
 * macros to allow constant folding when possible:
 * ___swab16, ___swab32, ___swab64, ___swahw32, ___swahb32
 */

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) Model0___u16 Model0___fswab16(Model0___u16 Model0_val)
{



 return ((Model0___u16)( (((Model0___u16)(Model0_val) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(Model0_val) & (Model0___u16)0xff00U) >> 8)));

}

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u32 Model0___fswab32(__u32 Model0_val)
{

 return Model0___arch_swab32(Model0_val);



}

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u64 Model0___fswab64(__u64 Model0_val)
{

 return Model0___arch_swab64(Model0_val);







}

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u32 Model0___fswahw32(__u32 Model0_val)
{



 return ((__u32)( (((__u32)(Model0_val) & (__u32)0x0000ffffUL) << 16) | (((__u32)(Model0_val) & (__u32)0xffff0000UL) >> 16)));

}

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u32 Model0___fswahb32(__u32 Model0_val)
{



 return ((__u32)( (((__u32)(Model0_val) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(Model0_val) & (__u32)0xff00ff00UL) >> 8)));

}

/**
 * __swab16 - return a byteswapped 16-bit value
 * @x: value to byteswap
 */
/**
 * __swab32 - return a byteswapped 32-bit value
 * @x: value to byteswap
 */
/**
 * __swab64 - return a byteswapped 64-bit value
 * @x: value to byteswap
 */
/**
 * __swahw32 - return a word-swapped 32-bit value
 * @x: value to wordswap
 *
 * __swahw32(0x12340000) is 0x00001234
 */





/**
 * __swahb32 - return a high and low byte-swapped 32-bit value
 * @x: value to byteswap
 *
 * __swahb32(0x12345678) is 0x34127856
 */





/**
 * __swab16p - return a byteswapped 16-bit value from a pointer
 * @p: pointer to a naturally-aligned 16-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___u16 Model0___swab16p(const Model0___u16 *Model0_p)
{



 return (__builtin_constant_p((Model0___u16)(*Model0_p)) ? ((Model0___u16)( (((Model0___u16)(*Model0_p) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(*Model0_p) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(*Model0_p));

}

/**
 * __swab32p - return a byteswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u32 Model0___swab32p(const __u32 *Model0_p)
{



 return (__builtin_constant_p((__u32)(*Model0_p)) ? ((__u32)( (((__u32)(*Model0_p) & (__u32)0x000000ffUL) << 24) | (((__u32)(*Model0_p) & (__u32)0x0000ff00UL) << 8) | (((__u32)(*Model0_p) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(*Model0_p) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(*Model0_p));

}

/**
 * __swab64p - return a byteswapped 64-bit value from a pointer
 * @p: pointer to a naturally-aligned 64-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u64 Model0___swab64p(const __u64 *Model0_p)
{



 return (__builtin_constant_p((__u64)(*Model0_p)) ? ((__u64)( (((__u64)(*Model0_p) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(*Model0_p) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(*Model0_p) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(*Model0_p) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(*Model0_p) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(*Model0_p) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(*Model0_p) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(*Model0_p) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(*Model0_p));

}

/**
 * __swahw32p - return a wordswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahw32() for details of wordswapping.
 */
static inline __attribute__((no_instrument_function)) __u32 Model0___swahw32p(const __u32 *Model0_p)
{



 return (__builtin_constant_p((__u32)(*Model0_p)) ? ((__u32)( (((__u32)(*Model0_p) & (__u32)0x0000ffffUL) << 16) | (((__u32)(*Model0_p) & (__u32)0xffff0000UL) >> 16))) : Model0___fswahw32(*Model0_p));

}

/**
 * __swahb32p - return a high and low byteswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahb32() for details of high/low byteswapping.
 */
static inline __attribute__((no_instrument_function)) __u32 Model0___swahb32p(const __u32 *Model0_p)
{



 return (__builtin_constant_p((__u32)(*Model0_p)) ? ((__u32)( (((__u32)(*Model0_p) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(*Model0_p) & (__u32)0xff00ff00UL) >> 8))) : Model0___fswahb32(*Model0_p));

}

/**
 * __swab16s - byteswap a 16-bit value in-place
 * @p: pointer to a naturally-aligned 16-bit value
 */
static inline __attribute__((no_instrument_function)) void Model0___swab16s(Model0___u16 *Model0_p)
{



 *Model0_p = Model0___swab16p(Model0_p);

}
/**
 * __swab32s - byteswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___swab32s(__u32 *Model0_p)
{



 *Model0_p = Model0___swab32p(Model0_p);

}

/**
 * __swab64s - byteswap a 64-bit value in-place
 * @p: pointer to a naturally-aligned 64-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___swab64s(__u64 *Model0_p)
{



 *Model0_p = Model0___swab64p(Model0_p);

}

/**
 * __swahw32s - wordswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahw32() for details of wordswapping
 */
static inline __attribute__((no_instrument_function)) void Model0___swahw32s(__u32 *Model0_p)
{



 *Model0_p = Model0___swahw32p(Model0_p);

}

/**
 * __swahb32s - high and low byteswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahb32() for details of high and low byte swapping
 */
static inline __attribute__((no_instrument_function)) void Model0___swahb32s(__u32 *Model0_p)
{



 *Model0_p = Model0___swahb32p(Model0_p);

}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___le64 Model0___cpu_to_le64p(const __u64 *Model0_p)
{
 return ( Model0___le64)*Model0_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u64 Model0___le64_to_cpup(const Model0___le64 *Model0_p)
{
 return ( __u64)*Model0_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___le32 Model0___cpu_to_le32p(const __u32 *Model0_p)
{
 return ( Model0___le32)*Model0_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u32 Model0___le32_to_cpup(const Model0___le32 *Model0_p)
{
 return ( __u32)*Model0_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___le16 Model0___cpu_to_le16p(const Model0___u16 *Model0_p)
{
 return ( Model0___le16)*Model0_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___u16 Model0___le16_to_cpup(const Model0___le16 *Model0_p)
{
 return ( Model0___u16)*Model0_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___be64 Model0___cpu_to_be64p(const __u64 *Model0_p)
{
 return ( Model0___be64)Model0___swab64p(Model0_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u64 Model0___be64_to_cpup(const Model0___be64 *Model0_p)
{
 return Model0___swab64p((__u64 *)Model0_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___be32 Model0___cpu_to_be32p(const __u32 *Model0_p)
{
 return ( Model0___be32)Model0___swab32p(Model0_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u32 Model0___be32_to_cpup(const Model0___be32 *Model0_p)
{
 return Model0___swab32p((__u32 *)Model0_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___be16 Model0___cpu_to_be16p(const Model0___u16 *Model0_p)
{
 return ( Model0___be16)Model0___swab16p(Model0_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0___u16 Model0___be16_to_cpup(const Model0___be16 *Model0_p)
{
 return Model0___swab16p((Model0___u16 *)Model0_p);
}




/*
 * linux/byteorder/generic.h
 * Generic Byte-reordering support
 *
 * The "... p" macros, like le64_to_cpup, can be used with pointers
 * to unaligned data, but there will be a performance penalty on 
 * some architectures.  Use get_unaligned for unaligned data.
 *
 * Francois-Rene Rideau <fare@tunes.org> 19970707
 *    gathered all the good ideas from all asm-foo/byteorder.h into one file,
 *    cleaned them up.
 *    I hope it is compliant with non-GCC compilers.
 *    I decided to put __BYTEORDER_HAS_U64__ in byteorder.h,
 *    because I wasn't sure it would be ok to put it in types.h
 *    Upgraded it to 2.1.43
 * Francois-Rene Rideau <fare@tunes.org> 19971012
 *    Upgraded it to 2.1.57
 *    to please Linus T., replaced huge #ifdef's between little/big endian
 *    by nestedly #include'd files.
 * Francois-Rene Rideau <fare@tunes.org> 19971205
 *    Made it to 2.1.71; now a facelift:
 *    Put files under include/linux/byteorder/
 *    Split swab from generic support.
 *
 * TODO:
 *   = Regular kernel maintainers could also replace all these manual
 *    byteswap macros that remain, disseminated among drivers,
 *    after some grep or the sources...
 *   = Linus might want to rename all these macros and files to fit his taste,
 *    to fit his personal naming scheme.
 *   = it seems that a few drivers would also appreciate
 *    nybble swapping support...
 *   = every architecture could add their byteswap macro in asm/byteorder.h
 *    see how some architectures already do (i386, alpha, ppc, etc)
 *   = cpu_to_beXX and beXX_to_cpu might some day need to be well
 *    distinguished throughout the kernel. This is not the case currently,
 *    since little endian, big endian, and pdp endian machines needn't it.
 *    But this might be the case for, say, a port of Linux to 20/21 bit
 *    architectures (and F21 Linux addict around?).
 */

/*
 * The following macros are to be defined by <asm/byteorder.h>:
 *
 * Conversion of long and short int between network and host format
 *	ntohl(__u32 x)
 *	ntohs(__u16 x)
 *	htonl(__u32 x)
 *	htons(__u16 x)
 * It seems that some programs (which? where? or perhaps a standard? POSIX?)
 * might like the above to be functions, not macros (why?).
 * if that's true, then detect them, and take measures.
 * Anyway, the measure is: define only ___ntohl as a macro instead,
 * and in a separate file, have
 * unsigned long inline ntohl(x){return ___ntohl(x);}
 *
 * The same for constant arguments
 *	__constant_ntohl(__u32 x)
 *	__constant_ntohs(__u16 x)
 *	__constant_htonl(__u32 x)
 *	__constant_htons(__u16 x)
 *
 * Conversion of XX-bit integers (16- 32- or 64-)
 * between native CPU format and little/big endian format
 * 64-bit stuff only defined for proper architectures
 *	cpu_to_[bl]eXX(__uXX x)
 *	[bl]eXX_to_cpu(__uXX x)
 *
 * The same, but takes a pointer to the value to convert
 *	cpu_to_[bl]eXXp(__uXX x)
 *	[bl]eXX_to_cpup(__uXX x)
 *
 * The same, but change in situ
 *	cpu_to_[bl]eXXs(__uXX x)
 *	[bl]eXX_to_cpus(__uXX x)
 *
 * See asm-foo/byteorder.h for examples of how to provide
 * architecture-optimized versions
 *
 */
/*
 * They have to be macros in order to do the constant folding
 * correctly - if the argument passed into a inline function
 * it is no longer constant according to gcc..
 */
static inline __attribute__((no_instrument_function)) void Model0_le16_add_cpu(Model0___le16 *Model0_var, Model0_u16 Model0_val)
{
 *Model0_var = (( Model0___le16)(Model0___u16)((( Model0___u16)(Model0___le16)(*Model0_var)) + Model0_val));
}

static inline __attribute__((no_instrument_function)) void Model0_le32_add_cpu(Model0___le32 *Model0_var, Model0_u32 Model0_val)
{
 *Model0_var = (( Model0___le32)(__u32)((( __u32)(Model0___le32)(*Model0_var)) + Model0_val));
}

static inline __attribute__((no_instrument_function)) void Model0_le64_add_cpu(Model0___le64 *Model0_var, Model0_u64 Model0_val)
{
 *Model0_var = (( Model0___le64)(__u64)((( __u64)(Model0___le64)(*Model0_var)) + Model0_val));
}

static inline __attribute__((no_instrument_function)) void Model0_be16_add_cpu(Model0___be16 *Model0_var, Model0_u16 Model0_val)
{
 *Model0_var = (( Model0___be16)(__builtin_constant_p((Model0___u16)(((__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(*Model0_var))) + Model0_val))) ? ((Model0___u16)( (((Model0___u16)(((__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(*Model0_var))) + Model0_val)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(((__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(*Model0_var))) + Model0_val)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(((__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(*Model0_var)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(*Model0_var))) + Model0_val))));
}

static inline __attribute__((no_instrument_function)) void Model0_be32_add_cpu(Model0___be32 *Model0_var, Model0_u32 Model0_val)
{
 *Model0_var = (( Model0___be32)(__builtin_constant_p((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model0___be32)(*Model0_var))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(*Model0_var))) + Model0_val))) ? ((__u32)( (((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model0___be32)(*Model0_var))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(*Model0_var))) + Model0_val)) & (__u32)0x000000ffUL) << 24) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model0___be32)(*Model0_var))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(*Model0_var))) + Model0_val)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model0___be32)(*Model0_var))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(*Model0_var))) + Model0_val)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model0___be32)(*Model0_var))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(*Model0_var))) + Model0_val)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(((__builtin_constant_p((__u32)(( __u32)(Model0___be32)(*Model0_var))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(*Model0_var)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(*Model0_var))) + Model0_val))));
}

static inline __attribute__((no_instrument_function)) void Model0_be64_add_cpu(Model0___be64 *Model0_var, Model0_u64 Model0_val)
{
 *Model0_var = (( Model0___be64)(__builtin_constant_p((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val))) ? ((__u64)( (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(*Model0_var))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(*Model0_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(*Model0_var))) + Model0_val))));
}





static inline __attribute__((no_instrument_function)) unsigned long Model0_find_next_zero_bit_le(const void *Model0_addr,
  unsigned long Model0_size, unsigned long Model0_offset)
{
 return Model0_find_next_zero_bit(Model0_addr, Model0_size, Model0_offset);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_find_next_bit_le(const void *Model0_addr,
  unsigned long Model0_size, unsigned long Model0_offset)
{
 return Model0_find_next_bit(Model0_addr, Model0_size, Model0_offset);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_find_first_zero_bit_le(const void *Model0_addr,
  unsigned long Model0_size)
{
 return Model0_find_first_zero_bit(Model0_addr, Model0_size);
}
static inline __attribute__((no_instrument_function)) int Model0_test_bit_le(int Model0_nr, const void *Model0_addr)
{
 return (__builtin_constant_p((Model0_nr ^ 0)) ? Model0_constant_test_bit((Model0_nr ^ 0), (Model0_addr)) : Model0_variable_test_bit((Model0_nr ^ 0), (Model0_addr)));
}

static inline __attribute__((no_instrument_function)) void Model0_set_bit_le(int Model0_nr, void *Model0_addr)
{
 Model0_set_bit(Model0_nr ^ 0, Model0_addr);
}

static inline __attribute__((no_instrument_function)) void Model0_clear_bit_le(int Model0_nr, void *Model0_addr)
{
 Model0_clear_bit(Model0_nr ^ 0, Model0_addr);
}

static inline __attribute__((no_instrument_function)) void Model0___set_bit_le(int Model0_nr, void *Model0_addr)
{
 Model0___set_bit(Model0_nr ^ 0, Model0_addr);
}

static inline __attribute__((no_instrument_function)) void Model0___clear_bit_le(int Model0_nr, void *Model0_addr)
{
 Model0___clear_bit(Model0_nr ^ 0, Model0_addr);
}

static inline __attribute__((no_instrument_function)) int Model0_test_and_set_bit_le(int Model0_nr, void *Model0_addr)
{
 return Model0_test_and_set_bit(Model0_nr ^ 0, Model0_addr);
}

static inline __attribute__((no_instrument_function)) int Model0_test_and_clear_bit_le(int Model0_nr, void *Model0_addr)
{
 return Model0_test_and_clear_bit(Model0_nr ^ 0, Model0_addr);
}

static inline __attribute__((no_instrument_function)) int Model0___test_and_set_bit_le(int Model0_nr, void *Model0_addr)
{
 return Model0___test_and_set_bit(Model0_nr ^ 0, Model0_addr);
}

static inline __attribute__((no_instrument_function)) int Model0___test_and_clear_bit_le(int Model0_nr, void *Model0_addr)
{
 return Model0___test_and_clear_bit(Model0_nr ^ 0, Model0_addr);
}




/*
 * Atomic bitops based version of ext2 atomic bitops
 */






/* same as for_each_set_bit() but use bit as value to start with */
/* same as for_each_clear_bit() but use bit as value to start with */





static inline __attribute__((no_instrument_function)) int Model0_get_bitmask_order(unsigned int Model0_count)
{
 int Model0_order;

 Model0_order = Model0_fls(Model0_count);
 return Model0_order; /* We could be slightly more clever with -1 here... */
}

static inline __attribute__((no_instrument_function)) int Model0_get_count_order(unsigned int Model0_count)
{
 int Model0_order;

 Model0_order = Model0_fls(Model0_count) - 1;
 if (Model0_count & (Model0_count - 1))
  Model0_order++;
 return Model0_order;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model0_hweight_long(unsigned long Model0_w)
{
 return sizeof(Model0_w) == 4 ? (__builtin_constant_p(Model0_w) ? ((((unsigned int) ((!!((Model0_w) & (1ULL << 0))) + (!!((Model0_w) & (1ULL << 1))) + (!!((Model0_w) & (1ULL << 2))) + (!!((Model0_w) & (1ULL << 3))) + (!!((Model0_w) & (1ULL << 4))) + (!!((Model0_w) & (1ULL << 5))) + (!!((Model0_w) & (1ULL << 6))) + (!!((Model0_w) & (1ULL << 7))))) + ((unsigned int) ((!!(((Model0_w) >> 8) & (1ULL << 0))) + (!!(((Model0_w) >> 8) & (1ULL << 1))) + (!!(((Model0_w) >> 8) & (1ULL << 2))) + (!!(((Model0_w) >> 8) & (1ULL << 3))) + (!!(((Model0_w) >> 8) & (1ULL << 4))) + (!!(((Model0_w) >> 8) & (1ULL << 5))) + (!!(((Model0_w) >> 8) & (1ULL << 6))) + (!!(((Model0_w) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!(((Model0_w) >> 16) & (1ULL << 0))) + (!!(((Model0_w) >> 16) & (1ULL << 1))) + (!!(((Model0_w) >> 16) & (1ULL << 2))) + (!!(((Model0_w) >> 16) & (1ULL << 3))) + (!!(((Model0_w) >> 16) & (1ULL << 4))) + (!!(((Model0_w) >> 16) & (1ULL << 5))) + (!!(((Model0_w) >> 16) & (1ULL << 6))) + (!!(((Model0_w) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!((((Model0_w) >> 16) >> 8) & (1ULL << 0))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 1))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 2))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 3))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 4))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 5))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 6))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 7))))))) : Model0___arch_hweight32(Model0_w)) : (__builtin_constant_p(Model0_w) ? (((((unsigned int) ((!!((Model0_w) & (1ULL << 0))) + (!!((Model0_w) & (1ULL << 1))) + (!!((Model0_w) & (1ULL << 2))) + (!!((Model0_w) & (1ULL << 3))) + (!!((Model0_w) & (1ULL << 4))) + (!!((Model0_w) & (1ULL << 5))) + (!!((Model0_w) & (1ULL << 6))) + (!!((Model0_w) & (1ULL << 7))))) + ((unsigned int) ((!!(((Model0_w) >> 8) & (1ULL << 0))) + (!!(((Model0_w) >> 8) & (1ULL << 1))) + (!!(((Model0_w) >> 8) & (1ULL << 2))) + (!!(((Model0_w) >> 8) & (1ULL << 3))) + (!!(((Model0_w) >> 8) & (1ULL << 4))) + (!!(((Model0_w) >> 8) & (1ULL << 5))) + (!!(((Model0_w) >> 8) & (1ULL << 6))) + (!!(((Model0_w) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!(((Model0_w) >> 16) & (1ULL << 0))) + (!!(((Model0_w) >> 16) & (1ULL << 1))) + (!!(((Model0_w) >> 16) & (1ULL << 2))) + (!!(((Model0_w) >> 16) & (1ULL << 3))) + (!!(((Model0_w) >> 16) & (1ULL << 4))) + (!!(((Model0_w) >> 16) & (1ULL << 5))) + (!!(((Model0_w) >> 16) & (1ULL << 6))) + (!!(((Model0_w) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!((((Model0_w) >> 16) >> 8) & (1ULL << 0))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 1))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 2))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 3))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 4))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 5))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 6))) + (!!((((Model0_w) >> 16) >> 8) & (1ULL << 7))))))) + ((((unsigned int) ((!!(((Model0_w) >> 32) & (1ULL << 0))) + (!!(((Model0_w) >> 32) & (1ULL << 1))) + (!!(((Model0_w) >> 32) & (1ULL << 2))) + (!!(((Model0_w) >> 32) & (1ULL << 3))) + (!!(((Model0_w) >> 32) & (1ULL << 4))) + (!!(((Model0_w) >> 32) & (1ULL << 5))) + (!!(((Model0_w) >> 32) & (1ULL << 6))) + (!!(((Model0_w) >> 32) & (1ULL << 7))))) + ((unsigned int) ((!!((((Model0_w) >> 32) >> 8) & (1ULL << 0))) + (!!((((Model0_w) >> 32) >> 8) & (1ULL << 1))) + (!!((((Model0_w) >> 32) >> 8) & (1ULL << 2))) + (!!((((Model0_w) >> 32) >> 8) & (1ULL << 3))) + (!!((((Model0_w) >> 32) >> 8) & (1ULL << 4))) + (!!((((Model0_w) >> 32) >> 8) & (1ULL << 5))) + (!!((((Model0_w) >> 32) >> 8) & (1ULL << 6))) + (!!((((Model0_w) >> 32) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!((((Model0_w) >> 32) >> 16) & (1ULL << 0))) + (!!((((Model0_w) >> 32) >> 16) & (1ULL << 1))) + (!!((((Model0_w) >> 32) >> 16) & (1ULL << 2))) + (!!((((Model0_w) >> 32) >> 16) & (1ULL << 3))) + (!!((((Model0_w) >> 32) >> 16) & (1ULL << 4))) + (!!((((Model0_w) >> 32) >> 16) & (1ULL << 5))) + (!!((((Model0_w) >> 32) >> 16) & (1ULL << 6))) + (!!((((Model0_w) >> 32) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!(((((Model0_w) >> 32) >> 16) >> 8) & (1ULL << 0))) + (!!(((((Model0_w) >> 32) >> 16) >> 8) & (1ULL << 1))) + (!!(((((Model0_w) >> 32) >> 16) >> 8) & (1ULL << 2))) + (!!(((((Model0_w) >> 32) >> 16) >> 8) & (1ULL << 3))) + (!!(((((Model0_w) >> 32) >> 16) >> 8) & (1ULL << 4))) + (!!(((((Model0_w) >> 32) >> 16) >> 8) & (1ULL << 5))) + (!!(((((Model0_w) >> 32) >> 16) >> 8) & (1ULL << 6))) + (!!(((((Model0_w) >> 32) >> 16) >> 8) & (1ULL << 7)))))))) : Model0___arch_hweight64(Model0_w));
}

/**
 * rol64 - rotate a 64-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u64 Model0_rol64(__u64 Model0_word, unsigned int Model0_shift)
{
 return (Model0_word << Model0_shift) | (Model0_word >> (64 - Model0_shift));
}

/**
 * ror64 - rotate a 64-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u64 Model0_ror64(__u64 Model0_word, unsigned int Model0_shift)
{
 return (Model0_word >> Model0_shift) | (Model0_word << (64 - Model0_shift));
}

/**
 * rol32 - rotate a 32-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u32 Model0_rol32(__u32 Model0_word, unsigned int Model0_shift)
{
 return (Model0_word << Model0_shift) | (Model0_word >> ((-Model0_shift) & 31));
}

/**
 * ror32 - rotate a 32-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u32 Model0_ror32(__u32 Model0_word, unsigned int Model0_shift)
{
 return (Model0_word >> Model0_shift) | (Model0_word << (32 - Model0_shift));
}

/**
 * rol16 - rotate a 16-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) Model0___u16 Model0_rol16(Model0___u16 Model0_word, unsigned int Model0_shift)
{
 return (Model0_word << Model0_shift) | (Model0_word >> (16 - Model0_shift));
}

/**
 * ror16 - rotate a 16-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) Model0___u16 Model0_ror16(Model0___u16 Model0_word, unsigned int Model0_shift)
{
 return (Model0_word >> Model0_shift) | (Model0_word << (16 - Model0_shift));
}

/**
 * rol8 - rotate an 8-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u8 Model0_rol8(__u8 Model0_word, unsigned int Model0_shift)
{
 return (Model0_word << Model0_shift) | (Model0_word >> (8 - Model0_shift));
}

/**
 * ror8 - rotate an 8-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u8 Model0_ror8(__u8 Model0_word, unsigned int Model0_shift)
{
 return (Model0_word >> Model0_shift) | (Model0_word << (8 - Model0_shift));
}

/**
 * sign_extend32 - sign extend a 32-bit value using specified bit as sign-bit
 * @value: value to sign extend
 * @index: 0 based bit index (0<=index<32) to sign bit
 *
 * This is safe to use for 16- and 8-bit types as well.
 */
static inline __attribute__((no_instrument_function)) Model0___s32 Model0_sign_extend32(__u32 Model0_value, int Model0_index)
{
 __u8 Model0_shift = 31 - Model0_index;
 return (Model0___s32)(Model0_value << Model0_shift) >> Model0_shift;
}

/**
 * sign_extend64 - sign extend a 64-bit value using specified bit as sign-bit
 * @value: value to sign extend
 * @index: 0 based bit index (0<=index<64) to sign bit
 */
static inline __attribute__((no_instrument_function)) Model0___s64 Model0_sign_extend64(__u64 Model0_value, int Model0_index)
{
 __u8 Model0_shift = 63 - Model0_index;
 return (Model0___s64)(Model0_value << Model0_shift) >> Model0_shift;
}

static inline __attribute__((no_instrument_function)) unsigned Model0_fls_long(unsigned long Model0_l)
{
 if (sizeof(Model0_l) == 4)
  return Model0_fls(Model0_l);
 return Model0_fls64(Model0_l);
}

/**
 * __ffs64 - find first set bit in a 64 bit word
 * @word: The 64 bit word
 *
 * On 64 bit arches this is a synomyn for __ffs
 * The result is not defined if no bits are set, so check that @word
 * is non-zero before calling this.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0___ffs64(Model0_u64 Model0_word)
{






 return Model0___ffs((unsigned long)Model0_word);
}
/**
 * find_last_bit - find the last set bit in a memory region
 * @addr: The address to start the search at
 * @size: The number of bits to search
 *
 * Returns the bit number of the last set bit, or size.
 */
extern unsigned long Model0_find_last_bit(const unsigned long *Model0_addr,
       unsigned long Model0_size);
/* Integer base 2 logarithm calculation
 *
 * Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 */







/*
 * deal with unrepresentable constant logarithms
 */
extern __attribute__((const, noreturn))
int Model0_____ilog2_NaN(void);

/*
 * non-constant log of base 2 calculators
 * - the arch may override these in asm/bitops.h if they can be implemented
 *   more efficiently than using fls() and fls64()
 * - the arch is not required to handle n==0 if implementing the fallback
 */

static inline __attribute__((no_instrument_function)) __attribute__((const))
int Model0___ilog2_u32(Model0_u32 Model0_n)
{
 return Model0_fls(Model0_n) - 1;
}



static inline __attribute__((no_instrument_function)) __attribute__((const))
int Model0___ilog2_u64(Model0_u64 Model0_n)
{
 return Model0_fls64(Model0_n) - 1;
}


/*
 *  Determine whether some value is a power of two, where zero is
 * *not* considered a power of two.
 */

static inline __attribute__((no_instrument_function)) __attribute__((const))
bool Model0_is_power_of_2(unsigned long Model0_n)
{
 return (Model0_n != 0 && ((Model0_n & (Model0_n - 1)) == 0));
}

/*
 * round up to nearest power of two
 */
static inline __attribute__((no_instrument_function)) __attribute__((const))
unsigned long Model0___roundup_pow_of_two(unsigned long Model0_n)
{
 return 1UL << Model0_fls_long(Model0_n - 1);
}

/*
 * round down to nearest power of two
 */
static inline __attribute__((no_instrument_function)) __attribute__((const))
unsigned long Model0___rounddown_pow_of_two(unsigned long Model0_n)
{
 return 1UL << (Model0_fls_long(Model0_n) - 1);
}

/**
 * ilog2 - log of base 2 of 32-bit or a 64-bit unsigned value
 * @n - parameter
 *
 * constant-capable log of base 2 calculation
 * - this can be used to initialise global variables from constant data, hence
 *   the massive ternary operator construction
 *
 * selects the appropriately-sized optimised version depending on sizeof(n)
 */
/**
 * roundup_pow_of_two - round the given value up to nearest power of two
 * @n - parameter
 *
 * round the given value up to the nearest power of two
 * - the result is undefined when n == 0
 * - this can be used to initialise global variables from constant data
 */
/**
 * rounddown_pow_of_two - round the given value down to nearest power of two
 * @n - parameter
 *
 * round the given value down to the nearest power of two
 * - the result is undefined when n == 0
 * - this can be used to initialise global variables from constant data
 */







/**
 * order_base_2 - calculate the (rounded up) base 2 order of the argument
 * @n: parameter
 *
 * The first few values calculated by this routine:
 *  ob2(0) = 0
 *  ob2(1) = 0
 *  ob2(2) = 1
 *  ob2(3) = 2
 *  ob2(4) = 2
 *  ob2(5) = 3
 *  ... and so on.
 */



/*
 * Check at compile time that something is of a particular type.
 * Always evaluates to 1 so you may use it easily in comparisons.
 */







/*
 * Check at compile time that 'function' is a certain type, or is a pointer
 * to that type (needs to use typedef for the function type.)
 */










/* These macros are used to mark some functions or 
 * initialized data (doesn't apply to uninitialized data)
 * as `initialization' functions. The kernel can take this
 * as hint that the function is used only during the initialization
 * phase and free up used memory resources after
 *
 * Usage:
 * For functions:
 * 
 * You should add __init immediately before the function name, like:
 *
 * static void __init initme(int x, int y)
 * {
 *    extern int z; z = x * y;
 * }
 *
 * If the function has a prototype somewhere, you can also add
 * __init between closing brace of the prototype and semicolon:
 *
 * extern int initialize_foobar_device(int, int, int) __init;
 *
 * For initialized data:
 * You should insert __initdata or __initconst between the variable name
 * and equal sign followed by value, e.g.:
 *
 * static int init_variable __initdata = 0;
 * static const char linux_logo[] __initconst = { 0x32, 0x36, ... };
 *
 * Don't forget to initialize data not at file scope, i.e. within a function,
 * as gcc otherwise puts the data into the bss section and not into the init
 * section.
 */

/* These are for everybody (although not all archs will actually
   discard it in modules) */






/*
 * Some architecture have tool chains which do not handle rodata attributes
 * correctly. For those disable special sections for const, so that other
 * architectures can annotate correctly.
 */






/*
 * modpost check for section mismatches during the kernel build.
 * A section mismatch happens when there are references from a
 * code or data section to an init section (both code or data).
 * The init sections are (for most archs) discarded by the kernel
 * when early init has completed so all such references are potential bugs.
 * For exit sections the same issue exists.
 *
 * The following markers are used for the cases where the reference to
 * the *init / *exit section (code or data) is valid and will teach
 * modpost not to issue a warning.  Intended semantics is that a code or
 * data tagged __ref* can reference code or data from init section without
 * producing a warning (of course, no warning does not mean code is
 * correct, so optimally document why the __ref is needed and why it's OK).
 *
 * The markers follow same syntax rules as __init / __initdata.
 */
/* Used for MEMORY_HOTPLUG */







/* For assembly routines */
/* silence warnings when references are OK */





/*
 * Used for initialization calls..
 */
typedef int (*Model0_initcall_t)(void);
typedef void (*Model0_exitcall_t)(void);

extern Model0_initcall_t Model0___con_initcall_start[], Model0___con_initcall_end[];
extern Model0_initcall_t Model0___security_initcall_start[], Model0___security_initcall_end[];

/* Used for contructor calls. */
typedef void (*Model0_ctor_fn_t)(void);

/* Defined in init/main.c */
extern int Model0_do_one_initcall(Model0_initcall_t Model0_fn);
extern char __attribute__ ((__section__(".init.data"))) Model0_boot_command_line[];
extern char *Model0_saved_command_line;
extern unsigned int Model0_reset_devices;

/* used by init/main.c */
void Model0_setup_arch(char **);
void Model0_prepare_namespace(void);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_load_default_modules(void);
int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_init_rootfs(void);


void Model0_mark_rodata_ro(void);


extern void (*Model0_late_time_init)(void);

extern bool Model0_initcall_debug;
/* initcalls are now grouped by functionality into separate 
 * subsections. Ordering inside the subsections is determined
 * by link order. 
 * For backwards compatibility, initcall() puts the call in 
 * the device init subsection.
 *
 * The `id' arg to __define_initcall() is needed so that multiple initcalls
 * can point at the same handler without causing duplicate-symbol build errors.
 */






/*
 * Early initcalls run before initializing SMP.
 *
 * Only for built-in code, not modules.
 */


/*
 * A "pure" initcall has no dependencies on anything else, and purely
 * initializes variables that couldn't be statically initialized.
 *
 * This only exists for built-in code, not for modules.
 * Keep main.c:initcall_level_names[] in sync.
 */
struct Model0_obs_kernel_param {
 const char *Model0_str;
 int (*Model0_setup_func)(char *);
 int Model0_early;
};

/*
 * Only for really core code.  See moduleparam.h for the normal way.
 *
 * Force the alignment so the compiler doesn't space elements of the
 * obs_kernel_param "array" too far apart in .init.setup.
 */
/*
 * NOTE: fn is as per module_param, not __setup!
 * Emits warning if fn returns non-zero.
 */
/* Relies on boot_command_line being set */
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_parse_early_param(void);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_parse_early_options(char *Model0_cmdline);
/* Data marked not to be saved by software suspend */
/*
 * Annotation for a "continued" line of log printout (only done after a
 * line that had no enclosing \n). Only to be used by core/arch code
 * during early bootup (a continued line is not SMP-safe otherwise).
 */


/* integer equivalents of KERN_<LEVEL> */













struct Model0_sysinfo {
 Model0___kernel_long_t Model0_uptime; /* Seconds since boot */
 Model0___kernel_ulong_t Model0_loads[3]; /* 1, 5, and 15 minute load averages */
 Model0___kernel_ulong_t Model0_totalram; /* Total usable main memory size */
 Model0___kernel_ulong_t Model0_freeram; /* Available memory size */
 Model0___kernel_ulong_t Model0_sharedram; /* Amount of shared memory */
 Model0___kernel_ulong_t Model0_bufferram; /* Memory used by buffers */
 Model0___kernel_ulong_t Model0_totalswap; /* Total swap space size */
 Model0___kernel_ulong_t Model0_freeswap; /* swap space still available */
 Model0___u16 Model0_procs; /* Number of current processes */
 Model0___u16 Model0_pad; /* Explicit padding for m68k */
 Model0___kernel_ulong_t Model0_totalhigh; /* Total high memory size */
 Model0___kernel_ulong_t Model0_freehigh; /* Available high memory size */
 __u32 Model0_mem_unit; /* Memory unit size in bytes */
 char Model0__f[20-2*sizeof(Model0___kernel_ulong_t)-sizeof(__u32)]; /* Padding: libc5 uses this.. */
};

/*
 * 'kernel.h' contains some often-used function prototypes etc
 */





/* L1 cache line size */
/*
 * __read_mostly is used to keep rarely changing variables out of frequently
 * updated cachelines. If an architecture doesn't support it, ignore the
 * hint.
 */




/*
 * __ro_after_init is used to mark things that are read-only after init (i.e.
 * after mark_rodata_ro() has been called). These are effectively read-only,
 * but may get written to during init, so can't live in .rodata (via "const").
 */
/*
 * The maximum alignment needed for some critical structures
 * These could be inter-node cacheline sizes/L3 cacheline
 * size etc.  Define this in asm/cache.h for your arch
 */

extern const char Model0_linux_banner[];
extern const char Model0_linux_proc_banner[];

static inline __attribute__((no_instrument_function)) int Model0_printk_get_level(const char *Model0_buffer)
{
 if (Model0_buffer[0] == '\001' && Model0_buffer[1]) {
  switch (Model0_buffer[1]) {
  case '0' ... '7':
  case 'd': /* KERN_DEFAULT */
   return Model0_buffer[1];
  }
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) const char *Model0_printk_skip_level(const char *Model0_buffer)
{
 if (Model0_printk_get_level(Model0_buffer))
  return Model0_buffer + 2;

 return Model0_buffer;
}



/* printk's without a loglevel use this.. */


/* We show everything that is MORE important than this.. */







extern int Model0_console_printk[];






static inline __attribute__((no_instrument_function)) void Model0_console_silent(void)
{
 (Model0_console_printk[0]) = 0;
}

static inline __attribute__((no_instrument_function)) void Model0_console_verbose(void)
{
 if ((Model0_console_printk[0]))
  (Model0_console_printk[0]) = 15;
}

/* strlen("ratelimit") + 1 */

extern char Model0_devkmsg_log_str[];
struct Model0_ctl_table;

struct Model0_va_format {
 const char *Model0_fmt;
 Model0_va_list *Model0_va;
};

/*
 * FW_BUG
 * Add this to a message where you are sure the firmware is buggy or behaves
 * really stupid or out of spec. Be aware that the responsible BIOS developer
 * should be able to fix this issue or at least get a concrete idea of the
 * problem by reading your message without the need of looking at the kernel
 * code.
 *
 * Use it for definite and high priority BIOS bugs.
 *
 * FW_WARN
 * Use it for not that clear (e.g. could the kernel messed up things already?)
 * and medium priority BIOS bugs.
 *
 * FW_INFO
 * Use this one if you want to tell the user or vendor about something
 * suspicious, but generally harmless related to the firmware.
 *
 * Use it for information or very low priority BIOS bugs.
 */




/*
 * HW_ERR
 * Add this to a message for hardware errors, so that user can report
 * it to hardware vendor instead of LKML or software vendor.
 */


/*
 * DEPRECATED
 * Add this to a message whenever you want to warn user space about the use
 * of a deprecated aspect of an API so they can stop using it
 */


/*
 * Dummy printk for disabled debugging statements to use whilst maintaining
 * gcc's format checking.
 */
extern __attribute__((format(printf, 1, 2)))
void Model0_early_printk(const char *Model0_fmt, ...);






extern void Model0_printk_nmi_init(void);
extern void Model0_printk_nmi_enter(void);
extern void Model0_printk_nmi_exit(void);
extern void Model0_printk_nmi_flush(void);
extern void Model0_printk_nmi_flush_on_panic(void);
           __attribute__((format(printf, 5, 0)))
int Model0_vprintk_emit(int Model0_facility, int Model0_level,
   const char *Model0_dict, Model0_size_t Model0_dictlen,
   const char *Model0_fmt, Model0_va_list Model0_args);

           __attribute__((format(printf, 1, 0)))
int Model0_vprintk(const char *Model0_fmt, Model0_va_list Model0_args);

           __attribute__((format(printf, 5, 6)))
int Model0_printk_emit(int Model0_facility, int Model0_level,
  const char *Model0_dict, Model0_size_t Model0_dictlen,
  const char *Model0_fmt, ...);

           __attribute__((format(printf, 1, 2)))
int Model0_printk(const char *Model0_fmt, ...);

/*
 * Special printk facility for scheduler/timekeeping use only, _DO_NOT_USE_ !
 */
__attribute__((format(printf, 1, 2))) int Model0_printk_deferred(const char *Model0_fmt, ...);

/*
 * Please don't use printk_ratelimit(), because it shares ratelimiting state
 * with all other unrelated printk_ratelimit() callsites.  Instead use
 * printk_ratelimited() or plain old __ratelimit().
 */
extern int Model0___printk_ratelimit(const char *func);

extern bool Model0_printk_timed_ratelimit(unsigned long *Model0_caller_jiffies,
       unsigned int Model0_interval_msec);

extern int Model0_printk_delay_msec;
extern int Model0_dmesg_restrict;
extern int Model0_kptr_restrict;

extern int
Model0_devkmsg_sysctl_set_loglvl(struct Model0_ctl_table *Model0_table, int Model0_write, void *Model0_buf,
     Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);

extern void Model0_wake_up_klogd(void);

char *Model0_log_buf_addr_get(void);
Model0_u32 Model0_log_buf_len_get(void);
void Model0_log_buf_kexec_setup(void);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_setup_log_buf(int Model0_early);
__attribute__((format(printf, 1, 2))) void Model0_dump_stack_set_arch_desc(const char *Model0_fmt, ...);
void Model0_dump_stack_print_info(const char *Model0_log_lvl);
void Model0_show_regs_print_info(const char *Model0_log_lvl);
extern void Model0_dump_stack(void) ;





/*
 * These can be used to print at the various log levels.
 * All of these will print unconditionally, although note that pr_debug()
 * and other debug macros are compiled out unless either DEBUG is defined
 * or CONFIG_DYNAMIC_DEBUG is set.
 */
/*
 * Like KERN_CONT, pr_cont() should only be used when continuing
 * a line with no newline ('\n') enclosed. Otherwise it defaults
 * back to KERN_DEFAULT.
 */



/* pr_devel() should produce zero code unless DEBUG is defined */
/* If you are writing a driver, please use dev_dbg instead */
/*
 * Print a one-time message (analogous to WARN_ONCE() et al):
 */
/* If you are writing a driver, please use dev_dbg instead */
/*
 * ratelimited messages with local ratelimit_state,
 * no local ratelimit_state used in the !PRINTK case
 */
/* no pr_cont_ratelimited, don't do that... */
/* If you are writing a driver, please use dev_dbg instead */
extern const struct Model0_file_operations Model0_kmsg_fops;

enum {
 Model0_DUMP_PREFIX_NONE,
 Model0_DUMP_PREFIX_ADDRESS,
 Model0_DUMP_PREFIX_OFFSET
};
extern int Model0_hex_dump_to_buffer(const void *Model0_buf, Model0_size_t Model0_len, int Model0_rowsize,
         int Model0_groupsize, char *Model0_linebuf, Model0_size_t Model0_linebuflen,
         bool Model0_ascii);

extern void Model0_print_hex_dump(const char *Model0_level, const char *Model0_prefix_str,
      int Model0_prefix_type, int Model0_rowsize, int Model0_groupsize,
      const void *Model0_buf, Model0_size_t Model0_len, bool Model0_ascii);




extern void Model0_print_hex_dump_bytes(const char *Model0_prefix_str, int Model0_prefix_type,
     const void *Model0_buf, Model0_size_t Model0_len);
static inline __attribute__((no_instrument_function)) void Model0_print_hex_dump_debug(const char *Model0_prefix_str, int Model0_prefix_type,
     int Model0_rowsize, int Model0_groupsize,
     const void *Model0_buf, Model0_size_t Model0_len, bool Model0_ascii)
{
}
/*
 * This looks more complex than it should be. But we need to
 * get the type for the ~ right in round_down (it needs to be
 * as wide as the result!), and we want to evaluate the macro
 * arguments just once each.
 */
/* The `const' in roundup() prevents gcc-3.3 from calling __divdi3 */
/*
 * Divide positive or negative dividend by positive divisor and round
 * to closest integer. Result is undefined for negative divisors and
 * for negative dividends if the divisor variable type is unsigned.
 */
/*
 * Same as above but for u64 dividends. divisor must be a 32-bit
 * number.
 */
/*
 * Multiplies an integer by a fraction, while avoiding unnecessary
 * overflow or loss of precision.
 */
/**
 * upper_32_bits - return bits 32-63 of a number
 * @n: the number we're accessing
 *
 * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress
 * the "right shift count >= width of type" warning when that quantity is
 * 32-bits.
 */


/**
 * lower_32_bits - return bits 0-31 of a number
 * @n: the number we're accessing
 */


struct Model0_completion;
struct Model0_pt_regs;
struct Model0_user;


extern int Model0__cond_resched(void);
  static inline __attribute__((no_instrument_function)) void Model0____might_sleep(const char *Model0_file, int Model0_line,
       int Model0_preempt_offset) { }
  static inline __attribute__((no_instrument_function)) void Model0___might_sleep(const char *Model0_file, int Model0_line,
       int Model0_preempt_offset) { }






/**
 * abs - return absolute value of an argument
 * @x: the value.  If it is unsigned type, it is converted to signed type first.
 *     char is treated as if it was signed (regardless of whether it really is)
 *     but the macro's return type is preserved as char.
 *
 * Return: an absolute value of x.
 */
/**
 * reciprocal_scale - "scale" a value into range [0, ep_ro)
 * @val: value
 * @ep_ro: right open interval endpoint
 *
 * Perform a "reciprocal multiplication" in order to "scale" a value into
 * range [0, ep_ro), where the upper interval endpoint is right-open.
 * This is useful, e.g. for accessing a index of an array containing
 * ep_ro elements, for example. Think of it as sort of modulus, only that
 * the result isn't that of modulo. ;) Note that if initial input is a
 * small value, then result will return 0.
 *
 * Return: a result based on val in interval [0, ep_ro).
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_reciprocal_scale(Model0_u32 Model0_val, Model0_u32 Model0_ep_ro)
{
 return (Model0_u32)(((Model0_u64) Model0_val * Model0_ep_ro) >> 32);
}






static inline __attribute__((no_instrument_function)) void Model0_might_fault(void) { }


extern struct Model0_atomic_notifier_head Model0_panic_notifier_list;
extern long (*Model0_panic_blink)(int Model0_state);
__attribute__((format(printf, 1, 2)))
void Model0_panic(const char *Model0_fmt, ...)
 __attribute__((noreturn)) ;
void Model0_nmi_panic(struct Model0_pt_regs *Model0_regs, const char *Model0_msg);
extern void Model0_oops_enter(void);
extern void Model0_oops_exit(void);
void Model0_print_oops_end_marker(void);
extern int Model0_oops_may_print(void);
void Model0_do_exit(long Model0_error_code)
 __attribute__((noreturn));
void Model0_complete_and_exit(struct Model0_completion *, long)
 __attribute__((noreturn));

/* Internal, do not use. */
int __attribute__((warn_unused_result)) Model0__kstrtoul(const char *Model0_s, unsigned int Model0_base, unsigned long *Model0_res);
int __attribute__((warn_unused_result)) Model0__kstrtol(const char *Model0_s, unsigned int Model0_base, long *Model0_res);

int __attribute__((warn_unused_result)) Model0_kstrtoull(const char *Model0_s, unsigned int Model0_base, unsigned long long *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtoll(const char *Model0_s, unsigned int Model0_base, long long *Model0_res);

/**
 * kstrtoul - convert a string to an unsigned long
 * @s: The start of the string. The string must be null-terminated, and may also
 *  include a single newline before its terminating null. The first character
 *  may also be a plus sign, but not a minus sign.
 * @base: The number base to use. The maximum supported base is 16. If base is
 *  given as 0, then the base of the string is automatically detected with the
 *  conventional semantics - If it begins with 0x the number will be parsed as a
 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be
 *  parsed as an octal number. Otherwise it will be parsed as a decimal.
 * @res: Where to write the result of the conversion on success.
 *
 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
 * Used as a replacement for the obsolete simple_strtoull. Return code must
 * be checked.
*/
static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtoul(const char *Model0_s, unsigned int Model0_base, unsigned long *Model0_res)
{
 /*
	 * We want to shortcut function call, but
	 * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.
	 */
 if (sizeof(unsigned long) == sizeof(unsigned long long) &&
     __alignof__(unsigned long) == __alignof__(unsigned long long))
  return Model0_kstrtoull(Model0_s, Model0_base, (unsigned long long *)Model0_res);
 else
  return Model0__kstrtoul(Model0_s, Model0_base, Model0_res);
}

/**
 * kstrtol - convert a string to a long
 * @s: The start of the string. The string must be null-terminated, and may also
 *  include a single newline before its terminating null. The first character
 *  may also be a plus sign or a minus sign.
 * @base: The number base to use. The maximum supported base is 16. If base is
 *  given as 0, then the base of the string is automatically detected with the
 *  conventional semantics - If it begins with 0x the number will be parsed as a
 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be
 *  parsed as an octal number. Otherwise it will be parsed as a decimal.
 * @res: Where to write the result of the conversion on success.
 *
 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
 * Used as a replacement for the obsolete simple_strtoull. Return code must
 * be checked.
 */
static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtol(const char *Model0_s, unsigned int Model0_base, long *Model0_res)
{
 /*
	 * We want to shortcut function call, but
	 * __builtin_types_compatible_p(long, long long) = 0.
	 */
 if (sizeof(long) == sizeof(long long) &&
     __alignof__(long) == __alignof__(long long))
  return Model0_kstrtoll(Model0_s, Model0_base, (long long *)Model0_res);
 else
  return Model0__kstrtol(Model0_s, Model0_base, Model0_res);
}

int __attribute__((warn_unused_result)) Model0_kstrtouint(const char *Model0_s, unsigned int Model0_base, unsigned int *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtoint(const char *Model0_s, unsigned int Model0_base, int *Model0_res);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtou64(const char *Model0_s, unsigned int Model0_base, Model0_u64 *Model0_res)
{
 return Model0_kstrtoull(Model0_s, Model0_base, Model0_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtos64(const char *Model0_s, unsigned int Model0_base, Model0_s64 *Model0_res)
{
 return Model0_kstrtoll(Model0_s, Model0_base, Model0_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtou32(const char *Model0_s, unsigned int Model0_base, Model0_u32 *Model0_res)
{
 return Model0_kstrtouint(Model0_s, Model0_base, Model0_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtos32(const char *Model0_s, unsigned int Model0_base, Model0_s32 *Model0_res)
{
 return Model0_kstrtoint(Model0_s, Model0_base, Model0_res);
}

int __attribute__((warn_unused_result)) Model0_kstrtou16(const char *Model0_s, unsigned int Model0_base, Model0_u16 *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtos16(const char *Model0_s, unsigned int Model0_base, Model0_s16 *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtou8(const char *Model0_s, unsigned int Model0_base, Model0_u8 *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtos8(const char *Model0_s, unsigned int Model0_base, Model0_s8 *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtobool(const char *Model0_s, bool *Model0_res);

int __attribute__((warn_unused_result)) Model0_kstrtoull_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, unsigned long long *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtoll_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, long long *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtoul_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, unsigned long *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtol_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, long *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtouint_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, unsigned int *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtoint_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, int *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtou16_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, Model0_u16 *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtos16_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, Model0_s16 *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtou8_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, Model0_u8 *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtos8_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, Model0_s8 *Model0_res);
int __attribute__((warn_unused_result)) Model0_kstrtobool_from_user(const char *Model0_s, Model0_size_t Model0_count, bool *Model0_res);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtou64_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, Model0_u64 *Model0_res)
{
 return Model0_kstrtoull_from_user(Model0_s, Model0_count, Model0_base, Model0_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtos64_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, Model0_s64 *Model0_res)
{
 return Model0_kstrtoll_from_user(Model0_s, Model0_count, Model0_base, Model0_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtou32_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, Model0_u32 *Model0_res)
{
 return Model0_kstrtouint_from_user(Model0_s, Model0_count, Model0_base, Model0_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kstrtos32_from_user(const char *Model0_s, Model0_size_t Model0_count, unsigned int Model0_base, Model0_s32 *Model0_res)
{
 return Model0_kstrtoint_from_user(Model0_s, Model0_count, Model0_base, Model0_res);
}

/* Obsolete, do not use.  Use kstrto<foo> instead */

extern unsigned long Model0_simple_strtoul(const char *,char **,unsigned int);
extern long Model0_simple_strtol(const char *,char **,unsigned int);
extern unsigned long long Model0_simple_strtoull(const char *,char **,unsigned int);
extern long long Model0_simple_strtoll(const char *,char **,unsigned int);

extern int Model0_num_to_str(char *Model0_buf, int Model0_size, unsigned long long Model0_num);

/* lib/printf utilities */

extern __attribute__((format(printf, 2, 3))) int Model0_sprintf(char *Model0_buf, const char * Model0_fmt, ...);
extern __attribute__((format(printf, 2, 0))) int Model0_vsprintf(char *Model0_buf, const char *, Model0_va_list);
extern __attribute__((format(printf, 3, 4)))
int Model0_snprintf(char *Model0_buf, Model0_size_t Model0_size, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 3, 0)))
int Model0_vsnprintf(char *Model0_buf, Model0_size_t Model0_size, const char *Model0_fmt, Model0_va_list Model0_args);
extern __attribute__((format(printf, 3, 4)))
int Model0_scnprintf(char *Model0_buf, Model0_size_t Model0_size, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 3, 0)))
int Model0_vscnprintf(char *Model0_buf, Model0_size_t Model0_size, const char *Model0_fmt, Model0_va_list Model0_args);
extern __attribute__((format(printf, 2, 3))) __attribute__((__malloc__))
char *Model0_kasprintf(Model0_gfp_t Model0_gfp, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 2, 0))) __attribute__((__malloc__))
char *Model0_kvasprintf(Model0_gfp_t Model0_gfp, const char *Model0_fmt, Model0_va_list Model0_args);
extern __attribute__((format(printf, 2, 0)))
const char *Model0_kvasprintf_const(Model0_gfp_t Model0_gfp, const char *Model0_fmt, Model0_va_list Model0_args);

extern __attribute__((format(scanf, 2, 3)))
int Model0_sscanf(const char *, const char *, ...);
extern __attribute__((format(scanf, 2, 0)))
int Model0_vsscanf(const char *, const char *, Model0_va_list);

extern int Model0_get_option(char **Model0_str, int *Model0_pint);
extern char *Model0_get_options(const char *Model0_str, int Model0_nints, int *Model0_ints);
extern unsigned long long Model0_memparse(const char *Model0_ptr, char **Model0_retptr);
extern bool Model0_parse_option_str(const char *Model0_str, const char *Model0_option);

extern int Model0_core_kernel_text(unsigned long Model0_addr);
extern int Model0_core_kernel_data(unsigned long Model0_addr);
extern int Model0___kernel_text_address(unsigned long Model0_addr);
extern int Model0_kernel_text_address(unsigned long Model0_addr);
extern int Model0_func_ptr_is_kernel_text(void *Model0_ptr);

unsigned long Model0_int_sqrt(unsigned long);

extern void Model0_bust_spinlocks(int Model0_yes);
extern int Model0_oops_in_progress; /* If set, an oops, panic(), BUG() or die() is in progress */
extern int Model0_panic_timeout;
extern int Model0_panic_on_oops;
extern int Model0_panic_on_unrecovered_nmi;
extern int Model0_panic_on_io_nmi;
extern int Model0_panic_on_warn;
extern int Model0_sysctl_panic_on_rcu_stall;
extern int Model0_sysctl_panic_on_stackoverflow;

extern bool Model0_crash_kexec_post_notifiers;

/*
 * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It
 * holds a CPU number which is executing panic() currently. A value of
 * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().
 */
extern Model0_atomic_t Model0_panic_cpu;


/*
 * Only to be used by arch init code. If the user over-wrote the default
 * CONFIG_PANIC_TIMEOUT, honor it.
 */
static inline __attribute__((no_instrument_function)) void Model0_set_arch_panic_timeout(int Model0_timeout, int Model0_arch_default_timeout)
{
 if (Model0_panic_timeout == Model0_arch_default_timeout)
  Model0_panic_timeout = Model0_timeout;
}
extern const char *Model0_print_tainted(void);
enum Model0_lockdep_ok {
 Model0_LOCKDEP_STILL_OK,
 Model0_LOCKDEP_NOW_UNRELIABLE
};
extern void Model0_add_taint(unsigned Model0_flag, enum Model0_lockdep_ok);
extern int Model0_test_taint(unsigned Model0_flag);
extern unsigned long Model0_get_taint(void);
extern int Model0_root_mountflags;

extern bool Model0_early_boot_irqs_disabled;

/* Values used for system_state */
extern enum Model0_system_states {
 Model0_SYSTEM_BOOTING,
 Model0_SYSTEM_RUNNING,
 Model0_SYSTEM_HALT,
 Model0_SYSTEM_POWER_OFF,
 Model0_SYSTEM_RESTART,
} Model0_system_state;
extern const char Model0_hex_asc[];



static inline __attribute__((no_instrument_function)) char *Model0_hex_byte_pack(char *Model0_buf, Model0_u8 Model0_byte)
{
 *Model0_buf++ = Model0_hex_asc[((Model0_byte) & 0xf0) >> 4];
 *Model0_buf++ = Model0_hex_asc[((Model0_byte) & 0x0f)];
 return Model0_buf;
}

extern const char Model0_hex_asc_upper[];



static inline __attribute__((no_instrument_function)) char *Model0_hex_byte_pack_upper(char *Model0_buf, Model0_u8 Model0_byte)
{
 *Model0_buf++ = Model0_hex_asc_upper[((Model0_byte) & 0xf0) >> 4];
 *Model0_buf++ = Model0_hex_asc_upper[((Model0_byte) & 0x0f)];
 return Model0_buf;
}

extern int Model0_hex_to_bin(char Model0_ch);
extern int __attribute__((warn_unused_result)) Model0_hex2bin(Model0_u8 *Model0_dst, const char *Model0_src, Model0_size_t Model0_count);
extern char *Model0_bin2hex(char *Model0_dst, const void *Model0_src, Model0_size_t Model0_count);

bool Model0_mac_pton(const char *Model0_s, Model0_u8 *Model0_mac);

/*
 * General tracing related utility functions - trace_printk(),
 * tracing_on/tracing_off and tracing_start()/tracing_stop
 *
 * Use tracing_on/tracing_off when you want to quickly turn on or off
 * tracing. It simply enables or disables the recording of the trace events.
 * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on
 * file, which gives a means for the kernel and userspace to interact.
 * Place a tracing_off() in the kernel where you want tracing to end.
 * From user space, examine the trace, and then echo 1 > tracing_on
 * to continue tracing.
 *
 * tracing_stop/tracing_start has slightly more overhead. It is used
 * by things like suspend to ram where disabling the recording of the
 * trace is not enough, but tracing must actually stop because things
 * like calling smp_processor_id() may crash the system.
 *
 * Most likely, you want to use tracing_on/tracing_off.
 */

enum Model0_ftrace_dump_mode {
 Model0_DUMP_NONE,
 Model0_DUMP_ALL,
 Model0_DUMP_ORIG,
};


void Model0_tracing_on(void);
void Model0_tracing_off(void);
int Model0_tracing_is_on(void);
void Model0_tracing_snapshot(void);
void Model0_tracing_snapshot_alloc(void);

extern void Model0_tracing_start(void);
extern void Model0_tracing_stop(void);

static inline __attribute__((no_instrument_function)) __attribute__((format(printf, 1, 2)))
void Model0_____trace_printk_check_format(const char *Model0_fmt, ...)
{
}






/**
 * trace_printk - printf formatting in the ftrace buffer
 * @fmt: the printf format for printing
 *
 * Note: __trace_printk is an internal function for trace_printk and
 *       the @ip is passed in via the trace_printk macro.
 *
 * This function allows a kernel developer to debug fast path sections
 * that printk is not appropriate for. By scattering in various
 * printk like tracing in the code, a developer can quickly see
 * where problems are occurring.
 *
 * This is intended as a debugging tool for the developer only.
 * Please refrain from leaving trace_printks scattered around in
 * your code. (Extra memory is used for special buffers that are
 * allocated when trace_printk() is used)
 *
 * A little optization trick is done here. If there's only one
 * argument, there's no need to scan the string for printf formats.
 * The trace_puts() will suffice. But how can we take advantage of
 * using trace_puts() when trace_printk() has only one argument?
 * By stringifying the args and checking the size we can tell
 * whether or not there are args. __stringify((__VA_ARGS__)) will
 * turn into "()\0" with a size of 3 when there are no args, anything
 * else will be bigger. All we need to do is define a string to this,
 * and then take its size and compare to 3. If it's bigger, use
 * do_trace_printk() otherwise, optimize it to trace_puts(). Then just
 * let gcc optimize the rest.
 */
extern __attribute__((format(printf, 2, 3)))
int Model0___trace_bprintk(unsigned long Model0_ip, const char *Model0_fmt, ...);

extern __attribute__((format(printf, 2, 3)))
int Model0___trace_printk(unsigned long Model0_ip, const char *Model0_fmt, ...);

/**
 * trace_puts - write a string into the ftrace buffer
 * @str: the string to record
 *
 * Note: __trace_bputs is an internal function for trace_puts and
 *       the @ip is passed in via the trace_puts macro.
 *
 * This is similar to trace_printk() but is made for those really fast
 * paths that a developer wants the least amount of "Heisenbug" affects,
 * where the processing of the print format is still too much.
 *
 * This function allows a kernel developer to debug fast path sections
 * that printk is not appropriate for. By scattering in various
 * printk like tracing in the code, a developer can quickly see
 * where problems are occurring.
 *
 * This is intended as a debugging tool for the developer only.
 * Please refrain from leaving trace_puts scattered around in
 * your code. (Extra memory is used for special buffers that are
 * allocated when trace_puts() is used)
 *
 * Returns: 0 if nothing was written, positive # if string was.
 *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)
 */
extern int Model0___trace_bputs(unsigned long Model0_ip, const char *Model0_str);
extern int Model0___trace_puts(unsigned long Model0_ip, const char *Model0_str, int Model0_size);

extern void Model0_trace_dump_stack(int Model0_skip);

/*
 * The double __builtin_constant_p is because gcc will give us an error
 * if we try to allocate the static variable to fmt if it is not a
 * constant. Even with the outer if statement.
 */
extern __attribute__((format(printf, 2, 0))) int
Model0___ftrace_vbprintk(unsigned long Model0_ip, const char *Model0_fmt, Model0_va_list Model0_ap);

extern __attribute__((format(printf, 2, 0))) int
Model0___ftrace_vprintk(unsigned long Model0_ip, const char *Model0_fmt, Model0_va_list Model0_ap);

extern void Model0_ftrace_dump(enum Model0_ftrace_dump_mode Model0_oops_dump_mode);
/*
 * min()/max()/clamp() macros that also do
 * strict type-checking.. See the
 * "unnecessary" pointer comparison.
 */
/**
 * min_not_zero - return the minimum that is _not_ zero, unless both are zero
 * @x: value1
 * @y: value2
 */





/**
 * clamp - return a value clamped to a given range with strict typechecking
 * @val: current value
 * @lo: lowest allowable value
 * @hi: highest allowable value
 *
 * This macro does strict typechecking of lo/hi to make sure they are of the
 * same type as val.  See the unnecessary pointer comparisons.
 */


/*
 * ..and if you can't take the strict
 * types, you can specify one yourself.
 *
 * Or not use min/max/clamp at all, of course.
 */
/**
 * clamp_t - return a value clamped to a given range using a given type
 * @type: the type of variable to use
 * @val: current value
 * @lo: minimum allowable value
 * @hi: maximum allowable value
 *
 * This macro does no typechecking and uses temporary variables of type
 * 'type' to make all the comparisons.
 */


/**
 * clamp_val - return a value clamped to a given range using val's type
 * @val: current value
 * @lo: minimum allowable value
 * @hi: maximum allowable value
 *
 * This macro does no typechecking and uses temporary variables of whatever
 * type the input argument 'val' is.  This is useful when val is an unsigned
 * type and min and max are literals that will otherwise be assigned a signed
 * integer type.
 */



/*
 * swap - swap value of @a and @b
 */



/**
 * container_of - cast a member of a structure out to the containing structure
 * @ptr:	the pointer to the member.
 * @type:	the type of the container struct this is embedded in.
 * @member:	the name of the member within the struct.
 *
 */




/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */




/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */




struct Model0_bug_entry {



 signed int Model0_bug_addr_disp;





 signed int Model0_file_disp;

 unsigned short Model0_line;

 unsigned short Model0_flags;
};


/*
 * Don't use BUG() or BUG_ON() unless there's really no way out; one
 * example might be detecting data structure corruption in the middle
 * of an operation that can't be backed out of.  If the (sub)system
 * can somehow continue operating, perhaps with reduced functionality,
 * it's probably not BUG-worthy.
 *
 * If you're tempted to BUG(), think again:  is completely giving up
 * really the *only* solution?  There are usually better options, where
 * users don't need to reboot ASAP and can mostly shut down cleanly.
 */
/*
 * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report
 * significant issues that need prompt attention if they should ever
 * appear at runtime.  Use the versions with printk format strings
 * to provide better diagnostics.
 */

extern __attribute__((format(printf, 3, 4)))
void Model0_warn_slowpath_fmt(const char *Model0_file, const int Model0_line,
         const char *Model0_fmt, ...);
extern __attribute__((format(printf, 4, 5)))
void Model0_warn_slowpath_fmt_taint(const char *Model0_file, const int Model0_line, unsigned Model0_taint,
        const char *Model0_fmt, ...);
extern void Model0_warn_slowpath_null(const char *Model0_file, const int Model0_line);
/* used internally by panic.c */
struct Model0_warn_args;

void Model0___warn(const char *Model0_file, int Model0_line, void *Model0_caller, unsigned Model0_taint,
     struct Model0_pt_regs *Model0_regs, struct Model0_warn_args *Model0_args);
/*
 * WARN_ON_SMP() is for cases that the warning is either
 * meaningless for !SMP or may even cause failures.
 * This is usually used for cases that we have
 * WARN_ON(!spin_is_locked(&lock)) checks, as spin_is_locked()
 * returns 0 for uniprocessor settings.
 * It can also be used with values that are only defined
 * on SMP:
 *
 * struct foo {
 *  [...]
 * #ifdef CONFIG_SMP
 *	int bar;
 * #endif
 * };
 *
 * void func(struct foo *zoot)
 * {
 *	WARN_ON_SMP(!zoot->bar);
 *
 * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),
 * and should be a nop and return false for uniprocessor.
 *
 * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set
 * and x is true.
 */


enum Model0_bug_trap_type {
 Model0_BUG_TRAP_TYPE_NONE = 0,
 Model0_BUG_TRAP_TYPE_WARN = 1,
 Model0_BUG_TRAP_TYPE_BUG = 2,
};

struct Model0_pt_regs;
/* Force a compilation error if a constant expression is not a power of 2 */



/* Force a compilation error if condition is true, but also produce a
   result (of value 0 and type size_t), so the expression can be used
   e.g. in a structure initializer (or where-ever else comma expressions
   aren't permitted). */



/*
 * BUILD_BUG_ON_INVALID() permits the compiler to check the validity of the
 * expression but avoids the generation of any code, even if that expression
 * has side-effects.
 */


/**
 * BUILD_BUG_ON_MSG - break compile if a condition is true & emit supplied
 *		      error message.
 * @condition: the condition which the compiler should know is false.
 *
 * See BUILD_BUG_ON for description.
 */


/**
 * BUILD_BUG_ON - break compile if a condition is true.
 * @condition: the condition which the compiler should know is false.
 *
 * If you have some code which relies on certain constants being equal, or
 * some other compile-time-evaluated condition, you should use BUILD_BUG_ON to
 * detect if someone changes it.
 *
 * The implementation uses gcc's reluctance to create a negative array, but gcc
 * (as of 4.4) only emits that error for obvious cases (e.g. not arguments to
 * inline functions).  Luckily, in 4.3 they added the "error" function
 * attribute just for this type of case.  Thus, we use a negative sized array
 * (should always create an error on gcc versions older than 4.4) and then call
 * an undefined function with the error attribute (should always create an
 * error on gcc 4.3 and later).  If for some reason, neither creates a
 * compile-time error, we'll still have a link-time error, which is harder to
 * track down.
 */







/**
 * BUILD_BUG - break compile if used.
 *
 * If you have some code that you expect the compiler to eliminate at
 * build time, you should use BUILD_BUG to detect if it is
 * unexpectedly used.
 */
static inline __attribute__((no_instrument_function)) int Model0_is_warning_bug(const struct Model0_bug_entry *Model0_bug)
{
 return Model0_bug->Model0_flags & (1 << 0);
}

const struct Model0_bug_entry *Model0_find_bug(unsigned long Model0_bugaddr);

enum Model0_bug_trap_type Model0_report_bug(unsigned long Model0_bug_addr, struct Model0_pt_regs *Model0_regs);

/* These are defined by the architecture */
int Model0_is_valid_bugaddr(unsigned long Model0_addr);


struct Model0_page;
struct Model0_vm_area_struct;
struct Model0_mm_struct;

extern void Model0_dump_page(struct Model0_page *Model0_page, const char *Model0_reason);
extern void Model0___dump_page(struct Model0_page *Model0_page, const char *Model0_reason);
void Model0_dump_vma(const struct Model0_vm_area_struct *Model0_vma);
void Model0_dump_mm(const struct Model0_mm_struct *Model0_mm);













/*
 * include/linux/spinlock.h - generic spinlock/rwlock declarations
 *
 * here's the role of the various spinlock/rwlock related include files:
 *
 * on SMP builds:
 *
 *  asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the
 *                        initializers
 *
 *  linux/spinlock_types.h:
 *                        defines the generic type and initializers
 *
 *  asm/spinlock.h:       contains the arch_spin_*()/etc. lowlevel
 *                        implementations, mostly inline assembly code
 *
 *   (also included on UP-debug builds:)
 *
 *  linux/spinlock_api_smp.h:
 *                        contains the prototypes for the _spin_*() APIs.
 *
 *  linux/spinlock.h:     builds the final spin_*() APIs.
 *
 * on UP builds:
 *
 *  linux/spinlock_type_up.h:
 *                        contains the generic, simplified UP spinlock type.
 *                        (which is an empty structure on non-debug builds)
 *
 *  linux/spinlock_types.h:
 *                        defines the generic type and initializers
 *
 *  linux/spinlock_up.h:
 *                        contains the arch_spin_*()/etc. version of UP
 *                        builds. (which are NOPs on non-debug, non-preempt
 *                        builds)
 *
 *   (included on UP-non-debug builds:)
 *
 *  linux/spinlock_api_up.h:
 *                        builds the _spin_*() APIs.
 *
 *  linux/spinlock.h:     builds the final spin_*() APIs.
 */






/*
 * include/linux/preempt.h - macros for accessing and manipulating
 * preempt_count (used for kernel preemption, interrupt count, etc.)
 */











/********** include/linux/list.h **********/

/*
 * Architectures might want to move the poison pointer offset
 * into some well-recognized area such as 0xdead000000000000,
 * that is also not mappable by user-space exploits:
 */






/*
 * These are non-NULL pointers that will result in page faults
 * under normal circumstances, used to verify that nobody uses
 * non-initialized list entries.
 */



/********** include/linux/timer.h **********/
/*
 * Magic number "tsta" to indicate a static timer initializer
 * for the object debugging code.
 */


/********** mm/debug-pagealloc.c **********/






/********** mm/page_alloc.c ************/



/********** mm/slab.c **********/
/*
 * Magic nums for obj red zoning.
 * Placed in the first word before and the first word after an obj.
 */






/* ...and for poisoning */




/********** arch/$ARCH/mm/init.c **********/


/********** arch/ia64/hp/common/sba_iommu.c **********/
/*
 * arch/ia64/hp/common/sba_iommu.c uses a 16-byte poison string with a
 * value of "SBAIOMMU POISON\0" for spill-over poisoning.
 */

/********** fs/jbd/journal.c **********/



/********** drivers/base/dmapool.c **********/



/********** drivers/atm/ **********/



/********** kernel/mutexes **********/



/********** lib/flex_array.c **********/


/********** security/ **********/
/* const.h: Macros for dealing with constants.  */




/* Some constant macros are used in both assembler and
 * C code.  Therefore we cannot annotate them always with
 * 'UL' and other type specifiers unilaterally.  We
 * use the following macros to deal with this.
 *
 * Similarly, _AT() will cast an expression with a type in C, but
 * leave it unchanged in asm.
 */


/*
 * Simple doubly linked list implementation.
 *
 * Some of the internal functions ("__xxx") are useful when
 * manipulating whole lists rather than single entries, as
 * sometimes we already know the next/prev entries and we can
 * generate better code by using them directly rather than
 * using the generic single-entry routines.
 */






static inline __attribute__((no_instrument_function)) void Model0_INIT_LIST_HEAD(struct Model0_list_head *Model0_list)
{
 ({ union { typeof(Model0_list->Model0_next) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_list->Model0_next)) (Model0_list) }; Model0___write_once_size(&(Model0_list->Model0_next), Model0___u.Model0___c, sizeof(Model0_list->Model0_next)); Model0___u.Model0___val; });
 Model0_list->Model0_prev = Model0_list;
}

/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */

static inline __attribute__((no_instrument_function)) void Model0___list_add(struct Model0_list_head *Model0_new,
         struct Model0_list_head *Model0_prev,
         struct Model0_list_head *Model0_next)
{
 Model0_next->Model0_prev = Model0_new;
 Model0_new->Model0_next = Model0_next;
 Model0_new->Model0_prev = Model0_prev;
 ({ union { typeof(Model0_prev->Model0_next) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_prev->Model0_next)) (Model0_new) }; Model0___write_once_size(&(Model0_prev->Model0_next), Model0___u.Model0___c, sizeof(Model0_prev->Model0_next)); Model0___u.Model0___val; });
}






/**
 * list_add - add a new entry
 * @new: new entry to be added
 * @head: list head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
static inline __attribute__((no_instrument_function)) void Model0_list_add(struct Model0_list_head *Model0_new, struct Model0_list_head *Model0_head)
{
 Model0___list_add(Model0_new, Model0_head, Model0_head->Model0_next);
}


/**
 * list_add_tail - add a new entry
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline __attribute__((no_instrument_function)) void Model0_list_add_tail(struct Model0_list_head *Model0_new, struct Model0_list_head *Model0_head)
{
 Model0___list_add(Model0_new, Model0_head->Model0_prev, Model0_head);
}

/*
 * Delete a list entry by making the prev/next entries
 * point to each other.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline __attribute__((no_instrument_function)) void Model0___list_del(struct Model0_list_head * Model0_prev, struct Model0_list_head * Model0_next)
{
 Model0_next->Model0_prev = Model0_prev;
 ({ union { typeof(Model0_prev->Model0_next) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_prev->Model0_next)) (Model0_next) }; Model0___write_once_size(&(Model0_prev->Model0_next), Model0___u.Model0___c, sizeof(Model0_prev->Model0_next)); Model0___u.Model0___val; });
}

/**
 * list_del - deletes entry from list.
 * @entry: the element to delete from the list.
 * Note: list_empty() on entry does not return true after this, the entry is
 * in an undefined state.
 */

static inline __attribute__((no_instrument_function)) void Model0___list_del_entry(struct Model0_list_head *Model0_entry)
{
 Model0___list_del(Model0_entry->Model0_prev, Model0_entry->Model0_next);
}

static inline __attribute__((no_instrument_function)) void Model0_list_del(struct Model0_list_head *Model0_entry)
{
 Model0___list_del(Model0_entry->Model0_prev, Model0_entry->Model0_next);
 Model0_entry->Model0_next = ((void *) 0x100 + (0xdead000000000000UL));
 Model0_entry->Model0_prev = ((void *) 0x200 + (0xdead000000000000UL));
}





/**
 * list_replace - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * If @old was empty, it will be overwritten.
 */
static inline __attribute__((no_instrument_function)) void Model0_list_replace(struct Model0_list_head *old,
    struct Model0_list_head *Model0_new)
{
 Model0_new->Model0_next = old->Model0_next;
 Model0_new->Model0_next->Model0_prev = Model0_new;
 Model0_new->Model0_prev = old->Model0_prev;
 Model0_new->Model0_prev->Model0_next = Model0_new;
}

static inline __attribute__((no_instrument_function)) void Model0_list_replace_init(struct Model0_list_head *old,
     struct Model0_list_head *Model0_new)
{
 Model0_list_replace(old, Model0_new);
 Model0_INIT_LIST_HEAD(old);
}

/**
 * list_del_init - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 */
static inline __attribute__((no_instrument_function)) void Model0_list_del_init(struct Model0_list_head *Model0_entry)
{
 Model0___list_del_entry(Model0_entry);
 Model0_INIT_LIST_HEAD(Model0_entry);
}

/**
 * list_move - delete from one list and add as another's head
 * @list: the entry to move
 * @head: the head that will precede our entry
 */
static inline __attribute__((no_instrument_function)) void Model0_list_move(struct Model0_list_head *Model0_list, struct Model0_list_head *Model0_head)
{
 Model0___list_del_entry(Model0_list);
 Model0_list_add(Model0_list, Model0_head);
}

/**
 * list_move_tail - delete from one list and add as another's tail
 * @list: the entry to move
 * @head: the head that will follow our entry
 */
static inline __attribute__((no_instrument_function)) void Model0_list_move_tail(struct Model0_list_head *Model0_list,
      struct Model0_list_head *Model0_head)
{
 Model0___list_del_entry(Model0_list);
 Model0_list_add_tail(Model0_list, Model0_head);
}

/**
 * list_is_last - tests whether @list is the last entry in list @head
 * @list: the entry to test
 * @head: the head of the list
 */
static inline __attribute__((no_instrument_function)) int Model0_list_is_last(const struct Model0_list_head *Model0_list,
    const struct Model0_list_head *Model0_head)
{
 return Model0_list->Model0_next == Model0_head;
}

/**
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline __attribute__((no_instrument_function)) int Model0_list_empty(const struct Model0_list_head *Model0_head)
{
 return ({ union { typeof(Model0_head->Model0_next) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_head->Model0_next), Model0___u.Model0___c, sizeof(Model0_head->Model0_next)); else Model0___read_once_size_nocheck(&(Model0_head->Model0_next), Model0___u.Model0___c, sizeof(Model0_head->Model0_next)); Model0___u.Model0___val; }) == Model0_head;
}

/**
 * list_empty_careful - tests whether a list is empty and not being modified
 * @head: the list to test
 *
 * Description:
 * tests whether a list is empty _and_ checks that no other CPU might be
 * in the process of modifying either member (next or prev)
 *
 * NOTE: using list_empty_careful() without synchronization
 * can only be safe if the only activity that can happen
 * to the list entry is list_del_init(). Eg. it cannot be used
 * if another CPU could re-list_add() it.
 */
static inline __attribute__((no_instrument_function)) int Model0_list_empty_careful(const struct Model0_list_head *Model0_head)
{
 struct Model0_list_head *Model0_next = Model0_head->Model0_next;
 return (Model0_next == Model0_head) && (Model0_next == Model0_head->Model0_prev);
}

/**
 * list_rotate_left - rotate the list to the left
 * @head: the head of the list
 */
static inline __attribute__((no_instrument_function)) void Model0_list_rotate_left(struct Model0_list_head *Model0_head)
{
 struct Model0_list_head *Model0_first;

 if (!Model0_list_empty(Model0_head)) {
  Model0_first = Model0_head->Model0_next;
  Model0_list_move_tail(Model0_first, Model0_head);
 }
}

/**
 * list_is_singular - tests whether a list has just one entry.
 * @head: the list to test.
 */
static inline __attribute__((no_instrument_function)) int Model0_list_is_singular(const struct Model0_list_head *Model0_head)
{
 return !Model0_list_empty(Model0_head) && (Model0_head->Model0_next == Model0_head->Model0_prev);
}

static inline __attribute__((no_instrument_function)) void Model0___list_cut_position(struct Model0_list_head *Model0_list,
  struct Model0_list_head *Model0_head, struct Model0_list_head *Model0_entry)
{
 struct Model0_list_head *Model0_new_first = Model0_entry->Model0_next;
 Model0_list->Model0_next = Model0_head->Model0_next;
 Model0_list->Model0_next->Model0_prev = Model0_list;
 Model0_list->Model0_prev = Model0_entry;
 Model0_entry->Model0_next = Model0_list;
 Model0_head->Model0_next = Model0_new_first;
 Model0_new_first->Model0_prev = Model0_head;
}

/**
 * list_cut_position - cut a list into two
 * @list: a new list to add all removed entries
 * @head: a list with entries
 * @entry: an entry within head, could be the head itself
 *	and if so we won't cut the list
 *
 * This helper moves the initial part of @head, up to and
 * including @entry, from @head to @list. You should
 * pass on @entry an element you know is on @head. @list
 * should be an empty list or a list you do not care about
 * losing its data.
 *
 */
static inline __attribute__((no_instrument_function)) void Model0_list_cut_position(struct Model0_list_head *Model0_list,
  struct Model0_list_head *Model0_head, struct Model0_list_head *Model0_entry)
{
 if (Model0_list_empty(Model0_head))
  return;
 if (Model0_list_is_singular(Model0_head) &&
  (Model0_head->Model0_next != Model0_entry && Model0_head != Model0_entry))
  return;
 if (Model0_entry == Model0_head)
  Model0_INIT_LIST_HEAD(Model0_list);
 else
  Model0___list_cut_position(Model0_list, Model0_head, Model0_entry);
}

static inline __attribute__((no_instrument_function)) void Model0___list_splice(const struct Model0_list_head *Model0_list,
     struct Model0_list_head *Model0_prev,
     struct Model0_list_head *Model0_next)
{
 struct Model0_list_head *Model0_first = Model0_list->Model0_next;
 struct Model0_list_head *Model0_last = Model0_list->Model0_prev;

 Model0_first->Model0_prev = Model0_prev;
 Model0_prev->Model0_next = Model0_first;

 Model0_last->Model0_next = Model0_next;
 Model0_next->Model0_prev = Model0_last;
}

/**
 * list_splice - join two lists, this is designed for stacks
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 */
static inline __attribute__((no_instrument_function)) void Model0_list_splice(const struct Model0_list_head *Model0_list,
    struct Model0_list_head *Model0_head)
{
 if (!Model0_list_empty(Model0_list))
  Model0___list_splice(Model0_list, Model0_head, Model0_head->Model0_next);
}

/**
 * list_splice_tail - join two lists, each list being a queue
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 */
static inline __attribute__((no_instrument_function)) void Model0_list_splice_tail(struct Model0_list_head *Model0_list,
    struct Model0_list_head *Model0_head)
{
 if (!Model0_list_empty(Model0_list))
  Model0___list_splice(Model0_list, Model0_head->Model0_prev, Model0_head);
}

/**
 * list_splice_init - join two lists and reinitialise the emptied list.
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 *
 * The list at @list is reinitialised
 */
static inline __attribute__((no_instrument_function)) void Model0_list_splice_init(struct Model0_list_head *Model0_list,
        struct Model0_list_head *Model0_head)
{
 if (!Model0_list_empty(Model0_list)) {
  Model0___list_splice(Model0_list, Model0_head, Model0_head->Model0_next);
  Model0_INIT_LIST_HEAD(Model0_list);
 }
}

/**
 * list_splice_tail_init - join two lists and reinitialise the emptied list
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 *
 * Each of the lists is a queue.
 * The list at @list is reinitialised
 */
static inline __attribute__((no_instrument_function)) void Model0_list_splice_tail_init(struct Model0_list_head *Model0_list,
      struct Model0_list_head *Model0_head)
{
 if (!Model0_list_empty(Model0_list)) {
  Model0___list_splice(Model0_list, Model0_head->Model0_prev, Model0_head);
  Model0_INIT_LIST_HEAD(Model0_list);
 }
}

/**
 * list_entry - get the struct for this entry
 * @ptr:	the &struct list_head pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_first_entry - get the first element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note, that list is expected to be not empty.
 */



/**
 * list_last_entry - get the last element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note, that list is expected to be not empty.
 */



/**
 * list_first_entry_or_null - get the first element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note that if the list is empty, it returns NULL.
 */



/**
 * list_next_entry - get the next element in list
 * @pos:	the type * to cursor
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_prev_entry - get the prev element in list
 * @pos:	the type * to cursor
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_for_each	-	iterate over a list
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */



/**
 * list_for_each_prev	-	iterate over a list backwards
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */



/**
 * list_for_each_safe - iterate over a list safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */




/**
 * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */





/**
 * list_for_each_entry	-	iterate over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */





/**
 * list_for_each_entry_reverse - iterate backwards over list of given type.
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */





/**
 * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()
 * @pos:	the type * to use as a start point
 * @head:	the head of the list
 * @member:	the name of the list_head within the struct.
 *
 * Prepares a pos entry for use as a start point in list_for_each_entry_continue().
 */



/**
 * list_for_each_entry_continue - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */





/**
 * list_for_each_entry_continue_reverse - iterate backwards from the given point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Start to iterate over list of given type backwards, continuing after
 * the current position.
 */





/**
 * list_for_each_entry_from - iterate over list of given type from the current point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type, continuing from current position.
 */




/**
 * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */






/**
 * list_for_each_entry_safe_continue - continue list iteration safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type, continuing after current point,
 * safe against removal of list entry.
 */






/**
 * list_for_each_entry_safe_from - iterate over list from current point safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type from current point, safe against
 * removal of list entry.
 */





/**
 * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate backwards over list of given type, safe against removal
 * of list entry.
 */






/**
 * list_safe_reset_next - reset a stale list_for_each_entry_safe loop
 * @pos:	the loop cursor used in the list_for_each_entry_safe loop
 * @n:		temporary storage used in list_for_each_entry_safe
 * @member:	the name of the list_head within the struct.
 *
 * list_safe_reset_next is not safe to use in general if the list may be
 * modified concurrently (eg. the lock is dropped in the loop body). An
 * exception to this is if the cursor element (pos) is pinned in the list,
 * and list_safe_reset_next is called after re-taking the lock and before
 * completing the current iteration of the loop body.
 */



/*
 * Double linked lists with a single pointer list head.
 * Mostly useful for hash tables where the two pointer list head is
 * too wasteful.
 * You lose the ability to access the tail in O(1).
 */




static inline __attribute__((no_instrument_function)) void Model0_INIT_HLIST_NODE(struct Model0_hlist_node *Model0_h)
{
 Model0_h->Model0_next = ((void *)0);
 Model0_h->Model0_pprev = ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_hlist_unhashed(const struct Model0_hlist_node *Model0_h)
{
 return !Model0_h->Model0_pprev;
}

static inline __attribute__((no_instrument_function)) int Model0_hlist_empty(const struct Model0_hlist_head *Model0_h)
{
 return !({ union { typeof(Model0_h->Model0_first) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_h->Model0_first), Model0___u.Model0___c, sizeof(Model0_h->Model0_first)); else Model0___read_once_size_nocheck(&(Model0_h->Model0_first), Model0___u.Model0___c, sizeof(Model0_h->Model0_first)); Model0___u.Model0___val; });
}

static inline __attribute__((no_instrument_function)) void Model0___hlist_del(struct Model0_hlist_node *Model0_n)
{
 struct Model0_hlist_node *Model0_next = Model0_n->Model0_next;
 struct Model0_hlist_node **Model0_pprev = Model0_n->Model0_pprev;

 ({ union { typeof(*Model0_pprev) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*Model0_pprev)) (Model0_next) }; Model0___write_once_size(&(*Model0_pprev), Model0___u.Model0___c, sizeof(*Model0_pprev)); Model0___u.Model0___val; });
 if (Model0_next)
  Model0_next->Model0_pprev = Model0_pprev;
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_del(struct Model0_hlist_node *Model0_n)
{
 Model0___hlist_del(Model0_n);
 Model0_n->Model0_next = ((void *) 0x100 + (0xdead000000000000UL));
 Model0_n->Model0_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_del_init(struct Model0_hlist_node *Model0_n)
{
 if (!Model0_hlist_unhashed(Model0_n)) {
  Model0___hlist_del(Model0_n);
  Model0_INIT_HLIST_NODE(Model0_n);
 }
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_add_head(struct Model0_hlist_node *Model0_n, struct Model0_hlist_head *Model0_h)
{
 struct Model0_hlist_node *Model0_first = Model0_h->Model0_first;
 Model0_n->Model0_next = Model0_first;
 if (Model0_first)
  Model0_first->Model0_pprev = &Model0_n->Model0_next;
 ({ union { typeof(Model0_h->Model0_first) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_h->Model0_first)) (Model0_n) }; Model0___write_once_size(&(Model0_h->Model0_first), Model0___u.Model0___c, sizeof(Model0_h->Model0_first)); Model0___u.Model0___val; });
 Model0_n->Model0_pprev = &Model0_h->Model0_first;
}

/* next must be != NULL */
static inline __attribute__((no_instrument_function)) void Model0_hlist_add_before(struct Model0_hlist_node *Model0_n,
     struct Model0_hlist_node *Model0_next)
{
 Model0_n->Model0_pprev = Model0_next->Model0_pprev;
 Model0_n->Model0_next = Model0_next;
 Model0_next->Model0_pprev = &Model0_n->Model0_next;
 ({ union { typeof(*(Model0_n->Model0_pprev)) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*(Model0_n->Model0_pprev))) (Model0_n) }; Model0___write_once_size(&(*(Model0_n->Model0_pprev)), Model0___u.Model0___c, sizeof(*(Model0_n->Model0_pprev))); Model0___u.Model0___val; });
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_add_behind(struct Model0_hlist_node *Model0_n,
        struct Model0_hlist_node *Model0_prev)
{
 Model0_n->Model0_next = Model0_prev->Model0_next;
 ({ union { typeof(Model0_prev->Model0_next) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_prev->Model0_next)) (Model0_n) }; Model0___write_once_size(&(Model0_prev->Model0_next), Model0___u.Model0___c, sizeof(Model0_prev->Model0_next)); Model0___u.Model0___val; });
 Model0_n->Model0_pprev = &Model0_prev->Model0_next;

 if (Model0_n->Model0_next)
  Model0_n->Model0_next->Model0_pprev = &Model0_n->Model0_next;
}

/* after that we'll appear to be on some hlist and hlist_del will work */
static inline __attribute__((no_instrument_function)) void Model0_hlist_add_fake(struct Model0_hlist_node *Model0_n)
{
 Model0_n->Model0_pprev = &Model0_n->Model0_next;
}

static inline __attribute__((no_instrument_function)) bool Model0_hlist_fake(struct Model0_hlist_node *Model0_h)
{
 return Model0_h->Model0_pprev == &Model0_h->Model0_next;
}

/*
 * Check whether the node is the only node of the head without
 * accessing head:
 */
static inline __attribute__((no_instrument_function)) bool
Model0_hlist_is_singular_node(struct Model0_hlist_node *Model0_n, struct Model0_hlist_head *Model0_h)
{
 return !Model0_n->Model0_next && Model0_n->Model0_pprev == &Model0_h->Model0_first;
}

/*
 * Move a list from one list head to another. Fixup the pprev
 * reference of the first entry if it exists.
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_move_list(struct Model0_hlist_head *old,
       struct Model0_hlist_head *Model0_new)
{
 Model0_new->Model0_first = old->Model0_first;
 if (Model0_new->Model0_first)
  Model0_new->Model0_first->Model0_pprev = &Model0_new->Model0_first;
 old->Model0_first = ((void *)0);
}
/**
 * hlist_for_each_entry	- iterate over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */





/**
 * hlist_for_each_entry_continue - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */





/**
 * hlist_for_each_entry_from - iterate over a hlist continuing from current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */




/**
 * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another &struct hlist_node to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */

/*
 * We put the hardirq and softirq counter into the preemption
 * counter. The bitmask has the following meaning:
 *
 * - bits 0-7 are the preemption count (max preemption depth: 256)
 * - bits 8-15 are the softirq count (max # of softirqs: 256)
 *
 * The hardirq count could in theory be the same as the number of
 * interrupts in the system, but we run all interrupt handlers with
 * interrupts disabled, so we cannot have nesting interrupts. Though
 * there are a few palaeontologic drivers which reenable interrupts in
 * the handler, so we need more than one bit here.
 *
 *         PREEMPT_MASK:	0x000000ff
 *         SOFTIRQ_MASK:	0x0000ff00
 *         HARDIRQ_MASK:	0x000f0000
 *             NMI_MASK:	0x00100000
 * PREEMPT_NEED_RESCHED:	0x80000000
 */
/* We use the MSB mostly because its available */


/* preempt_count() and related functions, depends on PREEMPT_NEED_RESCHED */





/*
 * Compared to the generic __my_cpu_offset version, the following
 * saves one instruction and avoids clobbering a temp register.
 */
/*
 * Initialized pointers to per-cpu variables needed for the boot
 * processor need to use these macros to get the proper address
 * offset from __per_cpu_load on SMP.
 *
 * There also must be an entry in vmlinux_64.lds.S
 */
/* For arch-specific code, we can use direct single-insn ops (they
 * don't give an lvalue though). */
extern void Model0___bad_percpu_size(void);
/*
 * Generate a percpu add to memory instruction and optimize code
 * if one is added or subtracted.
 */
/*
 * Add return operation
 */
/*
 * xchg is implemented using cmpxchg without a lock prefix. xchg is
 * expensive due to the implied lock prefix.  The processor cannot prefetch
 * cachelines if xchg is used.
 */
/*
 * cmpxchg has no such implied lock semantics as a result it is much
 * more efficient for cpu local operations.
 */
/*
 * this_cpu_read() makes gcc load the percpu variable every time it is
 * accessed while this_cpu_read_stable() allows the value to be cached.
 * this_cpu_read_stable() is more efficient and can be used if its value
 * is guaranteed to be valid across cpus.  The current users include
 * get_current() and get_thread_info() both of which are actually
 * per-thread variables implemented as per-cpu variables and thus
 * stable for the duration of the respective task.
 */
/*
 * Per cpu atomic 64 bit operations are only available under 64 bit.
 * 32 bit must fall back to generic operations.
 */
/*
 * Pretty complex macro to generate cmpxchg16 instruction.  The instruction
 * is not supported on early AMD64 processors so we must be able to emulate
 * it in software.  The address used in the cmpxchg16 instruction must be
 * aligned to a 16 byte boundary.
 */
/* This is not atomic against other CPUs -- CPU preemption needs to be off */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_x86_this_cpu_constant_test_bit(unsigned int Model0_nr,
                        const unsigned long *Model0_addr)
{
 unsigned long *Model0_a = (unsigned long *)Model0_addr + Model0_nr / 64;


 return ((1UL << (Model0_nr % 64)) & ({ typeof(*Model0_a) Model0_pfo_ret__; switch (sizeof(*Model0_a)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (*Model0_a)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (*Model0_a)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (*Model0_a)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (*Model0_a)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; })) != 0;



}

static inline __attribute__((no_instrument_function)) bool Model0_x86_this_cpu_variable_test_bit(int Model0_nr,
                        const unsigned long *Model0_addr)
{
 bool Model0_oldbit;

 asm volatile("bt ""%%""gs"":" "%" "2"",%1\n\t"
   "\n\tset" "c" " %[_cc_" "c" "]\n"
   : [_cc_c] "=qm" (Model0_oldbit)
   : "m" (*(unsigned long *)Model0_addr), "Ir" (Model0_nr));

 return Model0_oldbit;
}
















/*
 * The default limit for the nr of threads is now in
 * /proc/sys/kernel/threads-max.
 */

/*
 * Maximum supported processors.  Setting this smaller saves quite a
 * bit of memory.  Use nr_cpu_ids instead of this except for static bitmaps.
 */





/* Places which use this should consider cpumask_var_t. */




/*
 * This controls the default maximum pid allocated to a process
 */


/*
 * A maximum of 4 million PIDs should be enough for a while.
 * [NOTE: PID/TIDs are limited to 2^29 ~= 500+ million, see futex.h.]
 */



/*
 * Define a minimum number of pids per cpu.  Heuristically based
 * on original pid max of 32k for 32 cpus.  Also, increase the
 * minimum settable value for pid_max on the running system based
 * on similar defaults.  See kernel/pid.c:pidmap_init() for details.
 */
/*
 * linux/percpu-defs.h - basic definitions for percpu areas
 *
 * DO NOT INCLUDE DIRECTLY OUTSIDE PERCPU IMPLEMENTATION PROPER.
 *
 * This file is separate from linux/percpu.h to avoid cyclic inclusion
 * dependency from arch header files.  Only to be included from
 * asm/percpu.h.
 *
 * This file includes macros necessary to declare percpu sections and
 * variables, and definitions of percpu accessors and operations.  It
 * should provide enough percpu features to arch header files even when
 * they can only include asm/percpu.h to avoid cyclic inclusion dependency.
 */
/*
 * Base implementations of per-CPU variable declarations and definitions, where
 * the section in which the variable is to be placed is provided by the
 * 'sec' argument.  This may be used to affect the parameters governing the
 * variable's storage.
 *
 * NOTE!  The sections for the DECLARE and for the DEFINE must match, lest
 * linkage errors occur due the compiler generating the wrong code to access
 * that section.
 */







/*
 * s390 and alpha modules require percpu variables to be defined as
 * weak to force the compiler to generate GOT based external
 * references for them.  This is necessary because percpu sections
 * will be located outside of the usually addressable area.
 *
 * This definition puts the following two extra restrictions when
 * defining percpu variables.
 *
 * 1. The symbol must be globally unique, even the static ones.
 * 2. Static percpu variables cannot be defined inside a function.
 *
 * Archs which need weak percpu definitions should define
 * ARCH_NEEDS_WEAK_PER_CPU in asm/percpu.h when necessary.
 *
 * To ensure that the generic code observes the above two
 * restrictions, if CONFIG_DEBUG_FORCE_WEAK_PER_CPU is set weak
 * definition is used for all cases.
 */
/*
 * Normal declaration and definition macros.
 */
/*
 * Variant on the per-CPU variable declaration/definition theme used for
 * ordinary per-CPU variables.
 */






/*
 * Declaration/definition used for per-CPU variables that must come first in
 * the set of variables.
 */






/*
 * Declaration/definition used for per-CPU variables that must be cacheline
 * aligned under SMP conditions so that, whilst a particular instance of the
 * data corresponds to a particular CPU, inefficiencies due to direct access by
 * other CPUs are reduced by preventing the data from unnecessarily spanning
 * cachelines.
 *
 * An example of this would be statistical data, where each CPU's set of data
 * is updated by that CPU alone, but the data from across all CPUs is collated
 * by a CPU processing a read from a proc file.
 */
/*
 * Declaration/definition used for per-CPU variables that must be page aligned.
 */
/*
 * Declaration/definition used for per-CPU variables that must be read mostly.
 */






/*
 * Intermodule exports for per-CPU variables.  sparse forgets about
 * address space across EXPORT_SYMBOL(), change EXPORT_SYMBOL() to
 * noop if __CHECKER__.
 */
/*
 * Accessors and operations.
 */


/*
 * __verify_pcpu_ptr() verifies @ptr is a percpu pointer without evaluating
 * @ptr and is invoked once before a percpu area is accessed by all
 * accessors and operations.  This is performed in the generic part of
 * percpu and arch overrides don't need to worry about it; however, if an
 * arch wants to implement an arch-specific percpu accessor or operation,
 * it may use __verify_pcpu_ptr() to verify the parameters.
 *
 * + 0 is required in order to convert the pointer type from a
 * potential array type to a pointer to a single item of the array.
 */
/*
 * Add an offset to a pointer but keep the pointer as-is.  Use RELOC_HIDE()
 * to prevent the compiler from making incorrect assumptions about the
 * pointer value.  The weird cast keeps both GCC and sparse happy.
 */
/*
 * Must be an lvalue. Since @var must be a simple identifier,
 * we force a syntax error here if it isn't.
 */






/*
 * The weird & is necessary because sparse considers (void)(var) to be
 * a direct dereference of percpu variable (var).
 */
/*
 * Branching function to split up a function into a set of functions that
 * are called for different scalar sizes of the objects handled.
 */

extern void Model0___bad_size_call_parameter(void);




static inline __attribute__((no_instrument_function)) void Model0___this_cpu_preempt_check(const char *Model0_op) { }
/*
 * Special handling for cmpxchg_double.  cmpxchg_double is passed two
 * percpu variables.  The first has to be aligned to a double word
 * boundary and the second has to follow directly thereafter.
 * We enforce this on all architectures even if they don't support
 * a double cmpxchg instruction, since it's a cheap requirement, and it
 * avoids breaking the requirement for architectures with the instruction.
 */
/*
 * this_cpu operations (C) 2008-2013 Christoph Lameter <cl@linux.com>
 *
 * Optimized manipulation for memory allocated through the per cpu
 * allocator or for addresses of per cpu variables.
 *
 * These operation guarantee exclusivity of access for other operations
 * on the *same* processor. The assumption is that per cpu data is only
 * accessed by a single processor instance (the current one).
 *
 * The arch code can provide optimized implementation by defining macros
 * for certain scalar sizes. F.e. provide this_cpu_add_2() to provide per
 * cpu atomic operations for 2 byte sized RMW actions. If arch code does
 * not provide operations for a scalar size then the fallback in the
 * generic code will be used.
 *
 * cmpxchg_double replaces two adjacent scalars at once.  The first two
 * parameters are per cpu variables which have to be of the same size.  A
 * truth value is returned to indicate success or failure (since a double
 * register result is difficult to handle).  There is very limited hardware
 * support for these operations, so only certain sizes may work.
 */

/*
 * Operations for contexts where we do not want to do any checks for
 * preemptions.  Unless strictly necessary, always use [__]this_cpu_*()
 * instead.
 *
 * If there is no other protection through preempt disable and/or disabling
 * interupts then one of these RMW operations can show unexpected behavior
 * because the execution thread was rescheduled on another processor or an
 * interrupt occurred and the same percpu variable was modified from the
 * interrupt context.
 */
/*
 * Operations for contexts that are safe from preemption/interrupts.  These
 * operations verify that preemption is disabled.
 */
/*
 * Operations with implied preemption/interrupt protection.  These
 * operations can be used without worrying about preemption or interrupt.
 */



/*
 * per_cpu_offset() is the offset that has to be added to a
 * percpu variable to get to the instance for a certain processor.
 *
 * Most arches use the __per_cpu_offset array for those offsets but
 * some arches have their own ways of determining the offset (x86_64, s390).
 */

extern unsigned long Model0___per_cpu_offset[64];




/*
 * Determine the offset for the currently active processor.
 * An arch may define __my_cpu_offset to provide a more effective
 * means of obtaining the offset to the per cpu variables of the
 * current processor.
 */
/*
 * Arch may define arch_raw_cpu_ptr() to provide more efficient address
 * translations for raw_cpu_ptr().
 */





extern void Model0_setup_per_cpu_areas(void);

/* We can use this directly for local CPU (faster). */
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(unsigned long) Model0_this_cpu_off;





/*
 * Define the "EARLY_PER_CPU" macros.  These are used for some per_cpu
 * variables that are initialized and accessed before there are per_cpu
 * areas allocated.
 */
/* thread_info.h: common low-level thread information accessors
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds
 */







struct Model0_timespec;
struct Model0_compat_timespec;

/*
 * System call restart block.
 */
struct Model0_restart_block {
 long (*Model0_fn)(struct Model0_restart_block *);
 union {
  /* For futex_wait and futex_wait_requeue_pi */
  struct {
   Model0_u32 *Model0_uaddr;
   Model0_u32 Model0_val;
   Model0_u32 Model0_flags;
   Model0_u32 Model0_bitset;
   Model0_u64 Model0_time;
   Model0_u32 *Model0_uaddr2;
  } Model0_futex;
  /* For nanosleep */
  struct {
   Model0_clockid_t Model0_clockid;
   struct Model0_timespec *Model0_rmtp;

   struct Model0_compat_timespec *Model0_compat_rmtp;

   Model0_u64 Model0_expires;
  } Model0_nanosleep;
  /* For poll */
  struct {
   struct Model0_pollfd *Model0_ufds;
   int Model0_nfds;
   int Model0_has_timeout;
   unsigned long Model0_tv_sec;
   unsigned long Model0_tv_nsec;
  } Model0_poll;
 };
};

extern long Model0_do_no_restart_syscall(struct Model0_restart_block *Model0_parm);



/* thread_info.h: low-level thread information
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds and Dave Miller
 */



















/* PAGE_SHIFT determines the page size */
/* Cast *PAGE_MASK to a signed type so that it is sign-extended if
   virtual addresses are 32-bits but physical addresses are larger
   (ie, 32-bit PAE). */







unsigned long Model0_kaslr_get_random_long(const char *Model0_purpose);







static inline __attribute__((no_instrument_function)) void Model0_kernel_randomize_memory(void) { }
/*
 * Set __PAGE_OFFSET to the most negative possible address +
 * PGDIR_SIZE*16 (pgd slot 272).  The gap is to allow a space for a
 * hypervisor to fit.  Choosing 16 slots here is arbitrary, but it's
 * what Xen requires.
 */
/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */



/*
 * Kernel image size is limited to 1GiB due to the fixmap living in the
 * next 1GiB (see level2_kernel_pgt in arch/x86/kernel/head_64.S). Use
 * 512MiB by default, leaving 1.5GiB for modules once the page tables
 * are fully set up. If kernel ASLR is configured, it can extend the
 * kernel page table mapping, reducing the size of the modules area.
 */








extern int Model0_devmem_is_allowed(unsigned long Model0_pagenr);

extern unsigned long Model0_max_low_pfn_mapped;
extern unsigned long Model0_max_pfn_mapped;

static inline __attribute__((no_instrument_function)) Model0_phys_addr_t Model0_get_max_mapped(void)
{
 return (Model0_phys_addr_t)Model0_max_pfn_mapped << 12;
}

bool Model0_pfn_range_is_mapped(unsigned long Model0_start_pfn, unsigned long Model0_end_pfn);

extern unsigned long Model0_init_memory_mapping(unsigned long Model0_start,
      unsigned long Model0_end);

extern void Model0_initmem_init(void);









/* duplicated to the one in bootmem.h */
extern unsigned long Model0_max_pfn;
extern unsigned long Model0_phys_base;

static inline __attribute__((no_instrument_function)) unsigned long Model0___phys_addr_nodebug(unsigned long Model0_x)
{
 unsigned long Model0_y = Model0_x - (0xffffffff80000000UL);

 /* use the carry flag to determine if x was < __START_KERNEL_map */
 Model0_x = Model0_y + ((Model0_x > Model0_y) ? Model0_phys_base : ((0xffffffff80000000UL) - ((unsigned long)(0xffff880000000000UL))));

 return Model0_x;
}
void Model0_clear_page(void *Model0_page);
void Model0_copy_page(void *Model0_to, void *Model0_from);






struct Model0_page;





struct Model0_range {
 Model0_u64 Model0_start;
 Model0_u64 Model0_end;
};

int Model0_add_range(struct Model0_range *Model0_range, int Model0_az, int Model0_nr_range,
  Model0_u64 Model0_start, Model0_u64 Model0_end);


int Model0_add_range_with_merge(struct Model0_range *Model0_range, int Model0_az, int Model0_nr_range,
    Model0_u64 Model0_start, Model0_u64 Model0_end);

void Model0_subtract_range(struct Model0_range *Model0_range, int Model0_az, Model0_u64 Model0_start, Model0_u64 Model0_end);

int Model0_clean_sort_range(struct Model0_range *Model0_range, int Model0_az);

void Model0_sort_range(struct Model0_range *Model0_range, int Model0_nr_range);


static inline __attribute__((no_instrument_function)) Model0_resource_size_t Model0_cap_resource(Model0_u64 Model0_val)
{
 if (Model0_val > ((Model0_resource_size_t)~0))
  return ((Model0_resource_size_t)~0);

 return Model0_val;
}
extern struct Model0_range Model0_pfn_mapped[];
extern int Model0_nr_pfn_mapped;

static inline __attribute__((no_instrument_function)) void Model0_clear_user_page(void *Model0_page, unsigned long Model0_vaddr,
       struct Model0_page *Model0_pg)
{
 Model0_clear_page(Model0_page);
}

static inline __attribute__((no_instrument_function)) void Model0_copy_user_page(void *Model0_to, void *Model0_from, unsigned long Model0_vaddr,
      struct Model0_page *Model0_topage)
{
 Model0_copy_page(Model0_to, Model0_from);
}
/* __pa_symbol should be used for C visible symbols.
   This seems to be the official gcc blessed way to do such arithmetic. */
/*
 * We need __phys_reloc_hide() here because gcc may assume that there is no
 * overflow during __pa() calculation and can optimize it unexpectedly.
 * Newer versions of gcc provide -fno-strict-overflow switch to handle this
 * case properly. Once all supported versions of gcc understand it, we can
 * remove this Voodoo magic stuff. (i.e. once gcc3.x is deprecated)
 */
/*
 * virt_to_page(kaddr) returns a valid pointer if and only if
 * virt_addr_valid(kaddr) returns true.
 */


extern bool Model0___virt_addr_valid(unsigned long Model0_kaddr);














/*
 * pfn_t: encapsulates a page-frame number that is optionally backed
 * by memmap (struct page).  Whether a pfn_t has a 'struct page'
 * backing is indicated by flags in the high bits of the value.
 */
typedef struct {
 Model0_u64 Model0_val;
} Model0_pfn_t;
/*
 * supports 3 memory models.
 */
/* memmap is virtually contiguous.  */
/*
 * Convert a physical address to a Page Frame Number and back
 */








/*
 * Runtime evaluation of get_order()
 */
static inline __attribute__((no_instrument_function)) __attribute__((__const__))
int Model0___get_order(unsigned long Model0_size)
{
 int Model0_order;

 Model0_size--;
 Model0_size >>= 12;



 Model0_order = Model0_fls64(Model0_size);

 return Model0_order;
}

/**
 * get_order - Determine the allocation order of a memory size
 * @size: The size for which to get the order
 *
 * Determine the allocation order of a particular sized block of memory.  This
 * is on a logarithmic scale, where:
 *
 *	0 -> 2^0 * PAGE_SIZE and below
 *	1 -> 2^1 * PAGE_SIZE to 2^0 * PAGE_SIZE + 1
 *	2 -> 2^2 * PAGE_SIZE to 2^1 * PAGE_SIZE + 1
 *	3 -> 2^3 * PAGE_SIZE to 2^2 * PAGE_SIZE + 1
 *	4 -> 2^4 * PAGE_SIZE to 2^3 * PAGE_SIZE + 1
 *	...
 *
 * The order returned is used to find the smallest allocation granule required
 * to hold an object of the specified size.
 *
 * The result is undefined if the size is 0.
 *
 * This function may be used to initialise variables with compile time
 * evaluations of constants.
 */



/*
 * TOP_OF_KERNEL_STACK_PADDING is a number of unused bytes that we
 * reserve at the top of the kernel stack.  We do it because of a nasty
 * 32-bit corner case.  On x86_32, the hardware stack frame is
 * variable-length.  Except for vm86 mode, struct pt_regs assumes a
 * maximum-length frame.  If we enter from CPL 0, the top 8 bytes of
 * pt_regs don't actually exist.  Ordinarily this doesn't matter, but it
 * does in at least one case:
 *
 * If we take an NMI early enough in SYSENTER, then we can end up with
 * pt_regs that extends above sp0.  On the way out, in the espfix code,
 * we can read the saved SS value, but that value will be above sp0.
 * Without this offset, that can result in a page fault.  (We are
 * careful that, in this case, the value we read doesn't matter.)
 *
 * In vm86 mode, the hardware frame is much longer still, so add 16
 * bytes to make room for the real-mode segments.
 *
 * x86_64 has a fixed-length stack frame.
 */
/*
 * low level task data that entry.S needs immediate access to
 * - this struct should fit entirely inside of one cache line
 * - this struct shares the supervisor stack pages
 */

struct Model0_task_struct;












/* Various flags defined: can be included from assembler. */



/*
 * EFLAGS bits
 */
/*
 * Basic CPU control in CR0
 */
/*
 * Paging options in CR3
 */






/*
 * Intel CPU features in CR4
 */
/*
 * x86-64 Task Priority Register, CR8
 */


/*
 * AMD and Transmeta use MSRs for configuration; see <asm/msr-index.h>
 */

/*
 *      NSC/Cyrix CPU configuration register indexes
 */

/* Forward declaration, a strange C thing */
struct Model0_task_struct;
struct Model0_mm_struct;
struct Model0_vm86;














/*
 * Constructor for a conventional segment GDT (or LDT) entry.
 * This is a macro so it can be used in initializers.
 */







/* Simple and small GDT entries for booting only: */
/*
 * Bottom two bits of selector give the ring
 * privilege level
 */


/* User mode is privilege level 3: */


/* Bit 2 is Table Indicator (TI): selects between LDT or GDT */

/* LDT segment has TI set ... */

/* ... GDT has it cleared */
/*
 * We cannot use the same code segment descriptor for user and kernel mode,
 * not even in long flat mode, because of different DPL.
 *
 * GDT layout to get 64-bit SYSCALL/SYSRET support right. SYSRET hardcodes
 * selectors:
 *
 *   if returning to 32-bit userspace: cs = STAR.SYSRET_CS,
 *   if returning to 64-bit userspace: cs = STAR.SYSRET_CS+16,
 *
 * ss = STAR.SYSRET_CS+8 (in either case)
 *
 * thus USER_DS should be between 32-bit and 64-bit code selectors:
 */




/* Needs two entries */

/* Needs two entries */





/* Abused to load per CPU data from limit */


/*
 * Number of entries in the GDT table:
 */


/*
 * Segment selector values corresponding to the above entries:
 *
 * Note, selectors also need to have a correct RPL,
 * expressed with the +3 value for user-space selectors:
 */
/* Bitmask of exception vectors which push an error code on the stack: */
/*
 * early_idt_handler_array is an array of entry points referenced in the
 * early IDT.  For simplicity, it's a real array with one entry point
 * every nine bytes.  That leaves room for an optional 'push $0' if the
 * vector has no error code (two bytes), a 'push $vector_number' (two
 * bytes), and a jump to the common entry code (up to five bytes).
 */




extern const char Model0_early_idt_handler_array[32][9];




/*
 * Load a segment. Fall back on loading the zero segment if something goes
 * wrong.  This variant assumes that loading zero fully clears the segment.
 * This is always the case on Intel CPUs and, even on 64-bit AMD CPUs, any
 * failure to fully clear the cached descriptor is only observable for
 * FS and GS.
 */
static inline __attribute__((no_instrument_function)) void Model0___loadsegment_fs(unsigned short Model0_value)
{
 asm volatile("						\n"
       "1:	movw %0, %%fs			\n"
       "2:					\n"

       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "2b" ") - .\n" " .long (" "ex_handler_clear_fs" ") - .\n" " .popsection\n"

       : : "rm" (Model0_value) : "memory");
}

/* __loadsegment_gs is intentionally undefined.  Use load_gs_index instead. */





/*
 * Save a segment register away:
 */



/*
 * x86-32 user GS accessors:
 */





/* top of stack page */




/* Arbitrarily choose the same ptrace numbers as used by the Sparc code. */
/* only useful for access 32bit programs / kernels */
struct Model0_pt_regs {
/*
 * C ABI says these regs are callee-preserved. They aren't saved on kernel entry
 * unless syscall needs a complete, fully filled "struct pt_regs".
 */
 unsigned long Model0_r15;
 unsigned long Model0_r14;
 unsigned long Model0_r13;
 unsigned long Model0_r12;
 unsigned long Model0_bp;
 unsigned long Model0_bx;
/* These regs are callee-clobbered. Always saved on kernel entry. */
 unsigned long Model0_r11;
 unsigned long Model0_r10;
 unsigned long Model0_r9;
 unsigned long Model0_r8;
 unsigned long Model0_ax;
 unsigned long Model0_cx;
 unsigned long Model0_dx;
 unsigned long Model0_si;
 unsigned long Model0_di;
/*
 * On syscall entry, this is syscall#. On CPU exception, this is error code.
 * On hw interrupt, it's IRQ number:
 */
 unsigned long Model0_orig_ax;
/* Return frame for iretq */
 unsigned long Model0_ip;
 unsigned long Model0_cs;
 unsigned long Model0_flags;
 unsigned long Model0_sp;
 unsigned long Model0_ss;
/* top of stack page */
};







struct Model0_cpuinfo_x86;
struct Model0_task_struct;

extern unsigned long Model0_profile_pc(struct Model0_pt_regs *Model0_regs);


extern unsigned long
Model0_convert_ip_to_linear(struct Model0_task_struct *Model0_child, struct Model0_pt_regs *Model0_regs);
extern void Model0_send_sigtrap(struct Model0_task_struct *Model0_tsk, struct Model0_pt_regs *Model0_regs,
    int Model0_error_code, int Model0_si_code);


static inline __attribute__((no_instrument_function)) unsigned long Model0_regs_return_value(struct Model0_pt_regs *Model0_regs)
{
 return Model0_regs->Model0_ax;
}

/*
 * user_mode(regs) determines whether a register set came from user
 * mode.  On x86_32, this is true if V8086 mode was enabled OR if the
 * register set was from protected mode with RPL-3 CS value.  This
 * tricky test checks that with one comparison.
 *
 * On x86_64, vm86 mode is mercifully nonexistent, and we don't need
 * the extra check.
 */
static inline __attribute__((no_instrument_function)) int Model0_user_mode(struct Model0_pt_regs *Model0_regs)
{



 return !!(Model0_regs->Model0_cs & 3);

}

static inline __attribute__((no_instrument_function)) int Model0_v8086_mode(struct Model0_pt_regs *Model0_regs)
{



 return 0; /* No V86 mode support in long mode */

}


static inline __attribute__((no_instrument_function)) bool Model0_user_64bit_mode(struct Model0_pt_regs *Model0_regs)
{

 /*
	 * On non-paravirt systems, this is the only long mode CPL 3
	 * selector.  We do not allow long mode selectors in the LDT.
	 */
 return Model0_regs->Model0_cs == (6*8 + 3);




}
static inline __attribute__((no_instrument_function)) unsigned long Model0_kernel_stack_pointer(struct Model0_pt_regs *Model0_regs)
{
 return Model0_regs->Model0_sp;
}







/*
 * Common low level (register) ptrace helpers
 *
 * Copyright 2004-2011 Analog Devices Inc.
 *
 * Licensed under the GPL-2 or later.
 */






/* Helpers for working with the instruction pointer */







static inline __attribute__((no_instrument_function)) unsigned long Model0_instruction_pointer(struct Model0_pt_regs *Model0_regs)
{
 return ((Model0_regs)->Model0_ip);
}
static inline __attribute__((no_instrument_function)) void Model0_instruction_pointer_set(struct Model0_pt_regs *Model0_regs,
                                           unsigned long Model0_val)
{
 (((Model0_regs)->Model0_ip) = (Model0_val));
}





/* Helpers for working with the user stack pointer */







static inline __attribute__((no_instrument_function)) unsigned long Model0_user_stack_pointer(struct Model0_pt_regs *Model0_regs)
{
 return ((Model0_regs)->Model0_sp);
}
static inline __attribute__((no_instrument_function)) void Model0_user_stack_pointer_set(struct Model0_pt_regs *Model0_regs,
                                          unsigned long Model0_val)
{
 (((Model0_regs)->Model0_sp) = (Model0_val));
}

/* Helpers for working with the frame pointer */







static inline __attribute__((no_instrument_function)) unsigned long Model0_frame_pointer(struct Model0_pt_regs *Model0_regs)
{
 return ((Model0_regs)->Model0_bp);
}
static inline __attribute__((no_instrument_function)) void Model0_frame_pointer_set(struct Model0_pt_regs *Model0_regs,
                                     unsigned long Model0_val)
{
 (((Model0_regs)->Model0_bp) = (Model0_val));
}

/* Query offset/name of register from its name/offset */
extern int Model0_regs_query_register_offset(const char *Model0_name);
extern const char *Model0_regs_query_register_name(unsigned int Model0_offset);


/**
 * regs_get_register() - get register value from its offset
 * @regs:	pt_regs from which register value is gotten.
 * @offset:	offset number of the register.
 *
 * regs_get_register returns the value of a register. The @offset is the
 * offset of the register in struct pt_regs address which specified by @regs.
 * If @offset is bigger than MAX_REG_OFFSET, this returns 0.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_regs_get_register(struct Model0_pt_regs *Model0_regs,
           unsigned int Model0_offset)
{
 if (__builtin_expect(!!(Model0_offset > (__builtin_offsetof(struct Model0_pt_regs, Model0_ss))), 0))
  return 0;
 return *(unsigned long *)((unsigned long)Model0_regs + Model0_offset);
}

/**
 * regs_within_kernel_stack() - check the address in the stack
 * @regs:	pt_regs which contains kernel stack pointer.
 * @addr:	address which is checked.
 *
 * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).
 * If @addr is within the kernel stack, it returns true. If not, returns false.
 */
static inline __attribute__((no_instrument_function)) int Model0_regs_within_kernel_stack(struct Model0_pt_regs *Model0_regs,
        unsigned long Model0_addr)
{
 return ((Model0_addr & ~((((1UL) << 12) << (2 + 0)) - 1)) ==
  (Model0_kernel_stack_pointer(Model0_regs) & ~((((1UL) << 12) << (2 + 0)) - 1)));
}

/**
 * regs_get_kernel_stack_nth() - get Nth entry of the stack
 * @regs:	pt_regs which contains kernel stack pointer.
 * @n:		stack entry number.
 *
 * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which
 * is specified by @regs. If the @n th entry is NOT in the kernel stack,
 * this returns 0.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_regs_get_kernel_stack_nth(struct Model0_pt_regs *Model0_regs,
            unsigned int Model0_n)
{
 unsigned long *Model0_addr = (unsigned long *)Model0_kernel_stack_pointer(Model0_regs);
 Model0_addr += Model0_n;
 if (Model0_regs_within_kernel_stack(Model0_regs, (unsigned long)Model0_addr))
  return *Model0_addr;
 else
  return 0;
}
/*
 * When hitting ptrace_stop(), we cannot return using SYSRET because
 * that does not restore the full CPU state, only a minimal set.  The
 * ptracer can change arbitrary register values, which is usually okay
 * because the usual ptrace stops run off the signal delivery path which
 * forces IRET; however, ptrace_event() stops happen in arbitrary places
 * in the kernel and don't force IRET path.
 *
 * So force IRET path after a ptrace stop.
 */






struct Model0_user_desc;
extern int Model0_do_get_thread_area(struct Model0_task_struct *Model0_p, int Model0_idx,
         struct Model0_user_desc *Model0_info);
extern int Model0_do_set_thread_area(struct Model0_task_struct *Model0_p, int Model0_idx,
         struct Model0_user_desc *Model0_info, int Model0_can_allocate);

/* This structure matches the layout of the data saved to the stack
   following a device-not-present interrupt, part of it saved
   automatically by the 80386/80486.
   */
struct Model0_math_emu_info {
 long Model0____orig_eip;
 struct Model0_pt_regs *Model0_regs;
};





/*
 * Linux signal context definitions. The sigcontext includes a complex
 * hierarchy of CPU and FPU state, available to user-space (on the stack) when
 * a signal handler is executed.
 *
 * As over the years this ABI grew from its very simple roots towards
 * supporting more and more CPU state organically, some of the details (which
 * were rather clever hacks back in the days) became a bit quirky by today.
 *
 * The current ABI includes flexible provisions for future extensions, so we
 * won't have to grow new quirks for quite some time. Promise!
 */
/*
 * Bytes 464..511 in the current 512-byte layout of the FXSAVE/FXRSTOR frame
 * are reserved for SW usage. On CPUs supporting XSAVE/XRSTOR, these bytes are
 * used to extend the fpstate pointer in the sigcontext, which now includes the
 * extended state information along with fpstate information.
 *
 * If sw_reserved.magic1 == FP_XSTATE_MAGIC1 then there's a
 * sw_reserved.extended_size bytes large extended context area present. (The
 * last 32-bit word of this extended area (at the
 * fpstate+extended_size-FP_XSTATE_MAGIC2_SIZE address) is set to
 * FP_XSTATE_MAGIC2 so that you can sanity check your size calculations.)
 *
 * This extended area typically grows with newer CPUs that have larger and
 * larger XSAVE areas.
 */
struct Model0__fpx_sw_bytes {
 /*
	 * If set to FP_XSTATE_MAGIC1 then this is an xstate context.
	 * 0 if a legacy frame.
	 */
 __u32 Model0_magic1;

 /*
	 * Total size of the fpstate area:
	 *
	 *  - if magic1 == 0 then it's sizeof(struct _fpstate)
	 *  - if magic1 == FP_XSTATE_MAGIC1 then it's sizeof(struct _xstate)
	 *    plus extensions (if any)
	 */
 __u32 Model0_extended_size;

 /*
	 * Feature bit mask (including FP/SSE/extended state) that is present
	 * in the memory layout:
	 */
 __u64 Model0_xfeatures;

 /*
	 * Actual XSAVE state size, based on the xfeatures saved in the layout.
	 * 'extended_size' is greater than 'xstate_size':
	 */
 __u32 Model0_xstate_size;

 /* For future use: */
 __u32 Model0_padding[7];
};

/*
 * As documented in the iBCS2 standard:
 *
 * The first part of "struct _fpstate" is just the normal i387 hardware setup,
 * the extra "status" word is used to save the coprocessor status word before
 * entering the handler.
 *
 * The FPU state data structure has had to grow to accommodate the extended FPU
 * state required by the Streaming SIMD Extensions.  There is no documented
 * standard to accomplish this at the moment.
 */

/* 10-byte legacy floating point register: */
struct Model0__fpreg {
 Model0___u16 Model0_significand[4];
 Model0___u16 Model0_exponent;
};

/* 16-byte floating point register: */
struct Model0__fpxreg {
 Model0___u16 Model0_significand[4];
 Model0___u16 Model0_exponent;
 Model0___u16 Model0_padding[3];
};

/* 16-byte XMM register: */
struct Model0__xmmreg {
 __u32 Model0_element[4];
};



/*
 * The 32-bit FPU frame:
 */
struct Model0__fpstate_32 {
 /* Legacy FPU environment: */
 __u32 Model0_cw;
 __u32 Model0_sw;
 __u32 Model0_tag;
 __u32 Model0_ipoff;
 __u32 Model0_cssel;
 __u32 Model0_dataoff;
 __u32 Model0_datasel;
 struct Model0__fpreg Model0__st[8];
 Model0___u16 Model0_status;
 Model0___u16 Model0_magic; /* 0xffff: regular FPU data only */
       /* 0x0000: FXSR FPU data */

 /* FXSR FPU environment */
 __u32 Model0__fxsr_env[6]; /* FXSR FPU env is ignored */
 __u32 Model0_mxcsr;
 __u32 Model0_reserved;
 struct Model0__fpxreg Model0__fxsr_st[8]; /* FXSR FPU reg data is ignored */
 struct Model0__xmmreg Model0__xmm[8]; /* First 8 XMM registers */
 union {
  __u32 Model0_padding1[44]; /* Second 8 XMM registers plus padding */
  __u32 Model0_padding[44]; /* Alias name for old user-space */
 };

 union {
  __u32 Model0_padding2[12];
  struct Model0__fpx_sw_bytes Model0_sw_reserved; /* Potential extended state is encoded here */
 };
};

/*
 * The 64-bit FPU frame. (FXSAVE format and later)
 *
 * Note1: If sw_reserved.magic1 == FP_XSTATE_MAGIC1 then the structure is
 *        larger: 'struct _xstate'. Note that 'struct _xstate' embedds
 *        'struct _fpstate' so that you can always assume the _fpstate portion
 *        exists so that you can check the magic value.
 *
 * Note2: Reserved fields may someday contain valuable data. Always
 *	  save/restore them when you change signal frames.
 */
struct Model0__fpstate_64 {
 Model0___u16 Model0_cwd;
 Model0___u16 Model0_swd;
 /* Note this is not the same as the 32-bit/x87/FSAVE twd: */
 Model0___u16 Model0_twd;
 Model0___u16 Model0_fop;
 __u64 Model0_rip;
 __u64 Model0_rdp;
 __u32 Model0_mxcsr;
 __u32 Model0_mxcsr_mask;
 __u32 Model0_st_space[32]; /*  8x  FP registers, 16 bytes each */
 __u32 Model0_xmm_space[64]; /* 16x XMM registers, 16 bytes each */
 __u32 Model0_reserved2[12];
 union {
  __u32 Model0_reserved3[12];
  struct Model0__fpx_sw_bytes Model0_sw_reserved; /* Potential extended state is encoded here */
 };
};







struct Model0__header {
 __u64 Model0_xfeatures;
 __u64 Model0_reserved1[2];
 __u64 Model0_reserved2[5];
};

struct Model0__ymmh_state {
 /* 16x YMM registers, 16 bytes each: */
 __u32 Model0_ymmh_space[64];
};

/*
 * Extended state pointed to by sigcontext::fpstate.
 *
 * In addition to the fpstate, information encoded in _xstate::xstate_hdr
 * indicates the presence of other extended state information supported
 * by the CPU and kernel:
 */
struct Model0__xstate {
 struct Model0__fpstate_64 Model0_fpstate;
 struct Model0__header Model0_xstate_hdr;
 struct Model0__ymmh_state Model0_ymmh;
 /* New processor state extensions go here: */
};

/*
 * The 32-bit signal frame:
 */
struct Model0_sigcontext_32 {
 Model0___u16 Model0_gs, Model0___gsh;
 Model0___u16 Model0_fs, Model0___fsh;
 Model0___u16 Model0_es, Model0___esh;
 Model0___u16 Model0_ds, Model0___dsh;
 __u32 Model0_di;
 __u32 Model0_si;
 __u32 Model0_bp;
 __u32 Model0_sp;
 __u32 Model0_bx;
 __u32 Model0_dx;
 __u32 Model0_cx;
 __u32 Model0_ax;
 __u32 Model0_trapno;
 __u32 err;
 __u32 Model0_ip;
 Model0___u16 Model0_cs, Model0___csh;
 __u32 Model0_flags;
 __u32 Model0_sp_at_signal;
 Model0___u16 Model0_ss, Model0___ssh;

 /*
	 * fpstate is really (struct _fpstate *) or (struct _xstate *)
	 * depending on the FP_XSTATE_MAGIC1 encoded in the SW reserved
	 * bytes of (struct _fpstate) and FP_XSTATE_MAGIC2 present at the end
	 * of extended memory layout. See comments at the definition of
	 * (struct _fpx_sw_bytes)
	 */
 __u32 Model0_fpstate; /* Zero when no FPU/extended context */
 __u32 Model0_oldmask;
 __u32 Model0_cr2;
};

/*
 * The 64-bit signal frame:
 */
struct Model0_sigcontext_64 {
 __u64 Model0_r8;
 __u64 Model0_r9;
 __u64 Model0_r10;
 __u64 Model0_r11;
 __u64 Model0_r12;
 __u64 Model0_r13;
 __u64 Model0_r14;
 __u64 Model0_r15;
 __u64 Model0_di;
 __u64 Model0_si;
 __u64 Model0_bp;
 __u64 Model0_bx;
 __u64 Model0_dx;
 __u64 Model0_ax;
 __u64 Model0_cx;
 __u64 Model0_sp;
 __u64 Model0_ip;
 __u64 Model0_flags;
 Model0___u16 Model0_cs;
 Model0___u16 Model0_gs;
 Model0___u16 Model0_fs;
 Model0___u16 Model0_ss;
 __u64 err;
 __u64 Model0_trapno;
 __u64 Model0_oldmask;
 __u64 Model0_cr2;

 /*
	 * fpstate is really (struct _fpstate *) or (struct _xstate *)
	 * depending on the FP_XSTATE_MAGIC1 encoded in the SW reserved
	 * bytes of (struct _fpstate) and FP_XSTATE_MAGIC2 present at the end
	 * of extended memory layout. See comments at the definition of
	 * (struct _fpx_sw_bytes)
	 */
 __u64 Model0_fpstate; /* Zero when no FPU/extended context */
 __u64 Model0_reserved1[8];
};

/*
 * Create the real 'struct sigcontext' type:
 */
/*
 * The old user-space sigcontext definition, just in case user-space still
 * relies on it. The kernel definition (in asm/sigcontext.h) has unified
 * field names but otherwise the same layout.
 */







struct Model0_task_struct;

extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model0_task_struct *) Model0_current_task;

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model0_task_struct *Model0_get_current(void)
{
 return ({ typeof(Model0_current_task) Model0_pfo_ret__; switch (sizeof(Model0_current_task)) { case 1: asm("mov" "b ""%%""gs"":" "%" "P1"",%0" : "=q" (Model0_pfo_ret__) : "p" (&(Model0_current_task))); break; case 2: asm("mov" "w ""%%""gs"":" "%" "P1"",%0" : "=r" (Model0_pfo_ret__) : "p" (&(Model0_current_task))); break; case 4: asm("mov" "l ""%%""gs"":" "%" "P1"",%0" : "=r" (Model0_pfo_ret__) : "p" (&(Model0_current_task))); break; case 8: asm("mov" "q ""%%""gs"":" "%" "P1"",%0" : "=r" (Model0_pfo_ret__) : "p" (&(Model0_current_task))); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; });
}


/* If _PAGE_BIT_PRESENT is clear, we use these: */
/* - if the user mapped it with PROT_NONE; pte_present gives true */
/*
 * The same hidden bit is used by kmemcheck, but since kmemcheck
 * works on kernel pages while soft-dirty engine on user space,
 * they do not conflict with each other.
 */







/*
 * Tracking soft dirty bit when a page goes to a swap is tricky.
 * We need a bit which can be stored in pte _and_ not conflict
 * with swap entry format. On x86 bits 6 and 7 are *not* involved
 * into swap entry computation, but bit 6 is used for nonlinear
 * file mapping, so we borrow bit 7 for soft dirty tracking.
 *
 * Please note that this bit must be treated as swap dirty page
 * mark if and only if the PTE has present bit clear!
 */
/*
 * Set of bits not changed in pte_modify.  The pte's
 * protection key is treated like _PAGE_RW, for
 * instance, and is *not* included in this mask since
 * pte_modify() does modify it.
 */





/*
 * The cache modes defined here are used to translate between pure SW usage
 * and the HW defined cache mode bits and/or PAT entries.
 *
 * The resulting bits for PWT, PCD and PAT should be chosen in a way
 * to have the WB mode at index 0 (all bits clear). This is the default
 * right now and likely would break too much if changed.
 */

enum Model0_page_cache_mode {
 Model0__PAGE_CACHE_MODE_WB = 0,
 Model0__PAGE_CACHE_MODE_WC = 1,
 Model0__PAGE_CACHE_MODE_UC_MINUS = 2,
 Model0__PAGE_CACHE_MODE_UC = 3,
 Model0__PAGE_CACHE_MODE_WT = 4,
 Model0__PAGE_CACHE_MODE_WP = 5,
 Model0__PAGE_CACHE_MODE_NUM = 8
};
/*         xwr */
/*
 * early identity mapping  pte attrib macros.
 */







/*
 * generic non-linear memory support:
 *
 * 1) we will not split memory into more chunks than will fit into the flags
 *    field of the struct page
 *
 * SECTION_SIZE_BITS		2^n: size of each section
 * MAX_PHYSADDR_BITS		2^n: max size of physical address space
 * MAX_PHYSMEM_BITS		2^n: how much memory we can have in that space
 *
 */





/*
 * These are used to make use of C type-checking..
 */
typedef unsigned long Model0_pteval_t;
typedef unsigned long Model0_pmdval_t;
typedef unsigned long Model0_pudval_t;
typedef unsigned long Model0_pgdval_t;
typedef unsigned long Model0_pgprotval_t;

typedef struct { Model0_pteval_t Model0_pte; } Model0_pte_t;





/*
 * PGDIR_SHIFT determines what a top-level page table entry can map
 */



/*
 * 3rd level page
 */



/*
 * PMD_SHIFT determines the size of the area a middle-level
 * page table can map
 */



/*
 * entries per page directory level
 */
/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */






/* Extracts the PFN from a (pte|pmd|pud|pgd)val_t of a 4KB page */


/*
 *  Extracts the flags from a (pte|pmd|pud|pgd)val_t
 *  This includes the protection key value.
 */


typedef struct Model0_pgprot { Model0_pgprotval_t Model0_pgprot; } Model0_pgprot_t;

typedef struct { Model0_pgdval_t Model0_pgd; } Model0_pgd_t;

static inline __attribute__((no_instrument_function)) Model0_pgd_t Model0_native_make_pgd(Model0_pgdval_t Model0_val)
{
 return (Model0_pgd_t) { Model0_val };
}

static inline __attribute__((no_instrument_function)) Model0_pgdval_t Model0_native_pgd_val(Model0_pgd_t Model0_pgd)
{
 return Model0_pgd.Model0_pgd;
}

static inline __attribute__((no_instrument_function)) Model0_pgdval_t Model0_pgd_flags(Model0_pgd_t Model0_pgd)
{
 return Model0_native_pgd_val(Model0_pgd) & (~((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)))));
}


typedef struct { Model0_pudval_t Model0_pud; } Model0_pud_t;

static inline __attribute__((no_instrument_function)) Model0_pud_t Model0_native_make_pud(Model0_pmdval_t Model0_val)
{
 return (Model0_pud_t) { Model0_val };
}

static inline __attribute__((no_instrument_function)) Model0_pudval_t Model0_native_pud_val(Model0_pud_t Model0_pud)
{
 return Model0_pud.Model0_pud;
}
typedef struct { Model0_pmdval_t Model0_pmd; } Model0_pmd_t;

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_native_make_pmd(Model0_pmdval_t Model0_val)
{
 return (Model0_pmd_t) { Model0_val };
}

static inline __attribute__((no_instrument_function)) Model0_pmdval_t Model0_native_pmd_val(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd.Model0_pmd;
}
static inline __attribute__((no_instrument_function)) Model0_pudval_t Model0_pud_pfn_mask(Model0_pud_t Model0_pud)
{
 if (Model0_native_pud_val(Model0_pud) & (((Model0_pteval_t)(1)) << 7))
  return (((signed long)(~(((1UL) << 30)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)));
 else
  return ((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1))));
}

static inline __attribute__((no_instrument_function)) Model0_pudval_t Model0_pud_flags_mask(Model0_pud_t Model0_pud)
{
 return ~Model0_pud_pfn_mask(Model0_pud);
}

static inline __attribute__((no_instrument_function)) Model0_pudval_t Model0_pud_flags(Model0_pud_t Model0_pud)
{
 return Model0_native_pud_val(Model0_pud) & Model0_pud_flags_mask(Model0_pud);
}

static inline __attribute__((no_instrument_function)) Model0_pmdval_t Model0_pmd_pfn_mask(Model0_pmd_t Model0_pmd)
{
 if (Model0_native_pmd_val(Model0_pmd) & (((Model0_pteval_t)(1)) << 7))
  return (((signed long)(~(((1UL) << 21)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)));
 else
  return ((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1))));
}

static inline __attribute__((no_instrument_function)) Model0_pmdval_t Model0_pmd_flags_mask(Model0_pmd_t Model0_pmd)
{
 return ~Model0_pmd_pfn_mask(Model0_pmd);
}

static inline __attribute__((no_instrument_function)) Model0_pmdval_t Model0_pmd_flags(Model0_pmd_t Model0_pmd)
{
 return Model0_native_pmd_val(Model0_pmd) & Model0_pmd_flags_mask(Model0_pmd);
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_native_make_pte(Model0_pteval_t Model0_val)
{
 return (Model0_pte_t) { .Model0_pte = Model0_val };
}

static inline __attribute__((no_instrument_function)) Model0_pteval_t Model0_native_pte_val(Model0_pte_t Model0_pte)
{
 return Model0_pte.Model0_pte;
}

static inline __attribute__((no_instrument_function)) Model0_pteval_t Model0_pte_flags(Model0_pte_t Model0_pte)
{
 return Model0_native_pte_val(Model0_pte) & (~((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)))));
}




extern Model0_uint16_t Model0___cachemode2pte_tbl[Model0__PAGE_CACHE_MODE_NUM];
extern Model0_uint8_t Model0___pte2cachemode_tbl[8];
static inline __attribute__((no_instrument_function)) unsigned long Model0_cachemode2protval(enum Model0_page_cache_mode Model0_pcm)
{
 if (__builtin_expect(!!(Model0_pcm == 0), 1))
  return 0;
 return Model0___cachemode2pte_tbl[Model0_pcm];
}
static inline __attribute__((no_instrument_function)) Model0_pgprot_t Model0_cachemode2pgprot(enum Model0_page_cache_mode Model0_pcm)
{
 return ((Model0_pgprot_t) { (Model0_cachemode2protval(Model0_pcm)) } );
}
static inline __attribute__((no_instrument_function)) enum Model0_page_cache_mode Model0_pgprot2cachemode(Model0_pgprot_t Model0_pgprot)
{
 unsigned long Model0_masked;

 Model0_masked = ((Model0_pgprot).Model0_pgprot) & ((((Model0_pteval_t)(1)) << 7) | (((Model0_pteval_t)(1)) << 4) | (((Model0_pteval_t)(1)) << 3));
 if (__builtin_expect(!!(Model0_masked == 0), 1))
  return 0;
 return Model0___pte2cachemode_tbl[((((Model0_masked) >> (7 - 2)) & 4) | (((Model0_masked) >> (4 - 1)) & 2) | (((Model0_masked) >> 3) & 1))];
}
static inline __attribute__((no_instrument_function)) Model0_pgprot_t Model0_pgprot_4k_2_large(Model0_pgprot_t Model0_pgprot)
{
 Model0_pgprotval_t Model0_val = ((Model0_pgprot).Model0_pgprot);
 Model0_pgprot_t Model0_new;

 ((Model0_new).Model0_pgprot) = (Model0_val & ~((((Model0_pteval_t)(1)) << 7) | (((Model0_pteval_t)(1)) << 12))) |
  ((Model0_val & (((Model0_pteval_t)(1)) << 7)) << (12 - 7));
 return Model0_new;
}
static inline __attribute__((no_instrument_function)) Model0_pgprot_t Model0_pgprot_large_2_4k(Model0_pgprot_t Model0_pgprot)
{
 Model0_pgprotval_t Model0_val = ((Model0_pgprot).Model0_pgprot);
 Model0_pgprot_t Model0_new;

 ((Model0_new).Model0_pgprot) = (Model0_val & ~((((Model0_pteval_t)(1)) << 7) | (((Model0_pteval_t)(1)) << 12))) |
     ((Model0_val & (((Model0_pteval_t)(1)) << 12)) >>
      (12 - 7));
 return Model0_new;
}


typedef struct Model0_page *Model0_pgtable_t;

extern Model0_pteval_t Model0___supported_pte_mask;
extern void Model0_set_nx(void);
extern int Model0_nx_enabled;


extern Model0_pgprot_t Model0_pgprot_writecombine(Model0_pgprot_t Model0_prot);


extern Model0_pgprot_t Model0_pgprot_writethrough(Model0_pgprot_t Model0_prot);

/* Indicate that x86 has its own track and untrack pfn vma functions */



struct Model0_file;
Model0_pgprot_t Model0_phys_mem_access_prot(struct Model0_file *Model0_file, unsigned long Model0_pfn,
                              unsigned long Model0_size, Model0_pgprot_t Model0_vma_prot);
int Model0_phys_mem_access_prot_allowed(struct Model0_file *Model0_file, unsigned long Model0_pfn,
                              unsigned long Model0_size, Model0_pgprot_t *Model0_vma_prot);

/* Install a pte for a particular vaddr in kernel space. */
void Model0_set_pte_vaddr(unsigned long Model0_vaddr, Model0_pte_t Model0_pte);







struct Model0_seq_file;
extern void Model0_arch_report_meminfo(struct Model0_seq_file *Model0_m);

enum Model0_pg_level {
 Model0_PG_LEVEL_NONE,
 Model0_PG_LEVEL_4K,
 Model0_PG_LEVEL_2M,
 Model0_PG_LEVEL_1G,
 Model0_PG_LEVEL_NUM
};


extern void Model0_update_page_count(int Model0_level, unsigned long Model0_pages);




/*
 * Helper function that returns the kernel pagetable entry controlling
 * the virtual address 'address'. NULL means no pagetable entry present.
 * NOTE: the return type is pte_t but if the pmd is PSE then we return it
 * as a pte too.
 */
extern Model0_pte_t *Model0_lookup_address(unsigned long Model0_address, unsigned int *Model0_level);
extern Model0_pte_t *Model0_lookup_address_in_pgd(Model0_pgd_t *Model0_pgd, unsigned long Model0_address,
        unsigned int *Model0_level);
extern Model0_pmd_t *Model0_lookup_pmd_address(unsigned long Model0_address);
extern Model0_phys_addr_t Model0_slow_virt_to_phys(void *Model0___address);
extern int Model0_kernel_map_pages_in_pgd(Model0_pgd_t *Model0_pgd, Model0_u64 Model0_pfn, unsigned long Model0_address,
       unsigned Model0_numpages, unsigned long Model0_page_flags);







/*
 * CPU model specific register (MSR) numbers.
 *
 * Do not add new entries to this file unless the definitions are shared
 * between multiple compilation units.
 */

/* x86-64 specific MSRs */
/* EFER bits: */
/* Intel MSRs. Some also available on other CPUs */
/* DEBUGCTLMSR bits (others vary by model): */
/* C-state Residency Counters */
/* Interrupt Response Limit */







/* Run Time Average Power Limiting (RAPL) Interface */
/* Config TDP MSRs */
/* Hardware P state interface */
/* CPUID.6.EAX */






/* IA32_HWP_CAPABILITIES */





/* IA32_HWP_REQUEST */







/* IA32_HWP_STATUS */



/* IA32_HWP_INTERRUPT */
/* These are consecutive and not in the normal 4er MCE bank block */
/* Alternative perfctr range with full access. */


/* AMD64 MSRs. Not complete. See the architecture manual for a more
   complete list. */
/* Fam 17h MSRs */


/* Fam 16h MSRs */







/* Fam 15h MSRs */







/* Fam 10h MSRs */
/* K8 MSRs */




/* C1E active bits in int pending message */







/* K7 MSRs */
/* K6 MSRs */






/* Centaur-Hauls/IDT defined MSRs. */
/* VIA Cyrix defined MSRs*/





/* Transmeta defined MSRs */





/* Intel defined MSRs. */
/* Thermal Thresholds Support */
/* MISC_ENABLE bits: architectural */
/* MISC_ENABLE bits: model-specific, meaning may vary from core to core */
/* P4/Xeon+ specific */
/* Pentium IV performance counter MSRs */
/* Intel Core-based CPU performance counters */
/* Geode defined MSRs */


/* Intel VT MSRs */
/* VMX_BASIC bits and bitmasks */
/* MSR_IA32_VMX_MISC bits */


/* AMD-V MSRs */










/*
 * Cpumasks provide a bitmap suitable for representing the
 * set of CPU's in a system, one bit position per CPU number.  In general,
 * only nr_cpu_ids (<= NR_CPUS) bits are valid.
 */





















/* We don't want strings.h stuff being used by user stuff by accident */

extern char *Model0_strndup_user(const char *, long);
extern void *Model0_memdup_user(const void *, Model0_size_t);
extern void *Model0_memdup_user_nul(const void *, Model0_size_t);

/*
 * Include machine specific inline routines
 */









/* Written 2002 by Andi Kleen */

/* Only used for special circumstances. Stolen from i386/string.h */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model0___inline_memcpy(void *Model0_to, const void *Model0_from, Model0_size_t Model0_n)
{
 unsigned long Model0_d0, Model0_d1, Model0_d2;
 asm volatile("rep ; movsl\n\t"
       "testb $2,%b4\n\t"
       "je 1f\n\t"
       "movsw\n"
       "1:\ttestb $1,%b4\n\t"
       "je 2f\n\t"
       "movsb\n"
       "2:"
       : "=&c" (Model0_d0), "=&D" (Model0_d1), "=&S" (Model0_d2)
       : "0" (Model0_n / 4), "q" (Model0_n), "1" ((long)Model0_to), "2" ((long)Model0_from)
       : "memory");
 return Model0_to;
}

/* Even with __builtin_ the compiler may decide to use the out of line
   function. */


extern void *memcpy(void *Model0_to, const void *Model0_from, Model0_size_t Model0_len);
extern void *Model0___memcpy(void *Model0_to, const void *Model0_from, Model0_size_t Model0_len);
void *memset(void *Model0_s, int Model0_c, Model0_size_t Model0_n);
void *Model0___memset(void *Model0_s, int Model0_c, Model0_size_t Model0_n);


void *Model0_memmove(void *Model0_dest, const void *Model0_src, Model0_size_t Model0_count);
void *Model0___memmove(void *Model0_dest, const void *Model0_src, Model0_size_t Model0_count);

int Model0_memcmp(const void *Model0_cs, const void *Model0_ct, Model0_size_t Model0_count);
Model0_size_t Model0_strlen(const char *Model0_s);
char *Model0_strcpy(char *Model0_dest, const char *Model0_src);
char *Model0_strcat(char *Model0_dest, const char *Model0_src);
int Model0_strcmp(const char *Model0_cs, const char *Model0_ct);
/**
 * memcpy_mcsafe - copy memory with indication if a machine check happened
 *
 * @dst:	destination address
 * @src:	source address
 * @cnt:	number of bytes to copy
 *
 * Low level memory copy function that catches machine checks
 *
 * Return 0 for success, -EFAULT for fail
 */
int Model0_memcpy_mcsafe(void *Model0_dst, const void *Model0_src, Model0_size_t Model0_cnt);


extern char * Model0_strcpy(char *,const char *);


extern char * Model0_strncpy(char *,const char *, Model0___kernel_size_t);


Model0_size_t Model0_strlcpy(char *, const char *, Model0_size_t);


Model0_ssize_t __attribute__((warn_unused_result)) Model0_strscpy(char *, const char *, Model0_size_t);


extern char * Model0_strcat(char *, const char *);


extern char * Model0_strncat(char *, const char *, Model0___kernel_size_t);


extern Model0_size_t Model0_strlcat(char *, const char *, Model0___kernel_size_t);


extern int Model0_strcmp(const char *,const char *);


extern int Model0_strncmp(const char *,const char *,Model0___kernel_size_t);


extern int Model0_strcasecmp(const char *Model0_s1, const char *Model0_s2);


extern int Model0_strncasecmp(const char *Model0_s1, const char *Model0_s2, Model0_size_t Model0_n);


extern char * Model0_strchr(const char *,int);


extern char * Model0_strchrnul(const char *,int);


extern char * Model0_strnchr(const char *, Model0_size_t, int);


extern char * Model0_strrchr(const char *,int);

extern char * __attribute__((warn_unused_result)) Model0_skip_spaces(const char *);

extern char *Model0_strim(char *);

static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result)) char *Model0_strstrip(char *Model0_str)
{
 return Model0_strim(Model0_str);
}


extern char * Model0_strstr(const char *, const char *);


extern char * Model0_strnstr(const char *, const char *, Model0_size_t);


extern Model0___kernel_size_t Model0_strlen(const char *);


extern Model0___kernel_size_t Model0_strnlen(const char *,Model0___kernel_size_t);


extern char * Model0_strpbrk(const char *,const char *);


extern char * Model0_strsep(char **,const char *);


extern Model0___kernel_size_t Model0_strspn(const char *,const char *);


extern Model0___kernel_size_t Model0_strcspn(const char *,const char *);
extern void * Model0_memscan(void *,int,Model0___kernel_size_t);


extern int Model0_memcmp(const void *,const void *,Model0___kernel_size_t);


extern void * Model0_memchr(const void *,int,Model0___kernel_size_t);

void *Model0_memchr_inv(const void *Model0_s, int Model0_c, Model0_size_t Model0_n);
char *Model0_strreplace(char *Model0_s, char old, char Model0_new);

extern void Model0_kfree_const(const void *Model0_x);

extern char *Model0_kstrdup(const char *Model0_s, Model0_gfp_t Model0_gfp) __attribute__((__malloc__));
extern const char *Model0_kstrdup_const(const char *Model0_s, Model0_gfp_t Model0_gfp);
extern char *Model0_kstrndup(const char *Model0_s, Model0_size_t Model0_len, Model0_gfp_t Model0_gfp);
extern void *Model0_kmemdup(const void *Model0_src, Model0_size_t Model0_len, Model0_gfp_t Model0_gfp);

extern char **Model0_argv_split(Model0_gfp_t Model0_gfp, const char *Model0_str, int *Model0_argcp);
extern void Model0_argv_free(char **Model0_argv);

extern bool Model0_sysfs_streq(const char *Model0_s1, const char *Model0_s2);
extern int Model0_kstrtobool(const char *Model0_s, bool *Model0_res);
static inline __attribute__((no_instrument_function)) int Model0_strtobool(const char *Model0_s, bool *Model0_res)
{
 return Model0_kstrtobool(Model0_s, Model0_res);
}

int Model0_match_string(const char * const *Model0_array, Model0_size_t Model0_n, const char *Model0_string);


int Model0_vbin_printf(Model0_u32 *Model0_bin_buf, Model0_size_t Model0_size, const char *Model0_fmt, Model0_va_list Model0_args);
int Model0_bstr_printf(char *Model0_buf, Model0_size_t Model0_size, const char *Model0_fmt, const Model0_u32 *Model0_bin_buf);
int Model0_bprintf(Model0_u32 *Model0_bin_buf, Model0_size_t Model0_size, const char *Model0_fmt, ...) __attribute__((format(printf, 3, 4)));


extern Model0_ssize_t Model0_memory_read_from_buffer(void *Model0_to, Model0_size_t Model0_count, Model0_loff_t *Model0_ppos,
           const void *Model0_from, Model0_size_t Model0_available);

/**
 * strstarts - does @str start with @prefix?
 * @str: string to examine
 * @prefix: prefix to look for.
 */
static inline __attribute__((no_instrument_function)) bool Model0_strstarts(const char *Model0_str, const char *Model0_prefix)
{
 return Model0_strncmp(Model0_str, Model0_prefix, Model0_strlen(Model0_prefix)) == 0;
}

Model0_size_t Model0_memweight(const void *Model0_ptr, Model0_size_t Model0_bytes);
void Model0_memzero_explicit(void *Model0_s, Model0_size_t Model0_count);

/**
 * kbasename - return the last part of a pathname.
 *
 * @path: path to extract the filename from.
 */
static inline __attribute__((no_instrument_function)) const char *Model0_kbasename(const char *Model0_path)
{
 const char *Model0_tail = Model0_strrchr(Model0_path, '/');
 return Model0_tail ? Model0_tail + 1 : Model0_path;
}


/*
 * bitmaps provide bit arrays that consume one or more unsigned
 * longs.  The bitmap interface and available operations are listed
 * here, in bitmap.h
 *
 * Function implementations generic to all architectures are in
 * lib/bitmap.c.  Functions implementations that are architecture
 * specific are in various include/asm-<arch>/bitops.h headers
 * and other arch/<arch> specific files.
 *
 * See lib/bitmap.c for more details.
 */

/*
 * The available bitmap operations and their rough meaning in the
 * case that the bitmap is a single unsigned long are thus:
 *
 * Note that nbits should be always a compile time evaluable constant.
 * Otherwise many inlines will generate horrible code.
 *
 * bitmap_zero(dst, nbits)			*dst = 0UL
 * bitmap_fill(dst, nbits)			*dst = ~0UL
 * bitmap_copy(dst, src, nbits)			*dst = *src
 * bitmap_and(dst, src1, src2, nbits)		*dst = *src1 & *src2
 * bitmap_or(dst, src1, src2, nbits)		*dst = *src1 | *src2
 * bitmap_xor(dst, src1, src2, nbits)		*dst = *src1 ^ *src2
 * bitmap_andnot(dst, src1, src2, nbits)	*dst = *src1 & ~(*src2)
 * bitmap_complement(dst, src, nbits)		*dst = ~(*src)
 * bitmap_equal(src1, src2, nbits)		Are *src1 and *src2 equal?
 * bitmap_intersects(src1, src2, nbits) 	Do *src1 and *src2 overlap?
 * bitmap_subset(src1, src2, nbits)		Is *src1 a subset of *src2?
 * bitmap_empty(src, nbits)			Are all bits zero in *src?
 * bitmap_full(src, nbits)			Are all bits set in *src?
 * bitmap_weight(src, nbits)			Hamming Weight: number set bits
 * bitmap_set(dst, pos, nbits)			Set specified bit area
 * bitmap_clear(dst, pos, nbits)		Clear specified bit area
 * bitmap_find_next_zero_area(buf, len, pos, n, mask)	Find bit free area
 * bitmap_find_next_zero_area_off(buf, len, pos, n, mask)	as above
 * bitmap_shift_right(dst, src, n, nbits)	*dst = *src >> n
 * bitmap_shift_left(dst, src, n, nbits)	*dst = *src << n
 * bitmap_remap(dst, src, old, new, nbits)	*dst = map(old, new)(src)
 * bitmap_bitremap(oldbit, old, new, nbits)	newbit = map(old, new)(oldbit)
 * bitmap_onto(dst, orig, relmap, nbits)	*dst = orig relative to relmap
 * bitmap_fold(dst, orig, sz, nbits)		dst bits = orig bits mod sz
 * bitmap_parse(buf, buflen, dst, nbits)	Parse bitmap dst from kernel buf
 * bitmap_parse_user(ubuf, ulen, dst, nbits)	Parse bitmap dst from user buf
 * bitmap_parselist(buf, dst, nbits)		Parse bitmap dst from kernel buf
 * bitmap_parselist_user(buf, dst, nbits)	Parse bitmap dst from user buf
 * bitmap_find_free_region(bitmap, bits, order)	Find and allocate bit region
 * bitmap_release_region(bitmap, pos, order)	Free specified bit region
 * bitmap_allocate_region(bitmap, pos, order)	Allocate specified bit region
 * bitmap_from_u32array(dst, nbits, buf, nwords) *dst = *buf (nwords 32b words)
 * bitmap_to_u32array(buf, nwords, src, nbits)	*buf = *dst (nwords 32b words)
 */

/*
 * Also the following operations in asm/bitops.h apply to bitmaps.
 *
 * set_bit(bit, addr)			*addr |= bit
 * clear_bit(bit, addr)			*addr &= ~bit
 * change_bit(bit, addr)		*addr ^= bit
 * test_bit(bit, addr)			Is bit set in *addr?
 * test_and_set_bit(bit, addr)		Set bit and return old value
 * test_and_clear_bit(bit, addr)	Clear bit and return old value
 * test_and_change_bit(bit, addr)	Change bit and return old value
 * find_first_zero_bit(addr, nbits)	Position first zero bit in *addr
 * find_first_bit(addr, nbits)		Position first set bit in *addr
 * find_next_zero_bit(addr, nbits, bit)	Position next zero bit in *addr >= bit
 * find_next_bit(addr, nbits, bit)	Position next set bit in *addr >= bit
 */

/*
 * The DECLARE_BITMAP(name,bits) macro, in linux/types.h, can be used
 * to declare an array named 'name' of just enough unsigned longs to
 * contain all bit positions from 0 to 'bits' - 1.
 */

/*
 * lib/bitmap.c provides these functions:
 */

extern int Model0___bitmap_empty(const unsigned long *Model0_bitmap, unsigned int Model0_nbits);
extern int Model0___bitmap_full(const unsigned long *Model0_bitmap, unsigned int Model0_nbits);
extern int Model0___bitmap_equal(const unsigned long *Model0_bitmap1,
     const unsigned long *Model0_bitmap2, unsigned int Model0_nbits);
extern void Model0___bitmap_complement(unsigned long *Model0_dst, const unsigned long *Model0_src,
   unsigned int Model0_nbits);
extern void Model0___bitmap_shift_right(unsigned long *Model0_dst, const unsigned long *Model0_src,
    unsigned int Model0_shift, unsigned int Model0_nbits);
extern void Model0___bitmap_shift_left(unsigned long *Model0_dst, const unsigned long *Model0_src,
    unsigned int Model0_shift, unsigned int Model0_nbits);
extern int Model0___bitmap_and(unsigned long *Model0_dst, const unsigned long *Model0_bitmap1,
   const unsigned long *Model0_bitmap2, unsigned int Model0_nbits);
extern void Model0___bitmap_or(unsigned long *Model0_dst, const unsigned long *Model0_bitmap1,
   const unsigned long *Model0_bitmap2, unsigned int Model0_nbits);
extern void Model0___bitmap_xor(unsigned long *Model0_dst, const unsigned long *Model0_bitmap1,
   const unsigned long *Model0_bitmap2, unsigned int Model0_nbits);
extern int Model0___bitmap_andnot(unsigned long *Model0_dst, const unsigned long *Model0_bitmap1,
   const unsigned long *Model0_bitmap2, unsigned int Model0_nbits);
extern int Model0___bitmap_intersects(const unsigned long *Model0_bitmap1,
   const unsigned long *Model0_bitmap2, unsigned int Model0_nbits);
extern int Model0___bitmap_subset(const unsigned long *Model0_bitmap1,
   const unsigned long *Model0_bitmap2, unsigned int Model0_nbits);
extern int Model0___bitmap_weight(const unsigned long *Model0_bitmap, unsigned int Model0_nbits);

extern void Model0_bitmap_set(unsigned long *Model0_map, unsigned int Model0_start, int Model0_len);
extern void Model0_bitmap_clear(unsigned long *Model0_map, unsigned int Model0_start, int Model0_len);

extern unsigned long Model0_bitmap_find_next_zero_area_off(unsigned long *Model0_map,
          unsigned long Model0_size,
          unsigned long Model0_start,
          unsigned int Model0_nr,
          unsigned long Model0_align_mask,
          unsigned long Model0_align_offset);

/**
 * bitmap_find_next_zero_area - find a contiguous aligned zero area
 * @map: The address to base the search on
 * @size: The bitmap size in bits
 * @start: The bitnumber to start searching at
 * @nr: The number of zeroed bits we're looking for
 * @align_mask: Alignment mask for zero area
 *
 * The @align_mask should be one less than a power of 2; the effect is that
 * the bit offset of all zero areas this function finds is multiples of that
 * power of 2. A @align_mask of 0 means no alignment is required.
 */
static inline __attribute__((no_instrument_function)) unsigned long
Model0_bitmap_find_next_zero_area(unsigned long *Model0_map,
      unsigned long Model0_size,
      unsigned long Model0_start,
      unsigned int Model0_nr,
      unsigned long Model0_align_mask)
{
 return Model0_bitmap_find_next_zero_area_off(Model0_map, Model0_size, Model0_start, Model0_nr,
           Model0_align_mask, 0);
}

extern int Model0___bitmap_parse(const char *Model0_buf, unsigned int Model0_buflen, int Model0_is_user,
   unsigned long *Model0_dst, int Model0_nbits);
extern int Model0_bitmap_parse_user(const char *Model0_ubuf, unsigned int Model0_ulen,
   unsigned long *Model0_dst, int Model0_nbits);
extern int Model0_bitmap_parselist(const char *Model0_buf, unsigned long *Model0_maskp,
   int Model0_nmaskbits);
extern int Model0_bitmap_parselist_user(const char *Model0_ubuf, unsigned int Model0_ulen,
   unsigned long *Model0_dst, int Model0_nbits);
extern void Model0_bitmap_remap(unsigned long *Model0_dst, const unsigned long *Model0_src,
  const unsigned long *old, const unsigned long *Model0_new, unsigned int Model0_nbits);
extern int Model0_bitmap_bitremap(int Model0_oldbit,
  const unsigned long *old, const unsigned long *Model0_new, int Model0_bits);
extern void Model0_bitmap_onto(unsigned long *Model0_dst, const unsigned long *Model0_orig,
  const unsigned long *Model0_relmap, unsigned int Model0_bits);
extern void Model0_bitmap_fold(unsigned long *Model0_dst, const unsigned long *Model0_orig,
  unsigned int Model0_sz, unsigned int Model0_nbits);
extern int Model0_bitmap_find_free_region(unsigned long *Model0_bitmap, unsigned int Model0_bits, int Model0_order);
extern void Model0_bitmap_release_region(unsigned long *Model0_bitmap, unsigned int Model0_pos, int Model0_order);
extern int Model0_bitmap_allocate_region(unsigned long *Model0_bitmap, unsigned int Model0_pos, int Model0_order);
extern unsigned int Model0_bitmap_from_u32array(unsigned long *Model0_bitmap,
      unsigned int Model0_nbits,
      const Model0_u32 *Model0_buf,
      unsigned int Model0_nwords);
extern unsigned int Model0_bitmap_to_u32array(Model0_u32 *Model0_buf,
           unsigned int Model0_nwords,
           const unsigned long *Model0_bitmap,
           unsigned int Model0_nbits);





extern unsigned int Model0_bitmap_ord_to_pos(const unsigned long *Model0_bitmap, unsigned int Model0_ord, unsigned int Model0_nbits);
extern int Model0_bitmap_print_to_pagebuf(bool Model0_list, char *Model0_buf,
       const unsigned long *Model0_maskp, int Model0_nmaskbits);







static inline __attribute__((no_instrument_function)) void Model0_bitmap_zero(unsigned long *Model0_dst, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  *Model0_dst = 0UL;
 else {
  unsigned int Model0_len = (((Model0_nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(unsigned long);
  memset(Model0_dst, 0, Model0_len);
 }
}

static inline __attribute__((no_instrument_function)) void Model0_bitmap_fill(unsigned long *Model0_dst, unsigned int Model0_nbits)
{
 unsigned int Model0_nlongs = (((Model0_nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)));
 if (!(__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64)) {
  unsigned int Model0_len = (Model0_nlongs - 1) * sizeof(unsigned long);
  memset(Model0_dst, 0xff, Model0_len);
 }
 Model0_dst[Model0_nlongs - 1] = (~0UL >> (-(Model0_nbits) & (64 - 1)));
}

static inline __attribute__((no_instrument_function)) void Model0_bitmap_copy(unsigned long *Model0_dst, const unsigned long *Model0_src,
   unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  *Model0_dst = *Model0_src;
 else {
  unsigned int Model0_len = (((Model0_nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(unsigned long);
  ({ Model0_size_t Model0___len = (Model0_len); void *Model0___ret; if (__builtin_constant_p(Model0_len) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_dst), (Model0_src), Model0___len); else Model0___ret = __builtin_memcpy((Model0_dst), (Model0_src), Model0___len); Model0___ret; });
 }
}

static inline __attribute__((no_instrument_function)) int Model0_bitmap_and(unsigned long *Model0_dst, const unsigned long *Model0_src1,
   const unsigned long *Model0_src2, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  return (*Model0_dst = *Model0_src1 & *Model0_src2 & (~0UL >> (-(Model0_nbits) & (64 - 1)))) != 0;
 return Model0___bitmap_and(Model0_dst, Model0_src1, Model0_src2, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) void Model0_bitmap_or(unsigned long *Model0_dst, const unsigned long *Model0_src1,
   const unsigned long *Model0_src2, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  *Model0_dst = *Model0_src1 | *Model0_src2;
 else
  Model0___bitmap_or(Model0_dst, Model0_src1, Model0_src2, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) void Model0_bitmap_xor(unsigned long *Model0_dst, const unsigned long *Model0_src1,
   const unsigned long *Model0_src2, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  *Model0_dst = *Model0_src1 ^ *Model0_src2;
 else
  Model0___bitmap_xor(Model0_dst, Model0_src1, Model0_src2, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) int Model0_bitmap_andnot(unsigned long *Model0_dst, const unsigned long *Model0_src1,
   const unsigned long *Model0_src2, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  return (*Model0_dst = *Model0_src1 & ~(*Model0_src2) & (~0UL >> (-(Model0_nbits) & (64 - 1)))) != 0;
 return Model0___bitmap_andnot(Model0_dst, Model0_src1, Model0_src2, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) void Model0_bitmap_complement(unsigned long *Model0_dst, const unsigned long *Model0_src,
   unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  *Model0_dst = ~(*Model0_src);
 else
  Model0___bitmap_complement(Model0_dst, Model0_src, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) int Model0_bitmap_equal(const unsigned long *Model0_src1,
   const unsigned long *Model0_src2, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  return !((*Model0_src1 ^ *Model0_src2) & (~0UL >> (-(Model0_nbits) & (64 - 1))));




 return Model0___bitmap_equal(Model0_src1, Model0_src2, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) int Model0_bitmap_intersects(const unsigned long *Model0_src1,
   const unsigned long *Model0_src2, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  return ((*Model0_src1 & *Model0_src2) & (~0UL >> (-(Model0_nbits) & (64 - 1)))) != 0;
 else
  return Model0___bitmap_intersects(Model0_src1, Model0_src2, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) int Model0_bitmap_subset(const unsigned long *Model0_src1,
   const unsigned long *Model0_src2, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  return ! ((*Model0_src1 & ~(*Model0_src2)) & (~0UL >> (-(Model0_nbits) & (64 - 1))));
 else
  return Model0___bitmap_subset(Model0_src1, Model0_src2, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) int Model0_bitmap_empty(const unsigned long *Model0_src, unsigned Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  return ! (*Model0_src & (~0UL >> (-(Model0_nbits) & (64 - 1))));

 return Model0_find_first_bit(Model0_src, Model0_nbits) == Model0_nbits;
}

static inline __attribute__((no_instrument_function)) int Model0_bitmap_full(const unsigned long *Model0_src, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  return ! (~(*Model0_src) & (~0UL >> (-(Model0_nbits) & (64 - 1))));

 return Model0_find_first_zero_bit(Model0_src, Model0_nbits) == Model0_nbits;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_bitmap_weight(const unsigned long *Model0_src, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  return Model0_hweight_long(*Model0_src & (~0UL >> (-(Model0_nbits) & (64 - 1))));
 return Model0___bitmap_weight(Model0_src, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) void Model0_bitmap_shift_right(unsigned long *Model0_dst, const unsigned long *Model0_src,
    unsigned int Model0_shift, int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  *Model0_dst = (*Model0_src & (~0UL >> (-(Model0_nbits) & (64 - 1)))) >> Model0_shift;
 else
  Model0___bitmap_shift_right(Model0_dst, Model0_src, Model0_shift, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) void Model0_bitmap_shift_left(unsigned long *Model0_dst, const unsigned long *Model0_src,
    unsigned int Model0_shift, unsigned int Model0_nbits)
{
 if ((__builtin_constant_p(Model0_nbits) && (Model0_nbits) <= 64))
  *Model0_dst = (*Model0_src << Model0_shift) & (~0UL >> (-(Model0_nbits) & (64 - 1)));
 else
  Model0___bitmap_shift_left(Model0_dst, Model0_src, Model0_shift, Model0_nbits);
}

static inline __attribute__((no_instrument_function)) int Model0_bitmap_parse(const char *Model0_buf, unsigned int Model0_buflen,
   unsigned long *Model0_maskp, int Model0_nmaskbits)
{
 return Model0___bitmap_parse(Model0_buf, Model0_buflen, 0, Model0_maskp, Model0_nmaskbits);
}


/* Don't assign or return these: may not be this big! */
typedef struct Model0_cpumask { unsigned long Model0_bits[(((64) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } Model0_cpumask_t;

/**
 * cpumask_bits - get the bits in a cpumask
 * @maskp: the struct cpumask *
 *
 * You should only assume nr_cpu_ids bits of this mask are valid.  This is
 * a macro so it's const-correct.
 */


/**
 * cpumask_pr_args - printf args to output a cpumask
 * @maskp: cpumask to be printed
 *
 * Can be used to provide arguments for '%*pb[l]' when printing a cpumask.
 */





extern int Model0_nr_cpu_ids;
/*
 * The following particular system cpumasks and operations manage
 * possible, present, active and online cpus.
 *
 *     cpu_possible_mask- has bit 'cpu' set iff cpu is populatable
 *     cpu_present_mask - has bit 'cpu' set iff cpu is populated
 *     cpu_online_mask  - has bit 'cpu' set iff cpu available to scheduler
 *     cpu_active_mask  - has bit 'cpu' set iff cpu available to migration
 *
 *  If !CONFIG_HOTPLUG_CPU, present == possible, and active == online.
 *
 *  The cpu_possible_mask is fixed at boot time, as the set of CPU id's
 *  that it is possible might ever be plugged in at anytime during the
 *  life of that system boot.  The cpu_present_mask is dynamic(*),
 *  representing which CPUs are currently plugged in.  And
 *  cpu_online_mask is the dynamic subset of cpu_present_mask,
 *  indicating those CPUs available for scheduling.
 *
 *  If HOTPLUG is enabled, then cpu_possible_mask is forced to have
 *  all NR_CPUS bits set, otherwise it is just the set of CPUs that
 *  ACPI reports present at boot.
 *
 *  If HOTPLUG is enabled, then cpu_present_mask varies dynamically,
 *  depending on what ACPI reports as currently plugged in, otherwise
 *  cpu_present_mask is just a copy of cpu_possible_mask.
 *
 *  (*) Well, cpu_present_mask is dynamic in the hotplug case.  If not
 *      hotplug, it's a copy of cpu_possible_mask, hence fixed at boot.
 *
 * Subtleties:
 * 1) UP arch's (NR_CPUS == 1, CONFIG_SMP not defined) hardcode
 *    assumption that their single CPU is online.  The UP
 *    cpu_{online,possible,present}_masks are placebos.  Changing them
 *    will have no useful affect on the following num_*_cpus()
 *    and cpu_*() macros in the UP case.  This ugliness is a UP
 *    optimization - don't waste any instructions or memory references
 *    asking if you're online or how many CPUs there are if there is
 *    only one CPU.
 */

extern struct Model0_cpumask Model0___cpu_possible_mask;
extern struct Model0_cpumask Model0___cpu_online_mask;
extern struct Model0_cpumask Model0___cpu_present_mask;
extern struct Model0_cpumask Model0___cpu_active_mask;
/* verify cpu argument to cpumask_* operators */
static inline __attribute__((no_instrument_function)) unsigned int Model0_cpumask_check(unsigned int Model0_cpu)
{



 return Model0_cpu;
}
/**
 * cpumask_first - get the first cpu in a cpumask
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_cpumask_first(const struct Model0_cpumask *Model0_srcp)
{
 return Model0_find_first_bit(((Model0_srcp)->Model0_bits), 64);
}

/**
 * cpumask_next - get the next cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus set.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_cpumask_next(int Model0_n, const struct Model0_cpumask *Model0_srcp)
{
 /* -1 is a legal arg here. */
 if (Model0_n != -1)
  Model0_cpumask_check(Model0_n);
 return Model0_find_next_bit(((Model0_srcp)->Model0_bits), 64, Model0_n+1);
}

/**
 * cpumask_next_zero - get the next unset cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus unset.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_cpumask_next_zero(int Model0_n, const struct Model0_cpumask *Model0_srcp)
{
 /* -1 is a legal arg here. */
 if (Model0_n != -1)
  Model0_cpumask_check(Model0_n);
 return Model0_find_next_zero_bit(((Model0_srcp)->Model0_bits), 64, Model0_n+1);
}

int Model0_cpumask_next_and(int Model0_n, const struct Model0_cpumask *, const struct Model0_cpumask *);
int Model0_cpumask_any_but(const struct Model0_cpumask *Model0_mask, unsigned int Model0_cpu);
unsigned int Model0_cpumask_local_spread(unsigned int Model0_i, int Model0_node);

/**
 * for_each_cpu - iterate over every cpu in a mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */





/**
 * for_each_cpu_not - iterate over every cpu in a complemented mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */





/**
 * for_each_cpu_and - iterate over every cpu in both masks
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the first cpumask pointer
 * @and: the second cpumask pointer
 *
 * This saves a temporary CPU mask in many places.  It is equivalent to:
 *	struct cpumask tmp;
 *	cpumask_and(&tmp, &mask, &and);
 *	for_each_cpu(cpu, &tmp)
 *		...
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
/**
 * cpumask_set_cpu - set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_set_cpu(unsigned int Model0_cpu, struct Model0_cpumask *Model0_dstp)
{
 Model0_set_bit(Model0_cpumask_check(Model0_cpu), ((Model0_dstp)->Model0_bits));
}

/**
 * cpumask_clear_cpu - clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_clear_cpu(int Model0_cpu, struct Model0_cpumask *Model0_dstp)
{
 Model0_clear_bit(Model0_cpumask_check(Model0_cpu), ((Model0_dstp)->Model0_bits));
}

/**
 * cpumask_test_cpu - test for a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns 1 if @cpu is set in @cpumask, else returns 0
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_test_cpu(int Model0_cpu, const struct Model0_cpumask *Model0_cpumask)
{
 return (__builtin_constant_p((Model0_cpumask_check(Model0_cpu))) ? Model0_constant_test_bit((Model0_cpumask_check(Model0_cpu)), ((((Model0_cpumask))->Model0_bits))) : Model0_variable_test_bit((Model0_cpumask_check(Model0_cpu)), ((((Model0_cpumask))->Model0_bits))));
}

/**
 * cpumask_test_and_set_cpu - atomically test and set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns 1 if @cpu is set in old bitmap of @cpumask, else returns 0
 *
 * test_and_set_bit wrapper for cpumasks.
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_test_and_set_cpu(int Model0_cpu, struct Model0_cpumask *Model0_cpumask)
{
 return Model0_test_and_set_bit(Model0_cpumask_check(Model0_cpu), ((Model0_cpumask)->Model0_bits));
}

/**
 * cpumask_test_and_clear_cpu - atomically test and clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns 1 if @cpu is set in old bitmap of @cpumask, else returns 0
 *
 * test_and_clear_bit wrapper for cpumasks.
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_test_and_clear_cpu(int Model0_cpu, struct Model0_cpumask *Model0_cpumask)
{
 return Model0_test_and_clear_bit(Model0_cpumask_check(Model0_cpu), ((Model0_cpumask)->Model0_bits));
}

/**
 * cpumask_setall - set all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_setall(struct Model0_cpumask *Model0_dstp)
{
 Model0_bitmap_fill(((Model0_dstp)->Model0_bits), 64);
}

/**
 * cpumask_clear - clear all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_clear(struct Model0_cpumask *Model0_dstp)
{
 Model0_bitmap_zero(((Model0_dstp)->Model0_bits), 64);
}

/**
 * cpumask_and - *dstp = *src1p & *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 *
 * If *@dstp is empty, returns 0, else returns 1
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_and(struct Model0_cpumask *Model0_dstp,
          const struct Model0_cpumask *Model0_src1p,
          const struct Model0_cpumask *Model0_src2p)
{
 return Model0_bitmap_and(((Model0_dstp)->Model0_bits), ((Model0_src1p)->Model0_bits),
           ((Model0_src2p)->Model0_bits), 64);
}

/**
 * cpumask_or - *dstp = *src1p | *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_or(struct Model0_cpumask *Model0_dstp, const struct Model0_cpumask *Model0_src1p,
         const struct Model0_cpumask *Model0_src2p)
{
 Model0_bitmap_or(((Model0_dstp)->Model0_bits), ((Model0_src1p)->Model0_bits),
          ((Model0_src2p)->Model0_bits), 64);
}

/**
 * cpumask_xor - *dstp = *src1p ^ *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_xor(struct Model0_cpumask *Model0_dstp,
          const struct Model0_cpumask *Model0_src1p,
          const struct Model0_cpumask *Model0_src2p)
{
 Model0_bitmap_xor(((Model0_dstp)->Model0_bits), ((Model0_src1p)->Model0_bits),
           ((Model0_src2p)->Model0_bits), 64);
}

/**
 * cpumask_andnot - *dstp = *src1p & ~*src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 *
 * If *@dstp is empty, returns 0, else returns 1
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_andnot(struct Model0_cpumask *Model0_dstp,
      const struct Model0_cpumask *Model0_src1p,
      const struct Model0_cpumask *Model0_src2p)
{
 return Model0_bitmap_andnot(((Model0_dstp)->Model0_bits), ((Model0_src1p)->Model0_bits),
       ((Model0_src2p)->Model0_bits), 64);
}

/**
 * cpumask_complement - *dstp = ~*srcp
 * @dstp: the cpumask result
 * @srcp: the input to invert
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_complement(struct Model0_cpumask *Model0_dstp,
          const struct Model0_cpumask *Model0_srcp)
{
 Model0_bitmap_complement(((Model0_dstp)->Model0_bits), ((Model0_srcp)->Model0_bits),
           64);
}

/**
 * cpumask_equal - *src1p == *src2p
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((no_instrument_function)) bool Model0_cpumask_equal(const struct Model0_cpumask *Model0_src1p,
    const struct Model0_cpumask *Model0_src2p)
{
 return Model0_bitmap_equal(((Model0_src1p)->Model0_bits), ((Model0_src2p)->Model0_bits),
       64);
}

/**
 * cpumask_intersects - (*src1p & *src2p) != 0
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((no_instrument_function)) bool Model0_cpumask_intersects(const struct Model0_cpumask *Model0_src1p,
         const struct Model0_cpumask *Model0_src2p)
{
 return Model0_bitmap_intersects(((Model0_src1p)->Model0_bits), ((Model0_src2p)->Model0_bits),
            64);
}

/**
 * cpumask_subset - (*src1p & ~*src2p) == 0
 * @src1p: the first input
 * @src2p: the second input
 *
 * Returns 1 if *@src1p is a subset of *@src2p, else returns 0
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_subset(const struct Model0_cpumask *Model0_src1p,
     const struct Model0_cpumask *Model0_src2p)
{
 return Model0_bitmap_subset(((Model0_src1p)->Model0_bits), ((Model0_src2p)->Model0_bits),
        64);
}

/**
 * cpumask_empty - *srcp == 0
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are clear.
 */
static inline __attribute__((no_instrument_function)) bool Model0_cpumask_empty(const struct Model0_cpumask *Model0_srcp)
{
 return Model0_bitmap_empty(((Model0_srcp)->Model0_bits), 64);
}

/**
 * cpumask_full - *srcp == 0xFFFFFFFF...
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are set.
 */
static inline __attribute__((no_instrument_function)) bool Model0_cpumask_full(const struct Model0_cpumask *Model0_srcp)
{
 return Model0_bitmap_full(((Model0_srcp)->Model0_bits), 64);
}

/**
 * cpumask_weight - Count of bits in *srcp
 * @srcp: the cpumask to count bits (< nr_cpu_ids) in.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_cpumask_weight(const struct Model0_cpumask *Model0_srcp)
{
 return Model0_bitmap_weight(((Model0_srcp)->Model0_bits), 64);
}

/**
 * cpumask_shift_right - *dstp = *srcp >> n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_shift_right(struct Model0_cpumask *Model0_dstp,
           const struct Model0_cpumask *Model0_srcp, int Model0_n)
{
 Model0_bitmap_shift_right(((Model0_dstp)->Model0_bits), ((Model0_srcp)->Model0_bits), Model0_n,
            64);
}

/**
 * cpumask_shift_left - *dstp = *srcp << n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_shift_left(struct Model0_cpumask *Model0_dstp,
          const struct Model0_cpumask *Model0_srcp, int Model0_n)
{
 Model0_bitmap_shift_left(((Model0_dstp)->Model0_bits), ((Model0_srcp)->Model0_bits), Model0_n,
           64);
}

/**
 * cpumask_copy - *dstp = *srcp
 * @dstp: the result
 * @srcp: the input cpumask
 */
static inline __attribute__((no_instrument_function)) void Model0_cpumask_copy(struct Model0_cpumask *Model0_dstp,
    const struct Model0_cpumask *Model0_srcp)
{
 Model0_bitmap_copy(((Model0_dstp)->Model0_bits), ((Model0_srcp)->Model0_bits), 64);
}

/**
 * cpumask_any - pick a "random" cpu from *srcp
 * @srcp: the input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */


/**
 * cpumask_first_and - return the first cpu from *srcp1 & *srcp2
 * @src1p: the first input
 * @src2p: the second input
 *
 * Returns >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().
 */


/**
 * cpumask_any_and - pick a "random" cpu from *mask1 & *mask2
 * @mask1: the first input cpumask
 * @mask2: the second input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */


/**
 * cpumask_of - the cpumask containing just a given cpu
 * @cpu: the cpu (<= nr_cpu_ids)
 */


/**
 * cpumask_parse_user - extract a cpumask from a user string
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_parse_user(const char *Model0_buf, int Model0_len,
         struct Model0_cpumask *Model0_dstp)
{
 return Model0_bitmap_parse_user(Model0_buf, Model0_len, ((Model0_dstp)->Model0_bits), Model0_nr_cpu_ids);
}

/**
 * cpumask_parselist_user - extract a cpumask from a user string
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_parselist_user(const char *Model0_buf, int Model0_len,
         struct Model0_cpumask *Model0_dstp)
{
 return Model0_bitmap_parselist_user(Model0_buf, Model0_len, ((Model0_dstp)->Model0_bits),
         Model0_nr_cpu_ids);
}

/**
 * cpumask_parse - extract a cpumask from a string
 * @buf: the buffer to extract from
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((no_instrument_function)) int Model0_cpumask_parse(const char *Model0_buf, struct Model0_cpumask *Model0_dstp)
{
 char *Model0_nl = Model0_strchr(Model0_buf, '\n');
 unsigned int Model0_len = Model0_nl ? (unsigned int)(Model0_nl - Model0_buf) : Model0_strlen(Model0_buf);

 return Model0_bitmap_parse(Model0_buf, Model0_len, ((Model0_dstp)->Model0_bits), Model0_nr_cpu_ids);
}

/**
 * cpulist_parse - extract a cpumask from a user string of ranges
 * @buf: the buffer to extract from
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((no_instrument_function)) int Model0_cpulist_parse(const char *Model0_buf, struct Model0_cpumask *Model0_dstp)
{
 return Model0_bitmap_parselist(Model0_buf, ((Model0_dstp)->Model0_bits), Model0_nr_cpu_ids);
}

/**
 * cpumask_size - size to allocate for a 'struct cpumask' in bytes
 */
static inline __attribute__((no_instrument_function)) Model0_size_t Model0_cpumask_size(void)
{
 return (((64) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(long);
}

/*
 * cpumask_var_t: struct cpumask for stack usage.
 *
 * Oh, the wicked games we play!  In order to make kernel coding a
 * little more difficult, we typedef cpumask_var_t to an array or a
 * pointer: doing &mask on an array is a noop, so it still works.
 *
 * ie.
 *	cpumask_var_t tmpmask;
 *	if (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))
 *		return -ENOMEM;
 *
 *	  ... use 'tmpmask' like a normal struct cpumask * ...
 *
 *	free_cpumask_var(tmpmask);
 *
 *
 * However, one notable exception is there. alloc_cpumask_var() allocates
 * only nr_cpumask_bits bits (in the other hand, real cpumask_t always has
 * NR_CPUS bits). Therefore you don't have to dereference cpumask_var_t.
 *
 *	cpumask_var_t tmpmask;
 *	if (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))
 *		return -ENOMEM;
 *
 *	var = *tmpmask;
 *
 * This code makes NR_CPUS length memcopy and brings to a memory corruption.
 * cpumask_copy() provide safe copy functionality.
 *
 * Note that there is another evil here: If you define a cpumask_var_t
 * as a percpu variable then the way to obtain the address of the cpumask
 * structure differently influences what this_cpu_* operation needs to be
 * used. Please use this_cpu_cpumask_var_t in those cases. The direct use
 * of this_cpu_ptr() or this_cpu_read() will lead to failures when the
 * other type of cpumask_var_t implementation is configured.
 */
typedef struct Model0_cpumask Model0_cpumask_var_t[1];



static inline __attribute__((no_instrument_function)) bool Model0_alloc_cpumask_var(Model0_cpumask_var_t *Model0_mask, Model0_gfp_t Model0_flags)
{
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_alloc_cpumask_var_node(Model0_cpumask_var_t *Model0_mask, Model0_gfp_t Model0_flags,
       int Model0_node)
{
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_zalloc_cpumask_var(Model0_cpumask_var_t *Model0_mask, Model0_gfp_t Model0_flags)
{
 Model0_cpumask_clear(*Model0_mask);
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_zalloc_cpumask_var_node(Model0_cpumask_var_t *Model0_mask, Model0_gfp_t Model0_flags,
       int Model0_node)
{
 Model0_cpumask_clear(*Model0_mask);
 return true;
}

static inline __attribute__((no_instrument_function)) void Model0_alloc_bootmem_cpumask_var(Model0_cpumask_var_t *Model0_mask)
{
}

static inline __attribute__((no_instrument_function)) void Model0_free_cpumask_var(Model0_cpumask_var_t Model0_mask)
{
}

static inline __attribute__((no_instrument_function)) void Model0_free_bootmem_cpumask_var(Model0_cpumask_var_t Model0_mask)
{
}


/* It's common to want to use cpu_all_mask in struct member initializers,
 * so it has to refer to an address rather than a pointer. */
extern const unsigned long Model0_cpu_all_bits[(((64) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];


/* First bits of cpu_bit_bitmap are in fact unset. */






/* Wrappers for arch boot code to manipulate normally-constant masks */
void Model0_init_cpu_present(const struct Model0_cpumask *Model0_src);
void Model0_init_cpu_possible(const struct Model0_cpumask *Model0_src);
void Model0_init_cpu_online(const struct Model0_cpumask *Model0_src);

static inline __attribute__((no_instrument_function)) void
Model0_set_cpu_possible(unsigned int Model0_cpu, bool Model0_possible)
{
 if (Model0_possible)
  Model0_cpumask_set_cpu(Model0_cpu, &Model0___cpu_possible_mask);
 else
  Model0_cpumask_clear_cpu(Model0_cpu, &Model0___cpu_possible_mask);
}

static inline __attribute__((no_instrument_function)) void
Model0_set_cpu_present(unsigned int Model0_cpu, bool Model0_present)
{
 if (Model0_present)
  Model0_cpumask_set_cpu(Model0_cpu, &Model0___cpu_present_mask);
 else
  Model0_cpumask_clear_cpu(Model0_cpu, &Model0___cpu_present_mask);
}

static inline __attribute__((no_instrument_function)) void
Model0_set_cpu_online(unsigned int Model0_cpu, bool Model0_online)
{
 if (Model0_online)
  Model0_cpumask_set_cpu(Model0_cpu, &Model0___cpu_online_mask);
 else
  Model0_cpumask_clear_cpu(Model0_cpu, &Model0___cpu_online_mask);
}

static inline __attribute__((no_instrument_function)) void
Model0_set_cpu_active(unsigned int Model0_cpu, bool Model0_active)
{
 if (Model0_active)
  Model0_cpumask_set_cpu(Model0_cpu, &Model0___cpu_active_mask);
 else
  Model0_cpumask_clear_cpu(Model0_cpu, &Model0___cpu_active_mask);
}


/**
 * to_cpumask - convert an NR_CPUS bitmap to a struct cpumask *
 * @bitmap: the bitmap
 *
 * There are a few places where cpumask_var_t isn't appropriate and
 * static cpumasks must be used (eg. very early boot), yet we don't
 * expose the definition of 'struct cpumask'.
 *
 * This does the conversion, and can be used as a constant initializer.
 */




static inline __attribute__((no_instrument_function)) int Model0___check_is_bitmap(const unsigned long *Model0_bitmap)
{
 return 1;
}

/*
 * Special-case data structure for "single bit set only" constant CPU masks.
 *
 * We pre-generate all the 64 (or 32) possible bit positions, with enough
 * padding to the left and the right, and return the constant pointer
 * appropriately offset.
 */
extern const unsigned long
 Model0_cpu_bit_bitmap[64 +1][(((64) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];

static inline __attribute__((no_instrument_function)) const struct Model0_cpumask *Model0_get_cpu_mask(unsigned int Model0_cpu)
{
 const unsigned long *Model0_p = Model0_cpu_bit_bitmap[1 + Model0_cpu % 64];
 Model0_p -= Model0_cpu / 64;
 return ((struct Model0_cpumask *)(1 ? (Model0_p) : (void *)sizeof(Model0___check_is_bitmap(Model0_p))));
}
/**
 * cpumap_print_to_pagebuf  - copies the cpumask into the buffer either
 *	as comma-separated list of cpus or hex values of cpumask
 * @list: indicates whether the cpumap must be list
 * @mask: the cpumask to copy
 * @buf: the buffer to copy into
 *
 * Returns the length of the (null-terminated) @buf string, zero if
 * nothing is copied.
 */
static inline __attribute__((no_instrument_function)) Model0_ssize_t
Model0_cpumap_print_to_pagebuf(bool Model0_list, char *Model0_buf, const struct Model0_cpumask *Model0_mask)
{
 return Model0_bitmap_print_to_pagebuf(Model0_list, Model0_buf, ((Model0_mask)->Model0_bits),
          Model0_nr_cpu_ids);
}

extern Model0_cpumask_var_t Model0_cpu_callin_mask;
extern Model0_cpumask_var_t Model0_cpu_callout_mask;
extern Model0_cpumask_var_t Model0_cpu_initialized_mask;
extern Model0_cpumask_var_t Model0_cpu_sibling_setup_mask;

extern void Model0_setup_cpu_local_masks(void);















/* ioctl command encoding: 32 bits total, command in lower 16 bits,
 * size of the parameter structure in the lower 14 bits of the
 * upper 16 bits.
 * Encoding the size of the parameter structure in the ioctl request
 * is useful for catching programs compiled with old versions
 * and to avoid overwriting user space outside the user buffer area.
 * The highest 2 bits are reserved for indicating the ``access mode''.
 * NOTE: This limits the max parameter size to 16kB -1 !
 */

/*
 * The following is for compatibility across the various Linux
 * platforms.  The generic ioctl numbering scheme doesn't really enforce
 * a type field.  De facto, however, the top 8 bits of the lower 16
 * bits are indeed used as a type field, so we might just as well make
 * this explicit here.  Please be sure to use the decoding macros
 * below from now on.
 */



/*
 * Let any architecture override either of the following before
 * including this file.
 */
/*
 * Direction bits, which any architecture can choose to override
 * before including this file.
 */
/* used to create numbers */
/* used to decode ioctl numbers.. */





/* ...and for the drivers/sound files... */




/* provoke compile error for invalid uses of size argument */
extern unsigned int Model0___invalid_size_argument_for_IOC;

struct Model0_msr {
 union {
  struct {
   Model0_u32 Model0_l;
   Model0_u32 Model0_h;
  };
  Model0_u64 Model0_q;
 };
};

struct Model0_msr_info {
 Model0_u32 Model0_msr_no;
 struct Model0_msr Model0_reg;
 struct Model0_msr *Model0_msrs;
 int err;
};

struct Model0_msr_regs_info {
 Model0_u32 *Model0_regs;
 int err;
};

struct Model0_saved_msr {
 bool Model0_valid;
 struct Model0_msr_info Model0_info;
};

struct Model0_saved_msrs {
 unsigned int Model0_num;
 struct Model0_saved_msr *Model0_array;
};

/*
 * both i386 and x86_64 returns 64-bit value in edx:eax, but gcc's "A"
 * constraint has different meanings. For i386, "A" means exactly
 * edx:eax, while for x86_64 it doesn't mean rdx:rax or edx:eax. Instead,
 * it means rax *or* rdx.
 */

/* Using 64-bit values saves one instruction clearing the high half of low */
/*
 * Be very careful with includes. This header is prone to include loops.
 */














/*
 * Non-existant functions to indicate usage errors at link time
 * (or compile-time if the compiler implements __compiletime_error().
 */
extern void Model0___xchg_wrong_size(void)
                                                  ;
extern void Model0___cmpxchg_wrong_size(void)
                                                     ;
extern void Model0___xadd_wrong_size(void)
                                                  ;
extern void Model0___add_wrong_size(void)
                                                 ;

/*
 * Constants for operation sizes. On 32-bit, the 64-bit size it set to
 * -1 because sizeof will never return -1, thereby making those switch
 * case statements guaranteeed dead code which the compiler will
 * eliminate, and allowing the "missing symbol in the default case" to
 * indicate a usage error.
 */
/* 
 * An exchange-type operation, which takes a value and a pointer, and
 * returns the old value.
 */
/*
 * Note: no "lock" prefix even on SMP: xchg always implies lock anyway.
 * Since this is generally used to protect other memory information, we
 * use "asm volatile" and "memory" clobbers to prevent gcc from moving
 * information around.
 */


/*
 * Atomic compare and exchange.  Compare OLD with MEM, if identical,
 * store NEW in MEM.  Return the initial value in MEM.  Success is
 * indicated by comparing RETURN with OLD.
 */



static inline __attribute__((no_instrument_function)) void Model0_set_64bit(volatile Model0_u64 *Model0_ptr, Model0_u64 Model0_val)
{
 *Model0_ptr = Model0_val;
}
/*
 * xadd() adds "inc" to "*ptr" and atomically returns the previous
 * value of "*ptr".
 *
 * xadd() is locked when multiple CPUs are online
 * xadd_sync() is always locked
 * xadd_local() is never locked
 */
/*
 * add_*() adds "inc" to "*ptr"
 *
 * __add() takes a lock prefix
 * add_smp() is locked when multiple CPUs are online
 * add_sync() is always locked
 */



/*
 * Atomic operations that C can't guarantee us.  Useful for
 * resource counting etc..
 */



/**
 * atomic_read - read atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically reads the value of @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_atomic_read(const Model0_atomic_t *Model0_v)
{
 return ({ union { typeof((Model0_v)->Model0_counter) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_v)->Model0_counter), Model0___u.Model0___c, sizeof((Model0_v)->Model0_counter)); else Model0___read_once_size_nocheck(&((Model0_v)->Model0_counter), Model0___u.Model0___c, sizeof((Model0_v)->Model0_counter)); Model0___u.Model0___val; });
}

/**
 * atomic_set - set atomic variable
 * @v: pointer of type atomic_t
 * @i: required value
 *
 * Atomically sets the value of @v to @i.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_set(Model0_atomic_t *Model0_v, int Model0_i)
{
 ({ union { typeof(Model0_v->Model0_counter) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_v->Model0_counter)) (Model0_i) }; Model0___write_once_size(&(Model0_v->Model0_counter), Model0___u.Model0___c, sizeof(Model0_v->Model0_counter)); Model0___u.Model0___val; });
}

/**
 * atomic_add - add integer to atomic variable
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_add(int Model0_i, Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    Model0_v->Model0_counter += Model0_i;
#else
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addl %1,%0"
       : "+m" (Model0_v->Model0_counter)
       : "ir" (Model0_i));
#endif
}

/**
 * atomic_sub - subtract integer from atomic variable
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_sub(int Model0_i, Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    Model0_v->Model0_counter -= Model0_i;
#else
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "subl %1,%0"
       : "+m" (Model0_v->Model0_counter)
       : "ir" (Model0_i));
#endif
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_atomic_sub_return(int Model0_i, Model0_atomic_t *Model0_v);
/**
 * atomic_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_atomic_sub_and_test(int Model0_i, Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    return Model0_atomic_sub_return(Model0_i, Model0_v) == 0;
#else
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "subl" " %2, " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model0_v->Model0_counter), [_cc_e] "=qm" (Model0_c) : "er" (Model0_i) : "memory"); return Model0_c; } while (0);
#endif
}

/**
 * atomic_inc - increment atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_inc(Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    ++(Model0_v->Model0_counter);
#else
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "incl %0"
       : "+m" (Model0_v->Model0_counter));
#endif
}

/**
 * atomic_dec - decrement atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_dec(Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    --(Model0_v->Model0_counter);
#else
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "decl %0"
       : "+m" (Model0_v->Model0_counter));
#endif
}

/**
 * atomic_dec_and_test - decrement and test
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_atomic_dec_and_test(Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    return --(Model0_v->Model0_counter) == 0;
#else
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "decl" " " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model0_v->Model0_counter), [_cc_e] "=qm" (Model0_c) : : "memory"); return Model0_c; } while (0);
#endif
}

/**
 * atomic_inc_and_test - increment and test
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_atomic_inc_and_test(Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    return ++(Model0_v->Model0_counter) == 0;
#else
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "incl" " " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model0_v->Model0_counter), [_cc_e] "=qm" (Model0_c) : : "memory"); return Model0_c; } while (0);
#endif
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_atomic_add_return(int Model0_i, Model0_atomic_t *Model0_v);
/**
 * atomic_add_negative - add and test if negative
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_atomic_add_negative(int Model0_i, Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    return Model0_atomic_add_return(Model0_i, Model0_v) < 0;
#else
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addl" " %2, " "%0" ";" "\n\tset" "s" " %[_cc_" "s" "]\n" : "+m" (Model0_v->Model0_counter), [_cc_s] "=qm" (Model0_c) : "er" (Model0_i) : "memory"); return Model0_c; } while (0);
#endif
}

/**
 * atomic_add_return - add integer and return
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v and returns @i + @v
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_atomic_add_return(int Model0_i, Model0_atomic_t *Model0_v)
{
#if CY_ABSTRACT6
    return Model0_v->Model0_counter += Model0_i;
#else
 return Model0_i + ({ __typeof__ (*(((&Model0_v->Model0_counter)))) Model0___ret = (((Model0_i))); switch (sizeof(*(((&Model0_v->Model0_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; default: Model0___xadd_wrong_size(); } Model0___ret; });
#endif
}

/**
 * atomic_sub_return - subtract integer and return
 * @v: pointer of type atomic_t
 * @i: integer value to subtract
 *
 * Atomically subtracts @i from @v and returns @v - @i
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_atomic_sub_return(int Model0_i, Model0_atomic_t *Model0_v)
{
 return Model0_atomic_add_return(-Model0_i, Model0_v);
}




static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_atomic_fetch_add(int Model0_i, Model0_atomic_t *Model0_v)
{
 return ({ __typeof__ (*(((&Model0_v->Model0_counter)))) Model0___ret = (((Model0_i))); switch (sizeof(*(((&Model0_v->Model0_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; default: Model0___xadd_wrong_size(); } Model0___ret; });
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_atomic_fetch_sub(int Model0_i, Model0_atomic_t *Model0_v)
{
 return ({ __typeof__ (*(((&Model0_v->Model0_counter)))) Model0___ret = (((-Model0_i))); switch (sizeof(*(((&Model0_v->Model0_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; default: Model0___xadd_wrong_size(); } Model0___ret; });
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_atomic_cmpxchg(Model0_atomic_t *Model0_v, int old, int Model0_new)
{
#if CY_ABSTRACT6
    //reference [https://www.khronos.org/registry/OpenCL/sdk/1.1/docs/man/xhtml/atomic_cmpxchg.html]
    printf("Model0_new: %d\n", Model0_new);
    printf("old: %d\n", old);
    Model0_v->Model0_counter = (Model0_atomic_read(Model0_v) == old)? Model0_new : old;
    return old;
#else
 return ({ __typeof__(*((&Model0_v->Model0_counter))) Model0___ret; __typeof__(*((&Model0_v->Model0_counter))) Model0___old = ((old)); __typeof__(*((&Model0_v->Model0_counter))) Model0___new = ((Model0_new)); switch ((sizeof(*(&Model0_v->Model0_counter)))) { case 1: { volatile Model0_u8 *Model0___ptr = (volatile Model0_u8 *)((&Model0_v->Model0_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgb %2,%1" : "=a" (Model0___ret), "+m" (*Model0___ptr) : "q" (Model0___new), "0" (Model0___old) : "memory"); break; } case 2: { volatile Model0_u16 *Model0___ptr = (volatile Model0_u16 *)((&Model0_v->Model0_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgw %2,%1" : "=a" (Model0___ret), "+m" (*Model0___ptr) : "r" (Model0___new), "0" (Model0___old) : "memory"); break; } case 4: { volatile Model0_u32 *Model0___ptr = (volatile Model0_u32 *)((&Model0_v->Model0_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgl %2,%1" : "=a" (Model0___ret), "+m" (*Model0___ptr) : "r" (Model0___new), "0" (Model0___old) : "memory"); break; } case 8: { volatile Model0_u64 *Model0___ptr = (volatile Model0_u64 *)((&Model0_v->Model0_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgq %2,%1" : "=a" (Model0___ret), "+m" (*Model0___ptr) : "r" (Model0___new), "0" (Model0___old) : "memory"); break; } default: Model0___cmpxchg_wrong_size(); } Model0___ret; });
#endif
}

static inline __attribute__((no_instrument_function)) int Model0_atomic_xchg(Model0_atomic_t *Model0_v, int Model0_new)
{
 return ({ __typeof__ (*((&Model0_v->Model0_counter))) Model0___ret = ((Model0_new)); switch (sizeof(*((&Model0_v->Model0_counter)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*((&Model0_v->Model0_counter))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_v->Model0_counter))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_v->Model0_counter))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_v->Model0_counter))) : : "memory", "cc"); break; default: Model0___xchg_wrong_size(); } Model0___ret; });
}
static inline __attribute__((no_instrument_function)) void Model0_atomic_and(int Model0_i, Model0_atomic_t *Model0_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "and""l %1,%0" : "+m" (Model0_v->Model0_counter) : "ir" (Model0_i) : "memory"); } static inline __attribute__((no_instrument_function)) int Model0_atomic_fetch_and(int Model0_i, Model0_atomic_t *Model0_v) { int old, Model0_val = Model0_atomic_read(Model0_v); for (;;) { old = Model0_atomic_cmpxchg(Model0_v, Model0_val, Model0_val & Model0_i); if (old == Model0_val) break; Model0_val = old; } return old; }
static inline __attribute__((no_instrument_function)) void Model0_atomic_or(int Model0_i, Model0_atomic_t *Model0_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "or""l %1,%0" : "+m" (Model0_v->Model0_counter) : "ir" (Model0_i) : "memory"); } static inline __attribute__((no_instrument_function)) int Model0_atomic_fetch_or(int Model0_i, Model0_atomic_t *Model0_v) { int old, Model0_val = Model0_atomic_read(Model0_v); for (;;) { old = Model0_atomic_cmpxchg(Model0_v, Model0_val, Model0_val | Model0_i); if (old == Model0_val) break; Model0_val = old; } return old; }
static inline __attribute__((no_instrument_function)) void Model0_atomic_xor(int Model0_i, Model0_atomic_t *Model0_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xor""l %1,%0" : "+m" (Model0_v->Model0_counter) : "ir" (Model0_i) : "memory"); } static inline __attribute__((no_instrument_function)) int Model0_atomic_fetch_xor(int Model0_i, Model0_atomic_t *Model0_v) { int old, Model0_val = Model0_atomic_read(Model0_v); for (;;) { old = Model0_atomic_cmpxchg(Model0_v, Model0_val, Model0_val ^ Model0_i); if (old == Model0_val) break; Model0_val = old; } return old; }





/**
 * __atomic_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as @v was not already @u.
 * Returns the old value of @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0___atomic_add_unless(Model0_atomic_t *Model0_v, int Model0_a, int Model0_u)
{
 int Model0_c, old;
 Model0_c = Model0_atomic_read(Model0_v);
 for (;;) {
  if (__builtin_expect(!!(Model0_c == (Model0_u)), 0))
   break;
  old = Model0_atomic_cmpxchg((Model0_v), Model0_c, Model0_c + (Model0_a));
  if (__builtin_expect(!!(old == Model0_c), 1))
   break;
  Model0_c = old;
 }
 return Model0_c;
}

/**
 * atomic_inc_short - increment of a short integer
 * @v: pointer to type int
 *
 * Atomically adds 1 to @v
 * Returns the new value of @u
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) short int Model0_atomic_inc_short(short int *Model0_v)
{
 asm(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addw $1, %0" : "+m" (*Model0_v));
 return *Model0_v;
}












/* The 64-bit atomic type */



/**
 * atomic64_read - read atomic64 variable
 * @v: pointer of type atomic64_t
 *
 * Atomically reads the value of @v.
 * Doesn't imply a read memory barrier.
 */
static inline __attribute__((no_instrument_function)) long Model0_atomic64_read(const Model0_atomic64_t *Model0_v)
{
 return ({ union { typeof((Model0_v)->Model0_counter) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_v)->Model0_counter), Model0___u.Model0___c, sizeof((Model0_v)->Model0_counter)); else Model0___read_once_size_nocheck(&((Model0_v)->Model0_counter), Model0___u.Model0___c, sizeof((Model0_v)->Model0_counter)); Model0___u.Model0___val; });
}

/**
 * atomic64_set - set atomic64 variable
 * @v: pointer to type atomic64_t
 * @i: required value
 *
 * Atomically sets the value of @v to @i.
 */
static inline __attribute__((no_instrument_function)) void Model0_atomic64_set(Model0_atomic64_t *Model0_v, long Model0_i)
{
 ({ union { typeof(Model0_v->Model0_counter) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_v->Model0_counter)) (Model0_i) }; Model0___write_once_size(&(Model0_v->Model0_counter), Model0___u.Model0___c, sizeof(Model0_v->Model0_counter)); Model0___u.Model0___val; });
}

/**
 * atomic64_add - add integer to atomic64 variable
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic64_add(long Model0_i, Model0_atomic64_t *Model0_v)
{
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addq %1,%0"
       : "=m" (Model0_v->Model0_counter)
       : "er" (Model0_i), "m" (Model0_v->Model0_counter));
}

/**
 * atomic64_sub - subtract the atomic64 variable
 * @i: integer value to subtract
 * @v: pointer to type atomic64_t
 *
 * Atomically subtracts @i from @v.
 */
static inline __attribute__((no_instrument_function)) void Model0_atomic64_sub(long Model0_i, Model0_atomic64_t *Model0_v)
{
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "subq %1,%0"
       : "=m" (Model0_v->Model0_counter)
       : "er" (Model0_i), "m" (Model0_v->Model0_counter));
}

static inline __attribute__((no_instrument_function)) long Model0_atomic64_sub_return(long Model0_i, Model0_atomic64_t *Model0_v);
/**
 * atomic64_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer to type atomic64_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((no_instrument_function)) bool Model0_atomic64_sub_and_test(long Model0_i, Model0_atomic64_t *Model0_v)
{
#if CY_ABSTRACT6
    return Model0_atomic64_sub_return(Model0_i, Model0_v) == 0;
#else
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "subq" " %2, " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model0_v->Model0_counter), [_cc_e] "=qm" (Model0_c) : "er" (Model0_i) : "memory"); return Model0_c; } while (0);
#endif
}

/**
 * atomic64_inc - increment atomic64 variable
 * @v: pointer to type atomic64_t
 *
 * Atomically increments @v by 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic64_inc(Model0_atomic64_t *Model0_v)
{
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "incq %0"
       : "=m" (Model0_v->Model0_counter)
       : "m" (Model0_v->Model0_counter));
}

/**
 * atomic64_dec - decrement atomic64 variable
 * @v: pointer to type atomic64_t
 *
 * Atomically decrements @v by 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic64_dec(Model0_atomic64_t *Model0_v)
{
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "decq %0"
       : "=m" (Model0_v->Model0_counter)
       : "m" (Model0_v->Model0_counter));
}

/**
 * atomic64_dec_and_test - decrement and test
 * @v: pointer to type atomic64_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static inline __attribute__((no_instrument_function)) bool Model0_atomic64_dec_and_test(Model0_atomic64_t *Model0_v)
{
#if CY_ABSTRACT6
    return --(Model0_v->Model0_counter) == 0;
#else
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "decq" " " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model0_v->Model0_counter), [_cc_e] "=qm" (Model0_c) : : "memory"); return Model0_c; } while (0);
#endif
}

/**
 * atomic64_inc_and_test - increment and test
 * @v: pointer to type atomic64_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((no_instrument_function)) bool Model0_atomic64_inc_and_test(Model0_atomic64_t *Model0_v)
{
#if CY_ABSTRACT6
    return ++(Model0_v->Model0_counter) == 0;
#else
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "incq" " " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model0_v->Model0_counter), [_cc_e] "=qm" (Model0_c) : : "memory"); return Model0_c; } while (0);
#endif
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) long Model0_atomic64_add_return(long Model0_i, Model0_atomic64_t *Model0_v);
/**
 * atomic64_add_negative - add and test if negative
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static inline __attribute__((no_instrument_function)) bool Model0_atomic64_add_negative(long Model0_i, Model0_atomic64_t *Model0_v)
{
#if CY_ABSTRACT6
    return Model0_atomic64_add_return(Model0_i, Model0_v) < 0;
#else
 do { bool Model0_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addq" " %2, " "%0" ";" "\n\tset" "s" " %[_cc_" "s" "]\n" : "+m" (Model0_v->Model0_counter), [_cc_s] "=qm" (Model0_c) : "er" (Model0_i) : "memory"); return Model0_c; } while (0);
#endif
}

/**
 * atomic64_add_return - add and return
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v and returns @i + @v
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) long Model0_atomic64_add_return(long Model0_i, Model0_atomic64_t *Model0_v)
{
#if CY_ABSTRACT6
    Model0_v->Model0_counter += Model0_i;
    return Model0_v->Model0_counter;
#else
 return Model0_i + ({ __typeof__ (*(((&Model0_v->Model0_counter)))) Model0___ret = (((Model0_i))); switch (sizeof(*(((&Model0_v->Model0_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; default: Model0___xadd_wrong_size(); } Model0___ret; });
#endif
}

static inline __attribute__((no_instrument_function)) long Model0_atomic64_sub_return(long Model0_i, Model0_atomic64_t *Model0_v)
{
 return Model0_atomic64_add_return(-Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) long Model0_atomic64_fetch_add(long Model0_i, Model0_atomic64_t *Model0_v)
{
 return ({ __typeof__ (*(((&Model0_v->Model0_counter)))) Model0___ret = (((Model0_i))); switch (sizeof(*(((&Model0_v->Model0_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; default: Model0___xadd_wrong_size(); } Model0___ret; });
}

static inline __attribute__((no_instrument_function)) long Model0_atomic64_fetch_sub(long Model0_i, Model0_atomic64_t *Model0_v)
{
 return ({ __typeof__ (*(((&Model0_v->Model0_counter)))) Model0___ret = (((-Model0_i))); switch (sizeof(*(((&Model0_v->Model0_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*(((&Model0_v->Model0_counter)))) : : "memory", "cc"); break; default: Model0___xadd_wrong_size(); } Model0___ret; });
}




static inline __attribute__((no_instrument_function)) long Model0_atomic64_cmpxchg(Model0_atomic64_t *Model0_v, long old, long Model0_new)
{
 return ({ __typeof__(*((&Model0_v->Model0_counter))) Model0___ret; __typeof__(*((&Model0_v->Model0_counter))) Model0___old = ((old)); __typeof__(*((&Model0_v->Model0_counter))) Model0___new = ((Model0_new)); switch ((sizeof(*(&Model0_v->Model0_counter)))) { case 1: { volatile Model0_u8 *Model0___ptr = (volatile Model0_u8 *)((&Model0_v->Model0_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgb %2,%1" : "=a" (Model0___ret), "+m" (*Model0___ptr) : "q" (Model0___new), "0" (Model0___old) : "memory"); break; } case 2: { volatile Model0_u16 *Model0___ptr = (volatile Model0_u16 *)((&Model0_v->Model0_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgw %2,%1" : "=a" (Model0___ret), "+m" (*Model0___ptr) : "r" (Model0___new), "0" (Model0___old) : "memory"); break; } case 4: { volatile Model0_u32 *Model0___ptr = (volatile Model0_u32 *)((&Model0_v->Model0_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgl %2,%1" : "=a" (Model0___ret), "+m" (*Model0___ptr) : "r" (Model0___new), "0" (Model0___old) : "memory"); break; } case 8: { volatile Model0_u64 *Model0___ptr = (volatile Model0_u64 *)((&Model0_v->Model0_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgq %2,%1" : "=a" (Model0___ret), "+m" (*Model0___ptr) : "r" (Model0___new), "0" (Model0___old) : "memory"); break; } default: Model0___cmpxchg_wrong_size(); } Model0___ret; });
}

static inline __attribute__((no_instrument_function)) long Model0_atomic64_xchg(Model0_atomic64_t *Model0_v, long Model0_new)
{
 return ({ __typeof__ (*((&Model0_v->Model0_counter))) Model0___ret = ((Model0_new)); switch (sizeof(*((&Model0_v->Model0_counter)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*((&Model0_v->Model0_counter))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_v->Model0_counter))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_v->Model0_counter))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_v->Model0_counter))) : : "memory", "cc"); break; default: Model0___xchg_wrong_size(); } Model0___ret; });
}

/**
 * atomic64_add_unless - add unless the number is a given value
 * @v: pointer of type atomic64_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as it was not @u.
 * Returns the old value of @v.
 */
static inline __attribute__((no_instrument_function)) bool Model0_atomic64_add_unless(Model0_atomic64_t *Model0_v, long Model0_a, long Model0_u)
{
 long Model0_c, old;
 Model0_c = Model0_atomic64_read(Model0_v);
 for (;;) {
  if (__builtin_expect(!!(Model0_c == (Model0_u)), 0))
   break;
  old = Model0_atomic64_cmpxchg((Model0_v), Model0_c, Model0_c + (Model0_a));
  if (__builtin_expect(!!(old == Model0_c), 1))
   break;
  Model0_c = old;
 }
 return Model0_c != (Model0_u);
}



/*
 * atomic64_dec_if_positive - decrement by 1 if old value positive
 * @v: pointer of type atomic_t
 *
 * The function returns the old value of *v minus 1, even if
 * the atomic variable, v, was not decremented.
 */
static inline __attribute__((no_instrument_function)) long Model0_atomic64_dec_if_positive(Model0_atomic64_t *Model0_v)
{
 long Model0_c, old, Model0_dec;
 Model0_c = Model0_atomic64_read(Model0_v);
 for (;;) {
  Model0_dec = Model0_c - 1;
  if (__builtin_expect(!!(Model0_dec < 0), 0))
   break;
  old = Model0_atomic64_cmpxchg((Model0_v), Model0_c, Model0_dec);
  if (__builtin_expect(!!(old == Model0_c), 1))
   break;
  Model0_c = old;
 }
 return Model0_dec;
}
static inline __attribute__((no_instrument_function)) void Model0_atomic64_and(long Model0_i, Model0_atomic64_t *Model0_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "and""q %1,%0" : "+m" (Model0_v->Model0_counter) : "er" (Model0_i) : "memory"); } static inline __attribute__((no_instrument_function)) long Model0_atomic64_fetch_and(long Model0_i, Model0_atomic64_t *Model0_v) { long old, Model0_val = Model0_atomic64_read(Model0_v); for (;;) { old = Model0_atomic64_cmpxchg(Model0_v, Model0_val, Model0_val & Model0_i); if (old == Model0_val) break; Model0_val = old; } return old; }
static inline __attribute__((no_instrument_function)) void Model0_atomic64_or(long Model0_i, Model0_atomic64_t *Model0_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "or""q %1,%0" : "+m" (Model0_v->Model0_counter) : "er" (Model0_i) : "memory"); } static inline __attribute__((no_instrument_function)) long Model0_atomic64_fetch_or(long Model0_i, Model0_atomic64_t *Model0_v) { long old, Model0_val = Model0_atomic64_read(Model0_v); for (;;) { old = Model0_atomic64_cmpxchg(Model0_v, Model0_val, Model0_val | Model0_i); if (old == Model0_val) break; Model0_val = old; } return old; }
static inline __attribute__((no_instrument_function)) void Model0_atomic64_xor(long Model0_i, Model0_atomic64_t *Model0_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xor""q %1,%0" : "+m" (Model0_v->Model0_counter) : "er" (Model0_i) : "memory"); } static inline __attribute__((no_instrument_function)) long Model0_atomic64_fetch_xor(long Model0_i, Model0_atomic64_t *Model0_v) { long old, Model0_val = Model0_atomic64_read(Model0_v); for (;;) { old = Model0_atomic64_cmpxchg(Model0_v, Model0_val, Model0_val ^ Model0_i); if (old == Model0_val) break; Model0_val = old; } return old; }



/*
 * File can be included directly by headers who only want to access
 * tracepoint->key to guard out of line trace calls, or the definition of
 * trace_print_flags{_u64}. Otherwise linux/tracepoint.h should be used.
 */


/* Atomic operations usable in machine independent code */





/*
 * Relaxed variants of xchg, cmpxchg and some atomic operations.
 *
 * We support four variants:
 *
 * - Fully ordered: The default implementation, no suffix required.
 * - Acquire: Provides ACQUIRE semantics, _acquire suffix.
 * - Release: Provides RELEASE semantics, _release suffix.
 * - Relaxed: No ordering guarantees, _relaxed suffix.
 *
 * For compound atomics performing both a load and a store, ACQUIRE
 * semantics apply only to the load and RELEASE semantics only to the
 * store portion of the operation. Note that a failed cmpxchg_acquire
 * does -not- imply any memory ordering constraints.
 *
 * See Documentation/memory-barriers.txt for ACQUIRE/RELEASE definitions.
 */
/*
 * The idea here is to build acquire/release variants by adding explicit
 * barriers on top of the relaxed variant. In the case where the relaxed
 * variant is already fully ordered, no additional barriers are needed.
 *
 * Besides, if an arch has a special barrier for acquire/release, it could
 * implement its own __atomic_op_* and use the same framework for building
 * variants
 */
/* atomic_add_return_relaxed */
/* atomic_inc_return_relaxed */
/* atomic_sub_return_relaxed */
/* atomic_dec_return_relaxed */
/* atomic_fetch_add_relaxed */
/* atomic_fetch_inc_relaxed */
/* atomic_fetch_sub_relaxed */
/* atomic_fetch_dec_relaxed */
/* atomic_fetch_or_relaxed */
/* atomic_fetch_and_relaxed */
/* atomic_fetch_xor_relaxed */
/* atomic_xchg_relaxed */
/* atomic_cmpxchg_relaxed */
/* cmpxchg_relaxed */
/* cmpxchg64_relaxed */
/* xchg_relaxed */
/**
 * atomic_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as @v was not already @u.
 * Returns non-zero if @v was not @u, and zero otherwise.
 */
static inline __attribute__((no_instrument_function)) int Model0_atomic_add_unless(Model0_atomic_t *Model0_v, int Model0_a, int Model0_u)
{
 return Model0___atomic_add_unless(Model0_v, Model0_a, Model0_u) != Model0_u;
}

/**
 * atomic_inc_not_zero - increment unless the number is zero
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1, so long as @v is non-zero.
 * Returns non-zero if @v was non-zero, and zero otherwise.
 */





static inline __attribute__((no_instrument_function)) void Model0_atomic_andnot(int Model0_i, Model0_atomic_t *Model0_v)
{
 Model0_atomic_and(~Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) int Model0_atomic_fetch_andnot(int Model0_i, Model0_atomic_t *Model0_v)
{
 return Model0_atomic_fetch_and(~Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) int Model0_atomic_fetch_andnot_relaxed(int Model0_i, Model0_atomic_t *Model0_v)
{
 return Model0_atomic_fetch_and(~Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) int Model0_atomic_fetch_andnot_acquire(int Model0_i, Model0_atomic_t *Model0_v)
{
 return Model0_atomic_fetch_and(~Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) int Model0_atomic_fetch_andnot_release(int Model0_i, Model0_atomic_t *Model0_v)
{
 return Model0_atomic_fetch_and(~Model0_i, Model0_v);
}


/**
 * atomic_inc_not_zero_hint - increment if not null
 * @v: pointer of type atomic_t
 * @hint: probable value of the atomic before the increment
 *
 * This version of atomic_inc_not_zero() gives a hint of probable
 * value of the atomic. This helps processor to not read the memory
 * before doing the atomic read/modify/write cycle, lowering
 * number of bus transactions on some arches.
 *
 * Returns: 0 if increment was not done, 1 otherwise.
 */

static inline __attribute__((no_instrument_function)) int Model0_atomic_inc_not_zero_hint(Model0_atomic_t *Model0_v, int Model0_hint)
{
 int Model0_val, Model0_c = Model0_hint;

 /* sanity test, should be removed by compiler if hint is a constant */
 if (!Model0_hint)
  return Model0_atomic_add_unless((Model0_v), 1, 0);

 do {
  Model0_val = Model0_atomic_cmpxchg(Model0_v, Model0_c, Model0_c + 1);
  if (Model0_val == Model0_c)
   return 1;
  Model0_c = Model0_val;
 } while (Model0_c);

 return 0;
}



static inline __attribute__((no_instrument_function)) int Model0_atomic_inc_unless_negative(Model0_atomic_t *Model0_p)
{
 int Model0_v, Model0_v1;
 for (Model0_v = 0; Model0_v >= 0; Model0_v = Model0_v1) {
  Model0_v1 = Model0_atomic_cmpxchg(Model0_p, Model0_v, Model0_v + 1);
  if (__builtin_expect(!!(Model0_v1 == Model0_v), 1))
   return 1;
 }
 return 0;
}



static inline __attribute__((no_instrument_function)) int Model0_atomic_dec_unless_positive(Model0_atomic_t *Model0_p)
{
 int Model0_v, Model0_v1;
 for (Model0_v = 0; Model0_v <= 0; Model0_v = Model0_v1) {
  Model0_v1 = Model0_atomic_cmpxchg(Model0_p, Model0_v, Model0_v - 1);
  if (__builtin_expect(!!(Model0_v1 == Model0_v), 1))
   return 1;
 }
 return 0;
}


/*
 * atomic_dec_if_positive - decrement by 1 if old value positive
 * @v: pointer of type atomic_t
 *
 * The function returns the old value of *v minus 1, even if
 * the atomic variable, v, was not decremented.
 */

static inline __attribute__((no_instrument_function)) int Model0_atomic_dec_if_positive(Model0_atomic_t *Model0_v)
{
 int Model0_c, old, Model0_dec;
 Model0_c = Model0_atomic_read(Model0_v);
 for (;;) {
  Model0_dec = Model0_c - 1;
  if (__builtin_expect(!!(Model0_dec < 0), 0))
   break;
  old = Model0_atomic_cmpxchg((Model0_v), Model0_c, Model0_dec);
  if (__builtin_expect(!!(old == Model0_c), 1))
   break;
  Model0_c = old;
 }
 return Model0_dec;
}
/* atomic64_add_return_relaxed */
/* atomic64_inc_return_relaxed */
/* atomic64_sub_return_relaxed */
/* atomic64_dec_return_relaxed */
/* atomic64_fetch_add_relaxed */
/* atomic64_fetch_inc_relaxed */
/* atomic64_fetch_sub_relaxed */
/* atomic64_fetch_dec_relaxed */
/* atomic64_fetch_or_relaxed */
/* atomic64_fetch_and_relaxed */
/* atomic64_fetch_xor_relaxed */
/* atomic64_xchg_relaxed */
/* atomic64_cmpxchg_relaxed */
static inline __attribute__((no_instrument_function)) void Model0_atomic64_andnot(long long Model0_i, Model0_atomic64_t *Model0_v)
{
 Model0_atomic64_and(~Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) long long Model0_atomic64_fetch_andnot(long long Model0_i, Model0_atomic64_t *Model0_v)
{
 return Model0_atomic64_fetch_and(~Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) long long Model0_atomic64_fetch_andnot_relaxed(long long Model0_i, Model0_atomic64_t *Model0_v)
{
 return Model0_atomic64_fetch_and(~Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) long long Model0_atomic64_fetch_andnot_acquire(long long Model0_i, Model0_atomic64_t *Model0_v)
{
 return Model0_atomic64_fetch_and(~Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) long long Model0_atomic64_fetch_andnot_release(long long Model0_i, Model0_atomic64_t *Model0_v)
{
 return Model0_atomic64_fetch_and(~Model0_i, Model0_v);
}





/*
 * Copyright (C) 2005 Silicon Graphics, Inc.
 *	Christoph Lameter
 *
 * Allows to provide arch independent atomic definitions without the need to
 * edit all arch specific atomic.h files.
 */



/*
 * Suppport for atomic_long_t
 *
 * Casts for parameters are avoided for existing atomic functions in order to
 * avoid issues with cast-as-lval under gcc 4.x and other limitations that the
 * macros of a platform may have.
 */



typedef Model0_atomic64_t Model0_atomic_long_t;
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_read(const Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_read(Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_read_acquire(const Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)({ typeof(*&(Model0_v)->Model0_counter) Model0____p1 = ({ union { typeof(*&(Model0_v)->Model0_counter) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(*&(Model0_v)->Model0_counter), Model0___u.Model0___c, sizeof(*&(Model0_v)->Model0_counter)); else Model0___read_once_size_nocheck(&(*&(Model0_v)->Model0_counter), Model0___u.Model0___c, sizeof(*&(Model0_v)->Model0_counter)); Model0___u.Model0___val; }); do { bool Model0___cond = !((sizeof(*&(Model0_v)->Model0_counter) == sizeof(char) || sizeof(*&(Model0_v)->Model0_counter) == sizeof(short) || sizeof(*&(Model0_v)->Model0_counter) == sizeof(int) || sizeof(*&(Model0_v)->Model0_counter) == sizeof(long))); extern void Model0___compiletime_assert_45(void) ; if (Model0___cond) Model0___compiletime_assert_45(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); Model0____p1; }); }
static inline __attribute__((no_instrument_function)) void Model0_atomic_long_set(Model0_atomic_long_t *Model0_l, long Model0_i) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; Model0_atomic64_set(Model0_v, Model0_i); }
static inline __attribute__((no_instrument_function)) void Model0_atomic_long_set_release(Model0_atomic_long_t *Model0_l, long Model0_i) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; do { do { bool Model0___cond = !((sizeof(*&(Model0_v)->Model0_counter) == sizeof(char) || sizeof(*&(Model0_v)->Model0_counter) == sizeof(short) || sizeof(*&(Model0_v)->Model0_counter) == sizeof(int) || sizeof(*&(Model0_v)->Model0_counter) == sizeof(long))); extern void Model0___compiletime_assert_57(void) ; if (Model0___cond) Model0___compiletime_assert_57(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(Model0_v)->Model0_counter) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(Model0_v)->Model0_counter)) ((Model0_i)) }; Model0___write_once_size(&(*&(Model0_v)->Model0_counter), Model0___u.Model0___c, sizeof(*&(Model0_v)->Model0_counter)); Model0___u.Model0___val; }); } while (0); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_add_return(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_add_return(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_add_return_relaxed(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_add_return(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_add_return_acquire(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_add_return(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_add_return_release(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_add_return(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_sub_return(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_sub_return(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_sub_return_relaxed(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_sub_return(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_sub_return_acquire(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_sub_return(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_sub_return_release(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_sub_return(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_long_inc(Model0_atomic_long_t *Model0_l)
{
 Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l;

 Model0_atomic64_inc(Model0_v);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_long_dec(Model0_atomic_long_t *Model0_l)
{
 Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l;

 Model0_atomic64_dec(Model0_v);
}
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_add(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_add(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_add_relaxed(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_add(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_add_acquire(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_add(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_add_release(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_add(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_sub(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_sub(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_sub_relaxed(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_sub(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_sub_acquire(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_sub(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_sub_release(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_sub(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_and(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_and(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_and_relaxed(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_and(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_and_acquire(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_and(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_and_release(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_and(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_andnot(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_andnot(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_andnot_relaxed(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_andnot_relaxed(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_andnot_acquire(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_andnot_acquire(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_andnot_release(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_andnot_release(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_or(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_or(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_or_relaxed(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_or(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_or_acquire(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_or(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_or_release(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_or(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_xor(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_xor(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_xor_relaxed(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_xor(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_xor_acquire(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_xor(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_xor_release(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_xor(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_inc(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_add(1, (Model0_v)); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_inc_relaxed(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_add(1, (Model0_v)); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_inc_acquire(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_add(1, (Model0_v)); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_inc_release(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_add(1, (Model0_v)); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_dec(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_sub(1, (Model0_v)); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_dec_relaxed(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_sub(1, (Model0_v)); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_dec_acquire(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_sub(1, (Model0_v)); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_fetch_dec_release(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)Model0_atomic64_fetch_sub(1, (Model0_v)); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_long_add(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; Model0_atomic64_add(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_long_sub(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; Model0_atomic64_sub(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_long_and(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; Model0_atomic64_and(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_long_andnot(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; Model0_atomic64_andnot(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_long_or(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; Model0_atomic64_or(Model0_i, Model0_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_atomic_long_xor(long Model0_i, Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; Model0_atomic64_xor(Model0_i, Model0_v); }



static inline __attribute__((no_instrument_function)) int Model0_atomic_long_sub_and_test(long Model0_i, Model0_atomic_long_t *Model0_l)
{
 Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l;

 return Model0_atomic64_sub_and_test(Model0_i, Model0_v);
}

static inline __attribute__((no_instrument_function)) int Model0_atomic_long_dec_and_test(Model0_atomic_long_t *Model0_l)
{
 Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l;

 return Model0_atomic64_dec_and_test(Model0_v);
}

static inline __attribute__((no_instrument_function)) int Model0_atomic_long_inc_and_test(Model0_atomic_long_t *Model0_l)
{
 Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l;

 return Model0_atomic64_inc_and_test(Model0_v);
}

static inline __attribute__((no_instrument_function)) int Model0_atomic_long_add_negative(long Model0_i, Model0_atomic_long_t *Model0_l)
{
 Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l;

 return Model0_atomic64_add_negative(Model0_i, Model0_v);
}
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_inc_return(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)(Model0_atomic64_add_return(1, (Model0_v))); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_inc_return_relaxed(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)(Model0_atomic64_add_return(1, (Model0_v))); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_inc_return_acquire(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)(Model0_atomic64_add_return(1, (Model0_v))); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_inc_return_release(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)(Model0_atomic64_add_return(1, (Model0_v))); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_dec_return(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)(Model0_atomic64_sub_return(1, (Model0_v))); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_dec_return_relaxed(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)(Model0_atomic64_sub_return(1, (Model0_v))); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_dec_return_acquire(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)(Model0_atomic64_sub_return(1, (Model0_v))); }
static inline __attribute__((no_instrument_function)) long Model0_atomic_long_dec_return_release(Model0_atomic_long_t *Model0_l) { Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l; return (long)(Model0_atomic64_sub_return(1, (Model0_v))); }



static inline __attribute__((no_instrument_function)) long Model0_atomic_long_add_unless(Model0_atomic_long_t *Model0_l, long Model0_a, long Model0_u)
{
 Model0_atomic64_t *Model0_v = (Model0_atomic64_t *)Model0_l;

 return (long)Model0_atomic64_add_unless(Model0_v, Model0_a, Model0_u);
}



/*
 * Jump label support
 *
 * Copyright (C) 2009-2012 Jason Baron <jbaron@redhat.com>
 * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra
 *
 * DEPRECATED API:
 *
 * The use of 'struct static_key' directly, is now DEPRECATED. In addition
 * static_key_{true,false}() is also DEPRECATED. IE DO NOT use the following:
 *
 * struct static_key false = STATIC_KEY_INIT_FALSE;
 * struct static_key true = STATIC_KEY_INIT_TRUE;
 * static_key_true()
 * static_key_false()
 *
 * The updated API replacements are:
 *
 * DEFINE_STATIC_KEY_TRUE(key);
 * DEFINE_STATIC_KEY_FALSE(key);
 * static_branch_likely()
 * static_branch_unlikely()
 *
 * Jump labels provide an interface to generate dynamic branches using
 * self-modifying code. Assuming toolchain and architecture support, if we
 * define a "key" that is initially false via "DEFINE_STATIC_KEY_FALSE(key)",
 * an "if (static_branch_unlikely(&key))" statement is an unconditional branch
 * (which defaults to false - and the true block is placed out of line).
 * Similarly, we can define an initially true key via
 * "DEFINE_STATIC_KEY_TRUE(key)", and use it in the same
 * "if (static_branch_unlikely(&key))", in which case we will generate an
 * unconditional branch to the out-of-line true branch. Keys that are
 * initially true or false can be using in both static_branch_unlikely()
 * and static_branch_likely() statements.
 *
 * At runtime we can change the branch target by setting the key
 * to true via a call to static_branch_enable(), or false using
 * static_branch_disable(). If the direction of the branch is switched by
 * these calls then we run-time modify the branch target via a
 * no-op -> jump or jump -> no-op conversion. For example, for an
 * initially false key that is used in an "if (static_branch_unlikely(&key))"
 * statement, setting the key to true requires us to patch in a jump
 * to the out-of-line of true branch.
 *
 * In addition to static_branch_{enable,disable}, we can also reference count
 * the key or branch direction via static_branch_{inc,dec}. Thus,
 * static_branch_inc() can be thought of as a 'make more true' and
 * static_branch_dec() as a 'make more false'.
 *
 * Since this relies on modifying code, the branch modifying functions
 * must be considered absolute slow paths (machine wide synchronization etc.).
 * OTOH, since the affected branches are unconditional, their runtime overhead
 * will be absolutely minimal, esp. in the default (off) case where the total
 * effect is a single NOP of appropriate size. The on case will patch in a jump
 * to the out-of-line block.
 *
 * When the control is directly exposed to userspace, it is prudent to delay the
 * decrement to avoid high frequency code modifications which can (and do)
 * cause significant performance degradation. Struct static_key_deferred and
 * static_key_slow_dec_deferred() provide for this.
 *
 * Lacking toolchain and or architecture support, static keys fall back to a
 * simple conditional branch.
 *
 * Additional babbling in: Documentation/static-keys.txt
 */
extern bool Model0_static_key_initialized;
struct Model0_static_key {
 Model0_atomic_t Model0_enabled;
};
enum Model0_jump_label_type {
 Model0_JUMP_LABEL_NOP = 0,
 Model0_JUMP_LABEL_JMP,
};

struct Model0_module;
static inline __attribute__((no_instrument_function)) int Model0_static_key_count(struct Model0_static_key *Model0_key)
{
 return Model0_atomic_read(&Model0_key->Model0_enabled);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_jump_label_init(void)
{
 Model0_static_key_initialized = true;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_static_key_false(struct Model0_static_key *Model0_key)
{
 if (__builtin_expect(!!(Model0_static_key_count(Model0_key) > 0), 0))
  return true;
 return false;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_static_key_true(struct Model0_static_key *Model0_key)
{
 if (__builtin_expect(!!(Model0_static_key_count(Model0_key) > 0), 1))
  return true;
 return false;
}

static inline __attribute__((no_instrument_function)) void Model0_static_key_slow_inc(struct Model0_static_key *Model0_key)
{
 ({ int Model0___ret_warn_on = !!(!Model0_static_key_initialized); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_fmt("./include/linux/jump_label.h", 196, "%s used before call to jump_label_init", __func__); __builtin_expect(!!(Model0___ret_warn_on), 0); });
 Model0_atomic_inc(&Model0_key->Model0_enabled);
}

static inline __attribute__((no_instrument_function)) void Model0_static_key_slow_dec(struct Model0_static_key *Model0_key)
{
 ({ int Model0___ret_warn_on = !!(!Model0_static_key_initialized); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_fmt("./include/linux/jump_label.h", 202, "%s used before call to jump_label_init", __func__); __builtin_expect(!!(Model0___ret_warn_on), 0); });
 Model0_atomic_dec(&Model0_key->Model0_enabled);
}

static inline __attribute__((no_instrument_function)) int Model0_jump_label_text_reserved(void *Model0_start, void *Model0_end)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_jump_label_lock(void) {}
static inline __attribute__((no_instrument_function)) void Model0_jump_label_unlock(void) {}

static inline __attribute__((no_instrument_function)) int Model0_jump_label_apply_nops(struct Model0_module *Model0_mod)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_static_key_enable(struct Model0_static_key *Model0_key)
{
 int Model0_count = Model0_static_key_count(Model0_key);

 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(Model0_count < 0 || Model0_count > 1); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/jump_label.h", 223); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });

 if (!Model0_count)
  Model0_static_key_slow_inc(Model0_key);
}

static inline __attribute__((no_instrument_function)) void Model0_static_key_disable(struct Model0_static_key *Model0_key)
{
 int Model0_count = Model0_static_key_count(Model0_key);

 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(Model0_count < 0 || Model0_count > 1); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/jump_label.h", 233); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });

 if (Model0_count)
  Model0_static_key_slow_dec(Model0_key);
}
/* -------------------------------------------------------------------------- */

/*
 * Two type wrappers around static_key, such that we can use compile time
 * type differentiation to emit the right code.
 *
 * All the below code is macros in order to play type games.
 */

struct Model0_static_key_true {
 struct Model0_static_key Model0_key;
};

struct Model0_static_key_false {
 struct Model0_static_key Model0_key;
};
extern bool Model0_____wrong_branch_error(void);
/*
 * Advanced usage; refcount, branch is enabled when: count != 0
 */




/*
 * Normal usage; boolean enable/disable.
 */

struct Model0_trace_print_flags {
 unsigned long Model0_mask;
 const char *Model0_name;
};

struct Model0_trace_print_flags_u64 {
 unsigned long long Model0_mask;
 const char *Model0_name;
};

struct Model0_tracepoint_func {
 void *func;
 void *Model0_data;
 int Model0_prio;
};

struct Model0_tracepoint {
 const char *Model0_name; /* Tracepoint name */
 struct Model0_static_key Model0_key;
 void (*Model0_regfunc)(void);
 void (*Model0_unregfunc)(void);
 struct Model0_tracepoint_func *Model0_funcs;
};

extern struct Model0_tracepoint Model0___tracepoint_read_msr;
extern struct Model0_tracepoint Model0___tracepoint_write_msr;
extern struct Model0_tracepoint Model0___tracepoint_rdpmc;

extern void Model0_do_trace_write_msr(unsigned Model0_msr, Model0_u64 Model0_val, int Model0_failed);
extern void Model0_do_trace_read_msr(unsigned Model0_msr, Model0_u64 Model0_val, int Model0_failed);
extern void Model0_do_trace_rdpmc(unsigned Model0_msr, Model0_u64 Model0_val, int Model0_failed);







static inline __attribute__((no_instrument_function)) unsigned long long Model0_native_read_msr(unsigned int Model0_msr)
{
 unsigned long Model0_low, Model0_high;

 asm volatile("1: rdmsr\n"
       "2:\n"
       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "2b" ") - .\n" " .long (" "ex_handler_rdmsr_unsafe" ") - .\n" " .popsection\n"
       : "=a" (Model0_low), "=d" (Model0_high) : "c" (Model0_msr));
 if (Model0_static_key_false(&(Model0___tracepoint_read_msr).Model0_key))
  Model0_do_trace_read_msr(Model0_msr, ((Model0_low) | (Model0_high) << 32), 0);
 return ((Model0_low) | (Model0_high) << 32);
}

static inline __attribute__((no_instrument_function)) unsigned long long Model0_native_read_msr_safe(unsigned int Model0_msr,
            int *err)
{
 unsigned long Model0_low, Model0_high;

 asm volatile("2: rdmsr ; xor %[err],%[err]\n"
       "1:\n\t"
       ".section .fixup,\"ax\"\n\t"
       "3: mov %[fault],%[err]\n\t"
       "xorl %%eax, %%eax\n\t"
       "xorl %%edx, %%edx\n\t"
       "jmp 1b\n\t"
       ".previous\n\t"
       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "2b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n"
       : [err] "=r" (*err), "=a" (Model0_low), "=d" (Model0_high)
       : "c" (Model0_msr), [fault] "i" (-5));
 if (Model0_static_key_false(&(Model0___tracepoint_read_msr).Model0_key))
  Model0_do_trace_read_msr(Model0_msr, ((Model0_low) | (Model0_high) << 32), *err);
 return ((Model0_low) | (Model0_high) << 32);
}

/* Can be uninlined because referenced by paravirt */
__attribute__((no_instrument_function)) static inline __attribute__((no_instrument_function)) void Model0_native_write_msr(unsigned int Model0_msr,
         unsigned Model0_low, unsigned Model0_high)
{
 asm volatile("1: wrmsr\n"
       "2:\n"
       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "2b" ") - .\n" " .long (" "ex_handler_wrmsr_unsafe" ") - .\n" " .popsection\n"
       : : "c" (Model0_msr), "a"(Model0_low), "d" (Model0_high) : "memory");
 if (Model0_static_key_false(&(Model0___tracepoint_write_msr).Model0_key))
  Model0_do_trace_write_msr(Model0_msr, ((Model0_u64)Model0_high << 32 | Model0_low), 0);
}

/* Can be uninlined because referenced by paravirt */
__attribute__((no_instrument_function)) static inline __attribute__((no_instrument_function)) int Model0_native_write_msr_safe(unsigned int Model0_msr,
     unsigned Model0_low, unsigned Model0_high)
{
 int err;
 asm volatile("2: wrmsr ; xor %[err],%[err]\n"
       "1:\n\t"
       ".section .fixup,\"ax\"\n\t"
       "3:  mov %[fault],%[err] ; jmp 1b\n\t"
       ".previous\n\t"
       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "2b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n"
       : [err] "=a" (err)
       : "c" (Model0_msr), "0" (Model0_low), "d" (Model0_high),
         [fault] "i" (-5)
       : "memory");
 if (Model0_static_key_false(&(Model0___tracepoint_write_msr).Model0_key))
  Model0_do_trace_write_msr(Model0_msr, ((Model0_u64)Model0_high << 32 | Model0_low), err);
 return err;
}

extern int Model0_rdmsr_safe_regs(Model0_u32 Model0_regs[8]);
extern int Model0_wrmsr_safe_regs(Model0_u32 Model0_regs[8]);

/**
 * rdtsc() - returns the current TSC without ordering constraints
 *
 * rdtsc() returns the result of RDTSC as a 64-bit integer.  The
 * only ordering constraint it supplies is the ordering implied by
 * "asm volatile": it will put the RDTSC in the place you expect.  The
 * CPU can and will speculatively execute that RDTSC, though, so the
 * results can be non-monotonic if compared on different CPUs.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long long Model0_rdtsc(void)
{
 unsigned long Model0_low, Model0_high;

 asm volatile("rdtsc" : "=a" (Model0_low), "=d" (Model0_high));

 return ((Model0_low) | (Model0_high) << 32);
}

/**
 * rdtsc_ordered() - read the current TSC in program order
 *
 * rdtsc_ordered() returns the result of RDTSC as a 64-bit integer.
 * It is ordered like a load to a global in-memory counter.  It should
 * be impossible to observe non-monotonic rdtsc_unordered() behavior
 * across multiple CPUs as long as the TSC is synced.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long long Model0_rdtsc_ordered(void)
{
 /*
	 * The RDTSC instruction is not ordered relative to memory
	 * access.  The Intel SDM and the AMD APM are both vague on this
	 * point, but empirically an RDTSC instruction can be
	 * speculatively executed before prior loads.  An RDTSC
	 * immediately after an appropriate barrier appears to be
	 * ordered as a normal load, that is, it provides the same
	 * ordering guarantees as reading from a global memory location
	 * that some other imaginary CPU is updating continuously with a
	 * time stamp.
	 */
 asm volatile("661:\n\t" "" "\n662:\n" ".skip -((" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")) > 0) * " "(" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")), 0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 3*32+17)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" " .long 661b - .\n" " .long " "664""2""f - .\n" " .word " "( 3*32+18)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""2""f-""664""2""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "mfence" "\n" "665""1" ":\n\t" "664""2"":\n\t" "lfence" "\n" "665""2" ":\n\t" ".popsection" ::: "memory");

 return Model0_rdtsc();
}

/* Deprecated, keep it for a cycle for easier merging: */


static inline __attribute__((no_instrument_function)) unsigned long long Model0_native_read_pmc(int Model0_counter)
{
 unsigned long Model0_low, Model0_high;

 asm volatile("rdpmc" : "=a" (Model0_low), "=d" (Model0_high) : "c" (Model0_counter));
 if (Model0_static_key_false(&(Model0___tracepoint_rdpmc).Model0_key))
  Model0_do_trace_rdpmc(Model0_counter, ((Model0_low) | (Model0_high) << 32), 0);
 return ((Model0_low) | (Model0_high) << 32);
}





/*
 * Access to machine-specific registers (available on 586 and better only)
 * Note: the rd* operations modify the parameters directly (without using
 * pointer indirection), this allows gcc to optimize better
 */
static inline __attribute__((no_instrument_function)) void Model0_wrmsr(unsigned Model0_msr, unsigned Model0_low, unsigned Model0_high)
{
 Model0_native_write_msr(Model0_msr, Model0_low, Model0_high);
}




static inline __attribute__((no_instrument_function)) void Model0_wrmsrl(unsigned Model0_msr, Model0_u64 Model0_val)
{
 Model0_native_write_msr(Model0_msr, (Model0_u32)(Model0_val & 0xffffffffULL), (Model0_u32)(Model0_val >> 32));
}

/* wrmsr with exception handling */
static inline __attribute__((no_instrument_function)) int Model0_wrmsr_safe(unsigned Model0_msr, unsigned Model0_low, unsigned Model0_high)
{
 return Model0_native_write_msr_safe(Model0_msr, Model0_low, Model0_high);
}

/* rdmsr with exception handling */
static inline __attribute__((no_instrument_function)) int Model0_rdmsrl_safe(unsigned Model0_msr, unsigned long long *Model0_p)
{
 int err;

 *Model0_p = Model0_native_read_msr_safe(Model0_msr, &err);
 return err;
}
/*
 * 64-bit version of wrmsr_safe():
 */
static inline __attribute__((no_instrument_function)) int Model0_wrmsrl_safe(Model0_u32 Model0_msr, Model0_u64 Model0_val)
{
 return Model0_wrmsr_safe(Model0_msr, (Model0_u32)Model0_val, (Model0_u32)(Model0_val >> 32));
}





struct Model0_msr *Model0_msrs_alloc(void);
void Model0_msrs_free(struct Model0_msr *Model0_msrs);
int Model0_msr_set_bit(Model0_u32 Model0_msr, Model0_u8 Model0_bit);
int Model0_msr_clear_bit(Model0_u32 Model0_msr, Model0_u8 Model0_bit);


int Model0_rdmsr_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_msr_no, Model0_u32 *Model0_l, Model0_u32 *Model0_h);
int Model0_wrmsr_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_msr_no, Model0_u32 Model0_l, Model0_u32 Model0_h);
int Model0_rdmsrl_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_msr_no, Model0_u64 *Model0_q);
int Model0_wrmsrl_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_msr_no, Model0_u64 Model0_q);
void Model0_rdmsr_on_cpus(const struct Model0_cpumask *Model0_mask, Model0_u32 Model0_msr_no, struct Model0_msr *Model0_msrs);
void Model0_wrmsr_on_cpus(const struct Model0_cpumask *Model0_mask, Model0_u32 Model0_msr_no, struct Model0_msr *Model0_msrs);
int Model0_rdmsr_safe_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_msr_no, Model0_u32 *Model0_l, Model0_u32 *Model0_h);
int Model0_wrmsr_safe_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_msr_no, Model0_u32 Model0_l, Model0_u32 Model0_h);
int Model0_rdmsrl_safe_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_msr_no, Model0_u64 *Model0_q);
int Model0_wrmsrl_safe_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_msr_no, Model0_u64 Model0_q);
int Model0_rdmsr_safe_regs_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_regs[8]);
int Model0_wrmsr_safe_regs_on_cpu(unsigned int Model0_cpu, Model0_u32 Model0_regs[8]);
/* Written 2000 by Andi Kleen */



/*
 * Segment descriptor structure definitions, usable from both x86_64 and i386
 * archs.
 */





/*
 * FIXME: Accessing the desc_struct through its fields is more elegant,
 * and should be the one valid thing to do. However, a lot of open code
 * still touches the a and b accessors, and doing this allow us to do it
 * incrementally. We keep the signature as a struct, rather than an union,
 * so we can get rid of it transparently in the future -- glommer
 */
/* 8 byte segment descriptor */
struct Model0_desc_struct {
 union {
  struct {
   unsigned int Model0_a;
   unsigned int Model0_b;
  };
  struct {
   Model0_u16 Model0_limit0;
   Model0_u16 Model0_base0;
   unsigned Model0_base1: 8, Model0_type: 4, Model0_s: 1, Model0_dpl: 2, Model0_p: 1;
   unsigned Model0_limit: 4, Model0_avl: 1, Model0_l: 1, Model0_d: 1, Model0_g: 1, Model0_base2: 8;
  };
 };
} __attribute__((packed));







enum {
 Model0_GATE_INTERRUPT = 0xE,
 Model0_GATE_TRAP = 0xF,
 Model0_GATE_CALL = 0xC,
 Model0_GATE_TASK = 0x5,
};

/* 16byte gate */
struct Model0_gate_struct64 {
 Model0_u16 Model0_offset_low;
 Model0_u16 Model0_segment;
 unsigned Model0_ist : 3, Model0_zero0 : 5, Model0_type : 5, Model0_dpl : 2, Model0_p : 1;
 Model0_u16 Model0_offset_middle;
 Model0_u32 Model0_offset_high;
 Model0_u32 Model0_zero1;
} __attribute__((packed));





enum {
 Model0_DESC_TSS = 0x9,
 Model0_DESC_LDT = 0x2,
 Model0_DESCTYPE_S = 0x10, /* !system */
};

/* LDT or TSS descriptor in the GDT. 16 bytes. */
struct Model0_ldttss_desc64 {
 Model0_u16 Model0_limit0;
 Model0_u16 Model0_base0;
 unsigned Model0_base1 : 8, Model0_type : 5, Model0_dpl : 2, Model0_p : 1;
 unsigned Model0_limit1 : 4, Model0_zero0 : 3, Model0_g : 1, Model0_base2 : 8;
 Model0_u32 Model0_base3;
 Model0_u32 Model0_zero1;
} __attribute__((packed));


typedef struct Model0_gate_struct64 Model0_gate_desc;
typedef struct Model0_ldttss_desc64 Model0_ldt_desc;
typedef struct Model0_ldttss_desc64 Model0_tss_desc;
struct Model0_desc_ptr {
 unsigned short Model0_size;
 unsigned long Model0_address;
} __attribute__((packed)) ;



/* Access rights as returned by LAR */









static inline __attribute__((no_instrument_function)) void Model0_native_clts(void)
{
 asm volatile("clts");
}

/*
 * Volatile isn't enough to prevent the compiler from reordering the
 * read/write functions for the control registers and messing everything up.
 * A memory clobber would solve the problem, but would prevent reordering of
 * all loads stores around it, which can hurt performance. Solution is to
 * use a variable and mimic reads and writes to it to enforce serialization
 */
extern unsigned long Model0___force_order;

static inline __attribute__((no_instrument_function)) unsigned long Model0_native_read_cr0(void)
{
 unsigned long Model0_val;
 asm volatile("mov %%cr0,%0\n\t" : "=r" (Model0_val), "=m" (Model0___force_order));
 return Model0_val;
}

static inline __attribute__((no_instrument_function)) void Model0_native_write_cr0(unsigned long Model0_val)
{
 asm volatile("mov %0,%%cr0": : "r" (Model0_val), "m" (Model0___force_order));
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_native_read_cr2(void)
{
 unsigned long Model0_val;
 asm volatile("mov %%cr2,%0\n\t" : "=r" (Model0_val), "=m" (Model0___force_order));
 return Model0_val;
}

static inline __attribute__((no_instrument_function)) void Model0_native_write_cr2(unsigned long Model0_val)
{
 asm volatile("mov %0,%%cr2": : "r" (Model0_val), "m" (Model0___force_order));
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_native_read_cr3(void)
{
 unsigned long Model0_val;
 asm volatile("mov %%cr3,%0\n\t" : "=r" (Model0_val), "=m" (Model0___force_order));
 return Model0_val;
}

static inline __attribute__((no_instrument_function)) void Model0_native_write_cr3(unsigned long Model0_val)
{
 asm volatile("mov %0,%%cr3": : "r" (Model0_val), "m" (Model0___force_order));
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_native_read_cr4(void)
{
 unsigned long Model0_val;
 asm volatile("mov %%cr4,%0\n\t" : "=r" (Model0_val), "=m" (Model0___force_order));
 return Model0_val;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_native_read_cr4_safe(void)
{
 unsigned long Model0_val;
 /* This could fault if %cr4 does not exist. In x86_64, a cr4 always
	 * exists, so it will never fail. */






 Model0_val = Model0_native_read_cr4();

 return Model0_val;
}

static inline __attribute__((no_instrument_function)) void Model0_native_write_cr4(unsigned long Model0_val)
{
 asm volatile("mov %0,%%cr4": : "r" (Model0_val), "m" (Model0___force_order));
}


static inline __attribute__((no_instrument_function)) unsigned long Model0_native_read_cr8(void)
{
 unsigned long Model0_cr8;
 asm volatile("movq %%cr8,%0" : "=r" (Model0_cr8));
 return Model0_cr8;
}

static inline __attribute__((no_instrument_function)) void Model0_native_write_cr8(unsigned long Model0_val)
{
 asm volatile("movq %0,%%cr8" :: "r" (Model0_val) : "memory");
}



static inline __attribute__((no_instrument_function)) Model0_u32 Model0___read_pkru(void)
{
 Model0_u32 Model0_ecx = 0;
 Model0_u32 Model0_edx, Model0_pkru;

 /*
	 * "rdpkru" instruction.  Places PKRU contents in to EAX,
	 * clears EDX and requires that ecx=0.
	 */
 asm volatile(".byte 0x0f,0x01,0xee\n\t"
       : "=a" (Model0_pkru), "=d" (Model0_edx)
       : "c" (Model0_ecx));
 return Model0_pkru;
}

static inline __attribute__((no_instrument_function)) void Model0___write_pkru(Model0_u32 Model0_pkru)
{
 Model0_u32 Model0_ecx = 0, Model0_edx = 0;

 /*
	 * "wrpkru" instruction.  Loads contents in EAX to PKRU,
	 * requires that ecx = edx = 0.
	 */
 asm volatile(".byte 0x0f,0x01,0xef\n\t"
       : : "a" (Model0_pkru), "c"(Model0_ecx), "d"(Model0_edx));
}
static inline __attribute__((no_instrument_function)) void Model0_native_wbinvd(void)
{
 asm volatile("wbinvd": : :"memory");
}

extern void Model0_native_load_gs_index(unsigned);





static inline __attribute__((no_instrument_function)) unsigned long Model0_read_cr0(void)
{
 return Model0_native_read_cr0();
}

static inline __attribute__((no_instrument_function)) void Model0_write_cr0(unsigned long Model0_x)
{
 Model0_native_write_cr0(Model0_x);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_read_cr2(void)
{
 return Model0_native_read_cr2();
}

static inline __attribute__((no_instrument_function)) void Model0_write_cr2(unsigned long Model0_x)
{
 Model0_native_write_cr2(Model0_x);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_read_cr3(void)
{
 return Model0_native_read_cr3();
}

static inline __attribute__((no_instrument_function)) void Model0_write_cr3(unsigned long Model0_x)
{
 Model0_native_write_cr3(Model0_x);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0___read_cr4(void)
{
 return Model0_native_read_cr4();
}

static inline __attribute__((no_instrument_function)) unsigned long Model0___read_cr4_safe(void)
{
 return Model0_native_read_cr4_safe();
}

static inline __attribute__((no_instrument_function)) void Model0___write_cr4(unsigned long Model0_x)
{
 Model0_native_write_cr4(Model0_x);
}

static inline __attribute__((no_instrument_function)) void Model0_wbinvd(void)
{
 Model0_native_wbinvd();
}



static inline __attribute__((no_instrument_function)) unsigned long Model0_read_cr8(void)
{
 return Model0_native_read_cr8();
}

static inline __attribute__((no_instrument_function)) void Model0_write_cr8(unsigned long Model0_x)
{
 Model0_native_write_cr8(Model0_x);
}

static inline __attribute__((no_instrument_function)) void Model0_load_gs_index(unsigned Model0_selector)
{
 Model0_native_load_gs_index(Model0_selector);
}



/* Clear the 'TS' bit */
static inline __attribute__((no_instrument_function)) void Model0_clts(void)
{
 Model0_native_clts();
}





static inline __attribute__((no_instrument_function)) void Model0_clflush(volatile void *Model0___p)
{
 asm volatile("clflush %0" : "+m" (*(volatile char *)Model0___p));
}

static inline __attribute__((no_instrument_function)) void Model0_clflushopt(volatile void *Model0___p)
{
 asm volatile ("661:\n\t" ".byte " "0x3e" "; clflush %P0" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 9*32+23)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0x66; clflush %P0" "\n" "665""1" ":\n\t" ".popsection" : "+m" (*(volatile char *)Model0___p) : "i" (0));



}

static inline __attribute__((no_instrument_function)) void Model0_clwb(volatile void *Model0___p)
{
 volatile struct { char Model0_x[64]; } *Model0_p = Model0___p;

 asm volatile("661:\n\t" ".byte " "0x3e" "; clflush (%[pax])" "\n662:\n" ".skip -((" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")) > 0) * " "(" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")), 0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 9*32+23)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" " .long 661b - .\n" " .long " "664""2""f - .\n" " .word " "( 9*32+24)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""2""f-""664""2""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0x66; clflush (%[pax])" "\n" "665""1" ":\n\t" "664""2"":\n\t" ".byte 0x66, 0x0f, 0xae, 0x30" "\n" "665""2" ":\n\t" ".popsection"





  : [Model0_p] "+m" (*Model0_p)
  : [pax] "a" (Model0_p));
}
/*
 * FPU data structures:
 */



/*
 * The legacy x87 FPU state format, as saved by FSAVE and
 * restored by the FRSTOR instructions:
 */
struct Model0_fregs_state {
 Model0_u32 Model0_cwd; /* FPU Control Word		*/
 Model0_u32 Model0_swd; /* FPU Status Word		*/
 Model0_u32 Model0_twd; /* FPU Tag Word			*/
 Model0_u32 Model0_fip; /* FPU IP Offset		*/
 Model0_u32 Model0_fcs; /* FPU IP Selector		*/
 Model0_u32 Model0_foo; /* FPU Operand Pointer Offset	*/
 Model0_u32 Model0_fos; /* FPU Operand Pointer Selector	*/

 /* 8*10 bytes for each FP-reg = 80 bytes:			*/
 Model0_u32 Model0_st_space[20];

 /* Software status information [not touched by FSAVE]:		*/
 Model0_u32 Model0_status;
};

/*
 * The legacy fx SSE/MMX FPU state format, as saved by FXSAVE and
 * restored by the FXRSTOR instructions. It's similar to the FSAVE
 * format, but differs in some areas, plus has extensions at
 * the end for the XMM registers.
 */
struct Model0_fxregs_state {
 Model0_u16 Model0_cwd; /* Control Word			*/
 Model0_u16 Model0_swd; /* Status Word			*/
 Model0_u16 Model0_twd; /* Tag Word			*/
 Model0_u16 Model0_fop; /* Last Instruction Opcode		*/
 union {
  struct {
   Model0_u64 Model0_rip; /* Instruction Pointer		*/
   Model0_u64 Model0_rdp; /* Data Pointer			*/
  };
  struct {
   Model0_u32 Model0_fip; /* FPU IP Offset			*/
   Model0_u32 Model0_fcs; /* FPU IP Selector			*/
   Model0_u32 Model0_foo; /* FPU Operand Offset		*/
   Model0_u32 Model0_fos; /* FPU Operand Selector		*/
  };
 };
 Model0_u32 Model0_mxcsr; /* MXCSR Register State */
 Model0_u32 Model0_mxcsr_mask; /* MXCSR Mask		*/

 /* 8*16 bytes for each FP-reg = 128 bytes:			*/
 Model0_u32 Model0_st_space[32];

 /* 16*16 bytes for each XMM-reg = 256 bytes:			*/
 Model0_u32 Model0_xmm_space[64];

 Model0_u32 Model0_padding[12];

 union {
  Model0_u32 Model0_padding1[12];
  Model0_u32 Model0_sw_reserved[12];
 };

} __attribute__((aligned(16)));

/* Default value for fxregs_state.mxcsr: */


/*
 * Software based FPU emulation state. This is arbitrary really,
 * it matches the x87 format to make it easier to understand:
 */
struct Model0_swregs_state {
 Model0_u32 Model0_cwd;
 Model0_u32 Model0_swd;
 Model0_u32 Model0_twd;
 Model0_u32 Model0_fip;
 Model0_u32 Model0_fcs;
 Model0_u32 Model0_foo;
 Model0_u32 Model0_fos;
 /* 8*10 bytes for each FP-reg = 80 bytes: */
 Model0_u32 Model0_st_space[20];
 Model0_u8 Model0_ftop;
 Model0_u8 Model0_changed;
 Model0_u8 Model0_lookahead;
 Model0_u8 Model0_no_update;
 Model0_u8 Model0_rm;
 Model0_u8 Model0_alimit;
 struct Model0_math_emu_info *Model0_info;
 Model0_u32 Model0_entry_eip;
};

/*
 * List of XSAVE features Linux knows about:
 */
enum Model0_xfeature {
 Model0_XFEATURE_FP,
 Model0_XFEATURE_SSE,
 /*
	 * Values above here are "legacy states".
	 * Those below are "extended states".
	 */
 Model0_XFEATURE_YMM,
 Model0_XFEATURE_BNDREGS,
 Model0_XFEATURE_BNDCSR,
 Model0_XFEATURE_OPMASK,
 Model0_XFEATURE_ZMM_Hi256,
 Model0_XFEATURE_Hi16_ZMM,
 Model0_XFEATURE_PT_UNIMPLEMENTED_SO_FAR,
 Model0_XFEATURE_PKRU,

 Model0_XFEATURE_MAX,
};
struct Model0_reg_128_bit {
 Model0_u8 Model0_regbytes[128/8];
};
struct Model0_reg_256_bit {
 Model0_u8 Model0_regbytes[256/8];
};
struct Model0_reg_512_bit {
 Model0_u8 Model0_regbytes[512/8];
};

/*
 * State component 2:
 *
 * There are 16x 256-bit AVX registers named YMM0-YMM15.
 * The low 128 bits are aliased to the 16 SSE registers (XMM0-XMM15)
 * and are stored in 'struct fxregs_state::xmm_space[]' in the
 * "legacy" area.
 *
 * The high 128 bits are stored here.
 */
struct Model0_ymmh_struct {
 struct Model0_reg_128_bit Model0_hi_ymm[16];
} __attribute__((packed));

/* Intel MPX support: */

struct Model0_mpx_bndreg {
 Model0_u64 Model0_lower_bound;
 Model0_u64 Model0_upper_bound;
} __attribute__((packed));
/*
 * State component 3 is used for the 4 128-bit bounds registers
 */
struct Model0_mpx_bndreg_state {
 struct Model0_mpx_bndreg Model0_bndreg[4];
} __attribute__((packed));

/*
 * State component 4 is used for the 64-bit user-mode MPX
 * configuration register BNDCFGU and the 64-bit MPX status
 * register BNDSTATUS.  We call the pair "BNDCSR".
 */
struct Model0_mpx_bndcsr {
 Model0_u64 Model0_bndcfgu;
 Model0_u64 Model0_bndstatus;
} __attribute__((packed));

/*
 * The BNDCSR state is padded out to be 64-bytes in size.
 */
struct Model0_mpx_bndcsr_state {
 union {
  struct Model0_mpx_bndcsr Model0_bndcsr;
  Model0_u8 Model0_pad_to_64_bytes[64];
 };
} __attribute__((packed));

/* AVX-512 Components: */

/*
 * State component 5 is used for the 8 64-bit opmask registers
 * k0-k7 (opmask state).
 */
struct Model0_avx_512_opmask_state {
 Model0_u64 Model0_opmask_reg[8];
} __attribute__((packed));

/*
 * State component 6 is used for the upper 256 bits of the
 * registers ZMM0-ZMM15. These 16 256-bit values are denoted
 * ZMM0_H-ZMM15_H (ZMM_Hi256 state).
 */
struct Model0_avx_512_zmm_uppers_state {
 struct Model0_reg_256_bit Model0_zmm_upper[16];
} __attribute__((packed));

/*
 * State component 7 is used for the 16 512-bit registers
 * ZMM16-ZMM31 (Hi16_ZMM state).
 */
struct Model0_avx_512_hi16_state {
 struct Model0_reg_512_bit Model0_hi16_zmm[16];
} __attribute__((packed));

/*
 * State component 9: 32-bit PKRU register.  The state is
 * 8 bytes long but only 4 bytes is used currently.
 */
struct Model0_pkru_state {
 Model0_u32 Model0_pkru;
 Model0_u32 Model0_pad;
} __attribute__((packed));

struct Model0_xstate_header {
 Model0_u64 Model0_xfeatures;
 Model0_u64 Model0_xcomp_bv;
 Model0_u64 Model0_reserved[6];
} __attribute__((packed));

/*
 * xstate_header.xcomp_bv[63] indicates that the extended_state_area
 * is in compacted format.
 */


/*
 * This is our most modern FPU state format, as saved by the XSAVE
 * and restored by the XRSTOR instructions.
 *
 * It consists of a legacy fxregs portion, an xstate header and
 * subsequent areas as defined by the xstate header.  Not all CPUs
 * support all the extensions, so the size of the extended area
 * can vary quite a bit between CPUs.
 */
struct Model0_xregs_state {
 struct Model0_fxregs_state Model0_i387;
 struct Model0_xstate_header Model0_header;
 Model0_u8 Model0_extended_state_area[0];
} __attribute__ ((packed, aligned (64)));

/*
 * This is a union of all the possible FPU state formats
 * put together, so that we can pick the right one runtime.
 *
 * The size of the structure is determined by the largest
 * member - which is the xsave area.  The padding is there
 * to ensure that statically-allocated task_structs (just
 * the init_task today) have enough space.
 */
union Model0_fpregs_state {
 struct Model0_fregs_state Model0_fsave;
 struct Model0_fxregs_state Model0_fxsave;
 struct Model0_swregs_state Model0_soft;
 struct Model0_xregs_state Model0_xsave;
 Model0_u8 Model0___padding[((1UL) << 12)];
};

/*
 * Highest level per task FPU state data structure that
 * contains the FPU register state plus various FPU
 * state fields:
 */
struct Model0_fpu {
 /*
	 * @last_cpu:
	 *
	 * Records the last CPU on which this context was loaded into
	 * FPU registers. (In the lazy-restore case we might be
	 * able to reuse FPU registers across multiple context switches
	 * this way, if no intermediate task used the FPU.)
	 *
	 * A value of -1 is used to indicate that the FPU state in context
	 * memory is newer than the FPU state in registers, and that the
	 * FPU state should be reloaded next time the task is run.
	 */
 unsigned int Model0_last_cpu;

 /*
	 * @fpstate_active:
	 *
	 * This flag indicates whether this context is active: if the task
	 * is not running then we can restore from this context, if the task
	 * is running then we should save into this context.
	 */
 unsigned char Model0_fpstate_active;

 /*
	 * @fpregs_active:
	 *
	 * This flag determines whether a given context is actively
	 * loaded into the FPU's registers and that those registers
	 * represent the task's current FPU state.
	 *
	 * Note the interaction with fpstate_active:
	 *
	 *   # task does not use the FPU:
	 *   fpstate_active == 0
	 *
	 *   # task uses the FPU and regs are active:
	 *   fpstate_active == 1 && fpregs_active == 1
	 *
	 *   # the regs are inactive but still match fpstate:
	 *   fpstate_active == 1 && fpregs_active == 0 && fpregs_owner == fpu
	 *
	 * The third state is what we use for the lazy restore optimization
	 * on lazy-switching CPUs.
	 */
 unsigned char Model0_fpregs_active;

 /*
	 * @counter:
	 *
	 * This counter contains the number of consecutive context switches
	 * during which the FPU stays used. If this is over a threshold, the
	 * lazy FPU restore logic becomes eager, to save the trap overhead.
	 * This is an unsigned char so that after 256 iterations the counter
	 * wraps and the context switch behavior turns lazy again; this is to
	 * deal with bursty apps that only use the FPU for a short time:
	 */
 unsigned char Model0_counter;
 /*
	 * @state:
	 *
	 * In-memory copy of all FPU registers that we save/restore
	 * over context switches. If the task is using the FPU then
	 * the registers in the FPU are more recent than this state
	 * copy. If the task context-switches away then they get
	 * saved here and represent the FPU state.
	 *
	 * After context switches there may be a (short) time period
	 * during which the in-FPU hardware registers are unchanged
	 * and still perfectly match this state, if the tasks
	 * scheduled afterwards are not using the FPU.
	 *
	 * This is the 'lazy restore' window of optimization, which
	 * we track though 'fpu_fpregs_owner_ctx' and 'fpu->last_cpu'.
	 *
	 * We detect whether a subsequent task uses the FPU via setting
	 * CR0::TS to 1, which causes any FPU use to raise a #NM fault.
	 *
	 * During this window, if the task gets scheduled again, we
	 * might be able to skip having to do a restore from this
	 * memory buffer to the hardware registers - at the cost of
	 * incurring the overhead of #NM fault traps.
	 *
	 * Note that on modern CPUs that support the XSAVEOPT (or other
	 * optimized XSAVE instructions), we don't use #NM traps anymore,
	 * as the hardware can track whether FPU registers need saving
	 * or not. On such CPUs we activate the non-lazy ('eagerfpu')
	 * logic, which unconditionally saves/restores all FPU state
	 * across context switches. (if FPU state exists.)
	 */
 union Model0_fpregs_state Model0_state;
 /*
	 * WARNING: 'state' is dynamically-sized.  Do not put
	 * anything after it here.
	 */
};








/*
 * Flags for bug emulation.
 *
 * These occupy the top three bytes.
 */
enum {
 Model0_UNAME26 = 0x0020000,
 Model0_ADDR_NO_RANDOMIZE = 0x0040000, /* disable randomization of VA space */
 Model0_FDPIC_FUNCPTRS = 0x0080000, /* userspace function ptrs point to descriptors
						 * (signal handling)
						 */
 Model0_MMAP_PAGE_ZERO = 0x0100000,
 Model0_ADDR_COMPAT_LAYOUT = 0x0200000,
 Model0_READ_IMPLIES_EXEC = 0x0400000,
 Model0_ADDR_LIMIT_32BIT = 0x0800000,
 Model0_SHORT_INODE = 0x1000000,
 Model0_WHOLE_SECONDS = 0x2000000,
 Model0_STICKY_TIMEOUTS = 0x4000000,
 Model0_ADDR_LIMIT_3GB = 0x8000000,
};

/*
 * Security-relevant compatibility flags that must be
 * cleared upon setuid or setgid exec:
 */





/*
 * Personality types.
 *
 * These go in the low byte.  Avoid using the top bit, it will
 * conflict with error returns.
 */
enum {
 Model0_PER_LINUX = 0x0000,
 Model0_PER_LINUX_32BIT = 0x0000 | Model0_ADDR_LIMIT_32BIT,
 Model0_PER_LINUX_FDPIC = 0x0000 | Model0_FDPIC_FUNCPTRS,
 Model0_PER_SVR4 = 0x0001 | Model0_STICKY_TIMEOUTS | Model0_MMAP_PAGE_ZERO,
 Model0_PER_SVR3 = 0x0002 | Model0_STICKY_TIMEOUTS | Model0_SHORT_INODE,
 Model0_PER_SCOSVR3 = 0x0003 | Model0_STICKY_TIMEOUTS |
      Model0_WHOLE_SECONDS | Model0_SHORT_INODE,
 Model0_PER_OSR5 = 0x0003 | Model0_STICKY_TIMEOUTS | Model0_WHOLE_SECONDS,
 Model0_PER_WYSEV386 = 0x0004 | Model0_STICKY_TIMEOUTS | Model0_SHORT_INODE,
 Model0_PER_ISCR4 = 0x0005 | Model0_STICKY_TIMEOUTS,
 Model0_PER_BSD = 0x0006,
 Model0_PER_SUNOS = 0x0006 | Model0_STICKY_TIMEOUTS,
 Model0_PER_XENIX = 0x0007 | Model0_STICKY_TIMEOUTS | Model0_SHORT_INODE,
 Model0_PER_LINUX32 = 0x0008,
 Model0_PER_LINUX32_3GB = 0x0008 | Model0_ADDR_LIMIT_3GB,
 Model0_PER_IRIX32 = 0x0009 | Model0_STICKY_TIMEOUTS,/* IRIX5 32-bit */
 Model0_PER_IRIXN32 = 0x000a | Model0_STICKY_TIMEOUTS,/* IRIX6 new 32-bit */
 Model0_PER_IRIX64 = 0x000b | Model0_STICKY_TIMEOUTS,/* IRIX6 64-bit */
 Model0_PER_RISCOS = 0x000c,
 Model0_PER_SOLARIS = 0x000d | Model0_STICKY_TIMEOUTS,
 Model0_PER_UW7 = 0x000e | Model0_STICKY_TIMEOUTS | Model0_MMAP_PAGE_ZERO,
 Model0_PER_OSF4 = 0x000f, /* OSF/1 v4 */
 Model0_PER_HPUX = 0x0010,
 Model0_PER_MASK = 0x00ff,
};

/*
 * Return the base personality without flags.
 */


/*
 * Change personality of the currently running process.
 */








/*
 * Copyright (C) 2003 Bernardo Innocenti <bernie@develer.com>
 * Based on former asm-ppc/div64.h and asm-m68knommu/div64.h
 *
 * Optimization for constant divisors on 32-bit machines:
 * Copyright (C) 2006-2015 Nicolas Pitre
 *
 * The semantics of do_div() are:
 *
 * uint32_t do_div(uint64_t *n, uint32_t base)
 * {
 * 	uint32_t remainder = *n % base;
 * 	*n = *n / base;
 * 	return remainder;
 * }
 *
 * NOTE: macro parameter n is evaluated multiple times,
 *       beware of side effects!
 */






/**
 * div_u64_rem - unsigned 64bit divide with 32bit divisor with remainder
 *
 * This is commonly provided by 32bit archs to provide an optimized 64bit
 * divide.
 */
static inline __attribute__((no_instrument_function)) Model0_u64 Model0_div_u64_rem(Model0_u64 Model0_dividend, Model0_u32 Model0_divisor, Model0_u32 *Model0_remainder)
{
 *Model0_remainder = Model0_dividend % Model0_divisor;
 return Model0_dividend / Model0_divisor;
}

/**
 * div_s64_rem - signed 64bit divide with 32bit divisor with remainder
 */
static inline __attribute__((no_instrument_function)) Model0_s64 Model0_div_s64_rem(Model0_s64 Model0_dividend, Model0_s32 Model0_divisor, Model0_s32 *Model0_remainder)
{
 *Model0_remainder = Model0_dividend % Model0_divisor;
 return Model0_dividend / Model0_divisor;
}

/**
 * div64_u64_rem - unsigned 64bit divide with 64bit divisor and remainder
 */
static inline __attribute__((no_instrument_function)) Model0_u64 Model0_div64_u64_rem(Model0_u64 Model0_dividend, Model0_u64 Model0_divisor, Model0_u64 *Model0_remainder)
{
 *Model0_remainder = Model0_dividend % Model0_divisor;
 return Model0_dividend / Model0_divisor;
}

/**
 * div64_u64 - unsigned 64bit divide with 64bit divisor
 */
static inline __attribute__((no_instrument_function)) Model0_u64 Model0_div64_u64(Model0_u64 Model0_dividend, Model0_u64 Model0_divisor)
{
 return Model0_dividend / Model0_divisor;
}

/**
 * div64_s64 - signed 64bit divide with 64bit divisor
 */
static inline __attribute__((no_instrument_function)) Model0_s64 Model0_div64_s64(Model0_s64 Model0_dividend, Model0_s64 Model0_divisor)
{
 return Model0_dividend / Model0_divisor;
}
/**
 * div_u64 - unsigned 64bit divide with 32bit divisor
 *
 * This is the most common 64bit divide and should be used if possible,
 * as many 32bit archs can optimize this variant better than a full 64bit
 * divide.
 */

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_div_u64(Model0_u64 Model0_dividend, Model0_u32 Model0_divisor)
{
 Model0_u32 Model0_remainder;
 return Model0_div_u64_rem(Model0_dividend, Model0_divisor, &Model0_remainder);
}


/**
 * div_s64 - signed 64bit divide with 32bit divisor
 */

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_div_s64(Model0_s64 Model0_dividend, Model0_s32 Model0_divisor)
{
 Model0_s32 Model0_remainder;
 return Model0_div_s64_rem(Model0_dividend, Model0_divisor, &Model0_remainder);
}


Model0_u32 Model0_iter_div_u64_rem(Model0_u64 Model0_dividend, Model0_u32 Model0_divisor, Model0_u64 *Model0_remainder);

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_u32
Model0___iter_div_u64_rem(Model0_u64 Model0_dividend, Model0_u32 Model0_divisor, Model0_u64 *Model0_remainder)
{
 Model0_u32 Model0_ret = 0;

 while (Model0_dividend >= Model0_divisor) {
  /* The following asm() prevents the compiler from
		   optimising this loop into a modulo operation.  */
  asm("" : "+rm"(Model0_dividend));

  Model0_dividend -= Model0_divisor;
  Model0_ret++;
 }

 *Model0_remainder = Model0_dividend;

 return Model0_ret;
}




static inline __attribute__((no_instrument_function)) Model0_u64 Model0_mul_u64_u32_shr(Model0_u64 Model0_a, Model0_u32 Model0_mul, unsigned int Model0_shift)
{
 return (Model0_u64)(((unsigned __int128)Model0_a * Model0_mul) >> Model0_shift);
}



static inline __attribute__((no_instrument_function)) Model0_u64 Model0_mul_u64_u64_shr(Model0_u64 Model0_a, Model0_u64 Model0_mul, unsigned int Model0_shift)
{
 return (Model0_u64)(((unsigned __int128)Model0_a * Model0_mul) >> Model0_shift);
}
static inline __attribute__((no_instrument_function)) Model0_u64 Model0_mul_u64_u32_div(Model0_u64 Model0_a, Model0_u32 Model0_mul, Model0_u32 Model0_divisor)
{
 union {
  Model0_u64 Model0_ll;
  struct {



   Model0_u32 Model0_low, Model0_high;

  } Model0_l;
 } Model0_u, Model0_rl, Model0_rh;

 Model0_u.Model0_ll = Model0_a;
 Model0_rl.Model0_ll = (Model0_u64)Model0_u.Model0_l.Model0_low * Model0_mul;
 Model0_rh.Model0_ll = (Model0_u64)Model0_u.Model0_l.Model0_high * Model0_mul + Model0_rl.Model0_l.Model0_high;

 /* Bits 32-63 of the result will be in rh.l.low. */
 Model0_rl.Model0_l.Model0_high = ({ Model0_uint32_t Model0___base = (Model0_divisor); Model0_uint32_t Model0___rem; Model0___rem = ((Model0_uint64_t)(Model0_rh.Model0_ll)) % Model0___base; (Model0_rh.Model0_ll) = ((Model0_uint64_t)(Model0_rh.Model0_ll)) / Model0___base; Model0___rem; });

 /* Bits 0-31 of the result will be in rl.l.low.	*/
 ({ Model0_uint32_t Model0___base = (Model0_divisor); Model0_uint32_t Model0___rem; Model0___rem = ((Model0_uint64_t)(Model0_rl.Model0_ll)) % Model0___base; (Model0_rl.Model0_ll) = ((Model0_uint64_t)(Model0_rl.Model0_ll)) / Model0___base; Model0___rem; });

 Model0_rl.Model0_l.Model0_high = Model0_rh.Model0_l.Model0_low;
 return Model0_rl.Model0_ll;
}







/*
 * Kernel pointers have redundant information, so we can use a
 * scheme where we can return either an error code or a normal
 * pointer with the same return value.
 *
 * This should be a per-architecture thing, to allow different
 * error and pointer decisions.
 */






static inline __attribute__((no_instrument_function)) void * __attribute__((warn_unused_result)) Model0_ERR_PTR(long error)
{
 return (void *) error;
}

static inline __attribute__((no_instrument_function)) long __attribute__((warn_unused_result)) Model0_PTR_ERR( const void *Model0_ptr)
{
 return (long) Model0_ptr;
}

static inline __attribute__((no_instrument_function)) bool __attribute__((warn_unused_result)) Model0_IS_ERR( const void *Model0_ptr)
{
 return __builtin_expect(!!((unsigned long)(void *)((unsigned long)Model0_ptr) >= (unsigned long)-4095), 0);
}

static inline __attribute__((no_instrument_function)) bool __attribute__((warn_unused_result)) Model0_IS_ERR_OR_NULL( const void *Model0_ptr)
{
 return __builtin_expect(!!(!Model0_ptr), 0) || __builtin_expect(!!((unsigned long)(void *)((unsigned long)Model0_ptr) >= (unsigned long)-4095), 0);
}

/**
 * ERR_CAST - Explicitly cast an error-valued pointer to another pointer type
 * @ptr: The pointer to cast.
 *
 * Explicitly cast an error-valued pointer to another pointer type in such a
 * way as to make it clear that's what's going on.
 */
static inline __attribute__((no_instrument_function)) void * __attribute__((warn_unused_result)) Model0_ERR_CAST( const void *Model0_ptr)
{
 /* cast away the const */
 return (void *) Model0_ptr;
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_PTR_ERR_OR_ZERO( const void *Model0_ptr)
{
 if (Model0_IS_ERR(Model0_ptr))
  return Model0_PTR_ERR(Model0_ptr);
 else
  return 0;
}

/* Deprecated */
/*
 * include/linux/irqflags.h
 *
 * IRQ flags tracing: follow the state of the hardirq and softirq flags and
 * provide callbacks for transitions between ON and OFF states.
 *
 * This file gets included from lowlevel asm headers too, to provide
 * wrapped versions of the local_irq_*() APIs, based on the
 * raw_local_irq_*() macros from the lowlevel headers.
 */











/*
 * Interrupt control:
 */

static inline __attribute__((no_instrument_function)) unsigned long Model0_native_save_fl(void)
{
 unsigned long Model0_flags;

 /*
	 * "=rm" is safe here, because "pop" adjusts the stack before
	 * it evaluates its effective address -- this is part of the
	 * documented behavior of the "pop" instruction.
	 */
 asm volatile("# __raw_save_flags\n\t"
       "pushf ; pop %0"
       : "=rm" (Model0_flags)
       : /* no input */
       : "memory");

 return Model0_flags;
}

static inline __attribute__((no_instrument_function)) void Model0_native_restore_fl(unsigned long Model0_flags)
{
 asm volatile("push %0 ; popf"
       : /* no output */
       :"g" (Model0_flags)
       :"memory", "cc");
}

static inline __attribute__((no_instrument_function)) void Model0_native_irq_disable(void)
{
 asm volatile("cli": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0_native_irq_enable(void)
{
 asm volatile("sti": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0_native_safe_halt(void)
{
 asm volatile("sti; hlt": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0_native_halt(void)
{
 asm volatile("hlt": : :"memory");
}
static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) unsigned long Model0_arch_local_save_flags(void)
{
 return Model0_native_save_fl();
}

static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model0_arch_local_irq_restore(unsigned long Model0_flags)
{
 Model0_native_restore_fl(Model0_flags);
}

static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model0_arch_local_irq_disable(void)
{
 Model0_native_irq_disable();
}

static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model0_arch_local_irq_enable(void)
{
 Model0_native_irq_enable();
}

/*
 * Used in the idle loop; sti takes one instruction cycle
 * to complete:
 */
static inline __attribute__((no_instrument_function)) void Model0_arch_safe_halt(void)
{
 Model0_native_safe_halt();
}

/*
 * Used when interrupts are already enabled or to
 * shutdown the processor:
 */
static inline __attribute__((no_instrument_function)) void Model0_halt(void)
{
 Model0_native_halt();
}

/*
 * For spinlocks, etc:
 */
static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) unsigned long Model0_arch_local_irq_save(void)
{
 unsigned long Model0_flags = Model0_arch_local_save_flags();
 Model0_arch_local_irq_disable();
 return Model0_flags;
}
static inline __attribute__((no_instrument_function)) int Model0_arch_irqs_disabled_flags(unsigned long Model0_flags)
{
 return !(Model0_flags & ((1UL) << (9)));
}

static inline __attribute__((no_instrument_function)) int Model0_arch_irqs_disabled(void)
{
 unsigned long Model0_flags = Model0_arch_local_save_flags();

 return Model0_arch_irqs_disabled_flags(Model0_flags);
}
/*
 * Wrap the arch provided IRQ routines to provide appropriate checks.
 */
/*
 * The local_irq_*() APIs are equal to the raw_local_irq*()
 * if !TRACE_IRQFLAGS.
 */
/*
 * Some architectures don't define arch_irqs_disabled(), so even if either
 * definition would be fine we need to use different ones for the time being
 * to avoid build issues.
 */

/*
 * We handle most unaligned accesses in hardware.  On the other hand
 * unaligned DMA can be quite expensive on some Nehalem processors.
 *
 * Based on this we disable the IP header alignment in network drivers.
 */



/*
 * Default implementation of macro that returns current
 * instruction pointer ("program counter").
 */
static inline __attribute__((no_instrument_function)) void *Model0_current_text_addr(void)
{
 void *Model0_pc;

 asm volatile("mov $1f, %0; 1:":"=r" (Model0_pc));

 return Model0_pc;
}

/*
 * These alignment constraints are for performance in the vSMP case,
 * but in the task_struct case we must also meet hardware imposed
 * alignment requirements of the FPU state:
 */
enum Model0_tlb_infos {
 Model0_ENTRIES,
 Model0_NR_INFO
};

extern Model0_u16 __attribute__((__section__(".data..read_mostly"))) Model0_tlb_lli_4k[Model0_NR_INFO];
extern Model0_u16 __attribute__((__section__(".data..read_mostly"))) Model0_tlb_lli_2m[Model0_NR_INFO];
extern Model0_u16 __attribute__((__section__(".data..read_mostly"))) Model0_tlb_lli_4m[Model0_NR_INFO];
extern Model0_u16 __attribute__((__section__(".data..read_mostly"))) Model0_tlb_lld_4k[Model0_NR_INFO];
extern Model0_u16 __attribute__((__section__(".data..read_mostly"))) Model0_tlb_lld_2m[Model0_NR_INFO];
extern Model0_u16 __attribute__((__section__(".data..read_mostly"))) Model0_tlb_lld_4m[Model0_NR_INFO];
extern Model0_u16 __attribute__((__section__(".data..read_mostly"))) Model0_tlb_lld_1g[Model0_NR_INFO];

/*
 *  CPU type and hardware bug flags. Kept separately for each CPU.
 *  Members of this structure are referenced in head.S, so think twice
 *  before touching them. [mj]
 */

struct Model0_cpuinfo_x86 {
 __u8 Model0_x86; /* CPU family */
 __u8 Model0_x86_vendor; /* CPU vendor */
 __u8 Model0_x86_model;
 __u8 Model0_x86_mask;
 /* Number of 4K pages in DTLB/ITLB combined(in pages): */
 int Model0_x86_tlbsize;

 __u8 Model0_x86_virt_bits;
 __u8 Model0_x86_phys_bits;
 /* CPUID returned core id bits: */
 __u8 Model0_x86_coreid_bits;
 /* Max extended CPUID function supported: */
 __u32 Model0_extended_cpuid_level;
 /* Maximum supported CPUID level, -1=no CPUID: */
 int Model0_cpuid_level;
 __u32 Model0_x86_capability[18 + 1];
 char Model0_x86_vendor_id[16];
 char Model0_x86_model_id[64];
 /* in KB - valid for CPUS which support this call: */
 int Model0_x86_cache_size;
 int Model0_x86_cache_alignment; /* In bytes */
 /* Cache QoS architectural values: */
 int Model0_x86_cache_max_rmid; /* max index */
 int Model0_x86_cache_occ_scale; /* scale to bytes */
 int Model0_x86_power;
 unsigned long Model0_loops_per_jiffy;
 /* cpuid returned max cores value: */
 Model0_u16 Model0_x86_max_cores;
 Model0_u16 Model0_apicid;
 Model0_u16 Model0_initial_apicid;
 Model0_u16 Model0_x86_clflush_size;
 /* number of cores as seen by the OS: */
 Model0_u16 Model0_booted_cores;
 /* Physical processor id: */
 Model0_u16 Model0_phys_proc_id;
 /* Logical processor id: */
 Model0_u16 Model0_logical_proc_id;
 /* Core id: */
 Model0_u16 Model0_cpu_core_id;
 /* Index into per_cpu list: */
 Model0_u16 Model0_cpu_index;
 Model0_u32 Model0_microcode;
};
/*
 * capabilities of CPUs
 */
extern struct Model0_cpuinfo_x86 Model0_boot_cpu_data;
extern struct Model0_cpuinfo_x86 Model0_new_cpu_data;

extern struct Model0_tss_struct Model0_doublefault_tss;
extern __u32 Model0_cpu_caps_cleared[18];
extern __u32 Model0_cpu_caps_set[18];


extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(struct Model0_cpuinfo_x86) Model0_cpu_info;






extern const struct Model0_seq_operations Model0_cpuinfo_op;



extern void Model0_cpu_detect(struct Model0_cpuinfo_x86 *Model0_c);

extern void Model0_early_cpu_init(void);
extern void Model0_identify_boot_cpu(void);
extern void Model0_identify_secondary_cpu(struct Model0_cpuinfo_x86 *);
extern void Model0_print_cpu_info(struct Model0_cpuinfo_x86 *);
void Model0_print_cpu_msr(struct Model0_cpuinfo_x86 *);
extern void Model0_init_scattered_cpuid_features(struct Model0_cpuinfo_x86 *Model0_c);
extern unsigned int Model0_init_intel_cacheinfo(struct Model0_cpuinfo_x86 *Model0_c);
extern void Model0_init_amd_cacheinfo(struct Model0_cpuinfo_x86 *Model0_c);

extern void Model0_detect_extended_topology(struct Model0_cpuinfo_x86 *Model0_c);
extern void Model0_detect_ht(struct Model0_cpuinfo_x86 *Model0_c);




static inline __attribute__((no_instrument_function)) int Model0_have_cpuid_p(void)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) void Model0_native_cpuid(unsigned int *Model0_eax, unsigned int *Model0_ebx,
    unsigned int *Model0_ecx, unsigned int *Model0_edx)
{
 /* ecx is often an input as well as an output. */
 asm volatile("cpuid"
     : "=a" (*Model0_eax),
       "=b" (*Model0_ebx),
       "=c" (*Model0_ecx),
       "=d" (*Model0_edx)
     : "0" (*Model0_eax), "2" (*Model0_ecx)
     : "memory");
}

static inline __attribute__((no_instrument_function)) void Model0_load_cr3(Model0_pgd_t *Model0_pgdir)
{
 Model0_write_cr3(Model0___phys_addr_nodebug((unsigned long)(Model0_pgdir)));
}
struct Model0_x86_hw_tss {
 Model0_u32 Model0_reserved1;
 Model0_u64 Model0_sp0;
 Model0_u64 Model0_sp1;
 Model0_u64 Model0_sp2;
 Model0_u64 Model0_reserved2;
 Model0_u64 Model0_ist[7];
 Model0_u32 Model0_reserved3;
 Model0_u32 Model0_reserved4;
 Model0_u16 Model0_reserved5;
 Model0_u16 Model0_io_bitmap_base;

} __attribute__((packed)) __attribute__((__aligned__((1 << (6)))));


/*
 * IO-bitmap sizes:
 */






struct Model0_tss_struct {
 /*
	 * The hardware state:
	 */
 struct Model0_x86_hw_tss Model0_x86_tss;

 /*
	 * The extra 1 is there because the CPU will access an
	 * additional byte beyond the end of the IO permission
	 * bitmap. The extra byte must be all 1 bits, and must
	 * be within the limit.
	 */
 unsigned long Model0_io_bitmap[((65536/8)/sizeof(long)) + 1];
} __attribute__((__aligned__((1 << (6)))));

extern __attribute__((section(".data..percpu" "..shared_aligned"))) __typeof__(struct Model0_tss_struct) Model0_cpu_tss __attribute__((__aligned__((1 << (6)))));





/*
 * Save the original ist values for checking stack pointers during debugging
 */
struct Model0_orig_ist {
 unsigned long Model0_ist[7];
};


extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model0_orig_ist) Model0_orig_ist;

union Model0_irq_stack_union {
 char Model0_irq_stack[(((1UL) << 12) << (2 + 0))];
 /*
	 * GCC hardcodes the stack canary as %gs:40.  Since the
	 * irq_stack is the object at %gs:0, we reserve the bottom
	 * 48 bytes of the irq stack for the canary.
	 */
 struct {
  char Model0_gs_base[40];
  unsigned long Model0_stack_canary;
 };
};

extern __attribute__((section(".data..percpu" "..first"))) __typeof__(union Model0_irq_stack_union) Model0_irq_stack_union ;
extern typeof(Model0_irq_stack_union) Model0_init_per_cpu__irq_stack_union;

extern __attribute__((section(".data..percpu" ""))) __typeof__(char *) Model0_irq_stack_ptr;
extern __attribute__((section(".data..percpu" ""))) __typeof__(unsigned int) Model0_irq_count;
extern void Model0_ignore_sysret(void);
extern unsigned int Model0_fpu_kernel_xstate_size;
extern unsigned int Model0_fpu_user_xstate_size;

struct Model0_perf_event;

typedef struct {
 unsigned long Model0_seg;
} Model0_mm_segment_t;

struct Model0_thread_struct {
 /* Cached TLS descriptors: */
 struct Model0_desc_struct Model0_tls_array[3];
 unsigned long Model0_sp0;
 unsigned long Model0_sp;



 unsigned short Model0_es;
 unsigned short Model0_ds;
 unsigned short Model0_fsindex;
 unsigned short Model0_gsindex;





 unsigned long Model0_fsbase;
 unsigned long Model0_gsbase;
 /* Save middle states of ptrace breakpoints */
 struct Model0_perf_event *Model0_ptrace_bps[4];
 /* Debug status used for traps, single steps, etc... */
 unsigned long Model0_debugreg6;
 /* Keep track of the exact dr7 value set by the user */
 unsigned long Model0_ptrace_dr7;
 /* Fault info: */
 unsigned long Model0_cr2;
 unsigned long Model0_trap_nr;
 unsigned long Model0_error_code;




 /* IO permissions: */
 unsigned long *Model0_io_bitmap_ptr;
 unsigned long Model0_iopl;
 /* Max allowed port in the bitmap, in bytes: */
 unsigned Model0_io_bitmap_max;

 Model0_mm_segment_t Model0_addr_limit;

 unsigned int Model0_sig_on_uaccess_err:1;
 unsigned int Model0_uaccess_err:1; /* uaccess failed */

 /* Floating point and extended processor state */
 struct Model0_fpu Model0_fpu;
 /*
	 * WARNING: 'fpu' is dynamically-sized.  It *MUST* be at
	 * the end.
	 */
};

/*
 * Set IOPL bits in EFLAGS from given mask
 */
static inline __attribute__((no_instrument_function)) void Model0_native_set_iopl_mask(unsigned Model0_mask)
{
}

static inline __attribute__((no_instrument_function)) void
Model0_native_load_sp0(struct Model0_tss_struct *Model0_tss, struct Model0_thread_struct *thread)
{
 Model0_tss->Model0_x86_tss.Model0_sp0 = thread->Model0_sp0;







}

static inline __attribute__((no_instrument_function)) void Model0_native_swapgs(void)
{

 asm volatile("swapgs" ::: "memory");

}

static inline __attribute__((no_instrument_function)) unsigned long Model0_current_top_of_stack(void)
{

 return ({ typeof(Model0_cpu_tss.Model0_x86_tss.Model0_sp0) Model0_pfo_ret__; switch (sizeof(Model0_cpu_tss.Model0_x86_tss.Model0_sp0)) { case 1: asm("mov" "b ""%%""gs"":" "%" "P1"",%0" : "=q" (Model0_pfo_ret__) : "p" (&(Model0_cpu_tss.Model0_x86_tss.Model0_sp0))); break; case 2: asm("mov" "w ""%%""gs"":" "%" "P1"",%0" : "=r" (Model0_pfo_ret__) : "p" (&(Model0_cpu_tss.Model0_x86_tss.Model0_sp0))); break; case 4: asm("mov" "l ""%%""gs"":" "%" "P1"",%0" : "=r" (Model0_pfo_ret__) : "p" (&(Model0_cpu_tss.Model0_x86_tss.Model0_sp0))); break; case 8: asm("mov" "q ""%%""gs"":" "%" "P1"",%0" : "=r" (Model0_pfo_ret__) : "p" (&(Model0_cpu_tss.Model0_x86_tss.Model0_sp0))); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; });




}






static inline __attribute__((no_instrument_function)) void Model0_load_sp0(struct Model0_tss_struct *Model0_tss,
       struct Model0_thread_struct *thread)
{
 Model0_native_load_sp0(Model0_tss, thread);
}




/* Free all resources held by a thread. */
extern void Model0_release_thread(struct Model0_task_struct *);

unsigned long Model0_get_wchan(struct Model0_task_struct *Model0_p);

/*
 * Generic CPUID function
 * clear %ecx since some cpus (Cyrix MII) do not set or clear %ecx
 * resulting in stale register contents being returned.
 */
static inline __attribute__((no_instrument_function)) void Model0_cpuid(unsigned int Model0_op,
    unsigned int *Model0_eax, unsigned int *Model0_ebx,
    unsigned int *Model0_ecx, unsigned int *Model0_edx)
{
 *Model0_eax = Model0_op;
 *Model0_ecx = 0;
 Model0_native_cpuid(Model0_eax, Model0_ebx, Model0_ecx, Model0_edx);
}

/* Some CPUID calls want 'count' to be placed in ecx */
static inline __attribute__((no_instrument_function)) void Model0_cpuid_count(unsigned int Model0_op, int Model0_count,
          unsigned int *Model0_eax, unsigned int *Model0_ebx,
          unsigned int *Model0_ecx, unsigned int *Model0_edx)
{
 *Model0_eax = Model0_op;
 *Model0_ecx = Model0_count;
 Model0_native_cpuid(Model0_eax, Model0_ebx, Model0_ecx, Model0_edx);
}

/*
 * CPUID functions returning a single datum
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_cpuid_eax(unsigned int Model0_op)
{
 unsigned int Model0_eax, Model0_ebx, Model0_ecx, Model0_edx;

 Model0_cpuid(Model0_op, &Model0_eax, &Model0_ebx, &Model0_ecx, &Model0_edx);

 return Model0_eax;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_cpuid_ebx(unsigned int Model0_op)
{
 unsigned int Model0_eax, Model0_ebx, Model0_ecx, Model0_edx;

 Model0_cpuid(Model0_op, &Model0_eax, &Model0_ebx, &Model0_ecx, &Model0_edx);

 return Model0_ebx;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_cpuid_ecx(unsigned int Model0_op)
{
 unsigned int Model0_eax, Model0_ebx, Model0_ecx, Model0_edx;

 Model0_cpuid(Model0_op, &Model0_eax, &Model0_ebx, &Model0_ecx, &Model0_edx);

 return Model0_ecx;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_cpuid_edx(unsigned int Model0_op)
{
 unsigned int Model0_eax, Model0_ebx, Model0_ecx, Model0_edx;

 Model0_cpuid(Model0_op, &Model0_eax, &Model0_ebx, &Model0_ecx, &Model0_edx);

 return Model0_edx;
}

/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_rep_nop(void)
{
 asm volatile("rep; nop" ::: "memory");
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_cpu_relax(void)
{
 Model0_rep_nop();
}



/* Stop speculative execution and prefetching of modified code. */
static inline __attribute__((no_instrument_function)) void Model0_sync_core(void)
{
 int Model0_tmp;
 /*
	 * CPUID is a barrier to speculative execution.
	 * Prefetched instructions are automatically
	 * invalidated when modified.
	 */
 asm volatile("cpuid"
       : "=a" (Model0_tmp)
       : "0" (1)
       : "ebx", "ecx", "edx", "memory");

}

extern void Model0_select_idle_routine(const struct Model0_cpuinfo_x86 *Model0_c);
extern void Model0_init_amd_e400_c1e_mask(void);

extern unsigned long Model0_boot_option_idle_override;
extern bool Model0_amd_e400_c1e_detected;

enum Model0_idle_boot_override {Model0_IDLE_NO_OVERRIDE=0, Model0_IDLE_HALT, Model0_IDLE_NOMWAIT,
    Model0_IDLE_POLL};

extern void Model0_enable_sep_cpu(void);
extern int Model0_sysenter_setup(void);

extern void Model0_early_trap_init(void);
void Model0_early_trap_pf_init(void);

/* Defined in head.S */
extern struct Model0_desc_ptr Model0_early_gdt_descr;

extern void Model0_cpu_set_gdt(int);
extern void Model0_switch_to_new_gdt(int);
extern void Model0_load_percpu_segment(int);
extern void Model0_cpu_init(void);

static inline __attribute__((no_instrument_function)) unsigned long Model0_get_debugctlmsr(void)
{
 unsigned long Model0_debugctlmsr = 0;





 ((Model0_debugctlmsr) = Model0_native_read_msr((0x000001d9)));

 return Model0_debugctlmsr;
}

static inline __attribute__((no_instrument_function)) void Model0_update_debugctlmsr(unsigned long Model0_debugctlmsr)
{




 Model0_wrmsrl(0x000001d9, Model0_debugctlmsr);
}

extern void Model0_set_task_blockstep(struct Model0_task_struct *Model0_task, bool Model0_on);

/* Boot loader type from the setup header: */
extern int Model0_bootloader_type;
extern int Model0_bootloader_version;

extern char Model0_ignore_fpu_irq;
/*
 * Prefetch instructions for Pentium III (+) and AMD Athlon (+)
 *
 * It's not worth to care about 3dnow prefetches for the K6
 * because they are microcoded there and very slow.
 */
static inline __attribute__((no_instrument_function)) void Model0_prefetch(const void *Model0_x)
{
 asm volatile ("661:\n\t" "prefetcht0 %P1" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 0*32+25)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "prefetchnta %P1" "\n" "665""1" ":\n\t" ".popsection" : : "i" (0), "m" (*(const char *)Model0_x));


}

/*
 * 3dnow prefetch to get an exclusive cache line.
 * Useful for spinlocks to avoid one state transition in the
 * cache coherency protocol:
 */
static inline __attribute__((no_instrument_function)) void Model0_prefetchw(const void *Model0_x)
{
#if CY_ABSTRACT6
    __builtin_prefetch(Model0_x, 1);
#else
 asm volatile ("661:\n\t" "prefetcht0 %P1" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 6*32+ 8)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "prefetchw %P1" "\n" "665""1" ":\n\t" ".popsection" : : "i" (0), "m" (*(const char *)Model0_x));
#endif
}

static inline __attribute__((no_instrument_function)) void Model0_spin_lock_prefetch(const void *Model0_x)
{
 Model0_prefetchw(Model0_x);
}
/*
 * User space process size. 47bits minus one guard page.  The guard
 * page is necessary on Intel CPUs: if a SYSCALL instruction is at
 * the highest possible canonical userspace address, then that
 * syscall will enter the kernel with a non-canonical return
 * address, and SYSRET will explode dangerously.  We avoid this
 * particular problem by preventing anything from being mapped
 * at the maximum canonical address.
 */


/* This decides where the kernel will search for a free chunk of vm
 * space during mmap's.
 */
/*
 * Return saved PC of a blocked thread.
 * What is this good for? it will be always the scheduler or ret_from_fork.
 */



extern unsigned long Model0_KSTK_ESP(struct Model0_task_struct *Model0_task);



extern void Model0_start_thread(struct Model0_pt_regs *Model0_regs, unsigned long Model0_new_ip,
            unsigned long Model0_new_sp);

/*
 * This decides where the kernel will search for a free chunk of vm
 * space during mmap's.
 */




/* Get/set a process' ability to use the timestamp counter instruction */



extern int Model0_get_tsc_mode(unsigned long Model0_adr);
extern int Model0_set_tsc_mode(unsigned int Model0_val);

/* Register/unregister a process' MPX related resource */







static inline __attribute__((no_instrument_function)) int Model0_mpx_enable_management(void)
{
 return -22;
}
static inline __attribute__((no_instrument_function)) int Model0_mpx_disable_management(void)
{
 return -22;
}


extern Model0_u16 Model0_amd_get_nb_id(int Model0_cpu);
extern Model0_u32 Model0_amd_get_nodes_per_socket(void);

static inline __attribute__((no_instrument_function)) Model0_uint32_t Model0_hypervisor_cpuid_base(const char *Model0_sig, Model0_uint32_t Model0_leaves)
{
 Model0_uint32_t Model0_base, Model0_eax, Model0_signature[3];

 for (Model0_base = 0x40000000; Model0_base < 0x40010000; Model0_base += 0x100) {
  Model0_cpuid(Model0_base, &Model0_eax, &Model0_signature[0], &Model0_signature[1], &Model0_signature[2]);

  if (!Model0_memcmp(Model0_sig, Model0_signature, 12) &&
      (Model0_leaves == 0 || ((Model0_eax - Model0_base) >= Model0_leaves)))
   return Model0_base;
 }

 return 0;
}

extern unsigned long Model0_arch_align_stack(unsigned long Model0_sp);
extern void Model0_free_init_pages(char *Model0_what, unsigned long Model0_begin, unsigned long Model0_end);

void Model0_default_idle(void);






void Model0_stop_this_cpu(void *Model0_dummy);
void Model0_df_debug(struct Model0_pt_regs *Model0_regs, long Model0_error_code);






enum Model0_cpuid_leafs
{
 Model0_CPUID_1_EDX = 0,
 Model0_CPUID_8000_0001_EDX,
 Model0_CPUID_8086_0001_EDX,
 Model0_CPUID_LNX_1,
 Model0_CPUID_1_ECX,
 Model0_CPUID_C000_0001_EDX,
 Model0_CPUID_8000_0001_ECX,
 Model0_CPUID_LNX_2,
 Model0_CPUID_LNX_3,
 Model0_CPUID_7_0_EBX,
 Model0_CPUID_D_1_EAX,
 Model0_CPUID_F_0_EDX,
 Model0_CPUID_F_1_EDX,
 Model0_CPUID_8000_0008_EBX,
 Model0_CPUID_6_EAX,
 Model0_CPUID_8000_000A_EDX,
 Model0_CPUID_7_ECX,
 Model0_CPUID_8000_0007_EBX,
};


extern const char * const Model0_x86_cap_flags[18*32];
extern const char * const Model0_x86_power_flags[32];







/*
 * In order to save room, we index into this array by doing
 * X86_BUG_<name> - NCAPINTS*32.
 */
extern const char * const Model0_x86_bug_flags[1*32];




/*
 * There are 32 bits/features in each mask word.  The high bits
 * (selected with (bit>>5) give us the word number and the low 5
 * bits give us the bit/feature number inside the word.
 * (1UL<<((bit)&31) gives us a mask for the feature_bit so we can
 * see if it is set in the mask word.
 */
/*
 * This macro is for detection of features which need kernel
 * infrastructure to be used.  It may *not* directly test the CPU
 * itself.  Use the cpu_has() family if you want true runtime
 * testing of CPU features, like in hypervisor code where you are
 * supporting a possible guest feature where host support for it
 * is not relevant.
 */
/*
 * Fall back to dynamic for gcc versions which don't support asm goto. Should be
 * a minority now anyway.
 */


struct Model0_thread_info {
 struct Model0_task_struct *Model0_task; /* main task structure */
 __u32 Model0_flags; /* low level flags */
 __u32 Model0_status; /* thread synchronous flags */
 __u32 Model0_cpu; /* current CPU */
};
/*
 * thread information flags
 * - these are process state flags that various assembly files
 *   may need to access
 * - pending work-to-be-done flags are in LSW
 * - other flags in MSW
 * Warning: layout of LSW is hardcoded in entry.S
 */
/*
 * work to do in syscall_trace_enter().  Also includes TIF_NOHZ for
 * enter_from_user_mode()
 */





/* work to do on any return to user space */




/* flags to check in __switch_to() */
/*
 * macros/functions for gaining access to the thread information structure
 *
 * preempt_count needs to be 1 initially, until the scheduler is functional.
 */


static inline __attribute__((no_instrument_function)) struct Model0_thread_info *Model0_current_thread_info(void)
{
 return (struct Model0_thread_info *)(Model0_current_top_of_stack() - (((1UL) << 12) << (2 + 0)));
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_current_stack_pointer(void)
{
 unsigned long Model0_sp;

 asm("mov %%rsp,%0" : "=g" (Model0_sp));



 return Model0_sp;
}

/*
 * Walks up the stack frames to make sure that the specified object is
 * entirely contained by a single stack frame.
 *
 * Returns:
 *		 1 if within a frame
 *		-1 if placed across a frame boundary (or outside stack)
 *		 0 unable to determine (no frame pointers, etc)
 */
static inline __attribute__((no_instrument_function)) int Model0_arch_within_stack_frames(const void * const Model0_stack,
        const void * const Model0_stackend,
        const void *Model0_obj, unsigned long Model0_len)
{

 const void *Model0_frame = ((void *)0);
 const void *Model0_oldframe;

 Model0_oldframe = __builtin_frame_address(1);
 if (Model0_oldframe)
  Model0_frame = __builtin_frame_address(2);
 /*
	 * low ----------------------------------------------> high
	 * [saved bp][saved ip][args][local vars][saved bp][saved ip]
	 *                     ^----------------^
	 *               allow copies only within here
	 */
 while (Model0_stack <= Model0_frame && Model0_frame < Model0_stackend) {
  /*
		 * If obj + len extends past the last frame, this
		 * check won't pass and the next frame will be 0,
		 * causing us to bail out and correctly report
		 * the copy as invalid.
		 */
  if (Model0_obj + Model0_len <= Model0_frame)
   return Model0_obj >= Model0_oldframe + 2 * sizeof(void *) ? 1 : -1;
  Model0_oldframe = Model0_frame;
  Model0_frame = *(const void * const *)Model0_frame;
 }
 return -1;



}
/*
 * Thread-synchronous status.
 *
 * This is different from the flags in that nobody else
 * ever touches our thread-synchronous status, so we don't
 * have to worry about atomic accesses.
 */







static inline __attribute__((no_instrument_function)) bool Model0_in_ia32_syscall(void)
{




 if (Model0_current_thread_info()->Model0_status & 0x0002)
  return true;

 return false;
}

/*
 * Force syscall return via IRET by making it look as if there was
 * some work pending. IRET is our most capable (but slowest) syscall
 * return path, which is able to restore modified SS, CS and certain
 * EFLAGS values that other (fast) syscall return instructions
 * are not able to restore properly.
 */


extern void Model0_arch_task_cache_init(void);
extern int Model0_arch_dup_task_struct(struct Model0_task_struct *Model0_dst, struct Model0_task_struct *Model0_src);
extern void Model0_arch_release_task_struct(struct Model0_task_struct *Model0_tsk);
/*
 * flag set/clear/test wrappers
 * - pass TIF_xxxx constants to these functions
 */

static inline __attribute__((no_instrument_function)) void Model0_set_ti_thread_flag(struct Model0_thread_info *Model0_ti, int Model0_flag)
{
 Model0_set_bit(Model0_flag, (unsigned long *)&Model0_ti->Model0_flags);
}

static inline __attribute__((no_instrument_function)) void Model0_clear_ti_thread_flag(struct Model0_thread_info *Model0_ti, int Model0_flag)
{
 Model0_clear_bit(Model0_flag, (unsigned long *)&Model0_ti->Model0_flags);
}

static inline __attribute__((no_instrument_function)) int Model0_test_and_set_ti_thread_flag(struct Model0_thread_info *Model0_ti, int Model0_flag)
{
 return Model0_test_and_set_bit(Model0_flag, (unsigned long *)&Model0_ti->Model0_flags);
}

static inline __attribute__((no_instrument_function)) int Model0_test_and_clear_ti_thread_flag(struct Model0_thread_info *Model0_ti, int Model0_flag)
{
 return Model0_test_and_clear_bit(Model0_flag, (unsigned long *)&Model0_ti->Model0_flags);
}

static inline __attribute__((no_instrument_function)) int Model0_test_ti_thread_flag(struct Model0_thread_info *Model0_ti, int Model0_flag)
{
 return (__builtin_constant_p((Model0_flag)) ? Model0_constant_test_bit((Model0_flag), ((unsigned long *)&Model0_ti->Model0_flags)) : Model0_variable_test_bit((Model0_flag), ((unsigned long *)&Model0_ti->Model0_flags)));
}
static inline __attribute__((no_instrument_function)) void Model0_check_object_size(const void *Model0_ptr, unsigned long Model0_n,
         bool Model0_to_user)
{ }

extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model0___preempt_count;

/*
 * We use the PREEMPT_NEED_RESCHED bit as an inverted NEED_RESCHED such
 * that a decrement hitting 0 means we can and should reschedule.
 */


/*
 * We mask the PREEMPT_NEED_RESCHED bit so as not to confuse all current users
 * that think a non-zero value indicates we cannot preempt.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_preempt_count(void)
{
 return ({ typeof(Model0___preempt_count) Model0_pfo_ret__; switch (sizeof(Model0___preempt_count)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }) & ~0x80000000;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_preempt_count_set(int Model0_pc)
{
 do { typedef typeof((Model0___preempt_count)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_pc); (void)Model0_pto_tmp__; } switch (sizeof((Model0___preempt_count))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "qi" ((Model0_pto_T__)(Model0_pc))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pto_T__)(Model0_pc))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pto_T__)(Model0_pc))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "re" ((Model0_pto_T__)(Model0_pc))); break; default: Model0___bad_percpu_size(); } } while (0);
}

/*
 * must be macros to avoid header recursion hell
 */






/*
 * We fold the NEED_RESCHED bit into the preempt count such that
 * preempt_enable() can decrement and test for needing to reschedule with a
 * single instruction.
 *
 * We invert the actual bit, so that when the decrement hits 0 we know we both
 * need to resched (the bit is cleared) and can resched (no preempt count).
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_set_preempt_need_resched(void)
{
 do { typedef typeof((Model0___preempt_count)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (~0x80000000); (void)Model0_pto_tmp__; } switch (sizeof((Model0___preempt_count))) { case 1: asm("and" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "qi" ((Model0_pto_T__)(~0x80000000))); break; case 2: asm("and" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pto_T__)(~0x80000000))); break; case 4: asm("and" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pto_T__)(~0x80000000))); break; case 8: asm("and" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "re" ((Model0_pto_T__)(~0x80000000))); break; default: Model0___bad_percpu_size(); } } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_clear_preempt_need_resched(void)
{
 do { typedef typeof((Model0___preempt_count)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (0x80000000); (void)Model0_pto_tmp__; } switch (sizeof((Model0___preempt_count))) { case 1: asm("or" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "qi" ((Model0_pto_T__)(0x80000000))); break; case 2: asm("or" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pto_T__)(0x80000000))); break; case 4: asm("or" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pto_T__)(0x80000000))); break; case 8: asm("or" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "re" ((Model0_pto_T__)(0x80000000))); break; default: Model0___bad_percpu_size(); } } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_test_preempt_need_resched(void)
{
 return !(({ typeof(Model0___preempt_count) Model0_pfo_ret__; switch (sizeof(Model0___preempt_count)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }) & 0x80000000);
}

/*
 * The various preempt_count add/sub methods
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___preempt_count_add(int Model0_val)
{
#if CY_ABSTRACT6
    //We assume only one interleaving would happen, so there's no preempt
#else
 do { typedef typeof((Model0___preempt_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_val) && ((Model0_val) == 1 || (Model0_val) == -1)) ? (int)(Model0_val) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_val); (void)Model0_pao_tmp__; } switch (sizeof((Model0___preempt_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "qi" ((Model0_pao_T__)(Model0_val))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pao_T__)(Model0_val))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pao_T__)(Model0_val))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "re" ((Model0_pao_T__)(Model0_val))); break; default: Model0___bad_percpu_size(); } } while (0);
#endif
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___preempt_count_sub(int Model0_val)
{
#if CY_ABSTRACT6
    //We assume only one interleaving would happen, so there's no preempt
#else
 do { typedef typeof((Model0___preempt_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-Model0_val) && ((-Model0_val) == 1 || (-Model0_val) == -1)) ? (int)(-Model0_val) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-Model0_val); (void)Model0_pao_tmp__; } switch (sizeof((Model0___preempt_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "qi" ((Model0_pao_T__)(-Model0_val))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pao_T__)(-Model0_val))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "ri" ((Model0_pao_T__)(-Model0_val))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0___preempt_count)) : "re" ((Model0_pao_T__)(-Model0_val))); break; default: Model0___bad_percpu_size(); } } while (0);
#endif
}

/*
 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
 * a decrement which hits zero means we have no preempt_count and should
 * reschedule.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0___preempt_count_dec_and_test(void)
{
 do { bool Model0_c; asm volatile ("decl" " " "%%""gs"":" "%" "0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model0___preempt_count), [_cc_e] "=qm" (Model0_c) : : "memory"); return Model0_c; } while (0);
}

/*
 * Returns true when we need to resched and can (barring IRQ state).
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_should_resched(int Model0_preempt_offset)
{
 return __builtin_expect(!!(({ typeof(Model0___preempt_count) Model0_pfo_ret__; switch (sizeof(Model0___preempt_count)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0___preempt_count)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }) == Model0_preempt_offset), 0);
}






/*
 * Are we doing bottom half or hardware interrupt processing?
 * Are we in a softirq context? Interrupt context?
 * in_softirq - Are we currently processing softirq or have bh disabled?
 * in_serving_softirq - Are we currently processing softirq?
 */





/*
 * Are we in NMI context?
 */


/*
 * The preempt_count offset after preempt_disable();
 */






/*
 * The preempt_count offset after spin_lock()
 */


/*
 * The preempt_count offset needed for things like:
 *
 *  spin_lock_bh()
 *
 * Which need to disable both preemption (CONFIG_PREEMPT_COUNT) and
 * softirqs, such that unlock sequences of:
 *
 *  spin_unlock();
 *  local_bh_enable();
 *
 * Work as expected.
 */


/*
 * Are we running in atomic context?  WARNING: this macro cannot
 * always detect atomic context; in particular, it cannot know about
 * held spinlocks in non-preemptible kernels.  Thus it should not be
 * used in the general case to determine whether sleeping is possible.
 * Do not use in_atomic() in driver code.
 */


/*
 * Check whether we were atomic before we did preempt_disable():
 * (used by the scheduler)
 */
/*
 * Even if we don't have any preemption, we need preempt disable/enable
 * to be barriers, so that we don't have things like get_user/put_user
 * that can cause faults and scheduling migrate into our preempt-protected
 * region.
 */














static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___local_bh_disable_ip(unsigned long Model0_ip, unsigned int Model0_cnt)
{
 Model0___preempt_count_add(Model0_cnt);
 __asm__ __volatile__("": : :"memory");
}


static inline __attribute__((no_instrument_function)) void Model0_local_bh_disable(void)
{
 Model0___local_bh_disable_ip(({ __label__ Model0___here; Model0___here: (unsigned long)&&Model0___here; }), (2 * (1UL << (0 + 8))));
}

extern void Model0__local_bh_enable(void);
extern void Model0___local_bh_enable_ip(unsigned long Model0_ip, unsigned int Model0_cnt);

static inline __attribute__((no_instrument_function)) void Model0_local_bh_enable_ip(unsigned long Model0_ip)
{
 Model0___local_bh_enable_ip(Model0_ip, (2 * (1UL << (0 + 8))));
}

static inline __attribute__((no_instrument_function)) void Model0_local_bh_enable(void)
{
 Model0___local_bh_enable_ip(({ __label__ Model0___here; Model0___here: (unsigned long)&&Model0___here; }), (2 * (1UL << (0 + 8))));
}



/*
 * Must define these before including other files, inline functions need them
 */
/*
 * Pull the arch_spinlock_t and arch_rwlock_t definitions:
 */




/*
 * include/linux/spinlock_types.h - generic spinlock type definitions
 *                                  and initializers
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */



typedef Model0_u8 Model0___ticket_t;
typedef Model0_u16 Model0___ticketpair_t;
/*
 * Queued spinlock
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.
 *
 * Authors: Waiman Long <waiman.long@hp.com>
 */



/*
 * Including atomic.h with PARAVIRT on will cause compilation errors because
 * of recursive header file incluson via paravirt_types.h. So don't include
 * it if PARAVIRT is on.
 */





typedef struct Model0_qspinlock {
 Model0_atomic_t Model0_val;
} Model0_arch_spinlock_t;

/*
 * Initializier
 */


/*
 * Bitfields in the atomic value:
 *
 * When NR_CPUS < 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-15: not used
 * 16-17: tail index
 * 18-31: tail cpu (+1)
 *
 * When NR_CPUS >= 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-10: tail index
 * 11-31: tail cpu (+1)
 */





/*
 * The queue read/write lock data structure
 */

typedef struct Model0_qrwlock {
 Model0_atomic_t Model0_cnts;
 Model0_arch_spinlock_t Model0_wait_lock;
} Model0_arch_rwlock_t;




/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra
 *
 * see Documentation/locking/lockdep-design.txt for more details.
 */



struct Model0_task_struct;
struct Model0_lockdep_map;

/* for sysctl */
extern int Model0_prove_locking;
extern int Model0_lock_stat;
static inline __attribute__((no_instrument_function)) void Model0_lockdep_off(void)
{
}

static inline __attribute__((no_instrument_function)) void Model0_lockdep_on(void)
{
}
/*
 * We don't define lockdep_match_class() and lockdep_match_key() for !LOCKDEP
 * case since the result is not well defined and the caller should rather
 * #ifdef the call himself.
 */





/*
 * The class key takes no space if lockdep is disabled:
 */
struct Model0_lock_class_key { };
struct Model0_pin_cookie { };
static inline __attribute__((no_instrument_function)) void Model0_print_irqtrace_events(struct Model0_task_struct *Model0_curr)
{
}


/*
 * For trivial one-depth nesting of a lock-class, the following
 * global define can be used. (Subsystems with multiple levels
 * of nesting should define their own lock-nesting subclasses.)
 */


/*
 * Map the dependency ops to NOP or to real lockdep ops, depending
 * on the per lock-class debug mode:
 */
static inline __attribute__((no_instrument_function)) void
Model0_lockdep_rcu_suspicious(const char *Model0_file, const int Model0_line, const char *Model0_s)
{
}

typedef struct Model0_raw_spinlock {
 Model0_arch_spinlock_t Model0_raw_lock;
} Model0_raw_spinlock_t;
typedef struct Model0_spinlock {
 union {
  struct Model0_raw_spinlock Model0_rlock;
 };
} Model0_spinlock_t;



/*
 * include/linux/rwlock_types.h - generic rwlock type definitions
 *				  and initializers
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
typedef struct {
 Model0_arch_rwlock_t Model0_raw_lock;
} Model0_rwlock_t;

/*
 * Pull the arch_spin*() functions/declarations (UP-nondebug doesn't need them):
 */












/* Various instructions on x86 need to be replaced for
 * para-virtualization: those hooks are defined here. */
static inline __attribute__((no_instrument_function)) void Model0_paravirt_arch_dup_mmap(struct Model0_mm_struct *Model0_oldmm,
       struct Model0_mm_struct *Model0_mm)
{
}

static inline __attribute__((no_instrument_function)) void Model0_paravirt_arch_exit_mmap(struct Model0_mm_struct *Model0_mm)
{
}


/*
 * Your basic SMP spinlocks, allowing only a single CPU anywhere
 *
 * Simple spin lock operations.  There are two variants, one clears IRQ's
 * on the local processor, one does not.
 *
 * These are fair FIFO ticket locks, which support up to 2^16 CPUs.
 *
 * (the type definitions are in asm/spinlock_types.h)
 */
/* How long a lock should spin before we consider blocking */


extern struct Model0_static_key Model0_paravirt_ticketlocks_enabled;
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_static_key_false(struct Model0_static_key *Model0_key);











/**
 * queued_spin_unlock - release a queued spinlock
 * @lock : Pointer to queued spinlock structure
 *
 * A smp_store_release() on the least-significant byte.
 */
static inline __attribute__((no_instrument_function)) void Model0_native_queued_spin_unlock(struct Model0_qspinlock *Model0_lock)
{
 do { do { bool Model0___cond = !((sizeof(*(Model0_u8 *)Model0_lock) == sizeof(char) || sizeof(*(Model0_u8 *)Model0_lock) == sizeof(short) || sizeof(*(Model0_u8 *)Model0_lock) == sizeof(int) || sizeof(*(Model0_u8 *)Model0_lock) == sizeof(long))); extern void Model0___compiletime_assert_17(void) ; if (Model0___cond) Model0___compiletime_assert_17(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*(Model0_u8 *)Model0_lock) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*(Model0_u8 *)Model0_lock)) (0) }; Model0___write_once_size(&(*(Model0_u8 *)Model0_lock), Model0___u.Model0___c, sizeof(*(Model0_u8 *)Model0_lock)); Model0___u.Model0___val; }); } while (0);
}
static inline __attribute__((no_instrument_function)) void Model0_queued_spin_unlock(struct Model0_qspinlock *Model0_lock)
{
 Model0_native_queued_spin_unlock(Model0_lock);
}
/*
 * Queued spinlock
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.
 * (C) Copyright 2015 Hewlett-Packard Enterprise Development LP
 *
 * Authors: Waiman Long <waiman.long@hpe.com>
 */





/**
 * queued_spin_unlock_wait - wait until the _current_ lock holder releases the lock
 * @lock : Pointer to queued spinlock structure
 *
 * There is a very slight possibility of live-lock if the lockers keep coming
 * and the waiter is just unfortunate enough to not see any unlock state.
 */

extern void Model0_queued_spin_unlock_wait(struct Model0_qspinlock *Model0_lock);


/**
 * queued_spin_is_locked - is the spinlock locked?
 * @lock: Pointer to queued spinlock structure
 * Return: 1 if it is locked, 0 otherwise
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_queued_spin_is_locked(struct Model0_qspinlock *Model0_lock)
{
 /*
	 * See queued_spin_unlock_wait().
	 *
	 * Any !0 state indicates it is locked, even if _Q_LOCKED_VAL
	 * isn't immediately observable.
	 */
 return Model0_atomic_read(&Model0_lock->Model0_val);
}


/**
 * queued_spin_value_unlocked - is the spinlock structure unlocked?
 * @lock: queued spinlock structure
 * Return: 1 if it is unlocked, 0 otherwise
 *
 * N.B. Whenever there are tasks waiting for the lock, it is considered
 *      locked wrt the lockref code to avoid lock stealing by the lockref
 *      code and change things underneath the lock. This also allows some
 *      optimizations to be applied without conflict with lockref.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_queued_spin_value_unlocked(struct Model0_qspinlock Model0_lock)
{
 return !Model0_atomic_read(&Model0_lock.Model0_val);
}

/**
 * queued_spin_is_contended - check if the lock is contended
 * @lock : Pointer to queued spinlock structure
 * Return: 1 if lock contended, 0 otherwise
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_queued_spin_is_contended(struct Model0_qspinlock *Model0_lock)
{
 return Model0_atomic_read(&Model0_lock->Model0_val) & ~(((1U << 8) - 1) << 0);
}
/**
 * queued_spin_trylock - try to acquire the queued spinlock
 * @lock : Pointer to queued spinlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_queued_spin_trylock(struct Model0_qspinlock *Model0_lock)
{
 if (!Model0_atomic_read(&Model0_lock->Model0_val) &&
    (Model0_atomic_cmpxchg(&Model0_lock->Model0_val, 0, (1U << 0)) == 0))
  return 1;
 return 0;
}

extern void Model0_queued_spin_lock_slowpath(struct Model0_qspinlock *Model0_lock, Model0_u32 Model0_val);

/**
 * queued_spin_lock - acquire a queued spinlock
 * @lock: Pointer to queued spinlock structure
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_queued_spin_lock(struct Model0_qspinlock *Model0_lock)
{
 Model0_u32 Model0_val;

 Model0_val = Model0_atomic_cmpxchg(&Model0_lock->Model0_val, 0, (1U << 0));
 if (__builtin_expect(!!(Model0_val == 0), 1))
  return;
 Model0_queued_spin_lock_slowpath(Model0_lock, Model0_val);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_virt_spin_lock(struct Model0_qspinlock *Model0_lock)
{
 return false;
}


/*
 * Remapping spinlock architecture specific functions to the corresponding
 * queued spinlock functions.
 */
/*
 * Read-write spinlocks, allowing multiple readers
 * but only one writer.
 *
 * NOTE! it is quite common to have readers in interrupts
 * but no interrupt writers. For those circumstances we
 * can "mix" irq-safe locks - any writer needs to get a
 * irq-safe write-lock, but readers can get non-irqsafe
 * read-locks.
 *
 * On x86, we implement read-write locks using the generic qrwlock with
 * x86 specific optimization.
 */






/*
 * Queue read/write lock
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * (C) Copyright 2013-2014 Hewlett-Packard Development Company, L.P.
 *
 * Authors: Waiman Long <waiman.long@hp.com>
 */
/*
 * Writer states & reader shift and bias.
 *
 *       | +0 | +1 | +2 | +3 |
 *   ----+----+----+----+----+
 *    LE | 78 | 56 | 34 | 12 | 0x12345678
 *   ----+----+----+----+----+
 *       | wr |      rd      |
 *       +----+----+----+----+
 *
 *   ----+----+----+----+----+
 *    BE | 12 | 34 | 56 | 78 | 0x12345678
 *   ----+----+----+----+----+
 *       |      rd      | wr |
 *       +----+----+----+----+
 */






/*
 * External function declarations
 */
extern void Model0_queued_read_lock_slowpath(struct Model0_qrwlock *Model0_lock, Model0_u32 Model0_cnts);
extern void Model0_queued_write_lock_slowpath(struct Model0_qrwlock *Model0_lock);

/**
 * queued_read_can_lock- would read_trylock() succeed?
 * @lock: Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) int Model0_queued_read_can_lock(struct Model0_qrwlock *Model0_lock)
{
 return !(Model0_atomic_read(&Model0_lock->Model0_cnts) & 0xff);
}

/**
 * queued_write_can_lock- would write_trylock() succeed?
 * @lock: Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) int Model0_queued_write_can_lock(struct Model0_qrwlock *Model0_lock)
{
 return !Model0_atomic_read(&Model0_lock->Model0_cnts);
}

/**
 * queued_read_trylock - try to acquire read lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((no_instrument_function)) int Model0_queued_read_trylock(struct Model0_qrwlock *Model0_lock)
{
 Model0_u32 Model0_cnts;

 Model0_cnts = Model0_atomic_read(&Model0_lock->Model0_cnts);
 if (__builtin_expect(!!(!(Model0_cnts & 0xff)), 1)) {
  Model0_cnts = (Model0_u32)Model0_atomic_add_return((1U << 8), &Model0_lock->Model0_cnts);
  if (__builtin_expect(!!(!(Model0_cnts & 0xff)), 1))
   return 1;
  Model0_atomic_sub((1U << 8), &Model0_lock->Model0_cnts);
 }
 return 0;
}

/**
 * queued_write_trylock - try to acquire write lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((no_instrument_function)) int Model0_queued_write_trylock(struct Model0_qrwlock *Model0_lock)
{
 Model0_u32 Model0_cnts;

 Model0_cnts = Model0_atomic_read(&Model0_lock->Model0_cnts);
 if (__builtin_expect(!!(Model0_cnts), 0))
  return 0;

 return __builtin_expect(!!(Model0_atomic_cmpxchg(&Model0_lock->Model0_cnts, Model0_cnts, Model0_cnts | 0xff) == Model0_cnts), 1);

}
/**
 * queued_read_lock - acquire read lock of a queue rwlock
 * @lock: Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) void Model0_queued_read_lock(struct Model0_qrwlock *Model0_lock)
{
 Model0_u32 Model0_cnts;

 Model0_cnts = Model0_atomic_add_return((1U << 8), &Model0_lock->Model0_cnts);
 if (__builtin_expect(!!(!(Model0_cnts & 0xff)), 1))
  return;

 /* The slowpath will decrement the reader count, if necessary. */
 Model0_queued_read_lock_slowpath(Model0_lock, Model0_cnts);
}

/**
 * queued_write_lock - acquire write lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) void Model0_queued_write_lock(struct Model0_qrwlock *Model0_lock)
{
 /* Optimize for the unfair lock case where the fair flag is 0. */
 if (Model0_atomic_cmpxchg(&Model0_lock->Model0_cnts, 0, 0xff) == 0)
  return;

 Model0_queued_write_lock_slowpath(Model0_lock);
}

/**
 * queued_read_unlock - release read lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) void Model0_queued_read_unlock(struct Model0_qrwlock *Model0_lock)
{
 /*
	 * Atomically decrement the reader count
	 */
 (void)Model0_atomic_sub_return((1U << 8), &Model0_lock->Model0_cnts);
}

/**
 * __qrwlock_write_byte - retrieve the write byte address of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 * Return: the write byte address of a queue rwlock
 */
static inline __attribute__((no_instrument_function)) Model0_u8 *Model0___qrwlock_write_byte(struct Model0_qrwlock *Model0_lock)
{
 return (Model0_u8 *)Model0_lock + 3 * 0;
}

/**
 * queued_write_unlock - release write lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) void Model0_queued_write_unlock(struct Model0_qrwlock *Model0_lock)
{
 do { do { bool Model0___cond = !((sizeof(*Model0___qrwlock_write_byte(Model0_lock)) == sizeof(char) || sizeof(*Model0___qrwlock_write_byte(Model0_lock)) == sizeof(short) || sizeof(*Model0___qrwlock_write_byte(Model0_lock)) == sizeof(int) || sizeof(*Model0___qrwlock_write_byte(Model0_lock)) == sizeof(long))); extern void Model0___compiletime_assert_165(void) ; if (Model0___cond) Model0___compiletime_assert_165(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*Model0___qrwlock_write_byte(Model0_lock)) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*Model0___qrwlock_write_byte(Model0_lock))) (0) }; Model0___write_once_size(&(*Model0___qrwlock_write_byte(Model0_lock)), Model0___u.Model0___c, sizeof(*Model0___qrwlock_write_byte(Model0_lock))); Model0___u.Model0___val; }); } while (0);
}

/*
 * Remapping rwlock architecture specific functions to the corresponding
 * queue rwlock functions.
 */
/*
 * Despite its name it doesn't necessarily has to be a full barrier.
 * It should only guarantee that a STORE before the critical section
 * can not be reordered with LOADs and STOREs inside this section.
 * spin_lock() is the one-way barrier, this LOAD can not escape out
 * of the region. So the default implementation simply ensures that
 * a STORE can not move into the critical section, smp_wmb() should
 * serialize it with another STORE done by spin_lock().
 */




/**
 * raw_spin_unlock_wait - wait until the spinlock gets unlocked
 * @lock: the spinlock in question.
 */
static inline __attribute__((no_instrument_function)) void Model0_do_raw_spin_lock(Model0_raw_spinlock_t *Model0_lock)
{
 (void)0;
 Model0_queued_spin_lock(&Model0_lock->Model0_raw_lock);
}

static inline __attribute__((no_instrument_function)) void
Model0_do_raw_spin_lock_flags(Model0_raw_spinlock_t *Model0_lock, unsigned long *Model0_flags)
{
 (void)0;
 Model0_queued_spin_lock(&Model0_lock->Model0_raw_lock);
}

static inline __attribute__((no_instrument_function)) int Model0_do_raw_spin_trylock(Model0_raw_spinlock_t *Model0_lock)
{
 return Model0_queued_spin_trylock(&(Model0_lock)->Model0_raw_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_do_raw_spin_unlock(Model0_raw_spinlock_t *Model0_lock)
{
 Model0_queued_spin_unlock(&Model0_lock->Model0_raw_lock);
 (void)0;
}


/*
 * Define the various spin_lock methods.  Note we define these
 * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The
 * various methods are defined as nops in the case they are not
 * required.
 */
/*
 * Always evaluate the 'subclass' argument to avoid that the compiler
 * warns about set-but-not-used variables when building with
 * CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.
 */
/**
 * raw_spin_can_lock - would raw_spin_trylock() succeed?
 * @lock: the spinlock in question.
 */


/* Include rwlock functions */








/*
 * rwlock related methods
 *
 * split out from spinlock.h
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
/*
 * Define the various rw_lock methods.  Note we define these
 * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The various
 * methods are defined as nops in the case they are not required.
 */

/*
 * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
 */









/*
 * include/linux/spinlock_api_smp.h
 *
 * spinlock API declarations on SMP (and debug)
 * (implemented in kernel/spinlock.c)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */

int Model0_in_lock_functions(unsigned long Model0_addr);



void __attribute__((section(".spinlock.text"))) Model0__raw_spin_lock(Model0_raw_spinlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_spin_lock_nested(Model0_raw_spinlock_t *Model0_lock, int Model0_subclass)
                        ;
void __attribute__((section(".spinlock.text"))) Model0__raw_spin_lock_bh_nested(Model0_raw_spinlock_t *Model0_lock, int Model0_subclass)
                        ;
void __attribute__((section(".spinlock.text")))
Model0__raw_spin_lock_nest_lock(Model0_raw_spinlock_t *Model0_lock, struct Model0_lockdep_map *Model0_map)
                        ;
void __attribute__((section(".spinlock.text"))) Model0__raw_spin_lock_bh(Model0_raw_spinlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_spin_lock_irq(Model0_raw_spinlock_t *Model0_lock)
                        ;

unsigned long __attribute__((section(".spinlock.text"))) Model0__raw_spin_lock_irqsave(Model0_raw_spinlock_t *Model0_lock)
                        ;
unsigned long __attribute__((section(".spinlock.text")))
Model0__raw_spin_lock_irqsave_nested(Model0_raw_spinlock_t *Model0_lock, int Model0_subclass)
                        ;
int __attribute__((section(".spinlock.text"))) Model0__raw_spin_trylock(Model0_raw_spinlock_t *Model0_lock);
int __attribute__((section(".spinlock.text"))) Model0__raw_spin_trylock_bh(Model0_raw_spinlock_t *Model0_lock);
void __attribute__((section(".spinlock.text"))) Model0__raw_spin_unlock(Model0_raw_spinlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_spin_unlock_bh(Model0_raw_spinlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_spin_unlock_irq(Model0_raw_spinlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text")))
Model0__raw_spin_unlock_irqrestore(Model0_raw_spinlock_t *Model0_lock, unsigned long Model0_flags)
                        ;
static inline __attribute__((no_instrument_function)) int Model0___raw_spin_trylock(Model0_raw_spinlock_t *Model0_lock)
{
 __asm__ __volatile__("": : :"memory");
 if (Model0_do_raw_spin_trylock(Model0_lock)) {
  do { } while (0);
  return 1;
 }
 __asm__ __volatile__("": : :"memory");
 return 0;
}

/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */


static inline __attribute__((no_instrument_function)) unsigned long Model0___raw_spin_lock_irqsave(Model0_raw_spinlock_t *Model0_lock)
{
 unsigned long Model0_flags;

 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_flags = Model0_arch_local_irq_save(); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 /*
	 * On lockdep we dont want the hand-coded irq-enable of
	 * do_raw_spin_lock_flags() code, because lockdep assumes
	 * that interrupts are not re-enabled during lock-acquire:
	 */



 Model0_do_raw_spin_lock_flags(Model0_lock, &Model0_flags);

 return Model0_flags;
}

static inline __attribute__((no_instrument_function)) void Model0___raw_spin_lock_irq(Model0_raw_spinlock_t *Model0_lock)
{
 do { Model0_arch_local_irq_disable(); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 Model0_do_raw_spin_lock(Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0___raw_spin_lock_bh(Model0_raw_spinlock_t *Model0_lock)
{
 Model0___local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 do { } while (0);
 Model0_do_raw_spin_lock(Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0___raw_spin_lock(Model0_raw_spinlock_t *Model0_lock)
{
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 Model0_do_raw_spin_lock(Model0_lock);
}



static inline __attribute__((no_instrument_function)) void Model0___raw_spin_unlock(Model0_raw_spinlock_t *Model0_lock)
{
 do { } while (0);
 Model0_do_raw_spin_unlock(Model0_lock);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___raw_spin_unlock_irqrestore(Model0_raw_spinlock_t *Model0_lock,
         unsigned long Model0_flags)
{
 do { } while (0);
 Model0_do_raw_spin_unlock(Model0_lock);
 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_arch_local_irq_restore(Model0_flags); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___raw_spin_unlock_irq(Model0_raw_spinlock_t *Model0_lock)
{
 do { } while (0);
 Model0_do_raw_spin_unlock(Model0_lock);
 do { Model0_arch_local_irq_enable(); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___raw_spin_unlock_bh(Model0_raw_spinlock_t *Model0_lock)
{
 do { } while (0);
 Model0_do_raw_spin_unlock(Model0_lock);
 Model0___local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
}

static inline __attribute__((no_instrument_function)) int Model0___raw_spin_trylock_bh(Model0_raw_spinlock_t *Model0_lock)
{
 Model0___local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 if (Model0_do_raw_spin_trylock(Model0_lock)) {
  do { } while (0);
  return 1;
 }
 Model0___local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 return 0;
}









/*
 * include/linux/rwlock_api_smp.h
 *
 * spinlock API declarations on SMP (and debug)
 * (implemented in kernel/spinlock.c)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */

void __attribute__((section(".spinlock.text"))) Model0__raw_read_lock(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_write_lock(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_read_lock_bh(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_write_lock_bh(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_read_lock_irq(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_write_lock_irq(Model0_rwlock_t *Model0_lock) ;
unsigned long __attribute__((section(".spinlock.text"))) Model0__raw_read_lock_irqsave(Model0_rwlock_t *Model0_lock)
                       ;
unsigned long __attribute__((section(".spinlock.text"))) Model0__raw_write_lock_irqsave(Model0_rwlock_t *Model0_lock)
                       ;
int __attribute__((section(".spinlock.text"))) Model0__raw_read_trylock(Model0_rwlock_t *Model0_lock);
int __attribute__((section(".spinlock.text"))) Model0__raw_write_trylock(Model0_rwlock_t *Model0_lock);
void __attribute__((section(".spinlock.text"))) Model0__raw_read_unlock(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_write_unlock(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_read_unlock_bh(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_write_unlock_bh(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_read_unlock_irq(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text"))) Model0__raw_write_unlock_irq(Model0_rwlock_t *Model0_lock) ;
void __attribute__((section(".spinlock.text")))
Model0__raw_read_unlock_irqrestore(Model0_rwlock_t *Model0_lock, unsigned long Model0_flags)
                       ;
void __attribute__((section(".spinlock.text")))
Model0__raw_write_unlock_irqrestore(Model0_rwlock_t *Model0_lock, unsigned long Model0_flags)
                       ;
static inline __attribute__((no_instrument_function)) int Model0___raw_read_trylock(Model0_rwlock_t *Model0_lock)
{
 __asm__ __volatile__("": : :"memory");
 if (Model0_queued_read_trylock(&(Model0_lock)->Model0_raw_lock)) {
  do { } while (0);
  return 1;
 }
 __asm__ __volatile__("": : :"memory");
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0___raw_write_trylock(Model0_rwlock_t *Model0_lock)
{
 __asm__ __volatile__("": : :"memory");
 if (Model0_queued_write_trylock(&(Model0_lock)->Model0_raw_lock)) {
  do { } while (0);
  return 1;
 }
 __asm__ __volatile__("": : :"memory");
 return 0;
}

/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */


static inline __attribute__((no_instrument_function)) void Model0___raw_read_lock(Model0_rwlock_t *Model0_lock)
{
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model0_queued_read_lock(&(Model0_lock)->Model0_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0___raw_read_lock_irqsave(Model0_rwlock_t *Model0_lock)
{
 unsigned long Model0_flags;

 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_flags = Model0_arch_local_irq_save(); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model0_queued_read_lock(&((Model0_lock))->Model0_raw_lock); } while (0);

 return Model0_flags;
}

static inline __attribute__((no_instrument_function)) void Model0___raw_read_lock_irq(Model0_rwlock_t *Model0_lock)
{
 do { Model0_arch_local_irq_disable(); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model0_queued_read_lock(&(Model0_lock)->Model0_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) void Model0___raw_read_lock_bh(Model0_rwlock_t *Model0_lock)
{
 Model0___local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 do { } while (0);
 do {(void)0; Model0_queued_read_lock(&(Model0_lock)->Model0_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0___raw_write_lock_irqsave(Model0_rwlock_t *Model0_lock)
{
 unsigned long Model0_flags;

 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_flags = Model0_arch_local_irq_save(); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model0_queued_write_lock(&((Model0_lock))->Model0_raw_lock); } while (0);

 return Model0_flags;
}

static inline __attribute__((no_instrument_function)) void Model0___raw_write_lock_irq(Model0_rwlock_t *Model0_lock)
{
 do { Model0_arch_local_irq_disable(); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model0_queued_write_lock(&(Model0_lock)->Model0_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) void Model0___raw_write_lock_bh(Model0_rwlock_t *Model0_lock)
{
 Model0___local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 do { } while (0);
 do {(void)0; Model0_queued_write_lock(&(Model0_lock)->Model0_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) void Model0___raw_write_lock(Model0_rwlock_t *Model0_lock)
{
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model0_queued_write_lock(&(Model0_lock)->Model0_raw_lock); } while (0);
}



static inline __attribute__((no_instrument_function)) void Model0___raw_write_unlock(Model0_rwlock_t *Model0_lock)
{
 do { } while (0);
 do {Model0_queued_write_unlock(&(Model0_lock)->Model0_raw_lock); (void)0; } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___raw_read_unlock(Model0_rwlock_t *Model0_lock)
{
 do { } while (0);
 do {Model0_queued_read_unlock(&(Model0_lock)->Model0_raw_lock); (void)0; } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void
Model0___raw_read_unlock_irqrestore(Model0_rwlock_t *Model0_lock, unsigned long Model0_flags)
{
 do { } while (0);
 do {Model0_queued_read_unlock(&(Model0_lock)->Model0_raw_lock); (void)0; } while (0);
 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_arch_local_irq_restore(Model0_flags); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___raw_read_unlock_irq(Model0_rwlock_t *Model0_lock)
{
 do { } while (0);
 do {Model0_queued_read_unlock(&(Model0_lock)->Model0_raw_lock); (void)0; } while (0);
 do { Model0_arch_local_irq_enable(); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___raw_read_unlock_bh(Model0_rwlock_t *Model0_lock)
{
 do { } while (0);
 do {Model0_queued_read_unlock(&(Model0_lock)->Model0_raw_lock); (void)0; } while (0);
 Model0___local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
}

static inline __attribute__((no_instrument_function)) void Model0___raw_write_unlock_irqrestore(Model0_rwlock_t *Model0_lock,
          unsigned long Model0_flags)
{
 do { } while (0);
 do {Model0_queued_write_unlock(&(Model0_lock)->Model0_raw_lock); (void)0; } while (0);
 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_arch_local_irq_restore(Model0_flags); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___raw_write_unlock_irq(Model0_rwlock_t *Model0_lock)
{
 do { } while (0);
 do {Model0_queued_write_unlock(&(Model0_lock)->Model0_raw_lock); (void)0; } while (0);
 do { Model0_arch_local_irq_enable(); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___raw_write_unlock_bh(Model0_rwlock_t *Model0_lock)
{
 do { } while (0);
 do {Model0_queued_write_unlock(&(Model0_lock)->Model0_raw_lock); (void)0; } while (0);
 Model0___local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
}




/*
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_raw_spinlock_t *Model0_spinlock_check(Model0_spinlock_t *Model0_lock)
{
 return &Model0_lock->Model0_rlock;
}







static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_spin_lock(Model0_spinlock_t *Model0_lock)
{
 Model0__raw_spin_lock(&Model0_lock->Model0_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_spin_lock_bh(Model0_spinlock_t *Model0_lock)
{
 Model0__raw_spin_lock_bh(&Model0_lock->Model0_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_spin_trylock(Model0_spinlock_t *Model0_lock)
{
 return (Model0__raw_spin_trylock(&Model0_lock->Model0_rlock));
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_spin_lock_irq(Model0_spinlock_t *Model0_lock)
{
 Model0__raw_spin_lock_irq(&Model0_lock->Model0_rlock);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_spin_unlock(Model0_spinlock_t *Model0_lock)
{
 Model0___raw_spin_unlock(&Model0_lock->Model0_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_spin_unlock_bh(Model0_spinlock_t *Model0_lock)
{
 Model0__raw_spin_unlock_bh(&Model0_lock->Model0_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_spin_unlock_irq(Model0_spinlock_t *Model0_lock)
{
 Model0___raw_spin_unlock_irq(&Model0_lock->Model0_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_spin_unlock_irqrestore(Model0_spinlock_t *Model0_lock, unsigned long Model0_flags)
{
 do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0__raw_spin_unlock_irqrestore(&Model0_lock->Model0_rlock, Model0_flags); } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_spin_trylock_bh(Model0_spinlock_t *Model0_lock)
{
 return (Model0__raw_spin_trylock_bh(&Model0_lock->Model0_rlock));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_spin_trylock_irq(Model0_spinlock_t *Model0_lock)
{
 return ({ do { Model0_arch_local_irq_disable(); } while (0); (Model0__raw_spin_trylock(&Model0_lock->Model0_rlock)) ? 1 : ({ do { Model0_arch_local_irq_enable(); } while (0); 0; }); });
}






static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_spin_unlock_wait(Model0_spinlock_t *Model0_lock)
{
 Model0_queued_spin_unlock_wait(&(&Model0_lock->Model0_rlock)->Model0_raw_lock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_spin_is_locked(Model0_spinlock_t *Model0_lock)
{
 return Model0_queued_spin_is_locked(&(&Model0_lock->Model0_rlock)->Model0_raw_lock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_spin_is_contended(Model0_spinlock_t *Model0_lock)
{
 return Model0_queued_spin_is_contended(&(&Model0_lock->Model0_rlock)->Model0_raw_lock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_spin_can_lock(Model0_spinlock_t *Model0_lock)
{
 return (!Model0_queued_spin_is_locked(&(&Model0_lock->Model0_rlock)->Model0_raw_lock));
}



/*
 * Pull the atomic_t declaration:
 * (asm-mips/atomic.h needs above definitions)
 */

/**
 * atomic_dec_and_lock - lock on reaching reference count zero
 * @atomic: the atomic counter
 * @lock: the spinlock in question
 *
 * Decrements @atomic by 1.  If the result is 0, returns true and locks
 * @lock.  Returns false for all other cases.
 */
extern int Model0__atomic_dec_and_lock(Model0_atomic_t *Model0_atomic, Model0_spinlock_t *Model0_lock);



/*
 * Linux wait queue related types and methods
 */





/* First argument to waitid: */

typedef struct Model0___wait_queue Model0_wait_queue_t;
typedef int (*Model0_wait_queue_func_t)(Model0_wait_queue_t *Model0_wait, unsigned Model0_mode, int Model0_flags, void *Model0_key);
int Model0_default_wake_function(Model0_wait_queue_t *Model0_wait, unsigned Model0_mode, int Model0_flags, void *Model0_key);

/* __wait_queue::flags */



struct Model0___wait_queue {
 unsigned int Model0_flags;
 void *Model0_private;
 Model0_wait_queue_func_t func;
 struct Model0_list_head Model0_task_list;
};

struct Model0_wait_bit_key {
 void *Model0_flags;
 int Model0_bit_nr;

 unsigned long Model0_timeout;
};

struct Model0_wait_bit_queue {
 struct Model0_wait_bit_key Model0_key;
 Model0_wait_queue_t Model0_wait;
};

struct Model0___wait_queue_head {
 Model0_spinlock_t Model0_lock;
 struct Model0_list_head Model0_task_list;
};
typedef struct Model0___wait_queue_head Model0_wait_queue_head_t;

struct Model0_task_struct;

/*
 * Macros for declaration and initialisaton of the datatypes
 */
extern void Model0___init_waitqueue_head(Model0_wait_queue_head_t *Model0_q, const char *Model0_name, struct Model0_lock_class_key *);
static inline __attribute__((no_instrument_function)) void Model0_init_waitqueue_entry(Model0_wait_queue_t *Model0_q, struct Model0_task_struct *Model0_p)
{
 Model0_q->Model0_flags = 0;
 Model0_q->Model0_private = Model0_p;
 Model0_q->func = Model0_default_wake_function;
}

static inline __attribute__((no_instrument_function)) void
Model0_init_waitqueue_func_entry(Model0_wait_queue_t *Model0_q, Model0_wait_queue_func_t func)
{
 Model0_q->Model0_flags = 0;
 Model0_q->Model0_private = ((void *)0);
 Model0_q->func = func;
}

/**
 * waitqueue_active -- locklessly test for waiters on the queue
 * @q: the waitqueue to test for waiters
 *
 * returns true if the wait list is not empty
 *
 * NOTE: this function is lockless and requires care, incorrect usage _will_
 * lead to sporadic and non-obvious failure.
 *
 * Use either while holding wait_queue_head_t::lock or when used for wakeups
 * with an extra smp_mb() like:
 *
 *      CPU0 - waker                    CPU1 - waiter
 *
 *                                      for (;;) {
 *      @cond = true;                     prepare_to_wait(&wq, &wait, state);
 *      smp_mb();                         // smp_mb() from set_current_state()
 *      if (waitqueue_active(wq))         if (@cond)
 *        wake_up(wq);                      break;
 *                                        schedule();
 *                                      }
 *                                      finish_wait(&wq, &wait);
 *
 * Because without the explicit smp_mb() it's possible for the
 * waitqueue_active() load to get hoisted over the @cond store such that we'll
 * observe an empty wait list while the waiter might not observe @cond.
 *
 * Also note that this 'optimization' trades a spin_lock() for an smp_mb(),
 * which (when the lock is uncontended) are of roughly equal cost.
 */
static inline __attribute__((no_instrument_function)) int Model0_waitqueue_active(Model0_wait_queue_head_t *Model0_q)
{
 return !Model0_list_empty(&Model0_q->Model0_task_list);
}

/**
 * wq_has_sleeper - check if there are any waiting processes
 * @wq: wait queue head
 *
 * Returns true if wq has waiting processes
 *
 * Please refer to the comment for waitqueue_active.
 */
static inline __attribute__((no_instrument_function)) bool Model0_wq_has_sleeper(Model0_wait_queue_head_t *Model0_wq)
{
 /*
	 * We need to be sure we are in sync with the
	 * add_wait_queue modifications to the wait queue.
	 *
	 * This memory barrier should be paired with one on the
	 * waiting side.
	 */
 asm volatile("mfence":::"memory");
 return Model0_waitqueue_active(Model0_wq);
}

extern void Model0_add_wait_queue(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait);
extern void Model0_add_wait_queue_exclusive(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait);
extern void Model0_remove_wait_queue(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait);

static inline __attribute__((no_instrument_function)) void Model0___add_wait_queue(Model0_wait_queue_head_t *Model0_head, Model0_wait_queue_t *Model0_new)
{
 Model0_list_add(&Model0_new->Model0_task_list, &Model0_head->Model0_task_list);
}

/*
 * Used for wake-one threads:
 */
static inline __attribute__((no_instrument_function)) void
Model0___add_wait_queue_exclusive(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait)
{
 Model0_wait->Model0_flags |= 0x01;
 Model0___add_wait_queue(Model0_q, Model0_wait);
}

static inline __attribute__((no_instrument_function)) void Model0___add_wait_queue_tail(Model0_wait_queue_head_t *Model0_head,
      Model0_wait_queue_t *Model0_new)
{
 Model0_list_add_tail(&Model0_new->Model0_task_list, &Model0_head->Model0_task_list);
}

static inline __attribute__((no_instrument_function)) void
Model0___add_wait_queue_tail_exclusive(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait)
{
 Model0_wait->Model0_flags |= 0x01;
 Model0___add_wait_queue_tail(Model0_q, Model0_wait);
}

static inline __attribute__((no_instrument_function)) void
Model0___remove_wait_queue(Model0_wait_queue_head_t *Model0_head, Model0_wait_queue_t *old)
{
 Model0_list_del(&old->Model0_task_list);
}

typedef int Model0_wait_bit_action_f(struct Model0_wait_bit_key *, int Model0_mode);
void Model0___wake_up(Model0_wait_queue_head_t *Model0_q, unsigned int Model0_mode, int Model0_nr, void *Model0_key);
void Model0___wake_up_locked_key(Model0_wait_queue_head_t *Model0_q, unsigned int Model0_mode, void *Model0_key);
void Model0___wake_up_sync_key(Model0_wait_queue_head_t *Model0_q, unsigned int Model0_mode, int Model0_nr, void *Model0_key);
void Model0___wake_up_locked(Model0_wait_queue_head_t *Model0_q, unsigned int Model0_mode, int Model0_nr);
void Model0___wake_up_sync(Model0_wait_queue_head_t *Model0_q, unsigned int Model0_mode, int Model0_nr);
void Model0___wake_up_bit(Model0_wait_queue_head_t *, void *, int);
int Model0___wait_on_bit(Model0_wait_queue_head_t *, struct Model0_wait_bit_queue *, Model0_wait_bit_action_f *, unsigned);
int Model0___wait_on_bit_lock(Model0_wait_queue_head_t *, struct Model0_wait_bit_queue *, Model0_wait_bit_action_f *, unsigned);
void Model0_wake_up_bit(void *, int);
void Model0_wake_up_atomic_t(Model0_atomic_t *);
int Model0_out_of_line_wait_on_bit(void *, int, Model0_wait_bit_action_f *, unsigned);
int Model0_out_of_line_wait_on_bit_timeout(void *, int, Model0_wait_bit_action_f *, unsigned, unsigned long);
int Model0_out_of_line_wait_on_bit_lock(void *, int, Model0_wait_bit_action_f *, unsigned);
int Model0_out_of_line_wait_on_atomic_t(Model0_atomic_t *, int (*)(Model0_atomic_t *), unsigned);
Model0_wait_queue_head_t *Model0_bit_waitqueue(void *, int);
/*
 * Wakeup macros to be used to report events to the targets.
 */
/*
 * The below macro ___wait_event() has an explicit shadow of the __ret
 * variable when used from the wait_event_*() macros.
 *
 * This is so that both can use the ___wait_cond_timeout() construct
 * to wrap the condition.
 *
 * The type inconsistency of the wait_event_*() __ret variable is also
 * on purpose; we use long where we can return timeout values and int
 * otherwise.
 */
/**
 * wait_event - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
/*
 * io_wait_event() -- like wait_event() but with io_schedule()
 */
/**
 * wait_event_freezable - sleep (or freeze) until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE -- so as not to contribute
 * to system load) until the @condition evaluates to true. The
 * @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
/**
 * wait_event_timeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * or the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed.
 */
/*
 * like wait_event_timeout() -- except it uses TASK_INTERRUPTIBLE to avoid
 * increasing load and is freezable.
 */
/*
 * Just like wait_event_cmd(), except it sets exclusive flag
 */
/**
 * wait_event_cmd - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @cmd1: the command will be executed before sleep
 * @cmd2: the command will be executed after sleep
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
/**
 * wait_event_interruptible - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
/**
 * wait_event_interruptible_timeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed, or -%ERESTARTSYS if it was
 * interrupted by a signal.
 */
/**
 * wait_event_hrtimeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, as a ktime_t
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function returns 0 if @condition became true, or -ETIME if the timeout
 * elapsed.
 */
/**
 * wait_event_interruptible_hrtimeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, as a ktime_t
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function returns 0 if @condition became true, -ERESTARTSYS if it was
 * interrupted by a signal, or -ETIME if the timeout elapsed.
 */
/**
 * wait_event_interruptible_locked - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock()/spin_unlock()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_locked_irq - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock_irq()/spin_unlock_irq()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_exclusive_locked - sleep exclusively until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock()/spin_unlock()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * The process is put on the wait queue with an WQ_FLAG_EXCLUSIVE flag
 * set thus when other process waits process on the list if this
 * process is awaken further processes are not considered.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_exclusive_locked_irq - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock_irq()/spin_unlock_irq()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * The process is put on the wait queue with an WQ_FLAG_EXCLUSIVE flag
 * set thus when other process waits process on the list if this
 * process is awaken further processes are not considered.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
/**
 * wait_event_killable - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_KILLABLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
/**
 * wait_event_lock_irq_cmd - sleep until a condition gets true. The
 *			     condition is checked under the lock. This
 *			     is expected to be called with the lock
 *			     taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before cmd
 *	  and schedule() and reacquired afterwards.
 * @cmd: a command which is invoked outside the critical section before
 *	 sleep
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before invoking the cmd and going to sleep and is reacquired
 * afterwards.
 */







/**
 * wait_event_lock_irq - sleep until a condition gets true. The
 *			 condition is checked under the lock. This
 *			 is expected to be called with the lock
 *			 taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 */
/**
 * wait_event_interruptible_lock_irq_cmd - sleep until a condition gets true.
 *		The condition is checked under the lock. This is expected to
 *		be called with the lock taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before cmd and
 *	  schedule() and reacquired afterwards.
 * @cmd: a command which is invoked outside the critical section before
 *	 sleep
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received. The @condition is
 * checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before invoking the cmd and going to sleep and is reacquired
 * afterwards.
 *
 * The macro will return -ERESTARTSYS if it was interrupted by a signal
 * and 0 if @condition evaluated to true.
 */
/**
 * wait_event_interruptible_lock_irq - sleep until a condition gets true.
 *		The condition is checked under the lock. This is expected
 *		to be called with the lock taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or signal is received. The @condition is
 * checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 *
 * The macro will return -ERESTARTSYS if it was interrupted by a signal
 * and 0 if @condition evaluated to true.
 */
/**
 * wait_event_interruptible_lock_irq_timeout - sleep until a condition gets
 *		true or a timeout elapses. The condition is checked under
 *		the lock. This is expected to be called with the lock taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or signal is received. The @condition is
 * checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 *
 * The function returns 0 if the @timeout elapsed, -ERESTARTSYS if it
 * was interrupted by a signal, and the remaining jiffies otherwise
 * if the condition evaluated to true before the timeout elapsed.
 */
/*
 * Waitqueues which are removed from the waitqueue_head at wakeup time
 */
void Model0_prepare_to_wait(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait, int Model0_state);
void Model0_prepare_to_wait_exclusive(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait, int Model0_state);
long Model0_prepare_to_wait_event(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait, int Model0_state);
void Model0_finish_wait(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait);
void Model0_abort_exclusive_wait(Model0_wait_queue_head_t *Model0_q, Model0_wait_queue_t *Model0_wait, unsigned int Model0_mode, void *Model0_key);
long Model0_wait_woken(Model0_wait_queue_t *Model0_wait, unsigned Model0_mode, long Model0_timeout);
int Model0_woken_wake_function(Model0_wait_queue_t *Model0_wait, unsigned Model0_mode, int Model0_sync, void *Model0_key);
int Model0_autoremove_wake_function(Model0_wait_queue_t *Model0_wait, unsigned Model0_mode, int Model0_sync, void *Model0_key);
int Model0_wake_bit_function(Model0_wait_queue_t *Model0_wait, unsigned Model0_mode, int Model0_sync, void *Model0_key);
extern int Model0_bit_wait(struct Model0_wait_bit_key *, int);
extern int Model0_bit_wait_io(struct Model0_wait_bit_key *, int);
extern int Model0_bit_wait_timeout(struct Model0_wait_bit_key *, int);
extern int Model0_bit_wait_io_timeout(struct Model0_wait_bit_key *, int);

/**
 * wait_on_bit - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * There is a standard hashed waitqueue table for generic use. This
 * is the part of the hashtable's accessor API that waits on a bit.
 * For instance, if one were to have waiters on a bitflag, one would
 * call wait_on_bit() in threads waiting for the bit to clear.
 * One uses wait_on_bit() where one is waiting for the bit to clear,
 * but has no intention of setting it.
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((no_instrument_function)) int
Model0_wait_on_bit(unsigned long *Model0_word, int Model0_bit, unsigned Model0_mode)
{
 do { Model0__cond_resched(); } while (0);
 if (!(__builtin_constant_p((Model0_bit)) ? Model0_constant_test_bit((Model0_bit), (Model0_word)) : Model0_variable_test_bit((Model0_bit), (Model0_word))))
  return 0;
 return Model0_out_of_line_wait_on_bit(Model0_word, Model0_bit,
           Model0_bit_wait,
           Model0_mode);
}

/**
 * wait_on_bit_io - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared.  This is similar to wait_on_bit(), but calls
 * io_schedule() instead of schedule() for the actual waiting.
 *
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((no_instrument_function)) int
Model0_wait_on_bit_io(unsigned long *Model0_word, int Model0_bit, unsigned Model0_mode)
{
 do { Model0__cond_resched(); } while (0);
 if (!(__builtin_constant_p((Model0_bit)) ? Model0_constant_test_bit((Model0_bit), (Model0_word)) : Model0_variable_test_bit((Model0_bit), (Model0_word))))
  return 0;
 return Model0_out_of_line_wait_on_bit(Model0_word, Model0_bit,
           Model0_bit_wait_io,
           Model0_mode);
}

/**
 * wait_on_bit_timeout - wait for a bit to be cleared or a timeout elapses
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 * @timeout: timeout, in jiffies
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared. This is similar to wait_on_bit(), except also takes a
 * timeout parameter.
 *
 * Returned value will be zero if the bit was cleared before the
 * @timeout elapsed, or non-zero if the @timeout elapsed or process
 * received a signal and the mode permitted wakeup on that signal.
 */
static inline __attribute__((no_instrument_function)) int
Model0_wait_on_bit_timeout(unsigned long *Model0_word, int Model0_bit, unsigned Model0_mode,
      unsigned long Model0_timeout)
{
 do { Model0__cond_resched(); } while (0);
 if (!(__builtin_constant_p((Model0_bit)) ? Model0_constant_test_bit((Model0_bit), (Model0_word)) : Model0_variable_test_bit((Model0_bit), (Model0_word))))
  return 0;
 return Model0_out_of_line_wait_on_bit_timeout(Model0_word, Model0_bit,
            Model0_bit_wait_timeout,
            Model0_mode, Model0_timeout);
}

/**
 * wait_on_bit_action - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared, and allow the waiting action to be specified.
 * This is like wait_on_bit() but allows fine control of how the waiting
 * is done.
 *
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((no_instrument_function)) int
Model0_wait_on_bit_action(unsigned long *Model0_word, int Model0_bit, Model0_wait_bit_action_f *Model0_action,
     unsigned Model0_mode)
{
 do { Model0__cond_resched(); } while (0);
 if (!(__builtin_constant_p((Model0_bit)) ? Model0_constant_test_bit((Model0_bit), (Model0_word)) : Model0_variable_test_bit((Model0_bit), (Model0_word))))
  return 0;
 return Model0_out_of_line_wait_on_bit(Model0_word, Model0_bit, Model0_action, Model0_mode);
}

/**
 * wait_on_bit_lock - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * There is a standard hashed waitqueue table for generic use. This
 * is the part of the hashtable's accessor API that waits on a bit
 * when one intends to set it, for instance, trying to lock bitflags.
 * For instance, if one were to have waiters trying to set bitflag
 * and waiting for it to clear before setting it, one would call
 * wait_on_bit() in threads waiting to be able to set the bit.
 * One uses wait_on_bit_lock() where one is waiting for the bit to
 * clear with the intention of setting it, and when done, clearing it.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((no_instrument_function)) int
Model0_wait_on_bit_lock(unsigned long *Model0_word, int Model0_bit, unsigned Model0_mode)
{
 do { Model0__cond_resched(); } while (0);
 if (!Model0_test_and_set_bit(Model0_bit, Model0_word))
  return 0;
 return Model0_out_of_line_wait_on_bit_lock(Model0_word, Model0_bit, Model0_bit_wait, Model0_mode);
}

/**
 * wait_on_bit_lock_io - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared and then to atomically set it.  This is similar
 * to wait_on_bit(), but calls io_schedule() instead of schedule()
 * for the actual waiting.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((no_instrument_function)) int
Model0_wait_on_bit_lock_io(unsigned long *Model0_word, int Model0_bit, unsigned Model0_mode)
{
 do { Model0__cond_resched(); } while (0);
 if (!Model0_test_and_set_bit(Model0_bit, Model0_word))
  return 0;
 return Model0_out_of_line_wait_on_bit_lock(Model0_word, Model0_bit, Model0_bit_wait_io, Model0_mode);
}

/**
 * wait_on_bit_lock_action - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared and then to set it, and allow the waiting action
 * to be specified.
 * This is like wait_on_bit() but allows fine control of how the waiting
 * is done.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((no_instrument_function)) int
Model0_wait_on_bit_lock_action(unsigned long *Model0_word, int Model0_bit, Model0_wait_bit_action_f *Model0_action,
   unsigned Model0_mode)
{
 do { Model0__cond_resched(); } while (0);
 if (!Model0_test_and_set_bit(Model0_bit, Model0_word))
  return 0;
 return Model0_out_of_line_wait_on_bit_lock(Model0_word, Model0_bit, Model0_action, Model0_mode);
}

/**
 * wait_on_atomic_t - Wait for an atomic_t to become 0
 * @val: The atomic value being waited on, a kernel virtual address
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * Wait for an atomic_t to become 0.  We abuse the bit-wait waitqueue table for
 * the purpose of getting a waitqueue, but we set the key to a bit number
 * outside of the target 'word'.
 */
static inline __attribute__((no_instrument_function))
int Model0_wait_on_atomic_t(Model0_atomic_t *Model0_val, int (*Model0_action)(Model0_atomic_t *), unsigned Model0_mode)
{
 do { Model0__cond_resched(); } while (0);
 if (Model0_atomic_read(Model0_val) == 0)
  return 0;
 return Model0_out_of_line_wait_on_atomic_t(Model0_val, Model0_action, Model0_mode);
}






/*
 * Reader/writer consistent mechanism without starving writers. This type of
 * lock for data where the reader wants a consistent set of information
 * and is willing to retry if the information changes. There are two types
 * of readers:
 * 1. Sequence readers which never block a writer but they may have to retry
 *    if a writer is in progress by detecting change in sequence number.
 *    Writers do not wait for a sequence reader.
 * 2. Locking readers which will wait if a writer or another locking reader
 *    is in progress. A locking reader in progress will also block a writer
 *    from going forward. Unlike the regular rwlock, the read lock here is
 *    exclusive so that only one locking reader can get it.
 *
 * This is not as cache friendly as brlock. Also, this may not work well
 * for data that contains pointers, because any writer could
 * invalidate a pointer that a reader was following.
 *
 * Expected non-blocking reader usage:
 * 	do {
 *	    seq = read_seqbegin(&foo);
 * 	...
 *      } while (read_seqretry(&foo, seq));
 *
 *
 * On non-SMP the spin locks disappear but the writer still needs
 * to increment the sequence variables because an interrupt routine could
 * change the state of the data.
 *
 * Based on x86_64 vsyscall gettimeofday 
 * by Keith Owens and Andrea Arcangeli
 */







/*
 * Version using sequence counter only.
 * This can be used when code has its own mutex protecting the
 * updating starting before the write_seqcountbeqin() and ending
 * after the write_seqcount_end().
 */
typedef struct Model0_seqcount {
 unsigned Model0_sequence;



} Model0_seqcount_t;

static inline __attribute__((no_instrument_function)) void Model0___seqcount_init(Model0_seqcount_t *Model0_s, const char *Model0_name,
       struct Model0_lock_class_key *Model0_key)
{
 /*
	 * Make sure we are not reinitializing a held lock:
	 */
 do { (void)(Model0_name); (void)(Model0_key); } while (0);
 Model0_s->Model0_sequence = 0;
}
/**
 * __read_seqcount_begin - begin a seq-read critical section (without barrier)
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * __read_seqcount_begin is like read_seqcount_begin, but has no smp_rmb()
 * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
 * provided before actually loading any of the variables that are to be
 * protected in this critical section.
 *
 * Use carefully, only in critical code, and comment how the barrier is
 * provided.
 */
static inline __attribute__((no_instrument_function)) unsigned Model0___read_seqcount_begin(const Model0_seqcount_t *Model0_s)
{
 unsigned Model0_ret;

Model0_repeat:
 Model0_ret = ({ union { typeof(Model0_s->Model0_sequence) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_s->Model0_sequence), Model0___u.Model0___c, sizeof(Model0_s->Model0_sequence)); else Model0___read_once_size_nocheck(&(Model0_s->Model0_sequence), Model0___u.Model0___c, sizeof(Model0_s->Model0_sequence)); Model0___u.Model0___val; });
 if (__builtin_expect(!!(Model0_ret & 1), 0)) {
  Model0_cpu_relax();
  goto Model0_repeat;
 }
 return Model0_ret;
}

/**
 * raw_read_seqcount - Read the raw seqcount
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * raw_read_seqcount opens a read critical section of the given
 * seqcount without any lockdep checking and without checking or
 * masking the LSB. Calling code is responsible for handling that.
 */
static inline __attribute__((no_instrument_function)) unsigned Model0_raw_read_seqcount(const Model0_seqcount_t *Model0_s)
{
 unsigned Model0_ret = ({ union { typeof(Model0_s->Model0_sequence) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_s->Model0_sequence), Model0___u.Model0___c, sizeof(Model0_s->Model0_sequence)); else Model0___read_once_size_nocheck(&(Model0_s->Model0_sequence), Model0___u.Model0___c, sizeof(Model0_s->Model0_sequence)); Model0___u.Model0___val; });
 __asm__ __volatile__("": : :"memory");
 return Model0_ret;
}

/**
 * raw_read_seqcount_begin - start seq-read critical section w/o lockdep
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * raw_read_seqcount_begin opens a read critical section of the given
 * seqcount, but without any lockdep checking. Validity of the critical
 * section is tested by checking read_seqcount_retry function.
 */
static inline __attribute__((no_instrument_function)) unsigned Model0_raw_read_seqcount_begin(const Model0_seqcount_t *Model0_s)
{
 unsigned Model0_ret = Model0___read_seqcount_begin(Model0_s);
 __asm__ __volatile__("": : :"memory");
 return Model0_ret;
}

/**
 * read_seqcount_begin - begin a seq-read critical section
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * read_seqcount_begin opens a read critical section of the given seqcount.
 * Validity of the critical section is tested by checking read_seqcount_retry
 * function.
 */
static inline __attribute__((no_instrument_function)) unsigned Model0_read_seqcount_begin(const Model0_seqcount_t *Model0_s)
{
                                  ;
 return Model0_raw_read_seqcount_begin(Model0_s);
}

/**
 * raw_seqcount_begin - begin a seq-read critical section
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * raw_seqcount_begin opens a read critical section of the given seqcount.
 * Validity of the critical section is tested by checking read_seqcount_retry
 * function.
 *
 * Unlike read_seqcount_begin(), this function will not wait for the count
 * to stabilize. If a writer is active when we begin, we will fail the
 * read_seqcount_retry() instead of stabilizing at the beginning of the
 * critical section.
 */
static inline __attribute__((no_instrument_function)) unsigned Model0_raw_seqcount_begin(const Model0_seqcount_t *Model0_s)
{
 unsigned Model0_ret = ({ union { typeof(Model0_s->Model0_sequence) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_s->Model0_sequence), Model0___u.Model0___c, sizeof(Model0_s->Model0_sequence)); else Model0___read_once_size_nocheck(&(Model0_s->Model0_sequence), Model0___u.Model0___c, sizeof(Model0_s->Model0_sequence)); Model0___u.Model0___val; });
 __asm__ __volatile__("": : :"memory");
 return Model0_ret & ~1;
}

/**
 * __read_seqcount_retry - end a seq-read critical section (without barrier)
 * @s: pointer to seqcount_t
 * @start: count, from read_seqcount_begin
 * Returns: 1 if retry is required, else 0
 *
 * __read_seqcount_retry is like read_seqcount_retry, but has no smp_rmb()
 * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
 * provided before actually loading any of the variables that are to be
 * protected in this critical section.
 *
 * Use carefully, only in critical code, and comment how the barrier is
 * provided.
 */
static inline __attribute__((no_instrument_function)) int Model0___read_seqcount_retry(const Model0_seqcount_t *Model0_s, unsigned Model0_start)
{
 return __builtin_expect(!!(Model0_s->Model0_sequence != Model0_start), 0);
}

/**
 * read_seqcount_retry - end a seq-read critical section
 * @s: pointer to seqcount_t
 * @start: count, from read_seqcount_begin
 * Returns: 1 if retry is required, else 0
 *
 * read_seqcount_retry closes a read critical section of the given seqcount.
 * If the critical section was invalid, it must be ignored (and typically
 * retried).
 */
static inline __attribute__((no_instrument_function)) int Model0_read_seqcount_retry(const Model0_seqcount_t *Model0_s, unsigned Model0_start)
{
 __asm__ __volatile__("": : :"memory");
 return Model0___read_seqcount_retry(Model0_s, Model0_start);
}



static inline __attribute__((no_instrument_function)) void Model0_raw_write_seqcount_begin(Model0_seqcount_t *Model0_s)
{
 Model0_s->Model0_sequence++;
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0_raw_write_seqcount_end(Model0_seqcount_t *Model0_s)
{
 __asm__ __volatile__("": : :"memory");
 Model0_s->Model0_sequence++;
}

/**
 * raw_write_seqcount_barrier - do a seq write barrier
 * @s: pointer to seqcount_t
 *
 * This can be used to provide an ordering guarantee instead of the
 * usual consistency guarantee. It is one wmb cheaper, because we can
 * collapse the two back-to-back wmb()s.
 *
 *      seqcount_t seq;
 *      bool X = true, Y = false;
 *
 *      void read(void)
 *      {
 *              bool x, y;
 *
 *              do {
 *                      int s = read_seqcount_begin(&seq);
 *
 *                      x = X; y = Y;
 *
 *              } while (read_seqcount_retry(&seq, s));
 *
 *              BUG_ON(!x && !y);
 *      }
 *
 *      void write(void)
 *      {
 *              Y = true;
 *
 *              raw_write_seqcount_barrier(seq);
 *
 *              X = false;
 *      }
 */
static inline __attribute__((no_instrument_function)) void Model0_raw_write_seqcount_barrier(Model0_seqcount_t *Model0_s)
{
 Model0_s->Model0_sequence++;
 __asm__ __volatile__("": : :"memory");
 Model0_s->Model0_sequence++;
}

static inline __attribute__((no_instrument_function)) int Model0_raw_read_seqcount_latch(Model0_seqcount_t *Model0_s)
{
 int Model0_seq = ({ union { typeof(Model0_s->Model0_sequence) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_s->Model0_sequence), Model0___u.Model0___c, sizeof(Model0_s->Model0_sequence)); else Model0___read_once_size_nocheck(&(Model0_s->Model0_sequence), Model0___u.Model0___c, sizeof(Model0_s->Model0_sequence)); Model0___u.Model0___val; });
 /* Pairs with the first smp_wmb() in raw_write_seqcount_latch() */
 do { } while (0);
 return Model0_seq;
}

/**
 * raw_write_seqcount_latch - redirect readers to even/odd copy
 * @s: pointer to seqcount_t
 *
 * The latch technique is a multiversion concurrency control method that allows
 * queries during non-atomic modifications. If you can guarantee queries never
 * interrupt the modification -- e.g. the concurrency is strictly between CPUs
 * -- you most likely do not need this.
 *
 * Where the traditional RCU/lockless data structures rely on atomic
 * modifications to ensure queries observe either the old or the new state the
 * latch allows the same for non-atomic updates. The trade-off is doubling the
 * cost of storage; we have to maintain two copies of the entire data
 * structure.
 *
 * Very simply put: we first modify one copy and then the other. This ensures
 * there is always one copy in a stable state, ready to give us an answer.
 *
 * The basic form is a data structure like:
 *
 * struct latch_struct {
 *	seqcount_t		seq;
 *	struct data_struct	data[2];
 * };
 *
 * Where a modification, which is assumed to be externally serialized, does the
 * following:
 *
 * void latch_modify(struct latch_struct *latch, ...)
 * {
 *	smp_wmb();	<- Ensure that the last data[1] update is visible
 *	latch->seq++;
 *	smp_wmb();	<- Ensure that the seqcount update is visible
 *
 *	modify(latch->data[0], ...);
 *
 *	smp_wmb();	<- Ensure that the data[0] update is visible
 *	latch->seq++;
 *	smp_wmb();	<- Ensure that the seqcount update is visible
 *
 *	modify(latch->data[1], ...);
 * }
 *
 * The query will have a form like:
 *
 * struct entry *latch_query(struct latch_struct *latch, ...)
 * {
 *	struct entry *entry;
 *	unsigned seq, idx;
 *
 *	do {
 *		seq = raw_read_seqcount_latch(&latch->seq);
 *
 *		idx = seq & 0x01;
 *		entry = data_query(latch->data[idx], ...);
 *
 *		smp_rmb();
 *	} while (seq != latch->seq);
 *
 *	return entry;
 * }
 *
 * So during the modification, queries are first redirected to data[1]. Then we
 * modify data[0]. When that is complete, we redirect queries back to data[0]
 * and we can modify data[1].
 *
 * NOTE: The non-requirement for atomic modifications does _NOT_ include
 *       the publishing of new entries in the case where data is a dynamic
 *       data structure.
 *
 *       An iteration might start in data[0] and get suspended long enough
 *       to miss an entire modification sequence, once it resumes it might
 *       observe the new entry.
 *
 * NOTE: When data is a dynamic data structure; one should use regular RCU
 *       patterns to manage the lifetimes of the objects within.
 */
static inline __attribute__((no_instrument_function)) void Model0_raw_write_seqcount_latch(Model0_seqcount_t *Model0_s)
{
       __asm__ __volatile__("": : :"memory"); /* prior stores before incrementing "sequence" */
       Model0_s->Model0_sequence++;
       __asm__ __volatile__("": : :"memory"); /* increment "sequence" before following stores */
}

/*
 * Sequence counter only version assumes that callers are using their
 * own mutexing.
 */
static inline __attribute__((no_instrument_function)) void Model0_write_seqcount_begin_nested(Model0_seqcount_t *Model0_s, int Model0_subclass)
{
 Model0_raw_write_seqcount_begin(Model0_s);
 do { } while (0);
}

static inline __attribute__((no_instrument_function)) void Model0_write_seqcount_begin(Model0_seqcount_t *Model0_s)
{
 Model0_write_seqcount_begin_nested(Model0_s, 0);
}

static inline __attribute__((no_instrument_function)) void Model0_write_seqcount_end(Model0_seqcount_t *Model0_s)
{
 do { } while (0);
 Model0_raw_write_seqcount_end(Model0_s);
}

/**
 * write_seqcount_invalidate - invalidate in-progress read-side seq operations
 * @s: pointer to seqcount_t
 *
 * After write_seqcount_invalidate, no read-side seq operations will complete
 * successfully and see data older than this.
 */
static inline __attribute__((no_instrument_function)) void Model0_write_seqcount_invalidate(Model0_seqcount_t *Model0_s)
{
 __asm__ __volatile__("": : :"memory");
 Model0_s->Model0_sequence+=2;
}

typedef struct {
 struct Model0_seqcount Model0_seqcount;
 Model0_spinlock_t Model0_lock;
} Model0_seqlock_t;

/*
 * These macros triggered gcc-3.x compile-time problems.  We think these are
 * OK now.  Be cautious.
 */
/*
 * Read side functions for starting and finalizing a read side section.
 */
static inline __attribute__((no_instrument_function)) unsigned Model0_read_seqbegin(const Model0_seqlock_t *Model0_sl)
{
 return Model0_read_seqcount_begin(&Model0_sl->Model0_seqcount);
}

static inline __attribute__((no_instrument_function)) unsigned Model0_read_seqretry(const Model0_seqlock_t *Model0_sl, unsigned Model0_start)
{
 return Model0_read_seqcount_retry(&Model0_sl->Model0_seqcount, Model0_start);
}

/*
 * Lock out other writers and update the count.
 * Acts like a normal spin_lock/unlock.
 * Don't need preempt_disable() because that is in the spin_lock already.
 */
static inline __attribute__((no_instrument_function)) void Model0_write_seqlock(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_lock(&Model0_sl->Model0_lock);
 Model0_write_seqcount_begin(&Model0_sl->Model0_seqcount);
}

static inline __attribute__((no_instrument_function)) void Model0_write_sequnlock(Model0_seqlock_t *Model0_sl)
{
 Model0_write_seqcount_end(&Model0_sl->Model0_seqcount);
 Model0_spin_unlock(&Model0_sl->Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_write_seqlock_bh(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_lock_bh(&Model0_sl->Model0_lock);
 Model0_write_seqcount_begin(&Model0_sl->Model0_seqcount);
}

static inline __attribute__((no_instrument_function)) void Model0_write_sequnlock_bh(Model0_seqlock_t *Model0_sl)
{
 Model0_write_seqcount_end(&Model0_sl->Model0_seqcount);
 Model0_spin_unlock_bh(&Model0_sl->Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_write_seqlock_irq(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_lock_irq(&Model0_sl->Model0_lock);
 Model0_write_seqcount_begin(&Model0_sl->Model0_seqcount);
}

static inline __attribute__((no_instrument_function)) void Model0_write_sequnlock_irq(Model0_seqlock_t *Model0_sl)
{
 Model0_write_seqcount_end(&Model0_sl->Model0_seqcount);
 Model0_spin_unlock_irq(&Model0_sl->Model0_lock);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0___write_seqlock_irqsave(Model0_seqlock_t *Model0_sl)
{
 unsigned long Model0_flags;

 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_flags = Model0__raw_spin_lock_irqsave(Model0_spinlock_check(&Model0_sl->Model0_lock)); } while (0); } while (0);
 Model0_write_seqcount_begin(&Model0_sl->Model0_seqcount);
 return Model0_flags;
}




static inline __attribute__((no_instrument_function)) void
Model0_write_sequnlock_irqrestore(Model0_seqlock_t *Model0_sl, unsigned long Model0_flags)
{
 Model0_write_seqcount_end(&Model0_sl->Model0_seqcount);
 Model0_spin_unlock_irqrestore(&Model0_sl->Model0_lock, Model0_flags);
}

/*
 * A locking reader exclusively locks out other writers and locking readers,
 * but doesn't update the sequence number. Acts like a normal spin_lock/unlock.
 * Don't need preempt_disable() because that is in the spin_lock already.
 */
static inline __attribute__((no_instrument_function)) void Model0_read_seqlock_excl(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_lock(&Model0_sl->Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_read_sequnlock_excl(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_unlock(&Model0_sl->Model0_lock);
}

/**
 * read_seqbegin_or_lock - begin a sequence number check or locking block
 * @lock: sequence lock
 * @seq : sequence number to be checked
 *
 * First try it once optimistically without taking the lock. If that fails,
 * take the lock. The sequence number is also used as a marker for deciding
 * whether to be a reader (even) or writer (odd).
 * N.B. seq must be initialized to an even number to begin with.
 */
static inline __attribute__((no_instrument_function)) void Model0_read_seqbegin_or_lock(Model0_seqlock_t *Model0_lock, int *Model0_seq)
{
 if (!(*Model0_seq & 1)) /* Even */
  *Model0_seq = Model0_read_seqbegin(Model0_lock);
 else /* Odd */
  Model0_read_seqlock_excl(Model0_lock);
}

static inline __attribute__((no_instrument_function)) int Model0_need_seqretry(Model0_seqlock_t *Model0_lock, int Model0_seq)
{
 return !(Model0_seq & 1) && Model0_read_seqretry(Model0_lock, Model0_seq);
}

static inline __attribute__((no_instrument_function)) void Model0_done_seqretry(Model0_seqlock_t *Model0_lock, int Model0_seq)
{
 if (Model0_seq & 1)
  Model0_read_sequnlock_excl(Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_read_seqlock_excl_bh(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_lock_bh(&Model0_sl->Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_read_sequnlock_excl_bh(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_unlock_bh(&Model0_sl->Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_read_seqlock_excl_irq(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_lock_irq(&Model0_sl->Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_read_sequnlock_excl_irq(Model0_seqlock_t *Model0_sl)
{
 Model0_spin_unlock_irq(&Model0_sl->Model0_lock);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0___read_seqlock_excl_irqsave(Model0_seqlock_t *Model0_sl)
{
 unsigned long Model0_flags;

 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_flags = Model0__raw_spin_lock_irqsave(Model0_spinlock_check(&Model0_sl->Model0_lock)); } while (0); } while (0);
 return Model0_flags;
}




static inline __attribute__((no_instrument_function)) void
Model0_read_sequnlock_excl_irqrestore(Model0_seqlock_t *Model0_sl, unsigned long Model0_flags)
{
 Model0_spin_unlock_irqrestore(&Model0_sl->Model0_lock, Model0_flags);
}

static inline __attribute__((no_instrument_function)) unsigned long
Model0_read_seqbegin_or_lock_irqsave(Model0_seqlock_t *Model0_lock, int *Model0_seq)
{
 unsigned long Model0_flags = 0;

 if (!(*Model0_seq & 1)) /* Even */
  *Model0_seq = Model0_read_seqbegin(Model0_lock);
 else /* Odd */
  do { Model0_flags = Model0___read_seqlock_excl_irqsave(Model0_lock); } while (0);

 return Model0_flags;
}

static inline __attribute__((no_instrument_function)) void
Model0_done_seqretry_irqrestore(Model0_seqlock_t *Model0_lock, int Model0_seq, unsigned long Model0_flags)
{
 if (Model0_seq & 1)
  Model0_read_sequnlock_excl_irqrestore(Model0_lock, Model0_flags);
}



/*
 * Nodemasks provide a bitmap suitable for representing the
 * set of Node's in a system, one bit position per Node number.
 *
 * See detailed comments in the file linux/bitmap.h describing the
 * data type on which these nodemasks are based.
 *
 * For details of nodemask_parse_user(), see bitmap_parse_user() in
 * lib/bitmap.c.  For details of nodelist_parse(), see bitmap_parselist(),
 * also in bitmap.c.  For details of node_remap(), see bitmap_bitremap in
 * lib/bitmap.c.  For details of nodes_remap(), see bitmap_remap in
 * lib/bitmap.c.  For details of nodes_onto(), see bitmap_onto in
 * lib/bitmap.c.  For details of nodes_fold(), see bitmap_fold in
 * lib/bitmap.c.
 *
 * The available nodemask operations are:
 *
 * void node_set(node, mask)		turn on bit 'node' in mask
 * void node_clear(node, mask)		turn off bit 'node' in mask
 * void nodes_setall(mask)		set all bits
 * void nodes_clear(mask)		clear all bits
 * int node_isset(node, mask)		true iff bit 'node' set in mask
 * int node_test_and_set(node, mask)	test and set bit 'node' in mask
 *
 * void nodes_and(dst, src1, src2)	dst = src1 & src2  [intersection]
 * void nodes_or(dst, src1, src2)	dst = src1 | src2  [union]
 * void nodes_xor(dst, src1, src2)	dst = src1 ^ src2
 * void nodes_andnot(dst, src1, src2)	dst = src1 & ~src2
 * void nodes_complement(dst, src)	dst = ~src
 *
 * int nodes_equal(mask1, mask2)	Does mask1 == mask2?
 * int nodes_intersects(mask1, mask2)	Do mask1 and mask2 intersect?
 * int nodes_subset(mask1, mask2)	Is mask1 a subset of mask2?
 * int nodes_empty(mask)		Is mask empty (no bits sets)?
 * int nodes_full(mask)			Is mask full (all bits sets)?
 * int nodes_weight(mask)		Hamming weight - number of set bits
 *
 * void nodes_shift_right(dst, src, n)	Shift right
 * void nodes_shift_left(dst, src, n)	Shift left
 *
 * int first_node(mask)			Number lowest set bit, or MAX_NUMNODES
 * int next_node(node, mask)		Next node past 'node', or MAX_NUMNODES
 * int next_node_in(node, mask)		Next node past 'node', or wrap to first,
 *					or MAX_NUMNODES
 * int first_unset_node(mask)		First node not set in mask, or 
 *					MAX_NUMNODES
 *
 * nodemask_t nodemask_of_node(node)	Return nodemask with bit 'node' set
 * NODE_MASK_ALL			Initializer - all bits set
 * NODE_MASK_NONE			Initializer - no bits set
 * unsigned long *nodes_addr(mask)	Array of unsigned long's in mask
 *
 * int nodemask_parse_user(ubuf, ulen, mask)	Parse ascii string as nodemask
 * int nodelist_parse(buf, map)		Parse ascii string as nodelist
 * int node_remap(oldbit, old, new)	newbit = map(old, new)(oldbit)
 * void nodes_remap(dst, src, old, new)	*dst = map(old, new)(src)
 * void nodes_onto(dst, orig, relmap)	*dst = orig relative to relmap
 * void nodes_fold(dst, orig, sz)	dst bits = orig bits mod sz
 *
 * for_each_node_mask(node, mask)	for-loop node over mask
 *
 * int num_online_nodes()		Number of online Nodes
 * int num_possible_nodes()		Number of all possible Nodes
 *
 * int node_random(mask)		Random node with set bit in mask
 *
 * int node_online(node)		Is some node online?
 * int node_possible(node)		Is some node possible?
 *
 * node_set_online(node)		set bit 'node' in node_online_map
 * node_set_offline(node)		clear bit 'node' in node_online_map
 *
 * for_each_node(node)			for-loop node over node_possible_map
 * for_each_online_node(node)		for-loop node over node_online_map
 *
 * Subtlety:
 * 1) The 'type-checked' form of node_isset() causes gcc (3.3.2, anyway)
 *    to generate slightly worse code.  So use a simple one-line #define
 *    for node_isset(), instead of wrapping an inline inside a macro, the
 *    way we do the other calls.
 *
 * NODEMASK_SCRATCH
 * When doing above logical AND, OR, XOR, Remap operations the callers tend to
 * need temporary nodemask_t's on the stack. But if NODES_SHIFT is large,
 * nodemask_t's consume too much stack space.  NODEMASK_SCRATCH is a helper
 * for such situations. See below and CPUMASK_ALLOC also.
 */






typedef struct { unsigned long Model0_bits[((((1 << 6)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } Model0_nodemask_t;
extern Model0_nodemask_t Model0__unused_nodemask_arg_;

/**
 * nodemask_pr_args - printf args to output a nodemask
 * @maskp: nodemask to be printed
 *
 * Can be used to provide arguments for '%*pb[l]' when printing a nodemask.
 */


/*
 * The inline keyword gives the compiler room to decide to inline, or
 * not inline a function as it sees best.  However, as these functions
 * are called in both __init and non-__init functions, if they are not
 * inlined we will end up with a section mis-match error (of the type of
 * freeable items not being freed).  So we must use __always_inline here
 * to fix the problem.  If other functions in the future also end up in
 * this situation they will also need to be annotated as __always_inline
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___node_set(int Model0_node, volatile Model0_nodemask_t *Model0_dstp)
{
 Model0_set_bit(Model0_node, Model0_dstp->Model0_bits);
}


static inline __attribute__((no_instrument_function)) void Model0___node_clear(int Model0_node, volatile Model0_nodemask_t *Model0_dstp)
{
 Model0_clear_bit(Model0_node, Model0_dstp->Model0_bits);
}


static inline __attribute__((no_instrument_function)) void Model0___nodes_setall(Model0_nodemask_t *Model0_dstp, unsigned int Model0_nbits)
{
 Model0_bitmap_fill(Model0_dstp->Model0_bits, Model0_nbits);
}


static inline __attribute__((no_instrument_function)) void Model0___nodes_clear(Model0_nodemask_t *Model0_dstp, unsigned int Model0_nbits)
{
 Model0_bitmap_zero(Model0_dstp->Model0_bits, Model0_nbits);
}

/* No static inline type checking - see Subtlety (1) above. */




static inline __attribute__((no_instrument_function)) int Model0___node_test_and_set(int Model0_node, Model0_nodemask_t *Model0_addr)
{
 return Model0_test_and_set_bit(Model0_node, Model0_addr->Model0_bits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_and(Model0_nodemask_t *Model0_dstp, const Model0_nodemask_t *Model0_src1p,
     const Model0_nodemask_t *Model0_src2p, unsigned int Model0_nbits)
{
 Model0_bitmap_and(Model0_dstp->Model0_bits, Model0_src1p->Model0_bits, Model0_src2p->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_or(Model0_nodemask_t *Model0_dstp, const Model0_nodemask_t *Model0_src1p,
     const Model0_nodemask_t *Model0_src2p, unsigned int Model0_nbits)
{
 Model0_bitmap_or(Model0_dstp->Model0_bits, Model0_src1p->Model0_bits, Model0_src2p->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_xor(Model0_nodemask_t *Model0_dstp, const Model0_nodemask_t *Model0_src1p,
     const Model0_nodemask_t *Model0_src2p, unsigned int Model0_nbits)
{
 Model0_bitmap_xor(Model0_dstp->Model0_bits, Model0_src1p->Model0_bits, Model0_src2p->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_andnot(Model0_nodemask_t *Model0_dstp, const Model0_nodemask_t *Model0_src1p,
     const Model0_nodemask_t *Model0_src2p, unsigned int Model0_nbits)
{
 Model0_bitmap_andnot(Model0_dstp->Model0_bits, Model0_src1p->Model0_bits, Model0_src2p->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_complement(Model0_nodemask_t *Model0_dstp,
     const Model0_nodemask_t *Model0_srcp, unsigned int Model0_nbits)
{
 Model0_bitmap_complement(Model0_dstp->Model0_bits, Model0_srcp->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) int Model0___nodes_equal(const Model0_nodemask_t *Model0_src1p,
     const Model0_nodemask_t *Model0_src2p, unsigned int Model0_nbits)
{
 return Model0_bitmap_equal(Model0_src1p->Model0_bits, Model0_src2p->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) int Model0___nodes_intersects(const Model0_nodemask_t *Model0_src1p,
     const Model0_nodemask_t *Model0_src2p, unsigned int Model0_nbits)
{
 return Model0_bitmap_intersects(Model0_src1p->Model0_bits, Model0_src2p->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) int Model0___nodes_subset(const Model0_nodemask_t *Model0_src1p,
     const Model0_nodemask_t *Model0_src2p, unsigned int Model0_nbits)
{
 return Model0_bitmap_subset(Model0_src1p->Model0_bits, Model0_src2p->Model0_bits, Model0_nbits);
}


static inline __attribute__((no_instrument_function)) int Model0___nodes_empty(const Model0_nodemask_t *Model0_srcp, unsigned int Model0_nbits)
{
 return Model0_bitmap_empty(Model0_srcp->Model0_bits, Model0_nbits);
}


static inline __attribute__((no_instrument_function)) int Model0___nodes_full(const Model0_nodemask_t *Model0_srcp, unsigned int Model0_nbits)
{
 return Model0_bitmap_full(Model0_srcp->Model0_bits, Model0_nbits);
}


static inline __attribute__((no_instrument_function)) int Model0___nodes_weight(const Model0_nodemask_t *Model0_srcp, unsigned int Model0_nbits)
{
 return Model0_bitmap_weight(Model0_srcp->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_shift_right(Model0_nodemask_t *Model0_dstp,
     const Model0_nodemask_t *Model0_srcp, int Model0_n, int Model0_nbits)
{
 Model0_bitmap_shift_right(Model0_dstp->Model0_bits, Model0_srcp->Model0_bits, Model0_n, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_shift_left(Model0_nodemask_t *Model0_dstp,
     const Model0_nodemask_t *Model0_srcp, int Model0_n, int Model0_nbits)
{
 Model0_bitmap_shift_left(Model0_dstp->Model0_bits, Model0_srcp->Model0_bits, Model0_n, Model0_nbits);
}

/* FIXME: better would be to fix all architectures to never return
          > MAX_NUMNODES, then the silly min_ts could be dropped. */


static inline __attribute__((no_instrument_function)) int Model0___first_node(const Model0_nodemask_t *Model0_srcp)
{
 return ({ int Model0___min1 = ((1 << 6)); int Model0___min2 = (Model0_find_first_bit(Model0_srcp->Model0_bits, (1 << 6))); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; });
}


static inline __attribute__((no_instrument_function)) int Model0___next_node(int Model0_n, const Model0_nodemask_t *Model0_srcp)
{
 return ({ int Model0___min1 = ((1 << 6)); int Model0___min2 = (Model0_find_next_bit(Model0_srcp->Model0_bits, (1 << 6), Model0_n+1)); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; });
}

/*
 * Find the next present node in src, starting after node n, wrapping around to
 * the first node in src if needed.  Returns MAX_NUMNODES if src is empty.
 */

int Model0___next_node_in(int Model0_node, const Model0_nodemask_t *Model0_srcp);

static inline __attribute__((no_instrument_function)) void Model0_init_nodemask_of_node(Model0_nodemask_t *Model0_mask, int Model0_node)
{
 Model0___nodes_clear(&(*Model0_mask), (1 << 6));
 Model0___node_set((Model0_node), &(*Model0_mask));
}
static inline __attribute__((no_instrument_function)) int Model0___first_unset_node(const Model0_nodemask_t *Model0_maskp)
{
 return ({ int Model0___min1 = ((1 << 6)); int Model0___min2 = (Model0_find_first_zero_bit(Model0_maskp->Model0_bits, (1 << 6))); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; });

}
static inline __attribute__((no_instrument_function)) int Model0___nodemask_parse_user(const char *Model0_buf, int Model0_len,
     Model0_nodemask_t *Model0_dstp, int Model0_nbits)
{
 return Model0_bitmap_parse_user(Model0_buf, Model0_len, Model0_dstp->Model0_bits, Model0_nbits);
}


static inline __attribute__((no_instrument_function)) int Model0___nodelist_parse(const char *Model0_buf, Model0_nodemask_t *Model0_dstp, int Model0_nbits)
{
 return Model0_bitmap_parselist(Model0_buf, Model0_dstp->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) int Model0___node_remap(int Model0_oldbit,
  const Model0_nodemask_t *Model0_oldp, const Model0_nodemask_t *Model0_newp, int Model0_nbits)
{
 return Model0_bitmap_bitremap(Model0_oldbit, Model0_oldp->Model0_bits, Model0_newp->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_remap(Model0_nodemask_t *Model0_dstp, const Model0_nodemask_t *Model0_srcp,
  const Model0_nodemask_t *Model0_oldp, const Model0_nodemask_t *Model0_newp, int Model0_nbits)
{
 Model0_bitmap_remap(Model0_dstp->Model0_bits, Model0_srcp->Model0_bits, Model0_oldp->Model0_bits, Model0_newp->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_onto(Model0_nodemask_t *Model0_dstp, const Model0_nodemask_t *Model0_origp,
  const Model0_nodemask_t *Model0_relmapp, int Model0_nbits)
{
 Model0_bitmap_onto(Model0_dstp->Model0_bits, Model0_origp->Model0_bits, Model0_relmapp->Model0_bits, Model0_nbits);
}



static inline __attribute__((no_instrument_function)) void Model0___nodes_fold(Model0_nodemask_t *Model0_dstp, const Model0_nodemask_t *Model0_origp,
  int Model0_sz, int Model0_nbits)
{
 Model0_bitmap_fold(Model0_dstp->Model0_bits, Model0_origp->Model0_bits, Model0_sz, Model0_nbits);
}
/*
 * Bitmasks that are kept for all the nodes.
 */
enum Model0_node_states {
 Model0_N_POSSIBLE, /* The node could become online at some point */
 Model0_N_ONLINE, /* The node is online */
 Model0_N_NORMAL_MEMORY, /* The node has regular memory */



 Model0_N_HIGH_MEMORY = Model0_N_NORMAL_MEMORY,




 Model0_N_MEMORY = Model0_N_HIGH_MEMORY,

 Model0_N_CPU, /* The node has one or more cpus */
 Model0_NR_NODE_STATES
};

/*
 * The following particular system nodemasks and operations
 * on them manage all possible and online nodes.
 */

extern Model0_nodemask_t Model0_node_states[Model0_NR_NODE_STATES];


static inline __attribute__((no_instrument_function)) int Model0_node_state(int Model0_node, enum Model0_node_states Model0_state)
{
 return (__builtin_constant_p(((Model0_node))) ? Model0_constant_test_bit(((Model0_node)), ((Model0_node_states[Model0_state]).Model0_bits)) : Model0_variable_test_bit(((Model0_node)), ((Model0_node_states[Model0_state]).Model0_bits)));
}

static inline __attribute__((no_instrument_function)) void Model0_node_set_state(int Model0_node, enum Model0_node_states Model0_state)
{
 Model0___node_set(Model0_node, &Model0_node_states[Model0_state]);
}

static inline __attribute__((no_instrument_function)) void Model0_node_clear_state(int Model0_node, enum Model0_node_states Model0_state)
{
 Model0___node_clear(Model0_node, &Model0_node_states[Model0_state]);
}

static inline __attribute__((no_instrument_function)) int Model0_num_node_state(enum Model0_node_states Model0_state)
{
 return Model0___nodes_weight(&(Model0_node_states[Model0_state]), (1 << 6));
}






static inline __attribute__((no_instrument_function)) int Model0_next_online_node(int Model0_nid)
{
 return Model0___next_node((Model0_nid), &(Model0_node_states[Model0_N_ONLINE]));
}
static inline __attribute__((no_instrument_function)) int Model0_next_memory_node(int Model0_nid)
{
 return Model0___next_node((Model0_nid), &(Model0_node_states[Model0_N_MEMORY]));
}

extern int Model0_nr_node_ids;
extern int Model0_nr_online_nodes;

static inline __attribute__((no_instrument_function)) void Model0_node_set_online(int Model0_nid)
{
 Model0_node_set_state(Model0_nid, Model0_N_ONLINE);
 Model0_nr_online_nodes = Model0_num_node_state(Model0_N_ONLINE);
}

static inline __attribute__((no_instrument_function)) void Model0_node_set_offline(int Model0_nid)
{
 Model0_node_clear_state(Model0_nid, Model0_N_ONLINE);
 Model0_nr_online_nodes = Model0_num_node_state(Model0_N_ONLINE);
}
extern int Model0_node_random(const Model0_nodemask_t *Model0_maskp);
/*
 * For nodemask scrach area.
 * NODEMASK_ALLOC(type, name) allocates an object with a specified type and
 * name.
 */
/* A example struture for using NODEMASK_ALLOC, used in mempolicy. */
struct Model0_nodemask_scratch {
 Model0_nodemask_t Model0_mask1;
 Model0_nodemask_t Model0_mask2;
};
/*
 * Macros for manipulating and testing flags related to a
 * pageblock_nr_pages number of pages.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation version 2 of the License
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 *
 * Copyright (C) IBM Corporation, 2006
 *
 * Original author, Mel Gorman
 * Major cleanups and reduction of bit operations, Andy Whitcroft
 */





/* Bit indices that affect a whole block of pages */
enum Model0_pageblock_bits {
 Model0_PB_migrate,
 Model0_PB_migrate_end = Model0_PB_migrate + 3 - 1,
   /* 3 bits required for migrate types */
 Model0_PB_migrate_skip,/* If set the block is skipped by compaction */

 /*
	 * Assume the bits will always align on a word. If this assumption
	 * changes then get/set pageblock needs updating.
	 */
 Model0_NR_PAGEBLOCK_BITS
};
/* Huge pages are a constant size */
/* Forward declaration */
struct Model0_page;

unsigned long Model0_get_pfnblock_flags_mask(struct Model0_page *Model0_page,
    unsigned long Model0_pfn,
    unsigned long Model0_end_bitidx,
    unsigned long Model0_mask);

void Model0_set_pfnblock_flags_mask(struct Model0_page *Model0_page,
    unsigned long Model0_flags,
    unsigned long Model0_pfn,
    unsigned long Model0_end_bitidx,
    unsigned long Model0_mask);

/* Declarations for getting and setting flags. See mm/page_alloc.c */






/*
 * DO NOT MODIFY.
 *
 * This file was generated by Kbuild
 */

/*
 * When a memory allocation must conform to specific limitations (such
 * as being suitable for DMA) the caller will pass in hints to the
 * allocator in the gfp_mask, in the zone modifier bits.  These bits
 * are used to select a priority ordered list of memory zones which
 * match the requested limits. See gfp_zone() in include/linux/gfp.h
 */
/* SECTION_SHIFT	#bits space required to store a section # */




/*
 * page->flags layout:
 *
 * There are five possibilities for how page->flags get laid out.  The first
 * pair is for the normal case without sparsemem. The second pair is for
 * sparsemem when there is plenty of space for node and section information.
 * The last is when there is insufficient space in page->flags and a separate
 * lookup is necessary.
 *
 * No sparsemem or sparsemem vmemmap: |       NODE     | ZONE |             ... | FLAGS |
 *      " plus space for last_cpupid: |       NODE     | ZONE | LAST_CPUPID ... | FLAGS |
 * classic sparse with space for node:| SECTION | NODE | ZONE |             ... | FLAGS |
 *      " plus space for last_cpupid: | SECTION | NODE | ZONE | LAST_CPUPID ... | FLAGS |
 * classic sparse no space for node:  | SECTION |     ZONE    | ... | FLAGS |
 */
/*
 * We are going to use the flags for the page to node mapping if its in
 * there.  This includes the case where there is no node, so it is implicit.
 */



/* Free memory management - zoned buddy allocator.  */







/*
 * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed
 * costly to service.  That is between allocation orders which should
 * coalesce naturally under reasonable reclaim pressure and those which
 * will not.
 */


enum {
 Model0_MIGRATE_UNMOVABLE,
 Model0_MIGRATE_MOVABLE,
 Model0_MIGRATE_RECLAIMABLE,
 Model0_MIGRATE_PCPTYPES, /* the number of types on the pcp lists */
 Model0_MIGRATE_HIGHATOMIC = Model0_MIGRATE_PCPTYPES,
 Model0_MIGRATE_TYPES
};

/* In mm/page_alloc.c; keep in sync also with show_migration_types() there */
extern char * const Model0_migratetype_names[Model0_MIGRATE_TYPES];
extern int Model0_page_group_by_mobility_disabled;
struct Model0_free_area {
 struct Model0_list_head Model0_free_list[Model0_MIGRATE_TYPES];
 unsigned long Model0_nr_free;
};

struct Model0_pglist_data;

/*
 * zone->lock and the zone lru_lock are two of the hottest locks in the kernel.
 * So add a wild amount of padding here to ensure that they fall into separate
 * cachelines.  There are very few zone structures in the machine, so space
 * consumption is not a concern here.
 */

struct Model0_zone_padding {
 char Model0_x[0];
} __attribute__((__aligned__(1 << (6))));





enum Model0_zone_stat_item {
 /* First 128 byte cacheline (assuming 64 bit words) */
 Model0_NR_FREE_PAGES,
 Model0_NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */
 Model0_NR_ZONE_INACTIVE_ANON = Model0_NR_ZONE_LRU_BASE,
 Model0_NR_ZONE_ACTIVE_ANON,
 Model0_NR_ZONE_INACTIVE_FILE,
 Model0_NR_ZONE_ACTIVE_FILE,
 Model0_NR_ZONE_UNEVICTABLE,
 Model0_NR_ZONE_WRITE_PENDING, /* Count of dirty, writeback and unstable pages */
 Model0_NR_MLOCK, /* mlock()ed pages found and moved off LRU */
 Model0_NR_SLAB_RECLAIMABLE,
 Model0_NR_SLAB_UNRECLAIMABLE,
 Model0_NR_PAGETABLE, /* used for pagetables */
 Model0_NR_KERNEL_STACK_KB, /* measured in KiB */
 /* Second 128 byte cacheline */
 Model0_NR_BOUNCE,




 Model0_NUMA_HIT, /* allocated in intended node */
 Model0_NUMA_MISS, /* allocated in non intended node */
 Model0_NUMA_FOREIGN, /* was intended here, hit elsewhere */
 Model0_NUMA_INTERLEAVE_HIT, /* interleaver preferred this zone */
 Model0_NUMA_LOCAL, /* allocation from local node */
 Model0_NUMA_OTHER, /* allocation from other node */

 Model0_NR_FREE_CMA_PAGES,
 Model0_NR_VM_ZONE_STAT_ITEMS };

enum Model0_node_stat_item {
 Model0_NR_LRU_BASE,
 Model0_NR_INACTIVE_ANON = Model0_NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
 Model0_NR_ACTIVE_ANON, /*  "     "     "   "       "         */
 Model0_NR_INACTIVE_FILE, /*  "     "     "   "       "         */
 Model0_NR_ACTIVE_FILE, /*  "     "     "   "       "         */
 Model0_NR_UNEVICTABLE, /*  "     "     "   "       "         */
 Model0_NR_ISOLATED_ANON, /* Temporary isolated pages from anon lru */
 Model0_NR_ISOLATED_FILE, /* Temporary isolated pages from file lru */
 Model0_NR_PAGES_SCANNED, /* pages scanned since last reclaim */
 Model0_WORKINGSET_REFAULT,
 Model0_WORKINGSET_ACTIVATE,
 Model0_WORKINGSET_NODERECLAIM,
 Model0_NR_ANON_MAPPED, /* Mapped anonymous pages */
 Model0_NR_FILE_MAPPED, /* pagecache pages mapped into pagetables.
			   only modified from process context */
 Model0_NR_FILE_PAGES,
 Model0_NR_FILE_DIRTY,
 Model0_NR_WRITEBACK,
 Model0_NR_WRITEBACK_TEMP, /* Writeback using temporary buffers */
 Model0_NR_SHMEM, /* shmem pages (included tmpfs/GEM pages) */
 Model0_NR_SHMEM_THPS,
 Model0_NR_SHMEM_PMDMAPPED,
 Model0_NR_ANON_THPS,
 Model0_NR_UNSTABLE_NFS, /* NFS unstable pages */
 Model0_NR_VMSCAN_WRITE,
 Model0_NR_VMSCAN_IMMEDIATE, /* Prioritise for reclaim when writeback ends */
 Model0_NR_DIRTIED, /* page dirtyings since bootup */
 Model0_NR_WRITTEN, /* page writings since bootup */
 Model0_NR_VM_NODE_STAT_ITEMS
};

/*
 * We do arithmetic on the LRU lists in various places in the code,
 * so it is important to keep the active lists LRU_ACTIVE higher in
 * the array than the corresponding inactive lists, and to keep
 * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.
 *
 * This has to be kept in sync with the statistics in zone_stat_item
 * above and the descriptions in vmstat_text in mm/vmstat.c
 */




enum Model0_lru_list {
 Model0_LRU_INACTIVE_ANON = 0,
 Model0_LRU_ACTIVE_ANON = 0 + 1,
 Model0_LRU_INACTIVE_FILE = 0 + 2,
 Model0_LRU_ACTIVE_FILE = 0 + 2 + 1,
 Model0_LRU_UNEVICTABLE,
 Model0_NR_LRU_LISTS
};





static inline __attribute__((no_instrument_function)) int Model0_is_file_lru(enum Model0_lru_list Model0_lru)
{
 return (Model0_lru == Model0_LRU_INACTIVE_FILE || Model0_lru == Model0_LRU_ACTIVE_FILE);
}

static inline __attribute__((no_instrument_function)) int Model0_is_active_lru(enum Model0_lru_list Model0_lru)
{
 return (Model0_lru == Model0_LRU_ACTIVE_ANON || Model0_lru == Model0_LRU_ACTIVE_FILE);
}

struct Model0_zone_reclaim_stat {
 /*
	 * The pageout code in vmscan.c keeps track of how many of the
	 * mem/swap backed and file backed pages are referenced.
	 * The higher the rotated/scanned ratio, the more valuable
	 * that cache is.
	 *
	 * The anon LRU stats live in [0], file LRU stats in [1]
	 */
 unsigned long Model0_recent_rotated[2];
 unsigned long Model0_recent_scanned[2];
};

struct Model0_lruvec {
 struct Model0_list_head Model0_lists[Model0_NR_LRU_LISTS];
 struct Model0_zone_reclaim_stat Model0_reclaim_stat;
 /* Evictions & activations on the inactive file list */
 Model0_atomic_long_t Model0_inactive_age;



};

/* Mask used at gathering information at once (see memcontrol.c) */




/* Isolate clean file */

/* Isolate unmapped file */

/* Isolate for asynchronous migration */

/* Isolate unevictable pages */


/* LRU Isolation modes. */
typedef unsigned Model0_isolate_mode_t;

enum Model0_zone_watermarks {
 Model0_WMARK_MIN,
 Model0_WMARK_LOW,
 Model0_WMARK_HIGH,
 Model0_NR_WMARK
};





struct Model0_per_cpu_pages {
 int Model0_count; /* number of pages in the list */
 int Model0_high; /* high watermark, emptying needed */
 int Model0_batch; /* chunk size for buddy add/remove */

 /* Lists of pages, one per migrate type stored on the pcp-lists */
 struct Model0_list_head Model0_lists[Model0_MIGRATE_PCPTYPES];
};

struct Model0_per_cpu_pageset {
 struct Model0_per_cpu_pages Model0_pcp;

 Model0_s8 Model0_expire;


 Model0_s8 Model0_stat_threshold;
 Model0_s8 Model0_vm_stat_diff[Model0_NR_VM_ZONE_STAT_ITEMS];

};

struct Model0_per_cpu_nodestat {
 Model0_s8 Model0_stat_threshold;
 Model0_s8 Model0_vm_node_stat_diff[Model0_NR_VM_NODE_STAT_ITEMS];
};



enum Model0_zone_type {

 /*
	 * ZONE_DMA is used when there are devices that are not able
	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
	 * carve out the portion of memory that is needed for these devices.
	 * The range is arch specific.
	 *
	 * Some examples
	 *
	 * Architecture		Limit
	 * ---------------------------
	 * parisc, ia64, sparc	<4G
	 * s390			<2G
	 * arm			Various
	 * alpha		Unlimited or 0-16MB.
	 *
	 * i386, x86_64 and multiple other arches
	 * 			<16M.
	 */
 Model0_ZONE_DMA,


 /*
	 * x86_64 needs two ZONE_DMAs because it supports devices that are
	 * only able to do DMA to the lower 16M but also 32 bit devices that
	 * can only do DMA areas below 4G.
	 */
 Model0_ZONE_DMA32,

 /*
	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	 * performed on pages in ZONE_NORMAL if the DMA devices support
	 * transfers to all addressable memory.
	 */
 Model0_ZONE_NORMAL,
 Model0_ZONE_MOVABLE,



 Model0___MAX_NR_ZONES

};



struct Model0_zone {
 /* Read-mostly fields */

 /* zone watermarks, access with *_wmark_pages(zone) macros */
 unsigned long Model0_watermark[Model0_NR_WMARK];

 unsigned long Model0_nr_reserved_highatomic;

 /*
	 * We don't know if the memory that we're going to allocate will be
	 * freeable or/and it will be released eventually, so to avoid totally
	 * wasting several GB of ram we must reserve some of the lower zone
	 * memory (otherwise we risk to run OOM on the lower zones despite
	 * there being tons of freeable ram on the higher zones).  This array is
	 * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl
	 * changes.
	 */
 long Model0_lowmem_reserve[4];


 int Model0_node;

 struct Model0_pglist_data *Model0_zone_pgdat;
 struct Model0_per_cpu_pageset *Model0_pageset;
 /* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
 unsigned long Model0_zone_start_pfn;

 /*
	 * spanned_pages is the total pages spanned by the zone, including
	 * holes, which is calculated as:
	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;
	 *
	 * present_pages is physical pages existing within the zone, which
	 * is calculated as:
	 *	present_pages = spanned_pages - absent_pages(pages in holes);
	 *
	 * managed_pages is present pages managed by the buddy system, which
	 * is calculated as (reserved_pages includes pages allocated by the
	 * bootmem allocator):
	 *	managed_pages = present_pages - reserved_pages;
	 *
	 * So present_pages may be used by memory hotplug or memory power
	 * management logic to figure out unmanaged pages by checking
	 * (present_pages - managed_pages). And managed_pages should be used
	 * by page allocator and vm scanner to calculate all kinds of watermarks
	 * and thresholds.
	 *
	 * Locking rules:
	 *
	 * zone_start_pfn and spanned_pages are protected by span_seqlock.
	 * It is a seqlock because it has to be read outside of zone->lock,
	 * and it is done in the main allocator path.  But, it is written
	 * quite infrequently.
	 *
	 * The span_seq lock is declared along with zone->lock because it is
	 * frequently read in proximity to zone->lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 *
	 * Write access to present_pages at runtime should be protected by
	 * mem_hotplug_begin/end(). Any reader who can't tolerant drift of
	 * present_pages should get_online_mems() to get a stable value.
	 *
	 * Read access to managed_pages should be safe because it's unsigned
	 * long. Write access to zone->managed_pages and totalram_pages are
	 * protected by managed_page_count_lock at runtime. Idealy only
	 * adjust_managed_page_count() should be used instead of directly
	 * touching zone->managed_pages and totalram_pages.
	 */
 unsigned long Model0_managed_pages;
 unsigned long Model0_spanned_pages;
 unsigned long Model0_present_pages;

 const char *Model0_name;
 /*
	 * wait_table		-- the array holding the hash table
	 * wait_table_hash_nr_entries	-- the size of the hash table array
	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
	 *
	 * The purpose of all these is to keep track of the people
	 * waiting for a page to become available and make them
	 * runnable again when possible. The trouble is that this
	 * consumes a lot of space, especially when so few things
	 * wait on pages at a given time. So instead of using
	 * per-page waitqueues, we use a waitqueue hash table.
	 *
	 * The bucket discipline is to sleep on the same queue when
	 * colliding and wake all in that wait queue when removing.
	 * When something wakes, it must check to be sure its page is
	 * truly available, a la thundering herd. The cost of a
	 * collision is great, but given the expected load of the
	 * table, they should be so rare as to be outweighed by the
	 * benefits from the saved space.
	 *
	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
	 * primary users of these fields, and in mm/page_alloc.c
	 * free_area_init_core() performs the initialization of them.
	 */
 Model0_wait_queue_head_t *Model0_wait_table;
 unsigned long Model0_wait_table_hash_nr_entries;
 unsigned long Model0_wait_table_bits;

 /* Write-intensive fields used from the page allocator */
 struct Model0_zone_padding Model0__pad1_;

 /* free areas of different sizes */
 struct Model0_free_area Model0_free_area[11];

 /* zone flags, see below */
 unsigned long Model0_flags;

 /* Primarily protects free_area */
 Model0_spinlock_t Model0_lock;

 /* Write-intensive fields used by compaction and vmstats. */
 struct Model0_zone_padding Model0__pad2_;

 /*
	 * When free pages are below this point, additional steps are taken
	 * when reading the number of free pages to avoid per-cpu counter
	 * drift allowing watermarks to be breached
	 */
 unsigned long Model0_percpu_drift_mark;


 /* pfn where compaction free scanner should start */
 unsigned long Model0_compact_cached_free_pfn;
 /* pfn where async and sync compaction migration scanner should start */
 unsigned long Model0_compact_cached_migrate_pfn[2];



 /*
	 * On compaction failure, 1<<compact_defer_shift compactions
	 * are skipped before trying again. The number attempted since
	 * last failure is tracked with compact_considered.
	 */
 unsigned int Model0_compact_considered;
 unsigned int Model0_compact_defer_shift;
 int Model0_compact_order_failed;



 /* Set to true when the PG_migrate_skip bits should be cleared */
 bool Model0_compact_blockskip_flush;


 bool Model0_contiguous;

 struct Model0_zone_padding Model0__pad3_;
 /* Zone statistics */
 Model0_atomic_long_t Model0_vm_stat[Model0_NR_VM_ZONE_STAT_ITEMS];
} __attribute__((__aligned__(1 << (6))));

enum Model0_pgdat_flags {
 Model0_PGDAT_CONGESTED, /* pgdat has many dirty pages backed by
					 * a congested BDI
					 */
 Model0_PGDAT_DIRTY, /* reclaim scanning has recently found
					 * many dirty file pages at the tail
					 * of the LRU.
					 */
 Model0_PGDAT_WRITEBACK, /* reclaim scanning has recently found
					 * many pages under writeback
					 */
 Model0_PGDAT_RECLAIM_LOCKED, /* prevents concurrent reclaim */
};

static inline __attribute__((no_instrument_function)) unsigned long Model0_zone_end_pfn(const struct Model0_zone *Model0_zone)
{
 return Model0_zone->Model0_zone_start_pfn + Model0_zone->Model0_spanned_pages;
}

static inline __attribute__((no_instrument_function)) bool Model0_zone_spans_pfn(const struct Model0_zone *Model0_zone, unsigned long Model0_pfn)
{
 return Model0_zone->Model0_zone_start_pfn <= Model0_pfn && Model0_pfn < Model0_zone_end_pfn(Model0_zone);
}

static inline __attribute__((no_instrument_function)) bool Model0_zone_is_initialized(struct Model0_zone *Model0_zone)
{
 return !!Model0_zone->Model0_wait_table;
}

static inline __attribute__((no_instrument_function)) bool Model0_zone_is_empty(struct Model0_zone *Model0_zone)
{
 return Model0_zone->Model0_spanned_pages == 0;
}

/*
 * The "priority" of VM scanning is how much of the queues we will scan in one
 * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the
 * queues ("queue_length >> 12") during an aging round.
 */


/* Maximum number of zones on a zonelist */


enum {
 Model0_ZONELIST_FALLBACK, /* zonelist with fallback */

 /*
	 * The NUMA zonelists are doubled because we need zonelists that
	 * restrict the allocations to a single node for __GFP_THISNODE.
	 */
 Model0_ZONELIST_NOFALLBACK, /* zonelist without fallback (__GFP_THISNODE) */

 Model0_MAX_ZONELISTS
};

/*
 * This struct contains information about a zone in a zonelist. It is stored
 * here to avoid dereferences into large structures and lookups of tables
 */
struct Model0_zoneref {
 struct Model0_zone *Model0_zone; /* Pointer to actual zone */
 int Model0_zone_idx; /* zone_idx(zoneref->zone) */
};

/*
 * One allocation request operates on a zonelist. A zonelist
 * is a list of zones, the first one is the 'goal' of the
 * allocation, the other zones are fallback zones, in decreasing
 * priority.
 *
 * To speed the reading of the zonelist, the zonerefs contain the zone index
 * of the entry being read. Helper functions to access information given
 * a struct zoneref are
 *
 * zonelist_zone()	- Return the struct zone * for an entry in _zonerefs
 * zonelist_zone_idx()	- Return the index of the zone for an entry
 * zonelist_node_idx()	- Return the index of the node for an entry
 */
struct Model0_zonelist {
 struct Model0_zoneref Model0__zonerefs[((1 << 6) * 4) + 1];
};


/* The array of struct pages - for discontigmem use pgdat->lmem_map */
extern struct Model0_page *Model0_mem_map;


/*
 * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM
 * (mostly NUMA machines?) to denote a higher-level memory zone than the
 * zone denotes.
 *
 * On NUMA machines, each NUMA node would have a pg_data_t to describe
 * it's memory layout.
 *
 * Memory statistics and page replacement data structures are maintained on a
 * per-zone basis.
 */
struct Model0_bootmem_data;
typedef struct Model0_pglist_data {
 struct Model0_zone Model0_node_zones[4];
 struct Model0_zonelist Model0_node_zonelists[Model0_MAX_ZONELISTS];
 int Model0_nr_zones;
 unsigned long Model0_node_start_pfn;
 unsigned long Model0_node_present_pages; /* total number of physical pages */
 unsigned long Model0_node_spanned_pages; /* total size of physical page
					     range, including holes */
 int Model0_node_id;
 Model0_wait_queue_head_t Model0_kswapd_wait;
 Model0_wait_queue_head_t Model0_pfmemalloc_wait;
 struct Model0_task_struct *Model0_kswapd; /* Protected by
					   mem_hotplug_begin/end() */
 int Model0_kswapd_order;
 enum Model0_zone_type Model0_kswapd_classzone_idx;


 int Model0_kcompactd_max_order;
 enum Model0_zone_type Model0_kcompactd_classzone_idx;
 Model0_wait_queue_head_t Model0_kcompactd_wait;
 struct Model0_task_struct *Model0_kcompactd;
 /*
	 * This is a per-node reserve of pages that are not available
	 * to userspace allocations.
	 */
 unsigned long Model0_totalreserve_pages;


 /*
	 * zone reclaim becomes active if more unmapped pages exist.
	 */
 unsigned long Model0_min_unmapped_pages;
 unsigned long Model0_min_slab_pages;


 /* Write-intensive fields used by page reclaim */
 struct Model0_zone_padding Model0__pad1_;
 Model0_spinlock_t Model0_lru_lock;
 /* Fields commonly accessed by the page reclaim scanner */
 struct Model0_lruvec Model0_lruvec;

 /*
	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
	 * this node's LRU.  Maintained by the pageout code.
	 */
 unsigned int Model0_inactive_ratio;

 unsigned long Model0_flags;

 struct Model0_zone_padding Model0__pad2_;

 /* Per-node vmstats */
 struct Model0_per_cpu_nodestat *Model0_per_cpu_nodestats;
 Model0_atomic_long_t Model0_vm_stat[Model0_NR_VM_NODE_STAT_ITEMS];
} Model0_pg_data_t;
static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_zone_lru_lock(struct Model0_zone *Model0_zone)
{
 return &Model0_zone->Model0_zone_pgdat->Model0_lru_lock;
}

static inline __attribute__((no_instrument_function)) struct Model0_lruvec *Model0_node_lruvec(struct Model0_pglist_data *Model0_pgdat)
{
 return &Model0_pgdat->Model0_lruvec;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_pgdat_end_pfn(Model0_pg_data_t *Model0_pgdat)
{
 return Model0_pgdat->Model0_node_start_pfn + Model0_pgdat->Model0_node_spanned_pages;
}

static inline __attribute__((no_instrument_function)) bool Model0_pgdat_is_empty(Model0_pg_data_t *Model0_pgdat)
{
 return !Model0_pgdat->Model0_node_start_pfn && !Model0_pgdat->Model0_node_spanned_pages;
}

static inline __attribute__((no_instrument_function)) int Model0_zone_id(const struct Model0_zone *Model0_zone)
{
 struct Model0_pglist_data *Model0_pgdat = Model0_zone->Model0_zone_pgdat;

 return Model0_zone - Model0_pgdat->Model0_node_zones;
}







static inline __attribute__((no_instrument_function)) bool Model0_is_dev_zone(const struct Model0_zone *Model0_zone)
{
 return false;
}







/*
 *	Routines to manage notifier chains for passing status changes to any
 *	interested routines. We need this instead of hard coded call lists so
 *	that modules can poke their nose into the innards. The network devices
 *	needed them so here they are for the rest of you.
 *
 *				Alan Cox <Alan.Cox@linux.org>
 */





/*
 * Mutexes: blocking mutual exclusion locks
 *
 * started by Ingo Molnar:
 *
 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *
 * This file contains the main data structure and API definitions.
 */



/*
 * An MCS like lock especially tailored for optimistic spinning for sleeping
 * lock implementations (mutex, rwsem, etc).
 */
struct Model0_optimistic_spin_node {
 struct Model0_optimistic_spin_node *Model0_next, *Model0_prev;
 int Model0_locked; /* 1 if lock acquired */
 int Model0_cpu; /* encoded CPU # + 1 value */
};

struct Model0_optimistic_spin_queue {
 /*
	 * Stores an encoded value of the CPU # of the tail node in the queue.
	 * If the queue is empty, then it's set to OSQ_UNLOCKED_VAL.
	 */
 Model0_atomic_t Model0_tail;
};



/* Init macro and function. */


static inline __attribute__((no_instrument_function)) void Model0_osq_lock_init(struct Model0_optimistic_spin_queue *Model0_lock)
{
 Model0_atomic_set(&Model0_lock->Model0_tail, (0));
}

extern bool Model0_osq_lock(struct Model0_optimistic_spin_queue *Model0_lock);
extern void Model0_osq_unlock(struct Model0_optimistic_spin_queue *Model0_lock);

static inline __attribute__((no_instrument_function)) bool Model0_osq_is_locked(struct Model0_optimistic_spin_queue *Model0_lock)
{
 return Model0_atomic_read(&Model0_lock->Model0_tail) != (0);
}

/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 *   contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * debugging code also implements a number of additional features
 * that make lock debugging easier and faster:
 *
 * - uses symbolic names of mutexes, whenever they are printed in debug output
 * - point-of-acquire tracking, symbolic lookup of function names
 * - list of all locks held in the system, printout of them
 * - owner tracking
 * - detects self-recursing locks and prints out all relevant info
 * - detects multi-task circular deadlocks and prints out all affected
 *   locks and tasks (and only those tasks)
 */
struct Model0_mutex {
 /* 1: unlocked, 0: locked, negative: locked, possible waiters */
 Model0_atomic_t Model0_count;
 Model0_spinlock_t Model0_wait_lock;
 struct Model0_list_head Model0_wait_list;

 struct Model0_task_struct *Model0_owner;


 struct Model0_optimistic_spin_queue Model0_osq; /* Spinner MCS lock */







};

/*
 * This is the control structure for tasks blocked on mutex,
 * which resides on the blocked task's kernel stack:
 */
struct Model0_mutex_waiter {
 struct Model0_list_head Model0_list;
 struct Model0_task_struct *Model0_task;



};





/**
 * mutex_init - initialize the mutex
 * @mutex: the mutex to be initialized
 *
 * Initialize the mutex to unlocked state.
 *
 * It is not allowed to initialize an already locked mutex.
 */






static inline __attribute__((no_instrument_function)) void Model0_mutex_destroy(struct Model0_mutex *Model0_lock) {}
extern void Model0___mutex_init(struct Model0_mutex *Model0_lock, const char *Model0_name,
    struct Model0_lock_class_key *Model0_key);

/**
 * mutex_is_locked - is the mutex locked
 * @lock: the mutex to be queried
 *
 * Returns 1 if the mutex is locked, 0 if unlocked.
 */
static inline __attribute__((no_instrument_function)) int Model0_mutex_is_locked(struct Model0_mutex *Model0_lock)
{
 return Model0_atomic_read(&Model0_lock->Model0_count) != 1;
}

/*
 * See kernel/locking/mutex.c for detailed documentation of these APIs.
 * Also see Documentation/locking/mutex-design.txt.
 */
extern void Model0_mutex_lock(struct Model0_mutex *Model0_lock);
extern int __attribute__((warn_unused_result)) Model0_mutex_lock_interruptible(struct Model0_mutex *Model0_lock);
extern int __attribute__((warn_unused_result)) Model0_mutex_lock_killable(struct Model0_mutex *Model0_lock);







/*
 * NOTE: mutex_trylock() follows the spin_trylock() convention,
 *       not the down_trylock() convention!
 *
 * Returns 1 if the mutex has been acquired successfully, and 0 on contention.
 */
extern int Model0_mutex_trylock(struct Model0_mutex *Model0_lock);
extern void Model0_mutex_unlock(struct Model0_mutex *Model0_lock);

extern int Model0_atomic_dec_and_mutex_lock(Model0_atomic_t *Model0_cnt, struct Model0_mutex *Model0_lock);
/* rwsem.h: R/W semaphores, public interface
 *
 * Written by David Howells (dhowells@redhat.com).
 * Derived from asm-i386/semaphore.h
 */
struct Model0_rw_semaphore;





/* All arch specific implementations share the same struct */
struct Model0_rw_semaphore {
 Model0_atomic_long_t Model0_count;
 struct Model0_list_head Model0_wait_list;
 Model0_raw_spinlock_t Model0_wait_lock;

 struct Model0_optimistic_spin_queue Model0_osq; /* spinner MCS lock */
 /*
	 * Write owner. Used as a speculative check to see
	 * if the owner is running on the cpu.
	 */
 struct Model0_task_struct *Model0_owner;




};

extern struct Model0_rw_semaphore *Model0_rwsem_down_read_failed(struct Model0_rw_semaphore *Model0_sem);
extern struct Model0_rw_semaphore *Model0_rwsem_down_write_failed(struct Model0_rw_semaphore *Model0_sem);
extern struct Model0_rw_semaphore *Model0_rwsem_down_write_failed_killable(struct Model0_rw_semaphore *Model0_sem);
extern struct Model0_rw_semaphore *Model0_rwsem_wake(struct Model0_rw_semaphore *);
extern struct Model0_rw_semaphore *Model0_rwsem_downgrade_wake(struct Model0_rw_semaphore *Model0_sem);

/* Include the arch specific part */

/* rwsem.h: R/W semaphores implemented using XADD/CMPXCHG for i486+
 *
 * Written by David Howells (dhowells@redhat.com).
 *
 * Derived from asm-x86/semaphore.h
 *
 *
 * The MSW of the count is the negated number of active writers and waiting
 * lockers, and the LSW is the total number of active locks
 *
 * The lock count is initialized to 0 (no active and no waiting lockers).
 *
 * When a writer subtracts WRITE_BIAS, it'll get 0xffff0001 for the case of an
 * uncontended lock. This can be determined because XADD returns the old value.
 * Readers increment by 1 and see a positive value when uncontended, negative
 * if there are writers (and maybe) readers waiting (in which case it goes to
 * sleep).
 *
 * The value of WAITING_BIAS supports up to 32766 waiting processes. This can
 * be extended to 65534 by manually checking the whole MSW rather than relying
 * on the S flag.
 *
 * The value of ACTIVE_BIAS supports up to 65535 active processes.
 *
 * This should be totally fair - if anything is waiting, a process that wants a
 * lock will go to the back of the queue. When the currently active lock is
 * released, if there's a writer at the front of the queue, then that and only
 * that will be woken up; if there's a bunch of consecutive readers at the
 * front, then they'll all be woken up, but no other readers will be.
 */
/*
 * The bias values and the counter type limits the number of
 * potential readers/writers to 32767 for 32 bits and 2147483647
 * for 64 bits.
 */
/*
 * lock for reading
 */
static inline __attribute__((no_instrument_function)) void Model0___down_read(struct Model0_rw_semaphore *Model0_sem)
{
 asm volatile("# beginning down_read\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " " " "incq" " " "(%1)\n\t"
       /* adds 0x00000001 */
       "  jns        1f\n"
       "  call call_rwsem_down_read_failed\n"
       "1:\n\t"
       "# ending down_read\n\t"
       : "+m" (Model0_sem->Model0_count)
       : "a" (Model0_sem)
       : "memory", "cc");
}

/*
 * trylock for reading -- returns 1 if successful, 0 if contention
 */
static inline __attribute__((no_instrument_function)) bool Model0___down_read_trylock(struct Model0_rw_semaphore *Model0_sem)
{
 long Model0_result, Model0_tmp;
 asm volatile("# beginning __down_read_trylock\n\t"
       "  mov          %0,%1\n\t"
       "1:\n\t"
       "  mov          %1,%2\n\t"
       "  add          %3,%2\n\t"
       "  jle	     2f\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  cmpxchg  %2,%0\n\t"
       "  jnz	     1b\n\t"
       "2:\n\t"
       "# ending __down_read_trylock\n\t"
       : "+m" (Model0_sem->Model0_count), "=&a" (Model0_result), "=&r" (Model0_tmp)
       : "i" (0x00000001L)
       : "memory", "cc");
 return Model0_result >= 0;
}

/*
 * lock for writing
 */
static inline __attribute__((no_instrument_function)) void Model0___down_write(struct Model0_rw_semaphore *Model0_sem)
{
 ({ long Model0_tmp; struct Model0_rw_semaphore* Model0_ret; asm volatile("# beginning down_write\n\t" ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  xadd      %1,(%3)\n\t" "  test " " " "%k1" " " "," " " "%k1" " " "\n\t" "  jz        1f\n" "  call " "call_rwsem_down_write_failed" "\n" "1:\n" "# ending down_write" : "+m" (Model0_sem->Model0_count), "=d" (Model0_tmp), "=a" (Model0_ret) : "a" (Model0_sem), "1" (((-0xffffffffL -1) + 0x00000001L)) : "memory", "cc"); Model0_ret; });
}

static inline __attribute__((no_instrument_function)) int Model0___down_write_killable(struct Model0_rw_semaphore *Model0_sem)
{
 if (Model0_IS_ERR(({ long Model0_tmp; struct Model0_rw_semaphore* Model0_ret; asm volatile("# beginning down_write\n\t" ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  xadd      %1,(%3)\n\t" "  test " " " "%k1" " " "," " " "%k1" " " "\n\t" "  jz        1f\n" "  call " "call_rwsem_down_write_failed_killable" "\n" "1:\n" "# ending down_write" : "+m" (Model0_sem->Model0_count), "=d" (Model0_tmp), "=a" (Model0_ret) : "a" (Model0_sem), "1" (((-0xffffffffL -1) + 0x00000001L)) : "memory", "cc"); Model0_ret; })))
  return -4;

 return 0;
}

/*
 * trylock for writing -- returns 1 if successful, 0 if contention
 */
static inline __attribute__((no_instrument_function)) bool Model0___down_write_trylock(struct Model0_rw_semaphore *Model0_sem)
{
 bool Model0_result;
 long Model0_tmp0, Model0_tmp1;
 asm volatile("# beginning __down_write_trylock\n\t"
       "  mov          %0,%1\n\t"
       "1:\n\t"
       "  test " " " "%k1" " " "," " " "%k1" " " "\n\t"
       /* was the active mask 0 before? */
       "  jnz          2f\n\t"
       "  mov          %1,%2\n\t"
       "  add          %4,%2\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  cmpxchg  %2,%0\n\t"
       "  jnz	     1b\n\t"
       "2:\n\t"
       "\n\tset" "e" " %[_cc_" "e" "]\n"
       "# ending __down_write_trylock\n\t"
       : "+m" (Model0_sem->Model0_count), "=&a" (Model0_tmp0), "=&r" (Model0_tmp1),
         [_cc_e] "=qm" (Model0_result)
       : "er" (((-0xffffffffL -1) + 0x00000001L))
       : "memory", "cc");
 return Model0_result;
}

/*
 * unlock after reading
 */
static inline __attribute__((no_instrument_function)) void Model0___up_read(struct Model0_rw_semaphore *Model0_sem)
{
 long Model0_tmp;
 asm volatile("# beginning __up_read\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  xadd      %1,(%2)\n\t"
       /* subtracts 1, returns the old value */
       "  jns        1f\n\t"
       "  call call_rwsem_wake\n" /* expects old value in %edx */
       "1:\n"
       "# ending __up_read\n"
       : "+m" (Model0_sem->Model0_count), "=d" (Model0_tmp)
       : "a" (Model0_sem), "1" (-0x00000001L)
       : "memory", "cc");
}

/*
 * unlock after writing
 */
static inline __attribute__((no_instrument_function)) void Model0___up_write(struct Model0_rw_semaphore *Model0_sem)
{
 long Model0_tmp;
 asm volatile("# beginning __up_write\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  xadd      %1,(%2)\n\t"
       /* subtracts 0xffff0001, returns the old value */
       "  jns        1f\n\t"
       "  call call_rwsem_wake\n" /* expects old value in %edx */
       "1:\n\t"
       "# ending __up_write\n"
       : "+m" (Model0_sem->Model0_count), "=d" (Model0_tmp)
       : "a" (Model0_sem), "1" (-((-0xffffffffL -1) + 0x00000001L))
       : "memory", "cc");
}

/*
 * downgrade write lock to read lock
 */
static inline __attribute__((no_instrument_function)) void Model0___downgrade_write(struct Model0_rw_semaphore *Model0_sem)
{
 asm volatile("# beginning __downgrade_write\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " " " "addq" " " "%2,(%1)\n\t"
       /*
		      * transitions 0xZZZZ0001 -> 0xYYYY0001 (i386)
		      *     0xZZZZZZZZ00000001 -> 0xYYYYYYYY00000001 (x86_64)
		      */
       "  jns       1f\n\t"
       "  call call_rwsem_downgrade_wake\n"
       "1:\n\t"
       "# ending __downgrade_write\n"
       : "+m" (Model0_sem->Model0_count)
       : "a" (Model0_sem), "er" (-(-0xffffffffL -1))
       : "memory", "cc");
}

/* In all implementations count != 0 means locked */
static inline __attribute__((no_instrument_function)) int Model0_rwsem_is_locked(struct Model0_rw_semaphore *Model0_sem)
{
 return Model0_atomic_long_read(&Model0_sem->Model0_count) != 0;
}




/* Common initializer macros and functions */
extern void Model0___init_rwsem(struct Model0_rw_semaphore *Model0_sem, const char *Model0_name,
    struct Model0_lock_class_key *Model0_key);
/*
 * This is the same regardless of which rwsem implementation that is being used.
 * It is just a heuristic meant to be called by somebody alreadying holding the
 * rwsem to see if somebody from an incompatible type is wanting access to the
 * lock.
 */
static inline __attribute__((no_instrument_function)) int Model0_rwsem_is_contended(struct Model0_rw_semaphore *Model0_sem)
{
 return !Model0_list_empty(&Model0_sem->Model0_wait_list);
}

/*
 * lock for reading
 */
extern void Model0_down_read(struct Model0_rw_semaphore *Model0_sem);

/*
 * trylock for reading -- returns 1 if successful, 0 if contention
 */
extern int Model0_down_read_trylock(struct Model0_rw_semaphore *Model0_sem);

/*
 * lock for writing
 */
extern void Model0_down_write(struct Model0_rw_semaphore *Model0_sem);
extern int __attribute__((warn_unused_result)) Model0_down_write_killable(struct Model0_rw_semaphore *Model0_sem);

/*
 * trylock for writing -- returns 1 if successful, 0 if contention
 */
extern int Model0_down_write_trylock(struct Model0_rw_semaphore *Model0_sem);

/*
 * release a read lock
 */
extern void Model0_up_read(struct Model0_rw_semaphore *Model0_sem);

/*
 * release a write lock
 */
extern void Model0_up_write(struct Model0_rw_semaphore *Model0_sem);

/*
 * downgrade write lock to read lock
 */
extern void Model0_downgrade_write(struct Model0_rw_semaphore *Model0_sem);
/*
 * Sleepable Read-Copy Update mechanism for mutual exclusion
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 * Copyright (C) IBM Corporation, 2006
 * Copyright (C) Fujitsu, 2012
 *
 * Author: Paul McKenney <paulmck@us.ibm.com>
 *	   Lai Jiangshan <laijs@cn.fujitsu.com>
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 * 		Documentation/RCU/ *.txt
 *
 */






/*
 * Read-Copy Update mechanism for mutual exclusion
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 * Copyright IBM Corporation, 2001
 *
 * Author: Dipankar Sarma <dipankar@in.ibm.com>
 *
 * Based on the original work by Paul McKenney <paulmck@us.ibm.com>
 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
 * Papers:
 * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf
 * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 *		http://lse.sourceforge.net/locking/rcupdate.html
 *
 */



/*
 * (C) Copyright 2001 Linus Torvalds
 *
 * Atomic wait-for-completion handler data structures.
 * See kernel/sched/completion.c for details.
 */



/*
 * struct completion - structure used to maintain state for a "completion"
 *
 * This is the opaque structure used to maintain the state for a "completion".
 * Completions currently use a FIFO to queue threads that have to wait for
 * the "completion" event.
 *
 * See also:  complete(), wait_for_completion() (and friends _timeout,
 * _interruptible, _interruptible_timeout, and _killable), init_completion(),
 * reinit_completion(), and macros DECLARE_COMPLETION(),
 * DECLARE_COMPLETION_ONSTACK().
 */
struct Model0_completion {
 unsigned int Model0_done;
 Model0_wait_queue_head_t Model0_wait;
};







/**
 * DECLARE_COMPLETION - declare and initialize a completion structure
 * @work:  identifier for the completion structure
 *
 * This macro declares and initializes a completion structure. Generally used
 * for static declarations. You should use the _ONSTACK variant for automatic
 * variables.
 */



/*
 * Lockdep needs to run a non-constant initializer for on-stack
 * completions - so we use the _ONSTACK() variant for those that
 * are on the kernel stack:
 */
/**
 * DECLARE_COMPLETION_ONSTACK - declare and initialize a completion structure
 * @work:  identifier for the completion structure
 *
 * This macro declares and initializes a completion structure on the kernel
 * stack.
 */







/**
 * init_completion - Initialize a dynamically allocated completion
 * @x:  pointer to completion structure that is to be initialized
 *
 * This inline function will initialize a dynamically created completion
 * structure.
 */
static inline __attribute__((no_instrument_function)) void Model0_init_completion(struct Model0_completion *Model0_x)
{
 Model0_x->Model0_done = 0;
 do { static struct Model0_lock_class_key Model0___key; Model0___init_waitqueue_head((&Model0_x->Model0_wait), "&x->wait", &Model0___key); } while (0);
}

/**
 * reinit_completion - reinitialize a completion structure
 * @x:  pointer to completion structure that is to be reinitialized
 *
 * This inline function should be used to reinitialize a completion structure so it can
 * be reused. This is especially important after complete_all() is used.
 */
static inline __attribute__((no_instrument_function)) void Model0_reinit_completion(struct Model0_completion *Model0_x)
{
 Model0_x->Model0_done = 0;
}

extern void Model0_wait_for_completion(struct Model0_completion *);
extern void Model0_wait_for_completion_io(struct Model0_completion *);
extern int Model0_wait_for_completion_interruptible(struct Model0_completion *Model0_x);
extern int Model0_wait_for_completion_killable(struct Model0_completion *Model0_x);
extern unsigned long Model0_wait_for_completion_timeout(struct Model0_completion *Model0_x,
         unsigned long Model0_timeout);
extern unsigned long Model0_wait_for_completion_io_timeout(struct Model0_completion *Model0_x,
          unsigned long Model0_timeout);
extern long Model0_wait_for_completion_interruptible_timeout(
 struct Model0_completion *Model0_x, unsigned long Model0_timeout);
extern long Model0_wait_for_completion_killable_timeout(
 struct Model0_completion *Model0_x, unsigned long Model0_timeout);
extern bool Model0_try_wait_for_completion(struct Model0_completion *Model0_x);
extern bool Model0_completion_done(struct Model0_completion *Model0_x);

extern void Model0_complete(struct Model0_completion *);
extern void Model0_complete_all(struct Model0_completion *);






enum Model0_debug_obj_state {
 Model0_ODEBUG_STATE_NONE,
 Model0_ODEBUG_STATE_INIT,
 Model0_ODEBUG_STATE_INACTIVE,
 Model0_ODEBUG_STATE_ACTIVE,
 Model0_ODEBUG_STATE_DESTROYED,
 Model0_ODEBUG_STATE_NOTAVAILABLE,
 Model0_ODEBUG_STATE_MAX,
};

struct Model0_debug_obj_descr;

/**
 * struct debug_obj - representaion of an tracked object
 * @node:	hlist node to link the object into the tracker list
 * @state:	tracked object state
 * @astate:	current active state
 * @object:	pointer to the real object
 * @descr:	pointer to an object type specific debug description structure
 */
struct Model0_debug_obj {
 struct Model0_hlist_node Model0_node;
 enum Model0_debug_obj_state Model0_state;
 unsigned int Model0_astate;
 void *Model0_object;
 struct Model0_debug_obj_descr *Model0_descr;
};

/**
 * struct debug_obj_descr - object type specific debug description structure
 *
 * @name:		name of the object typee
 * @debug_hint:		function returning address, which have associated
 *			kernel symbol, to allow identify the object
 * @is_static_object:	return true if the obj is static, otherwise return false
 * @fixup_init:		fixup function, which is called when the init check
 *			fails. All fixup functions must return true if fixup
 *			was successful, otherwise return false
 * @fixup_activate:	fixup function, which is called when the activate check
 *			fails
 * @fixup_destroy:	fixup function, which is called when the destroy check
 *			fails
 * @fixup_free:		fixup function, which is called when the free check
 *			fails
 * @fixup_assert_init:  fixup function, which is called when the assert_init
 *			check fails
 */
struct Model0_debug_obj_descr {
 const char *Model0_name;
 void *(*Model0_debug_hint)(void *Model0_addr);
 bool (*Model0_is_static_object)(void *Model0_addr);
 bool (*Model0_fixup_init)(void *Model0_addr, enum Model0_debug_obj_state Model0_state);
 bool (*Model0_fixup_activate)(void *Model0_addr, enum Model0_debug_obj_state Model0_state);
 bool (*Model0_fixup_destroy)(void *Model0_addr, enum Model0_debug_obj_state Model0_state);
 bool (*Model0_fixup_free)(void *Model0_addr, enum Model0_debug_obj_state Model0_state);
 bool (*Model0_fixup_assert_init)(void *Model0_addr, enum Model0_debug_obj_state Model0_state);
};
static inline __attribute__((no_instrument_function)) void
Model0_debug_object_init (void *Model0_addr, struct Model0_debug_obj_descr *Model0_descr) { }
static inline __attribute__((no_instrument_function)) void
Model0_debug_object_init_on_stack(void *Model0_addr, struct Model0_debug_obj_descr *Model0_descr) { }
static inline __attribute__((no_instrument_function)) int
Model0_debug_object_activate (void *Model0_addr, struct Model0_debug_obj_descr *Model0_descr) { return 0; }
static inline __attribute__((no_instrument_function)) void
Model0_debug_object_deactivate(void *Model0_addr, struct Model0_debug_obj_descr *Model0_descr) { }
static inline __attribute__((no_instrument_function)) void
Model0_debug_object_destroy (void *Model0_addr, struct Model0_debug_obj_descr *Model0_descr) { }
static inline __attribute__((no_instrument_function)) void
Model0_debug_object_free (void *Model0_addr, struct Model0_debug_obj_descr *Model0_descr) { }
static inline __attribute__((no_instrument_function)) void
Model0_debug_object_assert_init(void *Model0_addr, struct Model0_debug_obj_descr *Model0_descr) { }

static inline __attribute__((no_instrument_function)) void Model0_debug_objects_early_init(void) { }
static inline __attribute__((no_instrument_function)) void Model0_debug_objects_mem_init(void) { }





static inline __attribute__((no_instrument_function)) void
Model0_debug_check_no_obj_freed(const void *Model0_address, unsigned long Model0_size) { }


/*
 *  include/linux/ktime.h
 *
 *  ktime_t - nanosecond-resolution time format.
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes and macros.
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 *
 *  Credits:
 *
 *  	Roman Zippel provided the ideas and primary code snippets of
 *  	the ktime_t union and further simplifications of the original
 *  	code.
 *
 *  For licencing details see kernel-base/COPYING
 */





















struct Model0_timespec {
 Model0___kernel_time_t Model0_tv_sec; /* seconds */
 long Model0_tv_nsec; /* nanoseconds */
};


struct Model0_timeval {
 Model0___kernel_time_t Model0_tv_sec; /* seconds */
 Model0___kernel_suseconds_t Model0_tv_usec; /* microseconds */
};

struct Model0_timezone {
 int Model0_tz_minuteswest; /* minutes west of Greenwich */
 int Model0_tz_dsttime; /* type of dst correction */
};


/*
 * Names of the interval timers, and structure
 * defining a timer setting:
 */




struct Model0_itimerspec {
 struct Model0_timespec Model0_it_interval; /* timer period */
 struct Model0_timespec Model0_it_value; /* timer expiration */
};

struct Model0_itimerval {
 struct Model0_timeval Model0_it_interval; /* timer interval */
 struct Model0_timeval Model0_it_value; /* current value */
};

/*
 * The IDs of the various system clocks (for POSIX.1b interval timers):
 */
/*
 * The various flags for setting POSIX.1b interval timers:
 */


typedef Model0___s64 Model0_time64_t;

/*
 * This wants to go into uapi/linux/time.h once we agreed about the
 * userspace interfaces.
 */
/* Parameters used to convert the timespec values: */
/* Located here for timespec[64]_valid_strict */






static inline __attribute__((no_instrument_function)) struct Model0_timespec Model0_timespec64_to_timespec(const struct Model0_timespec Model0_ts64)
{
 return Model0_ts64;
}

static inline __attribute__((no_instrument_function)) struct Model0_timespec Model0_timespec_to_timespec64(const struct Model0_timespec Model0_ts)
{
 return Model0_ts;
}

static inline __attribute__((no_instrument_function)) struct Model0_itimerspec Model0_itimerspec64_to_itimerspec(struct Model0_itimerspec *Model0_its64)
{
 return *Model0_its64;
}

static inline __attribute__((no_instrument_function)) struct Model0_itimerspec Model0_itimerspec_to_itimerspec64(struct Model0_itimerspec *Model0_its)
{
 return *Model0_its;
}
/*
 * timespec64_add_safe assumes both values are positive and checks for
 * overflow. It will return TIME64_MAX in case of overflow.
 */
extern struct Model0_timespec Model0_timespec64_add_safe(const struct Model0_timespec Model0_lhs,
      const struct Model0_timespec Model0_rhs);

extern struct Model0_timezone Model0_sys_tz;



static inline __attribute__((no_instrument_function)) int Model0_timespec_equal(const struct Model0_timespec *Model0_a,
                                 const struct Model0_timespec *Model0_b)
{
 return (Model0_a->Model0_tv_sec == Model0_b->Model0_tv_sec) && (Model0_a->Model0_tv_nsec == Model0_b->Model0_tv_nsec);
}

/*
 * lhs < rhs:  return <0
 * lhs == rhs: return 0
 * lhs > rhs:  return >0
 */
static inline __attribute__((no_instrument_function)) int Model0_timespec_compare(const struct Model0_timespec *Model0_lhs, const struct Model0_timespec *Model0_rhs)
{
 if (Model0_lhs->Model0_tv_sec < Model0_rhs->Model0_tv_sec)
  return -1;
 if (Model0_lhs->Model0_tv_sec > Model0_rhs->Model0_tv_sec)
  return 1;
 return Model0_lhs->Model0_tv_nsec - Model0_rhs->Model0_tv_nsec;
}

static inline __attribute__((no_instrument_function)) int Model0_timeval_compare(const struct Model0_timeval *Model0_lhs, const struct Model0_timeval *Model0_rhs)
{
 if (Model0_lhs->Model0_tv_sec < Model0_rhs->Model0_tv_sec)
  return -1;
 if (Model0_lhs->Model0_tv_sec > Model0_rhs->Model0_tv_sec)
  return 1;
 return Model0_lhs->Model0_tv_usec - Model0_rhs->Model0_tv_usec;
}

extern Model0_time64_t Model0_mktime64(const unsigned int Model0_year, const unsigned int Model0_mon,
   const unsigned int Model0_day, const unsigned int Model0_hour,
   const unsigned int Model0_min, const unsigned int Model0_sec);

/**
 * Deprecated. Use mktime64().
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_mktime(const unsigned int Model0_year,
   const unsigned int Model0_mon, const unsigned int Model0_day,
   const unsigned int Model0_hour, const unsigned int Model0_min,
   const unsigned int Model0_sec)
{
 return Model0_mktime64(Model0_year, Model0_mon, Model0_day, Model0_hour, Model0_min, Model0_sec);
}

extern void Model0_set_normalized_timespec(struct Model0_timespec *Model0_ts, Model0_time_t Model0_sec, Model0_s64 Model0_nsec);

/*
 * timespec_add_safe assumes both values are positive and checks
 * for overflow. It will return TIME_T_MAX if the reutrn would be
 * smaller then either of the arguments.
 */
extern struct Model0_timespec Model0_timespec_add_safe(const struct Model0_timespec Model0_lhs,
      const struct Model0_timespec Model0_rhs);


static inline __attribute__((no_instrument_function)) struct Model0_timespec Model0_timespec_add(struct Model0_timespec Model0_lhs,
      struct Model0_timespec Model0_rhs)
{
 struct Model0_timespec Model0_ts_delta;
 Model0_set_normalized_timespec(&Model0_ts_delta, Model0_lhs.Model0_tv_sec + Model0_rhs.Model0_tv_sec,
    Model0_lhs.Model0_tv_nsec + Model0_rhs.Model0_tv_nsec);
 return Model0_ts_delta;
}

/*
 * sub = lhs - rhs, in normalized form
 */
static inline __attribute__((no_instrument_function)) struct Model0_timespec Model0_timespec_sub(struct Model0_timespec Model0_lhs,
      struct Model0_timespec Model0_rhs)
{
 struct Model0_timespec Model0_ts_delta;
 Model0_set_normalized_timespec(&Model0_ts_delta, Model0_lhs.Model0_tv_sec - Model0_rhs.Model0_tv_sec,
    Model0_lhs.Model0_tv_nsec - Model0_rhs.Model0_tv_nsec);
 return Model0_ts_delta;
}

/*
 * Returns true if the timespec is norm, false if denorm:
 */
static inline __attribute__((no_instrument_function)) bool Model0_timespec_valid(const struct Model0_timespec *Model0_ts)
{
 /* Dates before 1970 are bogus */
 if (Model0_ts->Model0_tv_sec < 0)
  return false;
 /* Can't have more nanoseconds then a second */
 if ((unsigned long)Model0_ts->Model0_tv_nsec >= 1000000000L)
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_timespec_valid_strict(const struct Model0_timespec *Model0_ts)
{
 if (!Model0_timespec_valid(Model0_ts))
  return false;
 /* Disallow values that could overflow ktime_t */
 if ((unsigned long long)Model0_ts->Model0_tv_sec >= (((Model0_s64)~((Model0_u64)1 << 63)) / 1000000000L))
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_timeval_valid(const struct Model0_timeval *Model0_tv)
{
 /* Dates before 1970 are bogus */
 if (Model0_tv->Model0_tv_sec < 0)
  return false;

 /* Can't have more microseconds then a second */
 if (Model0_tv->Model0_tv_usec < 0 || Model0_tv->Model0_tv_usec >= 1000000L)
  return false;

 return true;
}

extern struct Model0_timespec Model0_timespec_trunc(struct Model0_timespec Model0_t, unsigned Model0_gran);

/*
 * Validates if a timespec/timeval used to inject a time offset is valid.
 * Offsets can be postive or negative. The value of the timeval/timespec
 * is the sum of its fields, but *NOTE*: the field tv_usec/tv_nsec must
 * always be non-negative.
 */
static inline __attribute__((no_instrument_function)) bool Model0_timeval_inject_offset_valid(const struct Model0_timeval *Model0_tv)
{
 /* We don't check the tv_sec as it can be positive or negative */

 /* Can't have more microseconds then a second */
 if (Model0_tv->Model0_tv_usec < 0 || Model0_tv->Model0_tv_usec >= 1000000L)
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_timespec_inject_offset_valid(const struct Model0_timespec *Model0_ts)
{
 /* We don't check the tv_sec as it can be positive or negative */

 /* Can't have more nanoseconds then a second */
 if (Model0_ts->Model0_tv_nsec < 0 || Model0_ts->Model0_tv_nsec >= 1000000000L)
  return false;
 return true;
}




/* Some architectures do not supply their own clocksource.
 * This is mainly the case in architectures that get their
 * inter-tick times by reading the counter on their interval
 * timer. Since these timers wrap every tick, they're not really
 * useful as clocksources. Wrapping them to act like one is possible
 * but not very efficient. So we provide a callout these arches
 * can implement for use with the jiffies clocksource to provide
 * finer then tick granular time.
 */




struct Model0_itimerval;
extern int Model0_do_setitimer(int Model0_which, struct Model0_itimerval *Model0_value,
   struct Model0_itimerval *Model0_ovalue);
extern int Model0_do_getitimer(int Model0_which, struct Model0_itimerval *Model0_value);

extern unsigned int Model0_alarm_setitimer(unsigned int Model0_seconds);

extern long Model0_do_utimes(int Model0_dfd, const char *Model0_filename, struct Model0_timespec *Model0_times, int Model0_flags);

struct Model0_tms;
extern void Model0_do_sys_times(struct Model0_tms *);

/*
 * Similar to the struct tm in userspace <time.h>, but it needs to be here so
 * that the kernel source is self contained.
 */
struct Model0_tm {
 /*
	 * the number of seconds after the minute, normally in the range
	 * 0 to 59, but can be up to 60 to allow for leap seconds
	 */
 int Model0_tm_sec;
 /* the number of minutes after the hour, in the range 0 to 59*/
 int Model0_tm_min;
 /* the number of hours past midnight, in the range 0 to 23 */
 int Model0_tm_hour;
 /* the day of the month, in the range 1 to 31 */
 int Model0_tm_mday;
 /* the number of months since January, in the range 0 to 11 */
 int Model0_tm_mon;
 /* the number of years since 1900 */
 long Model0_tm_year;
 /* the number of days since Sunday, in the range 0 to 6 */
 int Model0_tm_wday;
 /* the number of days since January 1, in the range 0 to 365 */
 int Model0_tm_yday;
};

void Model0_time64_to_tm(Model0_time64_t Model0_totalsecs, int Model0_offset, struct Model0_tm *Model0_result);

/**
 * time_to_tm - converts the calendar time to local broken-down time
 *
 * @totalsecs	the number of seconds elapsed since 00:00:00 on January 1, 1970,
 *		Coordinated Universal Time (UTC).
 * @offset	offset seconds adding to totalsecs.
 * @result	pointer to struct tm variable to receive broken-down time
 */
static inline __attribute__((no_instrument_function)) void Model0_time_to_tm(Model0_time_t Model0_totalsecs, int Model0_offset, struct Model0_tm *Model0_result)
{
 Model0_time64_to_tm(Model0_totalsecs, Model0_offset, Model0_result);
}

/**
 * timespec_to_ns - Convert timespec to nanoseconds
 * @ts:		pointer to the timespec variable to be converted
 *
 * Returns the scalar nanosecond representation of the timespec
 * parameter.
 */
static inline __attribute__((no_instrument_function)) Model0_s64 Model0_timespec_to_ns(const struct Model0_timespec *Model0_ts)
{
 return ((Model0_s64) Model0_ts->Model0_tv_sec * 1000000000L) + Model0_ts->Model0_tv_nsec;
}

/**
 * timeval_to_ns - Convert timeval to nanoseconds
 * @ts:		pointer to the timeval variable to be converted
 *
 * Returns the scalar nanosecond representation of the timeval
 * parameter.
 */
static inline __attribute__((no_instrument_function)) Model0_s64 Model0_timeval_to_ns(const struct Model0_timeval *Model0_tv)
{
 return ((Model0_s64) Model0_tv->Model0_tv_sec * 1000000000L) +
  Model0_tv->Model0_tv_usec * 1000L;
}

/**
 * ns_to_timespec - Convert nanoseconds to timespec
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the timespec representation of the nsec parameter.
 */
extern struct Model0_timespec Model0_ns_to_timespec(const Model0_s64 Model0_nsec);

/**
 * ns_to_timeval - Convert nanoseconds to timeval
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the timeval representation of the nsec parameter.
 */
extern struct Model0_timeval Model0_ns_to_timeval(const Model0_s64 Model0_nsec);

/**
 * timespec_add_ns - Adds nanoseconds to a timespec
 * @a:		pointer to timespec to be incremented
 * @ns:		unsigned nanoseconds value to be added
 *
 * This must always be inlined because its used from the x86-64 vdso,
 * which cannot call other kernel functions.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_timespec_add_ns(struct Model0_timespec *Model0_a, Model0_u64 Model0_ns)
{
 Model0_a->Model0_tv_sec += Model0___iter_div_u64_rem(Model0_a->Model0_tv_nsec + Model0_ns, 1000000000L, &Model0_ns);
 Model0_a->Model0_tv_nsec = Model0_ns;
}







/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/

/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */




/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/

/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */







/*
 * syscall interface - used (mainly by NTP daemon)
 * to discipline kernel clock oscillator
 */
struct Model0_timex {
 unsigned int Model0_modes; /* mode selector */
 Model0___kernel_long_t Model0_offset; /* time offset (usec) */
 Model0___kernel_long_t Model0_freq; /* frequency offset (scaled ppm) */
 Model0___kernel_long_t Model0_maxerror;/* maximum error (usec) */
 Model0___kernel_long_t Model0_esterror;/* estimated error (usec) */
 int Model0_status; /* clock command/status */
 Model0___kernel_long_t Model0_constant;/* pll time constant */
 Model0___kernel_long_t Model0_precision;/* clock precision (usec) (read only) */
 Model0___kernel_long_t Model0_tolerance;/* clock frequency tolerance (ppm)
				   * (read only)
				   */
 struct Model0_timeval Model0_time; /* (read only, except for ADJ_SETOFFSET) */
 Model0___kernel_long_t Model0_tick; /* (modified) usecs between clock ticks */

 Model0___kernel_long_t Model0_ppsfreq;/* pps frequency (scaled ppm) (ro) */
 Model0___kernel_long_t Model0_jitter; /* pps jitter (us) (ro) */
 int Model0_shift; /* interval duration (s) (shift) (ro) */
 Model0___kernel_long_t Model0_stabil; /* pps stability (scaled ppm) (ro) */
 Model0___kernel_long_t Model0_jitcnt; /* jitter limit exceeded (ro) */
 Model0___kernel_long_t Model0_calcnt; /* calibration intervals (ro) */
 Model0___kernel_long_t Model0_errcnt; /* calibration errors (ro) */
 Model0___kernel_long_t Model0_stbcnt; /* stability limit exceeded (ro) */

 int Model0_tai; /* TAI offset (ro) */

 int :32; int :32; int :32; int :32;
 int :32; int :32; int :32; int :32;
 int :32; int :32; int :32;
};

/*
 * Mode codes (timex.mode)
 */
/* NTP userland likes the MOD_ prefix better */
/*
 * Status codes (timex.status)
 */
/* read-only bits */



/*
 * Clock states (time_state)
 */

















/*
 * x86 TSC related functions
 */
/*
 * Standard way to access the cycle counter.
 */
typedef unsigned long long Model0_cycles_t;

extern unsigned int Model0_cpu_khz;
extern unsigned int Model0_tsc_khz;

extern void Model0_disable_TSC(void);

static inline __attribute__((no_instrument_function)) Model0_cycles_t Model0_get_cycles(void)
{





 return Model0_rdtsc();
}

extern struct Model0_system_counterval_t Model0_convert_art_to_tsc(Model0_cycle_t Model0_art);

extern void Model0_tsc_init(void);
extern void Model0_mark_tsc_unstable(char *Model0_reason);
extern int Model0_unsynchronized_tsc(void);
extern int Model0_check_tsc_unstable(void);
extern unsigned long Model0_native_calibrate_cpu(void);
extern unsigned long Model0_native_calibrate_tsc(void);
extern unsigned long long Model0_native_sched_clock_from_tsc(Model0_u64 Model0_tsc);

extern int Model0_tsc_clocksource_reliable;

/*
 * Boot-time check whether the TSCs are synchronized across
 * all CPUs/cores:
 */
extern void Model0_check_tsc_sync_source(int Model0_cpu);
extern void Model0_check_tsc_sync_target(void);

extern int Model0_notsc_setup(char *);
extern void Model0_tsc_save_sched_clock_state(void);
extern void Model0_tsc_restore_sched_clock_state(void);

unsigned long Model0_cpu_khz_from_msr(void);

/* Assume we use the PIT time source for the clock tick */


/*
 * The random_get_entropy() function is used by the /dev/random driver
 * in order to extract entropy via the relative unpredictability of
 * when an interrupt takes places versus a high speed, fine-grained
 * timing source or cycle counter.  Since it will be occurred on every
 * single interrupt, it must have a very low cost/overhead.
 *
 * By default we use get_cycles() for this purpose, but individual
 * architectures may override this in their asm/timex.h header file.
 */



/*
 * SHIFT_PLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in PLL mode.
 * It also used in dampening the offset correction, to define how
 * much of the current value in time_offset we correct for each
 * second. Changing this value changes the stiffness of the ntp
 * adjustment code. A lower value makes it more flexible, reducing
 * NTP convergence time. A higher value makes it stiffer, increasing
 * convergence time, but making the clock more stable.
 *
 * In David Mills' nanokernel reference implementation SHIFT_PLL is 4.
 * However this seems to increase convergence time much too long.
 *
 * https://lists.ntp.org/pipermail/hackers/2008-January/003487.html
 *
 * In the above mailing list discussion, it seems the value of 4
 * was appropriate for other Unix systems with HZ=100, and that
 * SHIFT_PLL should be decreased as HZ increases. However, Linux's
 * clock steering implementation is HZ independent.
 *
 * Through experimentation, a SHIFT_PLL value of 2 was found to allow
 * for fast convergence (very similar to the NTPv3 code used prior to
 * v2.6.19), with good clock stability.
 *
 *
 * SHIFT_FLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in FLL mode.
 * In David Mills' nanokernel reference implementation SHIFT_FLL is 2.
 *
 * MAXTC establishes the maximum time constant of the PLL.
 */




/*
 * SHIFT_USEC defines the scaling (shift) of the time_freq and
 * time_tolerance variables, which represent the current frequency
 * offset and maximum frequency tolerance.
 */
/*
 * kernel variables
 * Note: maximum error = NTP synch distance = dispersion + delay / 2;
 * estimated error = NTP dispersion.
 */
extern unsigned long Model0_tick_usec; /* USER_HZ period (usec) */
extern unsigned long Model0_tick_nsec; /* SHIFTED_HZ period (nsec) */

/* Required to safely shift negative values */
extern int Model0_do_adjtimex(struct Model0_timex *);
extern void Model0_hardpps(const struct Model0_timespec *, const struct Model0_timespec *);

int Model0_read_current_timer(unsigned long *Model0_timer_val);
void Model0_ntp_notify_cmos_timer(void);

/* The clock frequency of the i8253/i8254 PIT */
/* Automatically generated by kernel/time/timeconst.bc */
/* Time conversion constants for HZ == 1000 */

/*
 * The following defines establish the engineering parameters of the PLL
 * model. The HZ variable establishes the timer interrupt frequency, 100 Hz
 * for the SunOS kernel, 256 Hz for the Ultrix kernel and 1024 Hz for the
 * OSF/1 kernel. The SHIFT_HZ define expresses the same value as the
 * nearest power of two in order to avoid hardware multiply operations.
 */
/* Suppose we want to divide two numbers NOM and DEN: NOM/DEN, then we can
 * improve accuracy by shifting LSH bits, hence calculating:
 *     (NOM << LSH) / DEN
 * This however means trouble for large NOM, because (NOM << LSH) may no
 * longer fit in 32 bits. The following way of calculating this gives us
 * some slack, under the following conditions:
 *   - (NOM / DEN) fits in (32 - LSH) bits.
 *   - (NOM % DEN) fits in (32 - LSH) bits.
 */



/* LATCH is used in the interval timer and ftape setup. */


extern int Model0_register_refined_jiffies(long Model0_clock_tick_rate);

/* TICK_NSEC is the time between ticks in nsec assuming SHIFTED_HZ */


/* TICK_USEC is the time between ticks in usec assuming fake USER_HZ */


/* some arch's have a small-data section that can be accessed register-relative
 * but that can only take up to, say, 4-byte variables. jiffies being part of
 * an 8-byte variable may not be correctly accessed unless we force the issue
 */


/*
 * The 64-bit value is not atomic - you MUST NOT read it
 * without sampling the sequence number in jiffies_lock.
 * get_jiffies_64() will do this for you as appropriate.
 */
extern Model0_u64 __attribute__((section(".data"))) Model0_jiffies_64;
extern unsigned long volatile __attribute__((section(".data"))) Model0_jiffies;




static inline __attribute__((no_instrument_function)) Model0_u64 Model0_get_jiffies_64(void)
{
 return (Model0_u64)Model0_jiffies;
}


/*
 *	These inlines deal with timer wrapping correctly. You are 
 *	strongly encouraged to use them
 *	1. Because people otherwise forget
 *	2. Because if the timer wrap changes in future you won't have to
 *	   alter your driver code.
 *
 * time_after(a,b) returns true if the time a is after time b.
 *
 * Do this with "<0" and ">=0" to only test the sign of the result. A
 * good compiler would generate better code (and a really good compiler
 * wouldn't care). Gcc is currently neither.
 */
/*
 * Calculate whether a is in the range of [b, c].
 */




/*
 * Calculate whether a is in the range of [b, c).
 */




/* Same as above, but does so with platform independent 64bit types.
 * These must be used when utilizing jiffies_64 (i.e. return value of
 * get_jiffies_64() */
/*
 * These four macros compare jiffies and 'a' for convenience.
 */

/* time_is_before_jiffies(a) return true if a is before jiffies */


/* time_is_after_jiffies(a) return true if a is after jiffies */


/* time_is_before_eq_jiffies(a) return true if a is before or equal to jiffies*/


/* time_is_after_eq_jiffies(a) return true if a is after or equal to jiffies*/


/*
 * Have the 32 bit jiffies value wrap 5 minutes after boot
 * so jiffies wrap bugs show up earlier.
 */


/*
 * Change timeval to jiffies, trying to avoid the
 * most obvious overflows..
 *
 * And some not so obvious.
 *
 * Note that we don't want to return LONG_MAX, because
 * for various timeout reasons we often end up having
 * to wait "jiffies+1" in order to guarantee that we wait
 * at _least_ "jiffies" - so "jiffies+1" had better still
 * be positive.
 */


extern unsigned long Model0_preset_lpj;

/*
 * We want to do realistic conversions of time so we need to use the same
 * values the update wall clock code uses as the jiffies size.  This value
 * is: TICK_NSEC (which is defined in timex.h).  This
 * is a constant and is in nanoseconds.  We will use scaled math
 * with a set of scales defined here as SEC_JIFFIE_SC,  USEC_JIFFIE_SC and
 * NSEC_JIFFIE_SC.  Note that these defines contain nothing but
 * constants and so are computed at compile time.  SHIFT_HZ (computed in
 * timex.h) adjusts the scaling for different HZ values.

 * Scaled math???  What is that?
 *
 * Scaled math is a way to do integer math on values that would,
 * otherwise, either overflow, underflow, or cause undesired div
 * instructions to appear in the execution path.  In short, we "scale"
 * up the operands so they take more bits (more precision, less
 * underflow), do the desired operation and then "scale" the result back
 * by the same amount.  If we do the scaling by shifting we avoid the
 * costly mpy and the dastardly div instructions.

 * Suppose, for example, we want to convert from seconds to jiffies
 * where jiffies is defined in nanoseconds as NSEC_PER_JIFFIE.  The
 * simple math is: jiff = (sec * NSEC_PER_SEC) / NSEC_PER_JIFFIE; We
 * observe that (NSEC_PER_SEC / NSEC_PER_JIFFIE) is a constant which we
 * might calculate at compile time, however, the result will only have
 * about 3-4 bits of precision (less for smaller values of HZ).
 *
 * So, we scale as follows:
 * jiff = (sec) * (NSEC_PER_SEC / NSEC_PER_JIFFIE);
 * jiff = ((sec) * ((NSEC_PER_SEC * SCALE)/ NSEC_PER_JIFFIE)) / SCALE;
 * Then we make SCALE a power of two so:
 * jiff = ((sec) * ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE)) >> SCALE;
 * Now we define:
 * #define SEC_CONV = ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE))
 * jiff = (sec * SEC_CONV) >> SCALE;
 *
 * Often the math we use will expand beyond 32-bits so we tell C how to
 * do this and pass the 64-bit result of the mpy through the ">> SCALE"
 * which should take the result back to 32-bits.  We want this expansion
 * to capture as much precision as possible.  At the same time we don't
 * want to overflow so we pick the SCALE to avoid this.  In this file,
 * that means using a different scale for each range of HZ values (as
 * defined in timex.h).
 *
 * For those who want to know, gcc will give a 64-bit result from a "*"
 * operator if the result is a long long AND at least one of the
 * operands is cast to long long (usually just prior to the "*" so as
 * not to confuse it into thinking it really has a 64-bit operand,
 * which, buy the way, it can do, but it takes more code and at least 2
 * mpys).

 * We also need to be aware that one second in nanoseconds is only a
 * couple of bits away from overflowing a 32-bit word, so we MUST use
 * 64-bits to get the full range time in nanoseconds.

 */

/*
 * Here are the scales we will use.  One for seconds, nanoseconds and
 * microseconds.
 *
 * Within the limits of cpp we do a rough cut at the SEC_JIFFIE_SC and
 * check if the sign bit is set.  If not, we bump the shift count by 1.
 * (Gets an extra bit of precision where we can use it.)
 * We know it is set for HZ = 1024 and HZ = 100 not for 1000.
 * Haven't tested others.

 * Limits of cpp (for #if expressions) only long (no long long), but
 * then we only need the most signicant bit.
 */
/*
 * The maximum jiffie value is (MAX_INT >> 1).  Here we translate that
 * into seconds.  The 64-bit case will overflow if we are not careful,
 * so use the messy SH_DIV macro to do it.  Still all constants.
 */
/*
 * Convert various time units to each other:
 */
extern unsigned int Model0_jiffies_to_msecs(const unsigned long Model0_j);
extern unsigned int Model0_jiffies_to_usecs(const unsigned long Model0_j);

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_jiffies_to_nsecs(const unsigned long Model0_j)
{
 return (Model0_u64)Model0_jiffies_to_usecs(Model0_j) * 1000L;
}

extern unsigned long Model0___msecs_to_jiffies(const unsigned int Model0_m);

/*
 * HZ is equal to or smaller than 1000, and 1000 is a nice round
 * multiple of HZ, divide with the factor between them, but round
 * upwards:
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0__msecs_to_jiffies(const unsigned int Model0_m)
{
 return (Model0_m + (1000L / 1000) - 1) / (1000L / 1000);
}
/**
 * msecs_to_jiffies: - convert milliseconds to jiffies
 * @m:	time in milliseconds
 *
 * conversion is done as follows:
 *
 * - negative values mean 'infinite timeout' (MAX_JIFFY_OFFSET)
 *
 * - 'too large' values [that would result in larger than
 *   MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.
 *
 * - all other values are converted to jiffies by either multiplying
 *   the input value by a factor or dividing it with a factor and
 *   handling any 32-bit overflows.
 *   for the details see __msecs_to_jiffies()
 *
 * msecs_to_jiffies() checks for the passed in value being a constant
 * via __builtin_constant_p() allowing gcc to eliminate most of the
 * code, __msecs_to_jiffies() is called if the value passed does not
 * allow constant folding and the actual conversion must be done at
 * runtime.
 * the HZ range specific helpers _msecs_to_jiffies() are called both
 * directly here and from __msecs_to_jiffies() in the case where
 * constant folding is not possible.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model0_msecs_to_jiffies(const unsigned int Model0_m)
{
 if (__builtin_constant_p(Model0_m)) {
  if ((int)Model0_m < 0)
   return ((((long)(~0UL>>1)) >> 1)-1);
  return Model0__msecs_to_jiffies(Model0_m);
 } else {
  return Model0___msecs_to_jiffies(Model0_m);
 }
}

extern unsigned long Model0___usecs_to_jiffies(const unsigned int Model0_u);

static inline __attribute__((no_instrument_function)) unsigned long Model0__usecs_to_jiffies(const unsigned int Model0_u)
{
 return (Model0_u + (1000000L / 1000) - 1) / (1000000L / 1000);
}
/**
 * usecs_to_jiffies: - convert microseconds to jiffies
 * @u:	time in microseconds
 *
 * conversion is done as follows:
 *
 * - 'too large' values [that would result in larger than
 *   MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.
 *
 * - all other values are converted to jiffies by either multiplying
 *   the input value by a factor or dividing it with a factor and
 *   handling any 32-bit overflows as for msecs_to_jiffies.
 *
 * usecs_to_jiffies() checks for the passed in value being a constant
 * via __builtin_constant_p() allowing gcc to eliminate most of the
 * code, __usecs_to_jiffies() is called if the value passed does not
 * allow constant folding and the actual conversion must be done at
 * runtime.
 * the HZ range specific helpers _usecs_to_jiffies() are called both
 * directly here and from __msecs_to_jiffies() in the case where
 * constant folding is not possible.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model0_usecs_to_jiffies(const unsigned int Model0_u)
{
 if (__builtin_constant_p(Model0_u)) {
  if (Model0_u > Model0_jiffies_to_usecs(((((long)(~0UL>>1)) >> 1)-1)))
   return ((((long)(~0UL>>1)) >> 1)-1);
  return Model0__usecs_to_jiffies(Model0_u);
 } else {
  return Model0___usecs_to_jiffies(Model0_u);
 }
}

extern unsigned long Model0_timespec64_to_jiffies(const struct Model0_timespec *Model0_value);
extern void Model0_jiffies_to_timespec64(const unsigned long Model0_jiffies,
      struct Model0_timespec *Model0_value);
static inline __attribute__((no_instrument_function)) unsigned long Model0_timespec_to_jiffies(const struct Model0_timespec *Model0_value)
{
 struct Model0_timespec Model0_ts = Model0_timespec_to_timespec64(*Model0_value);

 return Model0_timespec64_to_jiffies(&Model0_ts);
}

static inline __attribute__((no_instrument_function)) void Model0_jiffies_to_timespec(const unsigned long Model0_jiffies,
           struct Model0_timespec *Model0_value)
{
 struct Model0_timespec Model0_ts;

 Model0_jiffies_to_timespec64(Model0_jiffies, &Model0_ts);
 *Model0_value = Model0_timespec64_to_timespec(Model0_ts);
}

extern unsigned long Model0_timeval_to_jiffies(const struct Model0_timeval *Model0_value);
extern void Model0_jiffies_to_timeval(const unsigned long Model0_jiffies,
          struct Model0_timeval *Model0_value);

extern Model0_clock_t Model0_jiffies_to_clock_t(unsigned long Model0_x);
static inline __attribute__((no_instrument_function)) Model0_clock_t Model0_jiffies_delta_to_clock_t(long Model0_delta)
{
 return Model0_jiffies_to_clock_t(({ typeof(0L) Model0__max1 = (0L); typeof(Model0_delta) Model0__max2 = (Model0_delta); (void) (&Model0__max1 == &Model0__max2); Model0__max1 > Model0__max2 ? Model0__max1 : Model0__max2; }));
}

extern unsigned long Model0_clock_t_to_jiffies(unsigned long Model0_x);
extern Model0_u64 Model0_jiffies_64_to_clock_t(Model0_u64 Model0_x);
extern Model0_u64 Model0_nsec_to_clock_t(Model0_u64 Model0_x);
extern Model0_u64 Model0_nsecs_to_jiffies64(Model0_u64 Model0_n);
extern unsigned long Model0_nsecs_to_jiffies(Model0_u64 Model0_n);

/*
 * ktime_t:
 *
 * A single 64-bit variable is used to store the hrtimers
 * internal representation of time values in scalar nanoseconds. The
 * design plays out best on 64-bit CPUs, where most conversions are
 * NOPs and most arithmetic ktime_t operations are plain arithmetic
 * operations.
 *
 */
union Model0_ktime {
 Model0_s64 Model0_tv64;
};

typedef union Model0_ktime Model0_ktime_t; /* Kill this */

/**
 * ktime_set - Set a ktime_t variable from a seconds/nanoseconds value
 * @secs:	seconds to set
 * @nsecs:	nanoseconds to set
 *
 * Return: The ktime_t representation of the value.
 */
static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ktime_set(const Model0_s64 Model0_secs, const unsigned long Model0_nsecs)
{
 if (__builtin_expect(!!(Model0_secs >= (((Model0_s64)~((Model0_u64)1 << 63)) / 1000000000L)), 0))
  return (Model0_ktime_t){ .Model0_tv64 = ((Model0_s64)~((Model0_u64)1 << 63)) };

 return (Model0_ktime_t) { .Model0_tv64 = Model0_secs * 1000000000L + (Model0_s64)Model0_nsecs };
}

/* Subtract two ktime_t variables. rem = lhs -rhs: */



/* Add two ktime_t variables. res = lhs + rhs: */



/*
 * Add a ktime_t variable and a scalar nanosecond value.
 * res = kt + nsval:
 */



/*
 * Subtract a scalar nanosecod from a ktime_t variable
 * res = kt - nsval:
 */



/* convert a timespec to ktime_t format: */
static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_timespec_to_ktime(struct Model0_timespec Model0_ts)
{
 return Model0_ktime_set(Model0_ts.Model0_tv_sec, Model0_ts.Model0_tv_nsec);
}

/* convert a timespec64 to ktime_t format: */
static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_timespec64_to_ktime(struct Model0_timespec Model0_ts)
{
 return Model0_ktime_set(Model0_ts.Model0_tv_sec, Model0_ts.Model0_tv_nsec);
}

/* convert a timeval to ktime_t format: */
static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_timeval_to_ktime(struct Model0_timeval Model0_tv)
{
 return Model0_ktime_set(Model0_tv.Model0_tv_sec, Model0_tv.Model0_tv_usec * 1000L);
}

/* Map the ktime_t to timespec conversion to ns_to_timespec function */


/* Map the ktime_t to timespec conversion to ns_to_timespec function */


/* Map the ktime_t to timeval conversion to ns_to_timeval function */


/* Convert ktime_t to nanoseconds - NOP in the scalar storage format: */



/**
 * ktime_equal - Compares two ktime_t variables to see if they are equal
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Compare two ktime_t variables.
 *
 * Return: 1 if equal.
 */
static inline __attribute__((no_instrument_function)) int Model0_ktime_equal(const Model0_ktime_t Model0_cmp1, const Model0_ktime_t Model0_cmp2)
{
 return Model0_cmp1.Model0_tv64 == Model0_cmp2.Model0_tv64;
}

/**
 * ktime_compare - Compares two ktime_t variables for less, greater or equal
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: ...
 *   cmp1  < cmp2: return <0
 *   cmp1 == cmp2: return 0
 *   cmp1  > cmp2: return >0
 */
static inline __attribute__((no_instrument_function)) int Model0_ktime_compare(const Model0_ktime_t Model0_cmp1, const Model0_ktime_t Model0_cmp2)
{
 if (Model0_cmp1.Model0_tv64 < Model0_cmp2.Model0_tv64)
  return -1;
 if (Model0_cmp1.Model0_tv64 > Model0_cmp2.Model0_tv64)
  return 1;
 return 0;
}

/**
 * ktime_after - Compare if a ktime_t value is bigger than another one.
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: true if cmp1 happened after cmp2.
 */
static inline __attribute__((no_instrument_function)) bool Model0_ktime_after(const Model0_ktime_t Model0_cmp1, const Model0_ktime_t Model0_cmp2)
{
 return Model0_ktime_compare(Model0_cmp1, Model0_cmp2) > 0;
}

/**
 * ktime_before - Compare if a ktime_t value is smaller than another one.
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: true if cmp1 happened before cmp2.
 */
static inline __attribute__((no_instrument_function)) bool Model0_ktime_before(const Model0_ktime_t Model0_cmp1, const Model0_ktime_t Model0_cmp2)
{
 return Model0_ktime_compare(Model0_cmp1, Model0_cmp2) < 0;
}
static inline __attribute__((no_instrument_function)) Model0_s64 Model0_ktime_divns(const Model0_ktime_t Model0_kt, Model0_s64 Model0_div)
{
 /*
	 * 32-bit implementation cannot handle negative divisors,
	 * so catch them on 64bit as well.
	 */
 ({ int Model0___ret_warn_on = !!(Model0_div < 0); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/ktime.h", 194); __builtin_expect(!!(Model0___ret_warn_on), 0); });
 return Model0_kt.Model0_tv64 / Model0_div;
}


static inline __attribute__((no_instrument_function)) Model0_s64 Model0_ktime_to_us(const Model0_ktime_t Model0_kt)
{
 return Model0_ktime_divns(Model0_kt, 1000L);
}

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_ktime_to_ms(const Model0_ktime_t Model0_kt)
{
 return Model0_ktime_divns(Model0_kt, 1000000L);
}

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_ktime_us_delta(const Model0_ktime_t Model0_later, const Model0_ktime_t Model0_earlier)
{
       return Model0_ktime_to_us(({ (Model0_ktime_t){ .Model0_tv64 = (Model0_later).Model0_tv64 - (Model0_earlier).Model0_tv64 }; }));
}

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_ktime_ms_delta(const Model0_ktime_t Model0_later, const Model0_ktime_t Model0_earlier)
{
 return Model0_ktime_to_ms(({ (Model0_ktime_t){ .Model0_tv64 = (Model0_later).Model0_tv64 - (Model0_earlier).Model0_tv64 }; }));
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ktime_add_us(const Model0_ktime_t Model0_kt, const Model0_u64 Model0_usec)
{
 return ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_kt).Model0_tv64 + (Model0_usec * 1000L) }; });
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ktime_add_ms(const Model0_ktime_t Model0_kt, const Model0_u64 Model0_msec)
{
 return ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_kt).Model0_tv64 + (Model0_msec * 1000000L) }; });
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ktime_sub_us(const Model0_ktime_t Model0_kt, const Model0_u64 Model0_usec)
{
 return ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_kt).Model0_tv64 - (Model0_usec * 1000L) }; });
}

extern Model0_ktime_t Model0_ktime_add_safe(const Model0_ktime_t Model0_lhs, const Model0_ktime_t Model0_rhs);

/**
 * ktime_to_timespec_cond - convert a ktime_t variable to timespec
 *			    format only if the variable contains data
 * @kt:		the ktime_t variable to convert
 * @ts:		the timespec variable to store the result in
 *
 * Return: %true if there was a successful conversion, %false if kt was 0.
 */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result)) bool Model0_ktime_to_timespec_cond(const Model0_ktime_t Model0_kt,
             struct Model0_timespec *Model0_ts)
{
 if (Model0_kt.Model0_tv64) {
  *Model0_ts = Model0_ns_to_timespec((Model0_kt).Model0_tv64);
  return true;
 } else {
  return false;
 }
}

/**
 * ktime_to_timespec64_cond - convert a ktime_t variable to timespec64
 *			    format only if the variable contains data
 * @kt:		the ktime_t variable to convert
 * @ts:		the timespec variable to store the result in
 *
 * Return: %true if there was a successful conversion, %false if kt was 0.
 */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result)) bool Model0_ktime_to_timespec64_cond(const Model0_ktime_t Model0_kt,
             struct Model0_timespec *Model0_ts)
{
 if (Model0_kt.Model0_tv64) {
  *Model0_ts = Model0_ns_to_timespec((Model0_kt).Model0_tv64);
  return true;
 } else {
  return false;
 }
}

/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */



static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ns_to_ktime(Model0_u64 Model0_ns)
{
 static const Model0_ktime_t Model0_ktime_zero = { .Model0_tv64 = 0 };

 return ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_ktime_zero).Model0_tv64 + (Model0_ns) }; });
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ms_to_ktime(Model0_u64 Model0_ms)
{
 static const Model0_ktime_t Model0_ktime_zero = { .Model0_tv64 = 0 };

 return Model0_ktime_add_ms(Model0_ktime_zero, Model0_ms);
}







/* Included from linux/ktime.h */

void Model0_timekeeping_init(void);
extern int Model0_timekeeping_suspended;

/*
 * Get and set timeofday
 */
extern void Model0_do_gettimeofday(struct Model0_timeval *Model0_tv);
extern int Model0_do_settimeofday64(const struct Model0_timespec *Model0_ts);
extern int Model0_do_sys_settimeofday64(const struct Model0_timespec *Model0_tv,
     const struct Model0_timezone *Model0_tz);
static inline __attribute__((no_instrument_function)) int Model0_do_sys_settimeofday(const struct Model0_timespec *Model0_tv,
          const struct Model0_timezone *Model0_tz)
{
 struct Model0_timespec Model0_ts64;

 if (!Model0_tv)
  return Model0_do_sys_settimeofday64(((void *)0), Model0_tz);

 if (!Model0_timespec_valid(Model0_tv))
  return -22;

 Model0_ts64 = Model0_timespec_to_timespec64(*Model0_tv);
 return Model0_do_sys_settimeofday64(&Model0_ts64, Model0_tz);
}

/*
 * Kernel time accessors
 */
unsigned long Model0_get_seconds(void);
struct Model0_timespec Model0_current_kernel_time64(void);
/* does not take xtime_lock */
struct Model0_timespec Model0___current_kernel_time(void);

static inline __attribute__((no_instrument_function)) struct Model0_timespec Model0_current_kernel_time(void)
{
 struct Model0_timespec Model0_now = Model0_current_kernel_time64();

 return Model0_timespec64_to_timespec(Model0_now);
}

/*
 * timespec based interfaces
 */
struct Model0_timespec Model0_get_monotonic_coarse64(void);
extern void Model0_getrawmonotonic64(struct Model0_timespec *Model0_ts);
extern void Model0_ktime_get_ts64(struct Model0_timespec *Model0_ts);
extern Model0_time64_t Model0_ktime_get_seconds(void);
extern Model0_time64_t Model0_ktime_get_real_seconds(void);

extern int Model0___getnstimeofday64(struct Model0_timespec *Model0_tv);
extern void Model0_getnstimeofday64(struct Model0_timespec *Model0_tv);
extern void Model0_getboottime64(struct Model0_timespec *Model0_ts);


/**
 * Deprecated. Use do_settimeofday64().
 */
static inline __attribute__((no_instrument_function)) int Model0_do_settimeofday(const struct Model0_timespec *Model0_ts)
{
 return Model0_do_settimeofday64(Model0_ts);
}

static inline __attribute__((no_instrument_function)) int Model0___getnstimeofday(struct Model0_timespec *Model0_ts)
{
 return Model0___getnstimeofday64(Model0_ts);
}

static inline __attribute__((no_instrument_function)) void Model0_getnstimeofday(struct Model0_timespec *Model0_ts)
{
 Model0_getnstimeofday64(Model0_ts);
}

static inline __attribute__((no_instrument_function)) void Model0_ktime_get_ts(struct Model0_timespec *Model0_ts)
{
 Model0_ktime_get_ts64(Model0_ts);
}

static inline __attribute__((no_instrument_function)) void Model0_ktime_get_real_ts(struct Model0_timespec *Model0_ts)
{
 Model0_getnstimeofday64(Model0_ts);
}

static inline __attribute__((no_instrument_function)) void Model0_getrawmonotonic(struct Model0_timespec *Model0_ts)
{
 Model0_getrawmonotonic64(Model0_ts);
}

static inline __attribute__((no_instrument_function)) struct Model0_timespec Model0_get_monotonic_coarse(void)
{
 return Model0_get_monotonic_coarse64();
}

static inline __attribute__((no_instrument_function)) void Model0_getboottime(struct Model0_timespec *Model0_ts)
{
 return Model0_getboottime64(Model0_ts);
}
/*
 * ktime_t based interfaces
 */

enum Model0_tk_offsets {
 Model0_TK_OFFS_REAL,
 Model0_TK_OFFS_BOOT,
 Model0_TK_OFFS_TAI,
 Model0_TK_OFFS_MAX,
};

extern Model0_ktime_t Model0_ktime_get(void);
extern Model0_ktime_t Model0_ktime_get_with_offset(enum Model0_tk_offsets Model0_offs);
extern Model0_ktime_t Model0_ktime_mono_to_any(Model0_ktime_t Model0_tmono, enum Model0_tk_offsets Model0_offs);
extern Model0_ktime_t Model0_ktime_get_raw(void);
extern Model0_u32 Model0_ktime_get_resolution_ns(void);

/**
 * ktime_get_real - get the real (wall-) time in ktime_t format
 */
static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ktime_get_real(void)
{
 return Model0_ktime_get_with_offset(Model0_TK_OFFS_REAL);
}

/**
 * ktime_get_boottime - Returns monotonic time since boot in ktime_t format
 *
 * This is similar to CLOCK_MONTONIC/ktime_get, but also includes the
 * time spent in suspend.
 */
static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ktime_get_boottime(void)
{
 return Model0_ktime_get_with_offset(Model0_TK_OFFS_BOOT);
}

/**
 * ktime_get_clocktai - Returns the TAI time of day in ktime_t format
 */
static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ktime_get_clocktai(void)
{
 return Model0_ktime_get_with_offset(Model0_TK_OFFS_TAI);
}

/**
 * ktime_mono_to_real - Convert monotonic time to clock realtime
 */
static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_ktime_mono_to_real(Model0_ktime_t Model0_mono)
{
 return Model0_ktime_mono_to_any(Model0_mono, Model0_TK_OFFS_REAL);
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_ktime_get_ns(void)
{
 return ((Model0_ktime_get()).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_ktime_get_real_ns(void)
{
 return ((Model0_ktime_get_real()).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_ktime_get_boot_ns(void)
{
 return ((Model0_ktime_get_boottime()).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_ktime_get_tai_ns(void)
{
 return ((Model0_ktime_get_clocktai()).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_ktime_get_raw_ns(void)
{
 return ((Model0_ktime_get_raw()).Model0_tv64);
}

extern Model0_u64 Model0_ktime_get_mono_fast_ns(void);
extern Model0_u64 Model0_ktime_get_raw_fast_ns(void);

/*
 * Timespec interfaces utilizing the ktime based ones
 */
static inline __attribute__((no_instrument_function)) void Model0_get_monotonic_boottime(struct Model0_timespec *Model0_ts)
{
 *Model0_ts = Model0_ns_to_timespec((Model0_ktime_get_boottime()).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) void Model0_get_monotonic_boottime64(struct Model0_timespec *Model0_ts)
{
 *Model0_ts = Model0_ns_to_timespec((Model0_ktime_get_boottime()).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) void Model0_timekeeping_clocktai(struct Model0_timespec *Model0_ts)
{
 *Model0_ts = Model0_ns_to_timespec((Model0_ktime_get_clocktai()).Model0_tv64);
}

/*
 * RTC specific
 */
extern bool Model0_timekeeping_rtc_skipsuspend(void);
extern bool Model0_timekeeping_rtc_skipresume(void);

extern void Model0_timekeeping_inject_sleeptime64(struct Model0_timespec *Model0_delta);

/*
 * PPS accessor
 */
extern void Model0_ktime_get_raw_and_real_ts64(struct Model0_timespec *Model0_ts_raw,
            struct Model0_timespec *Model0_ts_real);

/*
 * struct system_time_snapshot - simultaneous raw/real time capture with
 *	counter value
 * @cycles:	Clocksource counter value to produce the system times
 * @real:	Realtime system time
 * @raw:	Monotonic raw system time
 * @clock_was_set_seq:	The sequence number of clock was set events
 * @cs_was_changed_seq:	The sequence number of clocksource change events
 */
struct Model0_system_time_snapshot {
 Model0_cycle_t Model0_cycles;
 Model0_ktime_t Model0_real;
 Model0_ktime_t Model0_raw;
 unsigned int Model0_clock_was_set_seq;
 Model0_u8 Model0_cs_was_changed_seq;
};

/*
 * struct system_device_crosststamp - system/device cross-timestamp
 *	(syncronized capture)
 * @device:		Device time
 * @sys_realtime:	Realtime simultaneous with device time
 * @sys_monoraw:	Monotonic raw simultaneous with device time
 */
struct Model0_system_device_crosststamp {
 Model0_ktime_t Model0_device;
 Model0_ktime_t Model0_sys_realtime;
 Model0_ktime_t Model0_sys_monoraw;
};

/*
 * struct system_counterval_t - system counter value with the pointer to the
 *	corresponding clocksource
 * @cycles:	System counter value
 * @cs:		Clocksource corresponding to system counter value. Used by
 *	timekeeping code to verify comparibility of two cycle values
 */
struct Model0_system_counterval_t {
 Model0_cycle_t Model0_cycles;
 struct Model0_clocksource *Model0_cs;
};

/*
 * Get cross timestamp between system clock and device clock
 */
extern int Model0_get_device_system_crosststamp(
   int (*Model0_get_time_fn)(Model0_ktime_t *Model0_device_time,
    struct Model0_system_counterval_t *Model0_system_counterval,
    void *Model0_ctx),
   void *Model0_ctx,
   struct Model0_system_time_snapshot *Model0_history,
   struct Model0_system_device_crosststamp *Model0_xtstamp);

/*
 * Simultaneously snapshot realtime and monotonic raw clocks
 */
extern void Model0_ktime_get_snapshot(struct Model0_system_time_snapshot *Model0_systime_snapshot);

/*
 * Persistent clock related interfaces
 */
extern int Model0_persistent_clock_is_local;

extern void Model0_read_persistent_clock(struct Model0_timespec *Model0_ts);
extern void Model0_read_persistent_clock64(struct Model0_timespec *Model0_ts);
extern void Model0_read_boot_clock64(struct Model0_timespec *Model0_ts);
extern int Model0_update_persistent_clock(struct Model0_timespec Model0_now);
extern int Model0_update_persistent_clock64(struct Model0_timespec Model0_now);





extern int Model0_rcu_expedited; /* for sysctl */
extern int Model0_rcu_normal; /* also for sysctl */
bool Model0_rcu_gp_is_normal(void); /* Internal RCU use. */
bool Model0_rcu_gp_is_expedited(void); /* Internal RCU use. */
void Model0_rcu_expedite_gp(void);
void Model0_rcu_unexpedite_gp(void);


enum Model0_rcutorture_type {
 Model0_RCU_FLAVOR,
 Model0_RCU_BH_FLAVOR,
 Model0_RCU_SCHED_FLAVOR,
 Model0_RCU_TASKS_FLAVOR,
 Model0_SRCU_FLAVOR,
 Model0_INVALID_RCU_FLAVOR
};


void Model0_rcutorture_get_gp_data(enum Model0_rcutorture_type Model0_test_type, int *Model0_flags,
       unsigned long *Model0_gpnum, unsigned long *Model0_completed);
void Model0_rcutorture_record_test_transition(void);
void Model0_rcutorture_record_progress(unsigned long Model0_vernum);
void Model0_do_trace_rcu_torture_read(const char *Model0_rcutorturename,
          struct Model0_callback_head *Model0_rhp,
          unsigned long Model0_secs,
          unsigned long Model0_c_old,
          unsigned long Model0_c);
/* Exported common interfaces */
/* In classic RCU, call_rcu() is just call_rcu_sched(). */




/**
 * call_rcu_bh() - Queue an RCU for invocation after a quicker grace period.
 * @head: structure to be used for queueing the RCU updates.
 * @func: actual callback function to be invoked after the grace period
 *
 * The callback function will be invoked some time after a full grace
 * period elapses, in other words after all currently executing RCU
 * read-side critical sections have completed. call_rcu_bh() assumes
 * that the read-side critical sections end on completion of a softirq
 * handler. This means that read-side critical sections in process
 * context must not be interrupted by softirqs. This interface is to be
 * used when most of the read-side critical sections are in softirq context.
 * RCU read-side critical sections are delimited by :
 *  - rcu_read_lock() and  rcu_read_unlock(), if in interrupt context.
 *  OR
 *  - rcu_read_lock_bh() and rcu_read_unlock_bh(), if in process context.
 *  These may be nested.
 *
 * See the description of call_rcu() for more detailed information on
 * memory ordering guarantees.
 */
void Model0_call_rcu_bh(struct Model0_callback_head *Model0_head,
   Model0_rcu_callback_t func);

/**
 * call_rcu_sched() - Queue an RCU for invocation after sched grace period.
 * @head: structure to be used for queueing the RCU updates.
 * @func: actual callback function to be invoked after the grace period
 *
 * The callback function will be invoked some time after a full grace
 * period elapses, in other words after all currently executing RCU
 * read-side critical sections have completed. call_rcu_sched() assumes
 * that the read-side critical sections end on enabling of preemption
 * or on voluntary preemption.
 * RCU read-side critical sections are delimited by :
 *  - rcu_read_lock_sched() and  rcu_read_unlock_sched(),
 *  OR
 *  anything that disables preemption.
 *  These may be nested.
 *
 * See the description of call_rcu() for more detailed information on
 * memory ordering guarantees.
 */
void Model0_call_rcu_sched(struct Model0_callback_head *Model0_head,
      Model0_rcu_callback_t func);

void Model0_synchronize_sched(void);

/*
 * Structure allowing asynchronous waiting on RCU.
 */
struct Model0_rcu_synchronize {
 struct Model0_callback_head Model0_head;
 struct Model0_completion Model0_completion;
};
void Model0_wakeme_after_rcu(struct Model0_callback_head *Model0_head);

void Model0___wait_rcu_gp(bool Model0_checktiny, int Model0_n, Model0_call_rcu_func_t *Model0_crcu_array,
     struct Model0_rcu_synchronize *Model0_rs_array);
/**
 * synchronize_rcu_mult - Wait concurrently for multiple grace periods
 * @...: List of call_rcu() functions for the flavors to wait on.
 *
 * This macro waits concurrently for multiple flavors of RCU grace periods.
 * For example, synchronize_rcu_mult(call_rcu, call_rcu_bh) would wait
 * on concurrent RCU and RCU-bh grace periods.  Waiting on a give SRCU
 * domain requires you to write a wrapper function for that SRCU domain's
 * call_srcu() function, supplying the corresponding srcu_struct.
 *
 * If Tiny RCU, tell _wait_rcu_gp() not to bother waiting for RCU
 * or RCU-bh, given that anywhere synchronize_rcu_mult() can be called
 * is automatically a grace period.
 */



/**
 * call_rcu_tasks() - Queue an RCU for invocation task-based grace period
 * @head: structure to be used for queueing the RCU updates.
 * @func: actual callback function to be invoked after the grace period
 *
 * The callback function will be invoked some time after a full grace
 * period elapses, in other words after all currently executing RCU
 * read-side critical sections have completed. call_rcu_tasks() assumes
 * that the read-side critical sections end at a voluntary context
 * switch (not a preemption!), entry into idle, or transition to usermode
 * execution.  As such, there are no read-side primitives analogous to
 * rcu_read_lock() and rcu_read_unlock() because this primitive is intended
 * to determine that all tasks have passed through a safe state, not so
 * much for data-strcuture synchronization.
 *
 * See the description of call_rcu() for more detailed information on
 * memory ordering guarantees.
 */
void Model0_call_rcu_tasks(struct Model0_callback_head *Model0_head, Model0_rcu_callback_t func);
void Model0_synchronize_rcu_tasks(void);
void Model0_rcu_barrier_tasks(void);
static inline __attribute__((no_instrument_function)) void Model0___rcu_read_lock(void)
{
 if (0)
  __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0___rcu_read_unlock(void)
{
 if (0)
  __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0_synchronize_rcu(void)
{
 Model0_synchronize_sched();
}

static inline __attribute__((no_instrument_function)) int Model0_rcu_preempt_depth(void)
{
 return 0;
}



/* Internal to kernel */
void Model0_rcu_init(void);
void Model0_rcu_sched_qs(void);
void Model0_rcu_bh_qs(void);
void Model0_rcu_check_callbacks(int Model0_user);
void Model0_rcu_report_dead(unsigned int Model0_cpu);


void Model0_rcu_end_inkernel_boot(void);





void Model0_rcu_sysrq_start(void);
void Model0_rcu_sysrq_end(void);
static inline __attribute__((no_instrument_function)) void Model0_rcu_user_enter(void) { }
static inline __attribute__((no_instrument_function)) void Model0_rcu_user_exit(void) { }





static inline __attribute__((no_instrument_function)) void Model0_rcu_init_nohz(void)
{
}


/**
 * RCU_NONIDLE - Indicate idle-loop code that needs RCU readers
 * @a: Code that RCU needs to pay attention to.
 *
 * RCU, RCU-bh, and RCU-sched read-side critical sections are forbidden
 * in the inner idle loop, that is, between the rcu_idle_enter() and
 * the rcu_idle_exit() -- RCU will happily ignore any such read-side
 * critical sections.  However, things like powertop need tracepoints
 * in the inner idle loop.
 *
 * This macro provides the way out:  RCU_NONIDLE(do_something_with_RCU())
 * will tell RCU that it needs to pay attention, invoke its argument
 * (in this example, calling the do_something_with_RCU() function),
 * and then tell RCU to go back to ignoring this CPU.  It is permissible
 * to nest RCU_NONIDLE() wrappers, but not indefinitely (but the limit is
 * on the order of a million or so, even on 32-bit systems).  It is
 * not legal to block within RCU_NONIDLE(), nor is it permissible to
 * transfer control either into or out of RCU_NONIDLE()'s statement.
 */







/*
 * Note a voluntary context switch for RCU-tasks benefit.  This is a
 * macro rather than an inline function to avoid #include hell.
 */
/**
 * cond_resched_rcu_qs - Report potential quiescent states to RCU
 *
 * This macro resembles cond_resched(), except that it is defined to
 * report potential quiescent states to RCU-tasks even if the cond_resched()
 * machinery were to be shut off, as some advocate for PREEMPT kernels.
 */







bool Model0___rcu_is_watching(void);


/*
 * Infrastructure to implement the synchronize_() primitives in
 * TREE_RCU and rcu_barrier_() primitives in TINY_RCU.
 */



/*
 * Read-Copy Update mechanism for mutual exclusion (tree-based version)
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 * Copyright IBM Corporation, 2008
 *
 * Author: Dipankar Sarma <dipankar@in.ibm.com>
 *	   Paul E. McKenney <paulmck@linux.vnet.ibm.com> Hierarchical algorithm
 *
 * Based on the original work by Paul McKenney <paulmck@us.ibm.com>
 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 *	Documentation/RCU
 */




void Model0_rcu_note_context_switch(void);
int Model0_rcu_needs_cpu(Model0_u64 Model0_basem, Model0_u64 *Model0_nextevt);
void Model0_rcu_cpu_stall_reset(void);

/*
 * Note a virtualization-based context switch.  This is simply a
 * wrapper around rcu_note_context_switch(), which allows TINY_RCU
 * to save a few bytes. The caller must have disabled interrupts.
 */
static inline __attribute__((no_instrument_function)) void Model0_rcu_virt_note_context_switch(int Model0_cpu)
{
 Model0_rcu_note_context_switch();
}

void Model0_synchronize_rcu_bh(void);
void Model0_synchronize_sched_expedited(void);
void Model0_synchronize_rcu_expedited(void);

void Model0_kfree_call_rcu(struct Model0_callback_head *Model0_head, Model0_rcu_callback_t func);

/**
 * synchronize_rcu_bh_expedited - Brute-force RCU-bh grace period
 *
 * Wait for an RCU-bh grace period to elapse, but use a "big hammer"
 * approach to force the grace period to end quickly.  This consumes
 * significant time on all CPUs and is unfriendly to real-time workloads,
 * so is thus not recommended for any sort of common-case code.  In fact,
 * if you are using synchronize_rcu_bh_expedited() in a loop, please
 * restructure your code to batch your updates, and then use a single
 * synchronize_rcu_bh() instead.
 *
 * Note that it is illegal to call this function while holding any lock
 * that is acquired by a CPU-hotplug notifier.  And yes, it is also illegal
 * to call this function from a CPU-hotplug notifier.  Failing to observe
 * these restriction will result in deadlock.
 */
static inline __attribute__((no_instrument_function)) void Model0_synchronize_rcu_bh_expedited(void)
{
 Model0_synchronize_sched_expedited();
}

void Model0_rcu_barrier(void);
void Model0_rcu_barrier_bh(void);
void Model0_rcu_barrier_sched(void);
unsigned long Model0_get_state_synchronize_rcu(void);
void Model0_cond_synchronize_rcu(unsigned long Model0_oldstate);
unsigned long Model0_get_state_synchronize_sched(void);
void Model0_cond_synchronize_sched(unsigned long Model0_oldstate);

extern unsigned long Model0_rcutorture_testseq;
extern unsigned long Model0_rcutorture_vernum;
unsigned long Model0_rcu_batches_started(void);
unsigned long Model0_rcu_batches_started_bh(void);
unsigned long Model0_rcu_batches_started_sched(void);
unsigned long Model0_rcu_batches_completed(void);
unsigned long Model0_rcu_batches_completed_bh(void);
unsigned long Model0_rcu_batches_completed_sched(void);
unsigned long Model0_rcu_exp_batches_completed(void);
unsigned long Model0_rcu_exp_batches_completed_sched(void);
void Model0_show_rcu_gp_kthreads(void);

void Model0_rcu_force_quiescent_state(void);
void Model0_rcu_bh_force_quiescent_state(void);
void Model0_rcu_sched_force_quiescent_state(void);

void Model0_rcu_idle_enter(void);
void Model0_rcu_idle_exit(void);
void Model0_rcu_irq_enter(void);
void Model0_rcu_irq_exit(void);
void Model0_rcu_irq_enter_irqson(void);
void Model0_rcu_irq_exit_irqson(void);

void Model0_exit_rcu(void);

void Model0_rcu_scheduler_starting(void);
extern int Model0_rcu_scheduler_active __attribute__((__section__(".data..read_mostly")));

bool Model0_rcu_is_watching(void);

void Model0_rcu_all_qs(void);

/* RCUtree hotplug events */
int Model0_rcutree_prepare_cpu(unsigned int Model0_cpu);
int Model0_rcutree_online_cpu(unsigned int Model0_cpu);
int Model0_rcutree_offline_cpu(unsigned int Model0_cpu);
int Model0_rcutree_dead_cpu(unsigned int Model0_cpu);
int Model0_rcutree_dying_cpu(unsigned int Model0_cpu);






/*
 * init_rcu_head_on_stack()/destroy_rcu_head_on_stack() are needed for dynamic
 * initialization and destruction of rcu_head on the stack. rcu_head structures
 * allocated dynamically in the heap or defined statically don't need any
 * initialization.
 */






static inline __attribute__((no_instrument_function)) void Model0_init_rcu_head(struct Model0_callback_head *Model0_head)
{
}

static inline __attribute__((no_instrument_function)) void Model0_destroy_rcu_head(struct Model0_callback_head *Model0_head)
{
}

static inline __attribute__((no_instrument_function)) void Model0_init_rcu_head_on_stack(struct Model0_callback_head *Model0_head)
{
}

static inline __attribute__((no_instrument_function)) void Model0_destroy_rcu_head_on_stack(struct Model0_callback_head *Model0_head)
{
}





static inline __attribute__((no_instrument_function)) bool Model0_rcu_lockdep_current_cpu_online(void)
{
 return true;
}
static inline __attribute__((no_instrument_function)) int Model0_rcu_read_lock_held(void)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) int Model0_rcu_read_lock_bh_held(void)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) int Model0_rcu_read_lock_sched_held(void)
{
 return !0;
}
/*
 * Helper functions for rcu_dereference_check(), rcu_dereference_protected()
 * and rcu_assign_pointer().  Some of these could be folded into their
 * callers, but they are left separate in order to ease introduction of
 * multiple flavors of pointers to match the multiple flavors of RCU
 * (e.g., __rcu_bh, * __rcu_sched, and __srcu), should this make sense in
 * the future.
 */
/**
 * RCU_INITIALIZER() - statically initialize an RCU-protected global variable
 * @v: The value to statically initialize with.
 */


/**
 * rcu_assign_pointer() - assign to RCU-protected pointer
 * @p: pointer to assign to
 * @v: value to assign (publish)
 *
 * Assigns the specified value to the specified RCU-protected
 * pointer, ensuring that any concurrent RCU readers will see
 * any prior initialization.
 *
 * Inserts memory barriers on architectures that require them
 * (which is most of them), and also prevents the compiler from
 * reordering the code that initializes the structure after the pointer
 * assignment.  More importantly, this call documents which pointers
 * will be dereferenced by RCU read-side code.
 *
 * In some special cases, you may use RCU_INIT_POINTER() instead
 * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due
 * to the fact that it does not constrain either the CPU or the compiler.
 * That said, using RCU_INIT_POINTER() when you should have used
 * rcu_assign_pointer() is a very bad thing that results in
 * impossible-to-diagnose memory corruption.  So please be careful.
 * See the RCU_INIT_POINTER() comment header for details.
 *
 * Note that rcu_assign_pointer() evaluates each of its arguments only
 * once, appearances notwithstanding.  One of the "extra" evaluations
 * is in typeof() and the other visible only to sparse (__CHECKER__),
 * neither of which actually execute the argument.  As with most cpp
 * macros, this execute-arguments-only-once property is important, so
 * please be careful when making changes to rcu_assign_pointer() and the
 * other macros that it invokes.
 */
/**
 * rcu_access_pointer() - fetch RCU pointer with no dereferencing
 * @p: The pointer to read
 *
 * Return the value of the specified RCU-protected pointer, but omit the
 * smp_read_barrier_depends() and keep the READ_ONCE().  This is useful
 * when the value of this pointer is accessed, but the pointer is not
 * dereferenced, for example, when testing an RCU-protected pointer against
 * NULL.  Although rcu_access_pointer() may also be used in cases where
 * update-side locks prevent the value of the pointer from changing, you
 * should instead use rcu_dereference_protected() for this use case.
 *
 * It is also permissible to use rcu_access_pointer() when read-side
 * access to the pointer was removed at least one grace period ago, as
 * is the case in the context of the RCU callback that is freeing up
 * the data, or after a synchronize_rcu() returns.  This can be useful
 * when tearing down multi-linked structures after a grace period
 * has elapsed.
 */


/**
 * rcu_dereference_check() - rcu_dereference with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * Do an rcu_dereference(), but check that the conditions under which the
 * dereference will take place are correct.  Typically the conditions
 * indicate the various locking conditions that should be held at that
 * point.  The check should return true if the conditions are satisfied.
 * An implicit check for being in an RCU read-side critical section
 * (rcu_read_lock()) is included.
 *
 * For example:
 *
 *	bar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock));
 *
 * could be used to indicate to lockdep that foo->bar may only be dereferenced
 * if either rcu_read_lock() is held, or that the lock required to replace
 * the bar struct at foo->bar is held.
 *
 * Note that the list of conditions may also include indications of when a lock
 * need not be held, for example during initialisation or destruction of the
 * target struct:
 *
 *	bar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock) ||
 *					      atomic_read(&foo->usage) == 0);
 *
 * Inserts memory barriers on architectures that require them
 * (currently only the Alpha), prevents the compiler from refetching
 * (and from merging fetches), and, more importantly, documents exactly
 * which pointers are protected by RCU and checks that the pointer is
 * annotated as __rcu.
 */



/**
 * rcu_dereference_bh_check() - rcu_dereference_bh with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * This is the RCU-bh counterpart to rcu_dereference_check().
 */



/**
 * rcu_dereference_sched_check() - rcu_dereference_sched with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * This is the RCU-sched counterpart to rcu_dereference_check().
 */




/*
 * The tracing infrastructure traces RCU (we want that), but unfortunately
 * some of the RCU checks causes tracing to lock up the system.
 *
 * The no-tracing version of rcu_dereference_raw() must not call
 * rcu_read_lock_held().
 */


/**
 * rcu_dereference_protected() - fetch RCU pointer when updates prevented
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * Return the value of the specified RCU-protected pointer, but omit
 * both the smp_read_barrier_depends() and the READ_ONCE().  This
 * is useful in cases where update-side locks prevent the value of the
 * pointer from changing.  Please note that this primitive does -not-
 * prevent the compiler from repeating this reference or combining it
 * with other references, so it should not be used without protection
 * of appropriate locks.
 *
 * This function is only for update-side use.  Using this function
 * when protected only by rcu_read_lock() will result in infrequent
 * but very ugly failures.
 */




/**
 * rcu_dereference() - fetch RCU-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * This is a simple wrapper around rcu_dereference_check().
 */


/**
 * rcu_dereference_bh() - fetch an RCU-bh-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * Makes rcu_dereference_check() do the dirty work.
 */


/**
 * rcu_dereference_sched() - fetch RCU-sched-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * Makes rcu_dereference_check() do the dirty work.
 */


/**
 * rcu_pointer_handoff() - Hand off a pointer from RCU to other mechanism
 * @p: The pointer to hand off
 *
 * This is simply an identity function, but it documents where a pointer
 * is handed off from RCU to some other synchronization mechanism, for
 * example, reference counting or locking.  In C11, it would map to
 * kill_dependency().  It could be used as follows:
 *
 *	rcu_read_lock();
 *	p = rcu_dereference(gp);
 *	long_lived = is_long_lived(p);
 *	if (long_lived) {
 *		if (!atomic_inc_not_zero(p->refcnt))
 *			long_lived = false;
 *		else
 *			p = rcu_pointer_handoff(p);
 *	}
 *	rcu_read_unlock();
 */


/**
 * rcu_read_lock() - mark the beginning of an RCU read-side critical section
 *
 * When synchronize_rcu() is invoked on one CPU while other CPUs
 * are within RCU read-side critical sections, then the
 * synchronize_rcu() is guaranteed to block until after all the other
 * CPUs exit their critical sections.  Similarly, if call_rcu() is invoked
 * on one CPU while other CPUs are within RCU read-side critical
 * sections, invocation of the corresponding RCU callback is deferred
 * until after the all the other CPUs exit their critical sections.
 *
 * Note, however, that RCU callbacks are permitted to run concurrently
 * with new RCU read-side critical sections.  One way that this can happen
 * is via the following sequence of events: (1) CPU 0 enters an RCU
 * read-side critical section, (2) CPU 1 invokes call_rcu() to register
 * an RCU callback, (3) CPU 0 exits the RCU read-side critical section,
 * (4) CPU 2 enters a RCU read-side critical section, (5) the RCU
 * callback is invoked.  This is legal, because the RCU read-side critical
 * section that was running concurrently with the call_rcu() (and which
 * therefore might be referencing something that the corresponding RCU
 * callback would free up) has completed before the corresponding
 * RCU callback is invoked.
 *
 * RCU read-side critical sections may be nested.  Any deferred actions
 * will be deferred until the outermost RCU read-side critical section
 * completes.
 *
 * You can avoid reading and understanding the next paragraph by
 * following this rule: don't put anything in an rcu_read_lock() RCU
 * read-side critical section that would block in a !PREEMPT kernel.
 * But if you want the full story, read on!
 *
 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
 * it is illegal to block while in an RCU read-side critical section.
 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
 * kernel builds, RCU read-side critical sections may be preempted,
 * but explicit blocking is illegal.  Finally, in preemptible RCU
 * implementations in real-time (with -rt patchset) kernel builds, RCU
 * read-side critical sections may be preempted and they may also block, but
 * only when acquiring spinlocks that are subject to priority inheritance.
 */
static inline __attribute__((no_instrument_function)) void Model0_rcu_read_lock(void)
{
 Model0___rcu_read_lock();
 (void)0;
 do { } while (0);
 do { } while (0);

}

/*
 * So where is rcu_write_lock()?  It does not exist, as there is no
 * way for writers to lock out RCU readers.  This is a feature, not
 * a bug -- this property is what provides RCU's performance benefits.
 * Of course, writers must coordinate with each other.  The normal
 * spinlock primitives work well for this, but any other technique may be
 * used as well.  RCU does not care how the writers keep out of each
 * others' way, as long as they do so.
 */

/**
 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
 *
 * In most situations, rcu_read_unlock() is immune from deadlock.
 * However, in kernels built with CONFIG_RCU_BOOST, rcu_read_unlock()
 * is responsible for deboosting, which it does via rt_mutex_unlock().
 * Unfortunately, this function acquires the scheduler's runqueue and
 * priority-inheritance spinlocks.  This means that deadlock could result
 * if the caller of rcu_read_unlock() already holds one of these locks or
 * any lock that is ever acquired while holding them; or any lock which
 * can be taken from interrupt context because rcu_boost()->rt_mutex_lock()
 * does not disable irqs while taking ->wait_lock.
 *
 * That said, RCU readers are never priority boosted unless they were
 * preempted.  Therefore, one way to avoid deadlock is to make sure
 * that preemption never happens within any RCU read-side critical
 * section whose outermost rcu_read_unlock() is called with one of
 * rt_mutex_unlock()'s locks held.  Such preemption can be avoided in
 * a number of ways, for example, by invoking preempt_disable() before
 * critical section's outermost rcu_read_lock().
 *
 * Given that the set of locks acquired by rt_mutex_unlock() might change
 * at any time, a somewhat more future-proofed approach is to make sure
 * that that preemption never happens within any RCU read-side critical
 * section whose outermost rcu_read_unlock() is called with irqs disabled.
 * This approach relies on the fact that rt_mutex_unlock() currently only
 * acquires irq-disabled locks.
 *
 * The second of these two approaches is best in most situations,
 * however, the first approach can also be useful, at least to those
 * developers willing to keep abreast of the set of locks acquired by
 * rt_mutex_unlock().
 *
 * See rcu_read_lock() for more information.
 */
static inline __attribute__((no_instrument_function)) void Model0_rcu_read_unlock(void)
{
 do { } while (0);

 (void)0;
 Model0___rcu_read_unlock();
 do { } while (0); /* Keep acq info for rls diags. */
}

/**
 * rcu_read_lock_bh() - mark the beginning of an RCU-bh critical section
 *
 * This is equivalent of rcu_read_lock(), but to be used when updates
 * are being done using call_rcu_bh() or synchronize_rcu_bh(). Since
 * both call_rcu_bh() and synchronize_rcu_bh() consider completion of a
 * softirq handler to be a quiescent state, a process in RCU read-side
 * critical section must be protected by disabling softirqs. Read-side
 * critical sections in interrupt context can use just rcu_read_lock(),
 * though this should at least be commented to avoid confusing people
 * reading the code.
 *
 * Note that rcu_read_lock_bh() and the matching rcu_read_unlock_bh()
 * must occur in the same context, for example, it is illegal to invoke
 * rcu_read_unlock_bh() from one task if the matching rcu_read_lock_bh()
 * was invoked from some other task.
 */
static inline __attribute__((no_instrument_function)) void Model0_rcu_read_lock_bh(void)
{
 Model0_local_bh_disable();
 (void)0;
 do { } while (0);
 do { } while (0);

}

/*
 * rcu_read_unlock_bh - marks the end of a softirq-only RCU critical section
 *
 * See rcu_read_lock_bh() for more information.
 */
static inline __attribute__((no_instrument_function)) void Model0_rcu_read_unlock_bh(void)
{
 do { } while (0);

 do { } while (0);
 (void)0;
 Model0_local_bh_enable();
}

/**
 * rcu_read_lock_sched() - mark the beginning of a RCU-sched critical section
 *
 * This is equivalent of rcu_read_lock(), but to be used when updates
 * are being done using call_rcu_sched() or synchronize_rcu_sched().
 * Read-side critical sections can also be introduced by anything that
 * disables preemption, including local_irq_disable() and friends.
 *
 * Note that rcu_read_lock_sched() and the matching rcu_read_unlock_sched()
 * must occur in the same context, for example, it is illegal to invoke
 * rcu_read_unlock_sched() from process context if the matching
 * rcu_read_lock_sched() was invoked from an NMI handler.
 */
static inline __attribute__((no_instrument_function)) void Model0_rcu_read_lock_sched(void)
{
 __asm__ __volatile__("": : :"memory");
 (void)0;
 do { } while (0);
 do { } while (0);

}

/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */
static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model0_rcu_read_lock_sched_notrace(void)
{
 __asm__ __volatile__("": : :"memory");
 (void)0;
}

/*
 * rcu_read_unlock_sched - marks the end of a RCU-classic critical section
 *
 * See rcu_read_lock_sched for more information.
 */
static inline __attribute__((no_instrument_function)) void Model0_rcu_read_unlock_sched(void)
{
 do { } while (0);

 do { } while (0);
 (void)0;
 __asm__ __volatile__("": : :"memory");
}

/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */
static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model0_rcu_read_unlock_sched_notrace(void)
{
 (void)0;
 __asm__ __volatile__("": : :"memory");
}

/**
 * RCU_INIT_POINTER() - initialize an RCU protected pointer
 *
 * Initialize an RCU-protected pointer in special cases where readers
 * do not need ordering constraints on the CPU or the compiler.  These
 * special cases are:
 *
 * 1.	This use of RCU_INIT_POINTER() is NULLing out the pointer -or-
 * 2.	The caller has taken whatever steps are required to prevent
 *	RCU readers from concurrently accessing this pointer -or-
 * 3.	The referenced data structure has already been exposed to
 *	readers either at compile time or via rcu_assign_pointer() -and-
 *	a.	You have not made -any- reader-visible changes to
 *		this structure since then -or-
 *	b.	It is OK for readers accessing this structure from its
 *		new location to see the old state of the structure.  (For
 *		example, the changes were to statistical counters or to
 *		other state where exact synchronization is not required.)
 *
 * Failure to follow these rules governing use of RCU_INIT_POINTER() will
 * result in impossible-to-diagnose memory corruption.  As in the structures
 * will look OK in crash dumps, but any concurrent RCU readers might
 * see pre-initialized values of the referenced data structure.  So
 * please be very careful how you use RCU_INIT_POINTER()!!!
 *
 * If you are creating an RCU-protected linked structure that is accessed
 * by a single external-to-structure RCU-protected pointer, then you may
 * use RCU_INIT_POINTER() to initialize the internal RCU-protected
 * pointers, but you must use rcu_assign_pointer() to initialize the
 * external-to-structure pointer -after- you have completely initialized
 * the reader-accessible portions of the linked structure.
 *
 * Note that unlike rcu_assign_pointer(), RCU_INIT_POINTER() provides no
 * ordering guarantees for either the CPU or the compiler.
 */






/**
 * RCU_POINTER_INITIALIZER() - statically initialize an RCU protected pointer
 *
 * GCC-style initialization for an RCU-protected pointer in a structure field.
 */



/*
 * Does the specified offset indicate that the corresponding rcu_head
 * structure can be handled by kfree_rcu()?
 */


/*
 * Helper macro for kfree_rcu() to prevent argument-expansion eyestrain.
 */






/**
 * kfree_rcu() - kfree an object after a grace period.
 * @ptr:	pointer to kfree
 * @rcu_head:	the name of the struct rcu_head within the type of @ptr.
 *
 * Many rcu callbacks functions just call kfree() on the base structure.
 * These functions are trivial, but their size adds up, and furthermore
 * when they are used in a kernel module, that module must invoke the
 * high-latency rcu_barrier() function at module-unload time.
 *
 * The kfree_rcu() function handles this issue.  Rather than encoding a
 * function address in the embedded rcu_head structure, kfree_rcu() instead
 * encodes the offset of the rcu_head structure within the base structure.
 * Because the functions are not allowed in the low-order 4096 bytes of
 * kernel virtual memory, offsets up to 4095 bytes can be accommodated.
 * If the offset is larger than 4095 bytes, a compile-time error will
 * be generated in __kfree_rcu().  If this error is triggered, you can
 * either fall back to use of call_rcu() or rearrange the structure to
 * position the rcu_head structure into the first 4096 bytes.
 *
 * Note that the allowable offset might decrease in the future, for example,
 * to allow something like kmem_cache_free_rcu().
 *
 * The BUILD_BUG_ON check must not involve any function calls, hence the
 * checks are done in macros here.
 */
static inline __attribute__((no_instrument_function)) bool Model0_rcu_is_nocb_cpu(int Model0_cpu) { return false; }



/* Only for use by adaptive-ticks code. */





static inline __attribute__((no_instrument_function)) bool Model0_rcu_sys_is_idle(void)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model0_rcu_sysidle_force_exit(void)
{
}




/*
 * Dump the ftrace buffer, but only one time per callsite per boot.
 */
/*
 * workqueue.h --- work queue handling for Linux.
 */





struct Model0_tvec_base;

struct Model0_timer_list {
 /*
	 * All fields that change during normal runtime grouped to the
	 * same cacheline
	 */
 struct Model0_hlist_node Model0_entry;
 unsigned long Model0_expires;
 void (*Model0_function)(unsigned long);
 unsigned long Model0_data;
 Model0_u32 Model0_flags;


 int Model0_start_pid;
 void *Model0_start_site;
 char Model0_start_comm[16];




};
/*
 * A deferrable timer will work normally when the system is busy, but
 * will not cause a CPU to come out of idle just to service it; instead,
 * the timer will be serviced when the CPU eventually wakes up with a
 * subsequent non-deferrable timer.
 *
 * An irqsafe timer is executed with IRQ disabled and it's safe to wait for
 * the completion of the running instance from IRQ handlers, for example,
 * by calling del_timer_sync().
 *
 * Note: The irq disabled callback execution is a special case for
 * workqueue locking issues. It's not meant for executing random crap
 * with interrupts disabled. Abuse is monitored!
 */
void Model0_init_timer_key(struct Model0_timer_list *Model0_timer, unsigned int Model0_flags,
      const char *Model0_name, struct Model0_lock_class_key *Model0_key);







static inline __attribute__((no_instrument_function)) void Model0_destroy_timer_on_stack(struct Model0_timer_list *Model0_timer) { }
static inline __attribute__((no_instrument_function)) void Model0_init_timer_on_stack_key(struct Model0_timer_list *Model0_timer,
        unsigned int Model0_flags, const char *Model0_name,
        struct Model0_lock_class_key *Model0_key)
{
 Model0_init_timer_key(Model0_timer, Model0_flags, Model0_name, Model0_key);
}
/**
 * timer_pending - is a timer pending?
 * @timer: the timer in question
 *
 * timer_pending will tell whether a given timer is currently pending,
 * or not. Callers must ensure serialization wrt. other operations done
 * to this timer, eg. interrupt contexts, or other CPUs on SMP.
 *
 * return value: 1 if the timer is pending, 0 if not.
 */
static inline __attribute__((no_instrument_function)) int Model0_timer_pending(const struct Model0_timer_list * Model0_timer)
{
 return Model0_timer->Model0_entry.Model0_pprev != ((void *)0);
}

extern void Model0_add_timer_on(struct Model0_timer_list *Model0_timer, int Model0_cpu);
extern int Model0_del_timer(struct Model0_timer_list * Model0_timer);
extern int Model0_mod_timer(struct Model0_timer_list *Model0_timer, unsigned long Model0_expires);
extern int Model0_mod_timer_pending(struct Model0_timer_list *Model0_timer, unsigned long Model0_expires);

/*
 * The jiffies value which is added to now, when there is no timer
 * in the timer wheel:
 */


/*
 * Timer-statistics info:
 */


extern int Model0_timer_stats_active;

extern void Model0_init_timer_stats(void);

extern void Model0_timer_stats_update_stats(void *Model0_timer, Model0_pid_t Model0_pid, void *Model0_startf,
         void *Model0_timerf, char *Model0_comm, Model0_u32 Model0_flags);

extern void Model0___timer_stats_timer_set_start_info(struct Model0_timer_list *Model0_timer,
            void *Model0_addr);

static inline __attribute__((no_instrument_function)) void Model0_timer_stats_timer_set_start_info(struct Model0_timer_list *Model0_timer)
{
 if (__builtin_expect(!!(!Model0_timer_stats_active), 1))
  return;
 Model0___timer_stats_timer_set_start_info(Model0_timer, __builtin_return_address(0));
}

static inline __attribute__((no_instrument_function)) void Model0_timer_stats_timer_clear_start_info(struct Model0_timer_list *Model0_timer)
{
 Model0_timer->Model0_start_site = ((void *)0);
}
extern void Model0_add_timer(struct Model0_timer_list *Model0_timer);

extern int Model0_try_to_del_timer_sync(struct Model0_timer_list *Model0_timer);


  extern int Model0_del_timer_sync(struct Model0_timer_list *Model0_timer);






extern void Model0_init_timers(void);
extern void Model0_run_local_timers(void);
struct Model0_hrtimer;
extern enum Model0_hrtimer_restart Model0_it_real_fn(struct Model0_hrtimer *);



/*
 * sysctl.h: General linux system control interface
 *
 * Begun 24 March 1995, Stephen Tweedie
 *
 ****************************************************************
 ****************************************************************
 **
 **  WARNING:
 **  The values in this file are exported to user space via 
 **  the sysctl() binary interface.  Do *NOT* change the
 **  numbering of any existing values here, and do not change
 **  any numbers within any one set of values.  If you have to
 **  redefine an existing interface, use a new number for it.
 **  The kernel will then return -ENOTDIR to any application using
 **  the old binary interface.
 **
 ****************************************************************
 ****************************************************************
 */







/*
  Red Black Trees
  (C) 1999  Andrea Arcangeli <andrea@suse.de>
  
  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 2 of the License, or
  (at your option) any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

  linux/include/linux/rbtree.h

  To use rbtrees you'll have to implement your own insert and search cores.
  This will avoid us to use callbacks and to drop drammatically performances.
  I know it's not the cleaner way,  but in C (not in C++) to get
  performances and genericity...

  See Documentation/rbtree.txt for documentation and samples.
*/
struct Model0_rb_node {
 unsigned long Model0___rb_parent_color;
 struct Model0_rb_node *Model0_rb_right;
 struct Model0_rb_node *Model0_rb_left;
} __attribute__((aligned(sizeof(long))));
    /* The alignment might seem pointless, but allegedly CRIS needs it */

struct Model0_rb_root {
 struct Model0_rb_node *Model0_rb_node;
};
/* 'empty' nodes are nodes that are known not to be inserted in an rbtree */






extern void Model0_rb_insert_color(struct Model0_rb_node *, struct Model0_rb_root *);
extern void Model0_rb_erase(struct Model0_rb_node *, struct Model0_rb_root *);


/* Find logical next and previous nodes in a tree */
extern struct Model0_rb_node *Model0_rb_next(const struct Model0_rb_node *);
extern struct Model0_rb_node *Model0_rb_prev(const struct Model0_rb_node *);
extern struct Model0_rb_node *Model0_rb_first(const struct Model0_rb_root *);
extern struct Model0_rb_node *Model0_rb_last(const struct Model0_rb_root *);

/* Postorder iteration - always visit the parent after its children */
extern struct Model0_rb_node *Model0_rb_first_postorder(const struct Model0_rb_root *);
extern struct Model0_rb_node *Model0_rb_next_postorder(const struct Model0_rb_node *);

/* Fast replacement of a single node without remove/rebalance/add/rebalance */
extern void Model0_rb_replace_node(struct Model0_rb_node *Model0_victim, struct Model0_rb_node *Model0_new,
       struct Model0_rb_root *Model0_root);
extern void Model0_rb_replace_node_rcu(struct Model0_rb_node *Model0_victim, struct Model0_rb_node *Model0_new,
    struct Model0_rb_root *Model0_root);

static inline __attribute__((no_instrument_function)) void Model0_rb_link_node(struct Model0_rb_node *Model0_node, struct Model0_rb_node *Model0_parent,
    struct Model0_rb_node **Model0_rb_link)
{
 Model0_node->Model0___rb_parent_color = (unsigned long)Model0_parent;
 Model0_node->Model0_rb_left = Model0_node->Model0_rb_right = ((void *)0);

 *Model0_rb_link = Model0_node;
}

static inline __attribute__((no_instrument_function)) void Model0_rb_link_node_rcu(struct Model0_rb_node *Model0_node, struct Model0_rb_node *Model0_parent,
        struct Model0_rb_node **Model0_rb_link)
{
 Model0_node->Model0___rb_parent_color = (unsigned long)Model0_parent;
 Model0_node->Model0_rb_left = Model0_node->Model0_rb_right = ((void *)0);

 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_node); if (__builtin_constant_p(Model0_node) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof((*Model0_rb_link)) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof((*Model0_rb_link))) ((typeof(*Model0_rb_link))(Model0__r_a_p__v)) }; Model0___write_once_size(&((*Model0_rb_link)), Model0___u.Model0___c, sizeof((*Model0_rb_link))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&*Model0_rb_link) == sizeof(char) || sizeof(*&*Model0_rb_link) == sizeof(short) || sizeof(*&*Model0_rb_link) == sizeof(int) || sizeof(*&*Model0_rb_link) == sizeof(long))); extern void Model0___compiletime_assert_97(void) ; if (Model0___cond) Model0___compiletime_assert_97(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&*Model0_rb_link) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&*Model0_rb_link)) ((typeof(*((typeof(*Model0_rb_link))Model0__r_a_p__v)) *)((typeof(*Model0_rb_link))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&*Model0_rb_link), Model0___u.Model0___c, sizeof(*&*Model0_rb_link)); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
}






/**
 * rbtree_postorder_for_each_entry_safe - iterate in post-order over rb_root of
 * given type allowing the backing memory of @pos to be invalidated
 *
 * @pos:	the 'type *' to use as a loop cursor.
 * @n:		another 'type *' to use as temporary storage
 * @root:	'rb_root *' of the rbtree.
 * @field:	the name of the rb_node field within 'type'.
 *
 * rbtree_postorder_for_each_entry_safe() provides a similar guarantee as
 * list_for_each_entry_safe() and allows the iteration to continue independent
 * of changes to @pos by the body of the loop.
 *
 * Note, however, that it cannot handle other modifications that re-order the
 * rbtree it is iterating over. This includes calling rb_erase() on @pos, as
 * rb_erase() may rebalance the tree, causing us to miss some nodes.
 */
/*
 * sysctl.h: General linux system control interface
 *
 * Begun 24 March 1995, Stephen Tweedie
 *
 ****************************************************************
 ****************************************************************
 **
 **  WARNING:
 **  The values in this file are exported to user space via 
 **  the sysctl() binary interface.  Do *NOT* change the
 **  numbering of any existing values here, and do not change
 **  any numbers within any one set of values.  If you have to
 **  redefine an existing interface, use a new number for it.
 **  The kernel will then return -ENOTDIR to any application using
 **  the old binary interface.
 **
 ****************************************************************
 ****************************************************************
 */
struct Model0___sysctl_args {
 int *Model0_name;
 int Model0_nlen;
 void *Model0_oldval;
 Model0_size_t *Model0_oldlenp;
 void *Model0_newval;
 Model0_size_t Model0_newlen;
 unsigned long Model0___unused[4];
};

/* Define sysctl names first */

/* Top-level names: */

enum
{
 Model0_CTL_KERN=1, /* General kernel info and control */
 Model0_CTL_VM=2, /* VM management */
 Model0_CTL_NET=3, /* Networking */
 Model0_CTL_PROC=4, /* removal breaks strace(1) compilation */
 Model0_CTL_FS=5, /* Filesystems */
 Model0_CTL_DEBUG=6, /* Debugging */
 Model0_CTL_DEV=7, /* Devices */
 Model0_CTL_BUS=8, /* Busses */
 Model0_CTL_ABI=9, /* Binary emulation */
 Model0_CTL_CPU=10, /* CPU stuff (speed scaling, etc) */
 Model0_CTL_ARLAN=254, /* arlan wireless driver */
 Model0_CTL_S390DBF=5677, /* s390 debug */
 Model0_CTL_SUNRPC=7249, /* sunrpc debug */
 Model0_CTL_PM=9899, /* frv power management */
 Model0_CTL_FRV=9898, /* frv specific sysctls */
};

/* CTL_BUS names: */
enum
{
 Model0_CTL_BUS_ISA=1 /* ISA */
};

/* /proc/sys/fs/inotify/ */
enum
{
 Model0_INOTIFY_MAX_USER_INSTANCES=1, /* max instances per user */
 Model0_INOTIFY_MAX_USER_WATCHES=2, /* max watches per user */
 Model0_INOTIFY_MAX_QUEUED_EVENTS=3 /* max queued events per instance */
};

/* CTL_KERN names: */
enum
{
 Model0_KERN_OSTYPE=1, /* string: system version */
 Model0_KERN_OSRELEASE=2, /* string: system release */
 Model0_KERN_OSREV=3, /* int: system revision */
 Model0_KERN_VERSION=4, /* string: compile time info */
 Model0_KERN_SECUREMASK=5, /* struct: maximum rights mask */
 Model0_KERN_PROF=6, /* table: profiling information */
 Model0_KERN_NODENAME=7, /* string: hostname */
 Model0_KERN_DOMAINNAME=8, /* string: domainname */

 Model0_KERN_PANIC=15, /* int: panic timeout */
 Model0_KERN_REALROOTDEV=16, /* real root device to mount after initrd */

 Model0_KERN_SPARC_REBOOT=21, /* reboot command on Sparc */
 Model0_KERN_CTLALTDEL=22, /* int: allow ctl-alt-del to reboot */
 Model0_KERN_PRINTK=23, /* struct: control printk logging parameters */
 Model0_KERN_NAMETRANS=24, /* Name translation */
 Model0_KERN_PPC_HTABRECLAIM=25, /* turn htab reclaimation on/off on PPC */
 Model0_KERN_PPC_ZEROPAGED=26, /* turn idle page zeroing on/off on PPC */
 Model0_KERN_PPC_POWERSAVE_NAP=27, /* use nap mode for power saving */
 Model0_KERN_MODPROBE=28, /* string: modprobe path */
 Model0_KERN_SG_BIG_BUFF=29, /* int: sg driver reserved buffer size */
 Model0_KERN_ACCT=30, /* BSD process accounting parameters */
 Model0_KERN_PPC_L2CR=31, /* l2cr register on PPC */

 Model0_KERN_RTSIGNR=32, /* Number of rt sigs queued */
 Model0_KERN_RTSIGMAX=33, /* Max queuable */

 Model0_KERN_SHMMAX=34, /* long: Maximum shared memory segment */
 Model0_KERN_MSGMAX=35, /* int: Maximum size of a messege */
 Model0_KERN_MSGMNB=36, /* int: Maximum message queue size */
 Model0_KERN_MSGPOOL=37, /* int: Maximum system message pool size */
 Model0_KERN_SYSRQ=38, /* int: Sysreq enable */
 Model0_KERN_MAX_THREADS=39, /* int: Maximum nr of threads in the system */
  Model0_KERN_RANDOM=40, /* Random driver */
  Model0_KERN_SHMALL=41, /* int: Maximum size of shared memory */
  Model0_KERN_MSGMNI=42, /* int: msg queue identifiers */
  Model0_KERN_SEM=43, /* struct: sysv semaphore limits */
  Model0_KERN_SPARC_STOP_A=44, /* int: Sparc Stop-A enable */
  Model0_KERN_SHMMNI=45, /* int: shm array identifiers */
 Model0_KERN_OVERFLOWUID=46, /* int: overflow UID */
 Model0_KERN_OVERFLOWGID=47, /* int: overflow GID */
 Model0_KERN_SHMPATH=48, /* string: path to shm fs */
 Model0_KERN_HOTPLUG=49, /* string: path to uevent helper (deprecated) */
 Model0_KERN_IEEE_EMULATION_WARNINGS=50, /* int: unimplemented ieee instructions */
 Model0_KERN_S390_USER_DEBUG_LOGGING=51, /* int: dumps of user faults */
 Model0_KERN_CORE_USES_PID=52, /* int: use core or core.%pid */
 Model0_KERN_TAINTED=53, /* int: various kernel tainted flags */
 Model0_KERN_CADPID=54, /* int: PID of the process to notify on CAD */
 Model0_KERN_PIDMAX=55, /* int: PID # limit */
   Model0_KERN_CORE_PATTERN=56, /* string: pattern for core-file names */
 Model0_KERN_PANIC_ON_OOPS=57, /* int: whether we will panic on an oops */
 Model0_KERN_HPPA_PWRSW=58, /* int: hppa soft-power enable */
 Model0_KERN_HPPA_UNALIGNED=59, /* int: hppa unaligned-trap enable */
 Model0_KERN_PRINTK_RATELIMIT=60, /* int: tune printk ratelimiting */
 Model0_KERN_PRINTK_RATELIMIT_BURST=61, /* int: tune printk ratelimiting */
 Model0_KERN_PTY=62, /* dir: pty driver */
 Model0_KERN_NGROUPS_MAX=63, /* int: NGROUPS_MAX */
 Model0_KERN_SPARC_SCONS_PWROFF=64, /* int: serial console power-off halt */
 Model0_KERN_HZ_TIMER=65, /* int: hz timer on or off */
 Model0_KERN_UNKNOWN_NMI_PANIC=66, /* int: unknown nmi panic flag */
 Model0_KERN_BOOTLOADER_TYPE=67, /* int: boot loader type */
 Model0_KERN_RANDOMIZE=68, /* int: randomize virtual address space */
 Model0_KERN_SETUID_DUMPABLE=69, /* int: behaviour of dumps for setuid core */
 Model0_KERN_SPIN_RETRY=70, /* int: number of spinlock retries */
 Model0_KERN_ACPI_VIDEO_FLAGS=71, /* int: flags for setting up video after ACPI sleep */
 Model0_KERN_IA64_UNALIGNED=72, /* int: ia64 unaligned userland trap enable */
 Model0_KERN_COMPAT_LOG=73, /* int: print compat layer  messages */
 Model0_KERN_MAX_LOCK_DEPTH=74, /* int: rtmutex's maximum lock depth */
 Model0_KERN_NMI_WATCHDOG=75, /* int: enable/disable nmi watchdog */
 Model0_KERN_PANIC_ON_NMI=76, /* int: whether we will panic on an unrecovered */
 Model0_KERN_PANIC_ON_WARN=77, /* int: call panic() in WARN() functions */
};



/* CTL_VM names: */
enum
{
 Model0_VM_UNUSED1=1, /* was: struct: Set vm swapping control */
 Model0_VM_UNUSED2=2, /* was; int: Linear or sqrt() swapout for hogs */
 Model0_VM_UNUSED3=3, /* was: struct: Set free page thresholds */
 Model0_VM_UNUSED4=4, /* Spare */
 Model0_VM_OVERCOMMIT_MEMORY=5, /* Turn off the virtual memory safety limit */
 Model0_VM_UNUSED5=6, /* was: struct: Set buffer memory thresholds */
 Model0_VM_UNUSED7=7, /* was: struct: Set cache memory thresholds */
 Model0_VM_UNUSED8=8, /* was: struct: Control kswapd behaviour */
 Model0_VM_UNUSED9=9, /* was: struct: Set page table cache parameters */
 Model0_VM_PAGE_CLUSTER=10, /* int: set number of pages to swap together */
 Model0_VM_DIRTY_BACKGROUND=11, /* dirty_background_ratio */
 Model0_VM_DIRTY_RATIO=12, /* dirty_ratio */
 Model0_VM_DIRTY_WB_CS=13, /* dirty_writeback_centisecs */
 Model0_VM_DIRTY_EXPIRE_CS=14, /* dirty_expire_centisecs */
 Model0_VM_NR_PDFLUSH_THREADS=15, /* nr_pdflush_threads */
 Model0_VM_OVERCOMMIT_RATIO=16, /* percent of RAM to allow overcommit in */
 Model0_VM_PAGEBUF=17, /* struct: Control pagebuf parameters */
 Model0_VM_HUGETLB_PAGES=18, /* int: Number of available Huge Pages */
 Model0_VM_SWAPPINESS=19, /* Tendency to steal mapped memory */
 Model0_VM_LOWMEM_RESERVE_RATIO=20,/* reservation ratio for lower memory zones */
 Model0_VM_MIN_FREE_KBYTES=21, /* Minimum free kilobytes to maintain */
 Model0_VM_MAX_MAP_COUNT=22, /* int: Maximum number of mmaps/address-space */
 Model0_VM_LAPTOP_MODE=23, /* vm laptop mode */
 Model0_VM_BLOCK_DUMP=24, /* block dump mode */
 Model0_VM_HUGETLB_GROUP=25, /* permitted hugetlb group */
 Model0_VM_VFS_CACHE_PRESSURE=26, /* dcache/icache reclaim pressure */
 Model0_VM_LEGACY_VA_LAYOUT=27, /* legacy/compatibility virtual address space layout */
 Model0_VM_SWAP_TOKEN_TIMEOUT=28, /* default time for token time out */
 Model0_VM_DROP_PAGECACHE=29, /* int: nuke lots of pagecache */
 Model0_VM_PERCPU_PAGELIST_FRACTION=30,/* int: fraction of pages in each percpu_pagelist */
 Model0_VM_ZONE_RECLAIM_MODE=31, /* reclaim local zone memory before going off node */
 Model0_VM_MIN_UNMAPPED=32, /* Set min percent of unmapped pages */
 Model0_VM_PANIC_ON_OOM=33, /* panic at out-of-memory */
 Model0_VM_VDSO_ENABLED=34, /* map VDSO into new processes? */
 Model0_VM_MIN_SLAB=35, /* Percent pages ignored by zone reclaim */
};


/* CTL_NET names: */
enum
{
 Model0_NET_CORE=1,
 Model0_NET_ETHER=2,
 Model0_NET_802=3,
 Model0_NET_UNIX=4,
 Model0_NET_IPV4=5,
 Model0_NET_IPX=6,
 Model0_NET_ATALK=7,
 Model0_NET_NETROM=8,
 Model0_NET_AX25=9,
 Model0_NET_BRIDGE=10,
 Model0_NET_ROSE=11,
 Model0_NET_IPV6=12,
 Model0_NET_X25=13,
 Model0_NET_TR=14,
 Model0_NET_DECNET=15,
 Model0_NET_ECONET=16,
 Model0_NET_SCTP=17,
 Model0_NET_LLC=18,
 Model0_NET_NETFILTER=19,
 Model0_NET_DCCP=20,
 Model0_NET_IRDA=412,
};

/* /proc/sys/kernel/random */
enum
{
 Model0_RANDOM_POOLSIZE=1,
 Model0_RANDOM_ENTROPY_COUNT=2,
 Model0_RANDOM_READ_THRESH=3,
 Model0_RANDOM_WRITE_THRESH=4,
 Model0_RANDOM_BOOT_ID=5,
 Model0_RANDOM_UUID=6
};

/* /proc/sys/kernel/pty */
enum
{
 Model0_PTY_MAX=1,
 Model0_PTY_NR=2
};

/* /proc/sys/bus/isa */
enum
{
 Model0_BUS_ISA_MEM_BASE=1,
 Model0_BUS_ISA_PORT_BASE=2,
 Model0_BUS_ISA_PORT_SHIFT=3
};

/* /proc/sys/net/core */
enum
{
 Model0_NET_CORE_WMEM_MAX=1,
 Model0_NET_CORE_RMEM_MAX=2,
 Model0_NET_CORE_WMEM_DEFAULT=3,
 Model0_NET_CORE_RMEM_DEFAULT=4,
/* was	NET_CORE_DESTROY_DELAY */
 Model0_NET_CORE_MAX_BACKLOG=6,
 Model0_NET_CORE_FASTROUTE=7,
 Model0_NET_CORE_MSG_COST=8,
 Model0_NET_CORE_MSG_BURST=9,
 Model0_NET_CORE_OPTMEM_MAX=10,
 Model0_NET_CORE_HOT_LIST_LENGTH=11,
 Model0_NET_CORE_DIVERT_VERSION=12,
 Model0_NET_CORE_NO_CONG_THRESH=13,
 Model0_NET_CORE_NO_CONG=14,
 Model0_NET_CORE_LO_CONG=15,
 Model0_NET_CORE_MOD_CONG=16,
 Model0_NET_CORE_DEV_WEIGHT=17,
 Model0_NET_CORE_SOMAXCONN=18,
 Model0_NET_CORE_BUDGET=19,
 Model0_NET_CORE_AEVENT_ETIME=20,
 Model0_NET_CORE_AEVENT_RSEQTH=21,
 Model0_NET_CORE_WARNINGS=22,
};

/* /proc/sys/net/ethernet */

/* /proc/sys/net/802 */

/* /proc/sys/net/unix */

enum
{
 Model0_NET_UNIX_DESTROY_DELAY=1,
 Model0_NET_UNIX_DELETE_DELAY=2,
 Model0_NET_UNIX_MAX_DGRAM_QLEN=3,
};

/* /proc/sys/net/netfilter */
enum
{
 Model0_NET_NF_CONNTRACK_MAX=1,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_SYN_SENT=2,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_SYN_RECV=3,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_ESTABLISHED=4,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_FIN_WAIT=5,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_CLOSE_WAIT=6,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_LAST_ACK=7,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_TIME_WAIT=8,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_CLOSE=9,
 Model0_NET_NF_CONNTRACK_UDP_TIMEOUT=10,
 Model0_NET_NF_CONNTRACK_UDP_TIMEOUT_STREAM=11,
 Model0_NET_NF_CONNTRACK_ICMP_TIMEOUT=12,
 Model0_NET_NF_CONNTRACK_GENERIC_TIMEOUT=13,
 Model0_NET_NF_CONNTRACK_BUCKETS=14,
 Model0_NET_NF_CONNTRACK_LOG_INVALID=15,
 Model0_NET_NF_CONNTRACK_TCP_TIMEOUT_MAX_RETRANS=16,
 Model0_NET_NF_CONNTRACK_TCP_LOOSE=17,
 Model0_NET_NF_CONNTRACK_TCP_BE_LIBERAL=18,
 Model0_NET_NF_CONNTRACK_TCP_MAX_RETRANS=19,
 Model0_NET_NF_CONNTRACK_SCTP_TIMEOUT_CLOSED=20,
 Model0_NET_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_WAIT=21,
 Model0_NET_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_ECHOED=22,
 Model0_NET_NF_CONNTRACK_SCTP_TIMEOUT_ESTABLISHED=23,
 Model0_NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_SENT=24,
 Model0_NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_RECD=25,
 Model0_NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_ACK_SENT=26,
 Model0_NET_NF_CONNTRACK_COUNT=27,
 Model0_NET_NF_CONNTRACK_ICMPV6_TIMEOUT=28,
 Model0_NET_NF_CONNTRACK_FRAG6_TIMEOUT=29,
 Model0_NET_NF_CONNTRACK_FRAG6_LOW_THRESH=30,
 Model0_NET_NF_CONNTRACK_FRAG6_HIGH_THRESH=31,
 Model0_NET_NF_CONNTRACK_CHECKSUM=32,
};

/* /proc/sys/net/ipv4 */
enum
{
 /* v2.0 compatibile variables */
 Model0_NET_IPV4_FORWARD=8,
 Model0_NET_IPV4_DYNADDR=9,

 Model0_NET_IPV4_CONF=16,
 Model0_NET_IPV4_NEIGH=17,
 Model0_NET_IPV4_ROUTE=18,
 Model0_NET_IPV4_FIB_HASH=19,
 Model0_NET_IPV4_NETFILTER=20,

 Model0_NET_IPV4_TCP_TIMESTAMPS=33,
 Model0_NET_IPV4_TCP_WINDOW_SCALING=34,
 Model0_NET_IPV4_TCP_SACK=35,
 Model0_NET_IPV4_TCP_RETRANS_COLLAPSE=36,
 Model0_NET_IPV4_DEFAULT_TTL=37,
 Model0_NET_IPV4_AUTOCONFIG=38,
 Model0_NET_IPV4_NO_PMTU_DISC=39,
 Model0_NET_IPV4_TCP_SYN_RETRIES=40,
 Model0_NET_IPV4_IPFRAG_HIGH_THRESH=41,
 Model0_NET_IPV4_IPFRAG_LOW_THRESH=42,
 Model0_NET_IPV4_IPFRAG_TIME=43,
 Model0_NET_IPV4_TCP_MAX_KA_PROBES=44,
 Model0_NET_IPV4_TCP_KEEPALIVE_TIME=45,
 Model0_NET_IPV4_TCP_KEEPALIVE_PROBES=46,
 Model0_NET_IPV4_TCP_RETRIES1=47,
 Model0_NET_IPV4_TCP_RETRIES2=48,
 Model0_NET_IPV4_TCP_FIN_TIMEOUT=49,
 Model0_NET_IPV4_IP_MASQ_DEBUG=50,
 Model0_NET_TCP_SYNCOOKIES=51,
 Model0_NET_TCP_STDURG=52,
 Model0_NET_TCP_RFC1337=53,
 Model0_NET_TCP_SYN_TAILDROP=54,
 Model0_NET_TCP_MAX_SYN_BACKLOG=55,
 Model0_NET_IPV4_LOCAL_PORT_RANGE=56,
 Model0_NET_IPV4_ICMP_ECHO_IGNORE_ALL=57,
 Model0_NET_IPV4_ICMP_ECHO_IGNORE_BROADCASTS=58,
 Model0_NET_IPV4_ICMP_SOURCEQUENCH_RATE=59,
 Model0_NET_IPV4_ICMP_DESTUNREACH_RATE=60,
 Model0_NET_IPV4_ICMP_TIMEEXCEED_RATE=61,
 Model0_NET_IPV4_ICMP_PARAMPROB_RATE=62,
 Model0_NET_IPV4_ICMP_ECHOREPLY_RATE=63,
 Model0_NET_IPV4_ICMP_IGNORE_BOGUS_ERROR_RESPONSES=64,
 Model0_NET_IPV4_IGMP_MAX_MEMBERSHIPS=65,
 Model0_NET_TCP_TW_RECYCLE=66,
 Model0_NET_IPV4_ALWAYS_DEFRAG=67,
 Model0_NET_IPV4_TCP_KEEPALIVE_INTVL=68,
 Model0_NET_IPV4_INET_PEER_THRESHOLD=69,
 Model0_NET_IPV4_INET_PEER_MINTTL=70,
 Model0_NET_IPV4_INET_PEER_MAXTTL=71,
 Model0_NET_IPV4_INET_PEER_GC_MINTIME=72,
 Model0_NET_IPV4_INET_PEER_GC_MAXTIME=73,
 Model0_NET_TCP_ORPHAN_RETRIES=74,
 Model0_NET_TCP_ABORT_ON_OVERFLOW=75,
 Model0_NET_TCP_SYNACK_RETRIES=76,
 Model0_NET_TCP_MAX_ORPHANS=77,
 Model0_NET_TCP_MAX_TW_BUCKETS=78,
 Model0_NET_TCP_FACK=79,
 Model0_NET_TCP_REORDERING=80,
 Model0_NET_TCP_ECN=81,
 Model0_NET_TCP_DSACK=82,
 Model0_NET_TCP_MEM=83,
 Model0_NET_TCP_WMEM=84,
 Model0_NET_TCP_RMEM=85,
 Model0_NET_TCP_APP_WIN=86,
 Model0_NET_TCP_ADV_WIN_SCALE=87,
 Model0_NET_IPV4_NONLOCAL_BIND=88,
 Model0_NET_IPV4_ICMP_RATELIMIT=89,
 Model0_NET_IPV4_ICMP_RATEMASK=90,
 Model0_NET_TCP_TW_REUSE=91,
 Model0_NET_TCP_FRTO=92,
 Model0_NET_TCP_LOW_LATENCY=93,
 Model0_NET_IPV4_IPFRAG_SECRET_INTERVAL=94,
 Model0_NET_IPV4_IGMP_MAX_MSF=96,
 Model0_NET_TCP_NO_METRICS_SAVE=97,
 Model0_NET_TCP_DEFAULT_WIN_SCALE=105,
 Model0_NET_TCP_MODERATE_RCVBUF=106,
 Model0_NET_TCP_TSO_WIN_DIVISOR=107,
 Model0_NET_TCP_BIC_BETA=108,
 Model0_NET_IPV4_ICMP_ERRORS_USE_INBOUND_IFADDR=109,
 Model0_NET_TCP_CONG_CONTROL=110,
 Model0_NET_TCP_ABC=111,
 Model0_NET_IPV4_IPFRAG_MAX_DIST=112,
  Model0_NET_TCP_MTU_PROBING=113,
 Model0_NET_TCP_BASE_MSS=114,
 Model0_NET_IPV4_TCP_WORKAROUND_SIGNED_WINDOWS=115,
 Model0_NET_TCP_DMA_COPYBREAK=116,
 Model0_NET_TCP_SLOW_START_AFTER_IDLE=117,
 Model0_NET_CIPSOV4_CACHE_ENABLE=118,
 Model0_NET_CIPSOV4_CACHE_BUCKET_SIZE=119,
 Model0_NET_CIPSOV4_RBM_OPTFMT=120,
 Model0_NET_CIPSOV4_RBM_STRICTVALID=121,
 Model0_NET_TCP_AVAIL_CONG_CONTROL=122,
 Model0_NET_TCP_ALLOWED_CONG_CONTROL=123,
 Model0_NET_TCP_MAX_SSTHRESH=124,
 Model0_NET_TCP_FRTO_RESPONSE=125,
};

enum {
 Model0_NET_IPV4_ROUTE_FLUSH=1,
 Model0_NET_IPV4_ROUTE_MIN_DELAY=2, /* obsolete since 2.6.25 */
 Model0_NET_IPV4_ROUTE_MAX_DELAY=3, /* obsolete since 2.6.25 */
 Model0_NET_IPV4_ROUTE_GC_THRESH=4,
 Model0_NET_IPV4_ROUTE_MAX_SIZE=5,
 Model0_NET_IPV4_ROUTE_GC_MIN_INTERVAL=6,
 Model0_NET_IPV4_ROUTE_GC_TIMEOUT=7,
 Model0_NET_IPV4_ROUTE_GC_INTERVAL=8, /* obsolete since 2.6.38 */
 Model0_NET_IPV4_ROUTE_REDIRECT_LOAD=9,
 Model0_NET_IPV4_ROUTE_REDIRECT_NUMBER=10,
 Model0_NET_IPV4_ROUTE_REDIRECT_SILENCE=11,
 Model0_NET_IPV4_ROUTE_ERROR_COST=12,
 Model0_NET_IPV4_ROUTE_ERROR_BURST=13,
 Model0_NET_IPV4_ROUTE_GC_ELASTICITY=14,
 Model0_NET_IPV4_ROUTE_MTU_EXPIRES=15,
 Model0_NET_IPV4_ROUTE_MIN_PMTU=16,
 Model0_NET_IPV4_ROUTE_MIN_ADVMSS=17,
 Model0_NET_IPV4_ROUTE_SECRET_INTERVAL=18,
 Model0_NET_IPV4_ROUTE_GC_MIN_INTERVAL_MS=19,
};

enum
{
 Model0_NET_PROTO_CONF_ALL=-2,
 Model0_NET_PROTO_CONF_DEFAULT=-3

 /* And device ifindices ... */
};

enum
{
 Model0_NET_IPV4_CONF_FORWARDING=1,
 Model0_NET_IPV4_CONF_MC_FORWARDING=2,
 Model0_NET_IPV4_CONF_PROXY_ARP=3,
 Model0_NET_IPV4_CONF_ACCEPT_REDIRECTS=4,
 Model0_NET_IPV4_CONF_SECURE_REDIRECTS=5,
 Model0_NET_IPV4_CONF_SEND_REDIRECTS=6,
 Model0_NET_IPV4_CONF_SHARED_MEDIA=7,
 Model0_NET_IPV4_CONF_RP_FILTER=8,
 Model0_NET_IPV4_CONF_ACCEPT_SOURCE_ROUTE=9,
 Model0_NET_IPV4_CONF_BOOTP_RELAY=10,
 Model0_NET_IPV4_CONF_LOG_MARTIANS=11,
 Model0_NET_IPV4_CONF_TAG=12,
 Model0_NET_IPV4_CONF_ARPFILTER=13,
 Model0_NET_IPV4_CONF_MEDIUM_ID=14,
 Model0_NET_IPV4_CONF_NOXFRM=15,
 Model0_NET_IPV4_CONF_NOPOLICY=16,
 Model0_NET_IPV4_CONF_FORCE_IGMP_VERSION=17,
 Model0_NET_IPV4_CONF_ARP_ANNOUNCE=18,
 Model0_NET_IPV4_CONF_ARP_IGNORE=19,
 Model0_NET_IPV4_CONF_PROMOTE_SECONDARIES=20,
 Model0_NET_IPV4_CONF_ARP_ACCEPT=21,
 Model0_NET_IPV4_CONF_ARP_NOTIFY=22,
};

/* /proc/sys/net/ipv4/netfilter */
enum
{
 Model0_NET_IPV4_NF_CONNTRACK_MAX=1,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_SYN_SENT=2,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_SYN_RECV=3,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_ESTABLISHED=4,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_FIN_WAIT=5,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_CLOSE_WAIT=6,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_LAST_ACK=7,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_TIME_WAIT=8,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_CLOSE=9,
 Model0_NET_IPV4_NF_CONNTRACK_UDP_TIMEOUT=10,
 Model0_NET_IPV4_NF_CONNTRACK_UDP_TIMEOUT_STREAM=11,
 Model0_NET_IPV4_NF_CONNTRACK_ICMP_TIMEOUT=12,
 Model0_NET_IPV4_NF_CONNTRACK_GENERIC_TIMEOUT=13,
 Model0_NET_IPV4_NF_CONNTRACK_BUCKETS=14,
 Model0_NET_IPV4_NF_CONNTRACK_LOG_INVALID=15,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_MAX_RETRANS=16,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_LOOSE=17,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_BE_LIBERAL=18,
 Model0_NET_IPV4_NF_CONNTRACK_TCP_MAX_RETRANS=19,
  Model0_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_CLOSED=20,
  Model0_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_WAIT=21,
  Model0_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_ECHOED=22,
  Model0_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_ESTABLISHED=23,
  Model0_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_SENT=24,
  Model0_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_RECD=25,
  Model0_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_ACK_SENT=26,
 Model0_NET_IPV4_NF_CONNTRACK_COUNT=27,
 Model0_NET_IPV4_NF_CONNTRACK_CHECKSUM=28,
};

/* /proc/sys/net/ipv6 */
enum {
 Model0_NET_IPV6_CONF=16,
 Model0_NET_IPV6_NEIGH=17,
 Model0_NET_IPV6_ROUTE=18,
 Model0_NET_IPV6_ICMP=19,
 Model0_NET_IPV6_BINDV6ONLY=20,
 Model0_NET_IPV6_IP6FRAG_HIGH_THRESH=21,
 Model0_NET_IPV6_IP6FRAG_LOW_THRESH=22,
 Model0_NET_IPV6_IP6FRAG_TIME=23,
 Model0_NET_IPV6_IP6FRAG_SECRET_INTERVAL=24,
 Model0_NET_IPV6_MLD_MAX_MSF=25,
};

enum {
 Model0_NET_IPV6_ROUTE_FLUSH=1,
 Model0_NET_IPV6_ROUTE_GC_THRESH=2,
 Model0_NET_IPV6_ROUTE_MAX_SIZE=3,
 Model0_NET_IPV6_ROUTE_GC_MIN_INTERVAL=4,
 Model0_NET_IPV6_ROUTE_GC_TIMEOUT=5,
 Model0_NET_IPV6_ROUTE_GC_INTERVAL=6,
 Model0_NET_IPV6_ROUTE_GC_ELASTICITY=7,
 Model0_NET_IPV6_ROUTE_MTU_EXPIRES=8,
 Model0_NET_IPV6_ROUTE_MIN_ADVMSS=9,
 Model0_NET_IPV6_ROUTE_GC_MIN_INTERVAL_MS=10
};

enum {
 Model0_NET_IPV6_FORWARDING=1,
 Model0_NET_IPV6_HOP_LIMIT=2,
 Model0_NET_IPV6_MTU=3,
 Model0_NET_IPV6_ACCEPT_RA=4,
 Model0_NET_IPV6_ACCEPT_REDIRECTS=5,
 Model0_NET_IPV6_AUTOCONF=6,
 Model0_NET_IPV6_DAD_TRANSMITS=7,
 Model0_NET_IPV6_RTR_SOLICITS=8,
 Model0_NET_IPV6_RTR_SOLICIT_INTERVAL=9,
 Model0_NET_IPV6_RTR_SOLICIT_DELAY=10,
 Model0_NET_IPV6_USE_TEMPADDR=11,
 Model0_NET_IPV6_TEMP_VALID_LFT=12,
 Model0_NET_IPV6_TEMP_PREFERED_LFT=13,
 Model0_NET_IPV6_REGEN_MAX_RETRY=14,
 Model0_NET_IPV6_MAX_DESYNC_FACTOR=15,
 Model0_NET_IPV6_MAX_ADDRESSES=16,
 Model0_NET_IPV6_FORCE_MLD_VERSION=17,
 Model0_NET_IPV6_ACCEPT_RA_DEFRTR=18,
 Model0_NET_IPV6_ACCEPT_RA_PINFO=19,
 Model0_NET_IPV6_ACCEPT_RA_RTR_PREF=20,
 Model0_NET_IPV6_RTR_PROBE_INTERVAL=21,
 Model0_NET_IPV6_ACCEPT_RA_RT_INFO_MAX_PLEN=22,
 Model0_NET_IPV6_PROXY_NDP=23,
 Model0_NET_IPV6_ACCEPT_SOURCE_ROUTE=25,
 Model0_NET_IPV6_ACCEPT_RA_FROM_LOCAL=26,
 Model0___NET_IPV6_MAX
};

/* /proc/sys/net/ipv6/icmp */
enum {
 Model0_NET_IPV6_ICMP_RATELIMIT=1
};

/* /proc/sys/net/<protocol>/neigh/<dev> */
enum {
 Model0_NET_NEIGH_MCAST_SOLICIT=1,
 Model0_NET_NEIGH_UCAST_SOLICIT=2,
 Model0_NET_NEIGH_APP_SOLICIT=3,
 Model0_NET_NEIGH_RETRANS_TIME=4,
 Model0_NET_NEIGH_REACHABLE_TIME=5,
 Model0_NET_NEIGH_DELAY_PROBE_TIME=6,
 Model0_NET_NEIGH_GC_STALE_TIME=7,
 Model0_NET_NEIGH_UNRES_QLEN=8,
 Model0_NET_NEIGH_PROXY_QLEN=9,
 Model0_NET_NEIGH_ANYCAST_DELAY=10,
 Model0_NET_NEIGH_PROXY_DELAY=11,
 Model0_NET_NEIGH_LOCKTIME=12,
 Model0_NET_NEIGH_GC_INTERVAL=13,
 Model0_NET_NEIGH_GC_THRESH1=14,
 Model0_NET_NEIGH_GC_THRESH2=15,
 Model0_NET_NEIGH_GC_THRESH3=16,
 Model0_NET_NEIGH_RETRANS_TIME_MS=17,
 Model0_NET_NEIGH_REACHABLE_TIME_MS=18,
};

/* /proc/sys/net/dccp */
enum {
 Model0_NET_DCCP_DEFAULT=1,
};

/* /proc/sys/net/ipx */
enum {
 Model0_NET_IPX_PPROP_BROADCASTING=1,
 Model0_NET_IPX_FORWARDING=2
};

/* /proc/sys/net/llc */
enum {
 Model0_NET_LLC2=1,
 Model0_NET_LLC_STATION=2,
};

/* /proc/sys/net/llc/llc2 */
enum {
 Model0_NET_LLC2_TIMEOUT=1,
};

/* /proc/sys/net/llc/station */
enum {
 Model0_NET_LLC_STATION_ACK_TIMEOUT=1,
};

/* /proc/sys/net/llc/llc2/timeout */
enum {
 Model0_NET_LLC2_ACK_TIMEOUT=1,
 Model0_NET_LLC2_P_TIMEOUT=2,
 Model0_NET_LLC2_REJ_TIMEOUT=3,
 Model0_NET_LLC2_BUSY_TIMEOUT=4,
};

/* /proc/sys/net/appletalk */
enum {
 Model0_NET_ATALK_AARP_EXPIRY_TIME=1,
 Model0_NET_ATALK_AARP_TICK_TIME=2,
 Model0_NET_ATALK_AARP_RETRANSMIT_LIMIT=3,
 Model0_NET_ATALK_AARP_RESOLVE_TIME=4
};


/* /proc/sys/net/netrom */
enum {
 Model0_NET_NETROM_DEFAULT_PATH_QUALITY=1,
 Model0_NET_NETROM_OBSOLESCENCE_COUNT_INITIALISER=2,
 Model0_NET_NETROM_NETWORK_TTL_INITIALISER=3,
 Model0_NET_NETROM_TRANSPORT_TIMEOUT=4,
 Model0_NET_NETROM_TRANSPORT_MAXIMUM_TRIES=5,
 Model0_NET_NETROM_TRANSPORT_ACKNOWLEDGE_DELAY=6,
 Model0_NET_NETROM_TRANSPORT_BUSY_DELAY=7,
 Model0_NET_NETROM_TRANSPORT_REQUESTED_WINDOW_SIZE=8,
 Model0_NET_NETROM_TRANSPORT_NO_ACTIVITY_TIMEOUT=9,
 Model0_NET_NETROM_ROUTING_CONTROL=10,
 Model0_NET_NETROM_LINK_FAILS_COUNT=11,
 Model0_NET_NETROM_RESET=12
};

/* /proc/sys/net/ax25 */
enum {
 Model0_NET_AX25_IP_DEFAULT_MODE=1,
 Model0_NET_AX25_DEFAULT_MODE=2,
 Model0_NET_AX25_BACKOFF_TYPE=3,
 Model0_NET_AX25_CONNECT_MODE=4,
 Model0_NET_AX25_STANDARD_WINDOW=5,
 Model0_NET_AX25_EXTENDED_WINDOW=6,
 Model0_NET_AX25_T1_TIMEOUT=7,
 Model0_NET_AX25_T2_TIMEOUT=8,
 Model0_NET_AX25_T3_TIMEOUT=9,
 Model0_NET_AX25_IDLE_TIMEOUT=10,
 Model0_NET_AX25_N2=11,
 Model0_NET_AX25_PACLEN=12,
 Model0_NET_AX25_PROTOCOL=13,
 Model0_NET_AX25_DAMA_SLAVE_TIMEOUT=14
};

/* /proc/sys/net/rose */
enum {
 Model0_NET_ROSE_RESTART_REQUEST_TIMEOUT=1,
 Model0_NET_ROSE_CALL_REQUEST_TIMEOUT=2,
 Model0_NET_ROSE_RESET_REQUEST_TIMEOUT=3,
 Model0_NET_ROSE_CLEAR_REQUEST_TIMEOUT=4,
 Model0_NET_ROSE_ACK_HOLD_BACK_TIMEOUT=5,
 Model0_NET_ROSE_ROUTING_CONTROL=6,
 Model0_NET_ROSE_LINK_FAIL_TIMEOUT=7,
 Model0_NET_ROSE_MAX_VCS=8,
 Model0_NET_ROSE_WINDOW_SIZE=9,
 Model0_NET_ROSE_NO_ACTIVITY_TIMEOUT=10
};

/* /proc/sys/net/x25 */
enum {
 Model0_NET_X25_RESTART_REQUEST_TIMEOUT=1,
 Model0_NET_X25_CALL_REQUEST_TIMEOUT=2,
 Model0_NET_X25_RESET_REQUEST_TIMEOUT=3,
 Model0_NET_X25_CLEAR_REQUEST_TIMEOUT=4,
 Model0_NET_X25_ACK_HOLD_BACK_TIMEOUT=5,
 Model0_NET_X25_FORWARD=6
};

/* /proc/sys/net/token-ring */
enum
{
 Model0_NET_TR_RIF_TIMEOUT=1
};

/* /proc/sys/net/decnet/ */
enum {
 Model0_NET_DECNET_NODE_TYPE = 1,
 Model0_NET_DECNET_NODE_ADDRESS = 2,
 Model0_NET_DECNET_NODE_NAME = 3,
 Model0_NET_DECNET_DEFAULT_DEVICE = 4,
 Model0_NET_DECNET_TIME_WAIT = 5,
 Model0_NET_DECNET_DN_COUNT = 6,
 Model0_NET_DECNET_DI_COUNT = 7,
 Model0_NET_DECNET_DR_COUNT = 8,
 Model0_NET_DECNET_DST_GC_INTERVAL = 9,
 Model0_NET_DECNET_CONF = 10,
 Model0_NET_DECNET_NO_FC_MAX_CWND = 11,
 Model0_NET_DECNET_MEM = 12,
 Model0_NET_DECNET_RMEM = 13,
 Model0_NET_DECNET_WMEM = 14,
 Model0_NET_DECNET_DEBUG_LEVEL = 255
};

/* /proc/sys/net/decnet/conf/<dev> */
enum {
 Model0_NET_DECNET_CONF_LOOPBACK = -2,
 Model0_NET_DECNET_CONF_DDCMP = -3,
 Model0_NET_DECNET_CONF_PPP = -4,
 Model0_NET_DECNET_CONF_X25 = -5,
 Model0_NET_DECNET_CONF_GRE = -6,
 Model0_NET_DECNET_CONF_ETHER = -7

 /* ... and ifindex of devices */
};

/* /proc/sys/net/decnet/conf/<dev>/ */
enum {
 Model0_NET_DECNET_CONF_DEV_PRIORITY = 1,
 Model0_NET_DECNET_CONF_DEV_T1 = 2,
 Model0_NET_DECNET_CONF_DEV_T2 = 3,
 Model0_NET_DECNET_CONF_DEV_T3 = 4,
 Model0_NET_DECNET_CONF_DEV_FORWARDING = 5,
 Model0_NET_DECNET_CONF_DEV_BLKSIZE = 6,
 Model0_NET_DECNET_CONF_DEV_STATE = 7
};

/* /proc/sys/net/sctp */
enum {
 Model0_NET_SCTP_RTO_INITIAL = 1,
 Model0_NET_SCTP_RTO_MIN = 2,
 Model0_NET_SCTP_RTO_MAX = 3,
 Model0_NET_SCTP_RTO_ALPHA = 4,
 Model0_NET_SCTP_RTO_BETA = 5,
 Model0_NET_SCTP_VALID_COOKIE_LIFE = 6,
 Model0_NET_SCTP_ASSOCIATION_MAX_RETRANS = 7,
 Model0_NET_SCTP_PATH_MAX_RETRANS = 8,
 Model0_NET_SCTP_MAX_INIT_RETRANSMITS = 9,
 Model0_NET_SCTP_HB_INTERVAL = 10,
 Model0_NET_SCTP_PRESERVE_ENABLE = 11,
 Model0_NET_SCTP_MAX_BURST = 12,
 Model0_NET_SCTP_ADDIP_ENABLE = 13,
 Model0_NET_SCTP_PRSCTP_ENABLE = 14,
 Model0_NET_SCTP_SNDBUF_POLICY = 15,
 Model0_NET_SCTP_SACK_TIMEOUT = 16,
 Model0_NET_SCTP_RCVBUF_POLICY = 17,
};

/* /proc/sys/net/bridge */
enum {
 Model0_NET_BRIDGE_NF_CALL_ARPTABLES = 1,
 Model0_NET_BRIDGE_NF_CALL_IPTABLES = 2,
 Model0_NET_BRIDGE_NF_CALL_IP6TABLES = 3,
 Model0_NET_BRIDGE_NF_FILTER_VLAN_TAGGED = 4,
 Model0_NET_BRIDGE_NF_FILTER_PPPOE_TAGGED = 5,
};

/* proc/sys/net/irda */
enum {
 Model0_NET_IRDA_DISCOVERY=1,
 Model0_NET_IRDA_DEVNAME=2,
 Model0_NET_IRDA_DEBUG=3,
 Model0_NET_IRDA_FAST_POLL=4,
 Model0_NET_IRDA_DISCOVERY_SLOTS=5,
 Model0_NET_IRDA_DISCOVERY_TIMEOUT=6,
 Model0_NET_IRDA_SLOT_TIMEOUT=7,
 Model0_NET_IRDA_MAX_BAUD_RATE=8,
 Model0_NET_IRDA_MIN_TX_TURN_TIME=9,
 Model0_NET_IRDA_MAX_TX_DATA_SIZE=10,
 Model0_NET_IRDA_MAX_TX_WINDOW=11,
 Model0_NET_IRDA_MAX_NOREPLY_TIME=12,
 Model0_NET_IRDA_WARN_NOREPLY_TIME=13,
 Model0_NET_IRDA_LAP_KEEPALIVE_TIME=14,
};


/* CTL_FS names: */
enum
{
 Model0_FS_NRINODE=1, /* int:current number of allocated inodes */
 Model0_FS_STATINODE=2,
 Model0_FS_MAXINODE=3, /* int:maximum number of inodes that can be allocated */
 Model0_FS_NRDQUOT=4, /* int:current number of allocated dquots */
 Model0_FS_MAXDQUOT=5, /* int:maximum number of dquots that can be allocated */
 Model0_FS_NRFILE=6, /* int:current number of allocated filedescriptors */
 Model0_FS_MAXFILE=7, /* int:maximum number of filedescriptors that can be allocated */
 Model0_FS_DENTRY=8,
 Model0_FS_NRSUPER=9, /* int:current number of allocated super_blocks */
 Model0_FS_MAXSUPER=10, /* int:maximum number of super_blocks that can be allocated */
 Model0_FS_OVERFLOWUID=11, /* int: overflow UID */
 Model0_FS_OVERFLOWGID=12, /* int: overflow GID */
 Model0_FS_LEASES=13, /* int: leases enabled */
 Model0_FS_DIR_NOTIFY=14, /* int: directory notification enabled */
 Model0_FS_LEASE_TIME=15, /* int: maximum time to wait for a lease break */
 Model0_FS_DQSTATS=16, /* disc quota usage statistics and control */
 Model0_FS_XFS=17, /* struct: control xfs parameters */
 Model0_FS_AIO_NR=18, /* current system-wide number of aio requests */
 Model0_FS_AIO_MAX_NR=19, /* system-wide maximum number of aio requests */
 Model0_FS_INOTIFY=20, /* inotify submenu */
 Model0_FS_OCFS2=988, /* ocfs2 */
};

/* /proc/sys/fs/quota/ */
enum {
 Model0_FS_DQ_LOOKUPS = 1,
 Model0_FS_DQ_DROPS = 2,
 Model0_FS_DQ_READS = 3,
 Model0_FS_DQ_WRITES = 4,
 Model0_FS_DQ_CACHE_HITS = 5,
 Model0_FS_DQ_ALLOCATED = 6,
 Model0_FS_DQ_FREE = 7,
 Model0_FS_DQ_SYNCS = 8,
 Model0_FS_DQ_WARNINGS = 9,
};

/* CTL_DEBUG names: */

/* CTL_DEV names: */
enum {
 Model0_DEV_CDROM=1,
 Model0_DEV_HWMON=2,
 Model0_DEV_PARPORT=3,
 Model0_DEV_RAID=4,
 Model0_DEV_MAC_HID=5,
 Model0_DEV_SCSI=6,
 Model0_DEV_IPMI=7,
};

/* /proc/sys/dev/cdrom */
enum {
 Model0_DEV_CDROM_INFO=1,
 Model0_DEV_CDROM_AUTOCLOSE=2,
 Model0_DEV_CDROM_AUTOEJECT=3,
 Model0_DEV_CDROM_DEBUG=4,
 Model0_DEV_CDROM_LOCK=5,
 Model0_DEV_CDROM_CHECK_MEDIA=6
};

/* /proc/sys/dev/parport */
enum {
 Model0_DEV_PARPORT_DEFAULT=-3
};

/* /proc/sys/dev/raid */
enum {
 Model0_DEV_RAID_SPEED_LIMIT_MIN=1,
 Model0_DEV_RAID_SPEED_LIMIT_MAX=2
};

/* /proc/sys/dev/parport/default */
enum {
 Model0_DEV_PARPORT_DEFAULT_TIMESLICE=1,
 Model0_DEV_PARPORT_DEFAULT_SPINTIME=2
};

/* /proc/sys/dev/parport/parport n */
enum {
 Model0_DEV_PARPORT_SPINTIME=1,
 Model0_DEV_PARPORT_BASE_ADDR=2,
 Model0_DEV_PARPORT_IRQ=3,
 Model0_DEV_PARPORT_DMA=4,
 Model0_DEV_PARPORT_MODES=5,
 Model0_DEV_PARPORT_DEVICES=6,
 Model0_DEV_PARPORT_AUTOPROBE=16
};

/* /proc/sys/dev/parport/parport n/devices/ */
enum {
 Model0_DEV_PARPORT_DEVICES_ACTIVE=-3,
};

/* /proc/sys/dev/parport/parport n/devices/device n */
enum {
 Model0_DEV_PARPORT_DEVICE_TIMESLICE=1,
};

/* /proc/sys/dev/mac_hid */
enum {
 Model0_DEV_MAC_HID_KEYBOARD_SENDS_LINUX_KEYCODES=1,
 Model0_DEV_MAC_HID_KEYBOARD_LOCK_KEYCODES=2,
 Model0_DEV_MAC_HID_MOUSE_BUTTON_EMULATION=3,
 Model0_DEV_MAC_HID_MOUSE_BUTTON2_KEYCODE=4,
 Model0_DEV_MAC_HID_MOUSE_BUTTON3_KEYCODE=5,
 Model0_DEV_MAC_HID_ADB_MOUSE_SENDS_KEYCODES=6
};

/* /proc/sys/dev/scsi */
enum {
 Model0_DEV_SCSI_LOGGING_LEVEL=1,
};

/* /proc/sys/dev/ipmi */
enum {
 Model0_DEV_IPMI_POWEROFF_POWERCYCLE=1,
};

/* /proc/sys/abi */
enum
{
 Model0_ABI_DEFHANDLER_COFF=1, /* default handler for coff binaries */
 Model0_ABI_DEFHANDLER_ELF=2, /* default handler for ELF binaries */
 Model0_ABI_DEFHANDLER_LCALL7=3,/* default handler for procs using lcall7 */
 Model0_ABI_DEFHANDLER_LIBCSO=4,/* default handler for an libc.so ELF interp */
 Model0_ABI_TRACE=5, /* tracing flags */
 Model0_ABI_FAKE_UTSNAME=6, /* fake target utsname information */
};

/* For the /proc/sys support */
struct Model0_completion;
struct Model0_ctl_table;
struct Model0_nsproxy;
struct Model0_ctl_table_root;
struct Model0_ctl_table_header;
struct Model0_ctl_dir;

typedef int Model0_proc_handler (struct Model0_ctl_table *Model0_ctl, int Model0_write,
     void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);

extern int Model0_proc_dostring(struct Model0_ctl_table *, int,
    void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_dointvec(struct Model0_ctl_table *, int,
    void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_douintvec(struct Model0_ctl_table *, int,
    void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_dointvec_minmax(struct Model0_ctl_table *, int,
    void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_dointvec_jiffies(struct Model0_ctl_table *, int,
     void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_dointvec_userhz_jiffies(struct Model0_ctl_table *, int,
     void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_dointvec_ms_jiffies(struct Model0_ctl_table *, int,
        void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_doulongvec_minmax(struct Model0_ctl_table *, int,
      void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_doulongvec_ms_jiffies_minmax(struct Model0_ctl_table *Model0_table, int,
          void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_proc_do_large_bitmap(struct Model0_ctl_table *, int,
    void *, Model0_size_t *, Model0_loff_t *);

/*
 * Register a set of sysctl names by calling register_sysctl_table
 * with an initialised array of struct ctl_table's.  An entry with 
 * NULL procname terminates the table.  table->de will be
 * set up by the registration and need not be initialised in advance.
 *
 * sysctl names can be mirrored automatically under /proc/sys.  The
 * procname supplied controls /proc naming.
 *
 * The table's mode will be honoured both for sys_sysctl(2) and
 * proc-fs access.
 *
 * Leaf nodes in the sysctl tree will be represented by a single file
 * under /proc; non-leaf nodes will be represented by directories.  A
 * null procname disables /proc mirroring at this node.
 *
 * sysctl(2) can automatically manage read and write requests through
 * the sysctl table.  The data and maxlen fields of the ctl_table
 * struct enable minimal validation of the values being written to be
 * performed, and the mode field allows minimal authentication.
 * 
 * There must be a proc_handler routine for any terminal nodes
 * mirrored under /proc/sys (non-terminals are handled by a built-in
 * directory handler).  Several default handlers are available to
 * cover common cases.
 */

/* Support for userspace poll() to watch for changes */
struct Model0_ctl_table_poll {
 Model0_atomic_t Model0_event;
 Model0_wait_queue_head_t Model0_wait;
};

static inline __attribute__((no_instrument_function)) void *Model0_proc_sys_poll_event(struct Model0_ctl_table_poll *Model0_poll)
{
 return (void *)(unsigned long)Model0_atomic_read(&Model0_poll->Model0_event);
}
/* A sysctl table is an array of struct ctl_table: */
struct Model0_ctl_table
{
 const char *Model0_procname; /* Text ID for /proc/sys, or zero */
 void *Model0_data;
 int Model0_maxlen;
 Model0_umode_t Model0_mode;
 struct Model0_ctl_table *Model0_child; /* Deprecated */
 Model0_proc_handler *Model0_proc_handler; /* Callback for text formatting */
 struct Model0_ctl_table_poll *Model0_poll;
 void *Model0_extra1;
 void *Model0_extra2;
};

struct Model0_ctl_node {
 struct Model0_rb_node Model0_node;
 struct Model0_ctl_table_header *Model0_header;
};

/* struct ctl_table_header is used to maintain dynamic lists of
   struct ctl_table trees. */
struct Model0_ctl_table_header
{
 union {
  struct {
   struct Model0_ctl_table *Model0_ctl_table;
   int Model0_used;
   int Model0_count;
   int Model0_nreg;
  };
  struct Model0_callback_head Model0_rcu;
 };
 struct Model0_completion *Model0_unregistering;
 struct Model0_ctl_table *Model0_ctl_table_arg;
 struct Model0_ctl_table_root *Model0_root;
 struct Model0_ctl_table_set *Model0_set;
 struct Model0_ctl_dir *Model0_parent;
 struct Model0_ctl_node *Model0_node;
};

struct Model0_ctl_dir {
 /* Header must be at the start of ctl_dir */
 struct Model0_ctl_table_header Model0_header;
 struct Model0_rb_root Model0_root;
};

struct Model0_ctl_table_set {
 int (*Model0_is_seen)(struct Model0_ctl_table_set *);
 struct Model0_ctl_dir Model0_dir;
};

struct Model0_ctl_table_root {
 struct Model0_ctl_table_set Model0_default_set;
 struct Model0_ctl_table_set *(*Model0_lookup)(struct Model0_ctl_table_root *Model0_root,
        struct Model0_nsproxy *Model0_namespaces);
 int (*Model0_permissions)(struct Model0_ctl_table_header *Model0_head, struct Model0_ctl_table *Model0_table);
};

/* struct ctl_path describes where in the hierarchy a table is added */
struct Model0_ctl_path {
 const char *Model0_procname;
};



void Model0_proc_sys_poll_notify(struct Model0_ctl_table_poll *Model0_poll);

extern void Model0_setup_sysctl_set(struct Model0_ctl_table_set *Model0_p,
 struct Model0_ctl_table_root *Model0_root,
 int (*Model0_is_seen)(struct Model0_ctl_table_set *));
extern void Model0_retire_sysctl_set(struct Model0_ctl_table_set *Model0_set);

void Model0_register_sysctl_root(struct Model0_ctl_table_root *Model0_root);
struct Model0_ctl_table_header *Model0___register_sysctl_table(
 struct Model0_ctl_table_set *Model0_set,
 const char *Model0_path, struct Model0_ctl_table *Model0_table);
struct Model0_ctl_table_header *Model0___register_sysctl_paths(
 struct Model0_ctl_table_set *Model0_set,
 const struct Model0_ctl_path *Model0_path, struct Model0_ctl_table *Model0_table);
struct Model0_ctl_table_header *Model0_register_sysctl(const char *Model0_path, struct Model0_ctl_table *Model0_table);
struct Model0_ctl_table_header *Model0_register_sysctl_table(struct Model0_ctl_table * Model0_table);
struct Model0_ctl_table_header *Model0_register_sysctl_paths(const struct Model0_ctl_path *Model0_path,
      struct Model0_ctl_table *Model0_table);

void Model0_unregister_sysctl_table(struct Model0_ctl_table_header * Model0_table);

extern int Model0_sysctl_init(void);

extern struct Model0_ctl_table Model0_sysctl_mount_point[];
int Model0_sysctl_max_threads(struct Model0_ctl_table *Model0_table, int Model0_write,
         void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);

extern unsigned int Model0_sysctl_timer_migration;
int Model0_timer_migration_handler(struct Model0_ctl_table *Model0_table, int Model0_write,
       void *Model0_buffer, Model0_size_t *Model0_lenp,
       Model0_loff_t *Model0_ppos);


unsigned long Model0___round_jiffies(unsigned long Model0_j, int Model0_cpu);
unsigned long Model0___round_jiffies_relative(unsigned long Model0_j, int Model0_cpu);
unsigned long Model0_round_jiffies(unsigned long Model0_j);
unsigned long Model0_round_jiffies_relative(unsigned long Model0_j);

unsigned long Model0___round_jiffies_up(unsigned long Model0_j, int Model0_cpu);
unsigned long Model0___round_jiffies_up_relative(unsigned long Model0_j, int Model0_cpu);
unsigned long Model0_round_jiffies_up(unsigned long Model0_j);
unsigned long Model0_round_jiffies_up_relative(unsigned long Model0_j);


int Model0_timers_dead_cpu(unsigned int Model0_cpu);







struct Model0_workqueue_struct;

struct Model0_work_struct;
typedef void (*Model0_work_func_t)(struct Model0_work_struct *Model0_work);
void Model0_delayed_work_timer_fn(unsigned long Model0___data);

/*
 * The first word is the work queue pointer and the flags rolled into
 * one
 */


enum {
 Model0_WORK_STRUCT_PENDING_BIT = 0, /* work item is pending execution */
 Model0_WORK_STRUCT_DELAYED_BIT = 1, /* work item is delayed */
 Model0_WORK_STRUCT_PWQ_BIT = 2, /* data points to pwq */
 Model0_WORK_STRUCT_LINKED_BIT = 3, /* next work is linked to this one */




 Model0_WORK_STRUCT_COLOR_SHIFT = 4, /* color for workqueue flushing */


 Model0_WORK_STRUCT_COLOR_BITS = 4,

 Model0_WORK_STRUCT_PENDING = 1 << Model0_WORK_STRUCT_PENDING_BIT,
 Model0_WORK_STRUCT_DELAYED = 1 << Model0_WORK_STRUCT_DELAYED_BIT,
 Model0_WORK_STRUCT_PWQ = 1 << Model0_WORK_STRUCT_PWQ_BIT,
 Model0_WORK_STRUCT_LINKED = 1 << Model0_WORK_STRUCT_LINKED_BIT,



 Model0_WORK_STRUCT_STATIC = 0,


 /*
	 * The last color is no color used for works which don't
	 * participate in workqueue flushing.
	 */
 Model0_WORK_NR_COLORS = (1 << Model0_WORK_STRUCT_COLOR_BITS) - 1,
 Model0_WORK_NO_COLOR = Model0_WORK_NR_COLORS,

 /* not bound to any CPU, prefer the local CPU */
 Model0_WORK_CPU_UNBOUND = 64,

 /*
	 * Reserve 7 bits off of pwq pointer w/ debugobjects turned off.
	 * This makes pwqs aligned to 256 bytes and allows 15 workqueue
	 * flush colors.
	 */
 Model0_WORK_STRUCT_FLAG_BITS = Model0_WORK_STRUCT_COLOR_SHIFT +
      Model0_WORK_STRUCT_COLOR_BITS,

 /* data contains off-queue information when !WORK_STRUCT_PWQ */
 Model0_WORK_OFFQ_FLAG_BASE = Model0_WORK_STRUCT_COLOR_SHIFT,

 Model0___WORK_OFFQ_CANCELING = Model0_WORK_OFFQ_FLAG_BASE,
 Model0_WORK_OFFQ_CANCELING = (1 << Model0___WORK_OFFQ_CANCELING),

 /*
	 * When a work item is off queue, its high bits point to the last
	 * pool it was on.  Cap at 31 bits and use the highest number to
	 * indicate that no pool is associated.
	 */
 Model0_WORK_OFFQ_FLAG_BITS = 1,
 Model0_WORK_OFFQ_POOL_SHIFT = Model0_WORK_OFFQ_FLAG_BASE + Model0_WORK_OFFQ_FLAG_BITS,
 Model0_WORK_OFFQ_LEFT = 64 - Model0_WORK_OFFQ_POOL_SHIFT,
 Model0_WORK_OFFQ_POOL_BITS = Model0_WORK_OFFQ_LEFT <= 31 ? Model0_WORK_OFFQ_LEFT : 31,
 Model0_WORK_OFFQ_POOL_NONE = (1LU << Model0_WORK_OFFQ_POOL_BITS) - 1,

 /* convenience constants */
 Model0_WORK_STRUCT_FLAG_MASK = (1UL << Model0_WORK_STRUCT_FLAG_BITS) - 1,
 Model0_WORK_STRUCT_WQ_DATA_MASK = ~Model0_WORK_STRUCT_FLAG_MASK,
 Model0_WORK_STRUCT_NO_POOL = (unsigned long)Model0_WORK_OFFQ_POOL_NONE << Model0_WORK_OFFQ_POOL_SHIFT,

 /* bit mask for work_busy() return values */
 Model0_WORK_BUSY_PENDING = 1 << 0,
 Model0_WORK_BUSY_RUNNING = 1 << 1,

 /* maximum string length for set_worker_desc() */
 Model0_WORKER_DESC_LEN = 24,
};

struct Model0_work_struct {
 Model0_atomic_long_t Model0_data;
 struct Model0_list_head Model0_entry;
 Model0_work_func_t func;



};





struct Model0_delayed_work {
 struct Model0_work_struct Model0_work;
 struct Model0_timer_list Model0_timer;

 /* target workqueue and CPU ->timer uses to queue ->work */
 struct Model0_workqueue_struct *Model0_wq;
 int Model0_cpu;
};

/*
 * A struct for workqueue attributes.  This can be used to change
 * attributes of an unbound workqueue.
 *
 * Unlike other fields, ->no_numa isn't a property of a worker_pool.  It
 * only modifies how apply_workqueue_attrs() select pools and thus doesn't
 * participate in pool hash calculations or equality comparisons.
 */
struct Model0_workqueue_attrs {
 int Model0_nice; /* nice level */
 Model0_cpumask_var_t Model0_cpumask; /* allowed CPUs */
 bool Model0_no_numa; /* disable NUMA affinity */
};

static inline __attribute__((no_instrument_function)) struct Model0_delayed_work *Model0_to_delayed_work(struct Model0_work_struct *Model0_work)
{
 return ({ const typeof( ((struct Model0_delayed_work *)0)->Model0_work ) *Model0___mptr = (Model0_work); (struct Model0_delayed_work *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_delayed_work, Model0_work) );});
}

struct Model0_execute_work {
 struct Model0_work_struct Model0_work;
};
static inline __attribute__((no_instrument_function)) void Model0___init_work(struct Model0_work_struct *Model0_work, int Model0_onstack) { }
static inline __attribute__((no_instrument_function)) void Model0_destroy_work_on_stack(struct Model0_work_struct *Model0_work) { }
static inline __attribute__((no_instrument_function)) void Model0_destroy_delayed_work_on_stack(struct Model0_delayed_work *Model0_work) { }
static inline __attribute__((no_instrument_function)) unsigned int Model0_work_static(struct Model0_work_struct *Model0_work) { return 0; }


/*
 * initialize all of a work item in one go
 *
 * NOTE! No point in using "atomic_long_set()": using a direct
 * assignment of the work data initializer allows the compiler
 * to generate better code.
 */
/**
 * work_pending - Find out whether a work item is currently pending
 * @work: The work item in question
 */



/**
 * delayed_work_pending - Find out whether a delayable work item is currently
 * pending
 * @w: The work item in question
 */



/*
 * Workqueue flags and constants.  For details, please refer to
 * Documentation/workqueue.txt.
 */
enum {
 Model0_WQ_UNBOUND = 1 << 1, /* not bound to any cpu */
 Model0_WQ_FREEZABLE = 1 << 2, /* freeze during suspend */
 Model0_WQ_MEM_RECLAIM = 1 << 3, /* may be used for memory reclaim */
 Model0_WQ_HIGHPRI = 1 << 4, /* high priority */
 Model0_WQ_CPU_INTENSIVE = 1 << 5, /* cpu intensive workqueue */
 Model0_WQ_SYSFS = 1 << 6, /* visible in sysfs, see wq_sysfs_register() */

 /*
	 * Per-cpu workqueues are generally preferred because they tend to
	 * show better performance thanks to cache locality.  Per-cpu
	 * workqueues exclude the scheduler from choosing the CPU to
	 * execute the worker threads, which has an unfortunate side effect
	 * of increasing power consumption.
	 *
	 * The scheduler considers a CPU idle if it doesn't have any task
	 * to execute and tries to keep idle cores idle to conserve power;
	 * however, for example, a per-cpu work item scheduled from an
	 * interrupt handler on an idle CPU will force the scheduler to
	 * excute the work item on that CPU breaking the idleness, which in
	 * turn may lead to more scheduling choices which are sub-optimal
	 * in terms of power consumption.
	 *
	 * Workqueues marked with WQ_POWER_EFFICIENT are per-cpu by default
	 * but become unbound if workqueue.power_efficient kernel param is
	 * specified.  Per-cpu workqueues which are identified to
	 * contribute significantly to power-consumption are identified and
	 * marked with this flag and enabling the power_efficient mode
	 * leads to noticeable power saving at the cost of small
	 * performance disadvantage.
	 *
	 * http://thread.gmane.org/gmane.linux.kernel/1480396
	 */
 Model0_WQ_POWER_EFFICIENT = 1 << 7,

 Model0___WQ_DRAINING = 1 << 16, /* internal: workqueue is draining */
 Model0___WQ_ORDERED = 1 << 17, /* internal: workqueue is ordered */
 Model0___WQ_LEGACY = 1 << 18, /* internal: create*_workqueue() */

 Model0_WQ_MAX_ACTIVE = 512, /* I like 512, better ideas? */
 Model0_WQ_MAX_UNBOUND_PER_CPU = 4, /* 4 * #cpus for unbound wq */
 Model0_WQ_DFL_ACTIVE = Model0_WQ_MAX_ACTIVE / 2,
};

/* unbound wq's aren't per-cpu, scale max_active according to #cpus */



/*
 * System-wide workqueues which are always present.
 *
 * system_wq is the one used by schedule[_delayed]_work[_on]().
 * Multi-CPU multi-threaded.  There are users which expect relatively
 * short queue flush time.  Don't queue works which can run for too
 * long.
 *
 * system_highpri_wq is similar to system_wq but for work items which
 * require WQ_HIGHPRI.
 *
 * system_long_wq is similar to system_wq but may host long running
 * works.  Queue flushing might take relatively long.
 *
 * system_unbound_wq is unbound workqueue.  Workers are not bound to
 * any specific CPU, not concurrency managed, and all queued works are
 * executed immediately as long as max_active limit is not reached and
 * resources are available.
 *
 * system_freezable_wq is equivalent to system_wq except that it's
 * freezable.
 *
 * *_power_efficient_wq are inclined towards saving power and converted
 * into WQ_UNBOUND variants if 'wq_power_efficient' is enabled; otherwise,
 * they are same as their non-power-efficient counterparts - e.g.
 * system_power_efficient_wq is identical to system_wq if
 * 'wq_power_efficient' is disabled.  See WQ_POWER_EFFICIENT for more info.
 */
extern struct Model0_workqueue_struct *Model0_system_wq;
extern struct Model0_workqueue_struct *Model0_system_highpri_wq;
extern struct Model0_workqueue_struct *Model0_system_long_wq;
extern struct Model0_workqueue_struct *Model0_system_unbound_wq;
extern struct Model0_workqueue_struct *Model0_system_freezable_wq;
extern struct Model0_workqueue_struct *Model0_system_power_efficient_wq;
extern struct Model0_workqueue_struct *Model0_system_freezable_power_efficient_wq;

extern struct Model0_workqueue_struct *
Model0___alloc_workqueue_key(const char *Model0_fmt, unsigned int Model0_flags, int Model0_max_active,
 struct Model0_lock_class_key *Model0_key, const char *Model0_lock_name, ...) __attribute__((format(printf, 1, 6)));

/**
 * alloc_workqueue - allocate a workqueue
 * @fmt: printf format for the name of the workqueue
 * @flags: WQ_* flags
 * @max_active: max in-flight work items, 0 for default
 * @args...: args for @fmt
 *
 * Allocate a workqueue with the specified parameters.  For detailed
 * information on WQ_* flags, please refer to Documentation/workqueue.txt.
 *
 * The __lock_name macro dance is to guarantee that single lock_class_key
 * doesn't end up with different namesm, which isn't allowed by lockdep.
 *
 * RETURNS:
 * Pointer to the allocated workqueue on success, %NULL on failure.
 */
/**
 * alloc_ordered_workqueue - allocate an ordered workqueue
 * @fmt: printf format for the name of the workqueue
 * @flags: WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)
 * @args...: args for @fmt
 *
 * Allocate an ordered workqueue.  An ordered workqueue executes at
 * most one work item at any given time in the queued order.  They are
 * implemented as unbound workqueues with @max_active of one.
 *
 * RETURNS:
 * Pointer to the allocated workqueue on success, %NULL on failure.
 */
extern void Model0_destroy_workqueue(struct Model0_workqueue_struct *Model0_wq);

struct Model0_workqueue_attrs *Model0_alloc_workqueue_attrs(Model0_gfp_t Model0_gfp_mask);
void Model0_free_workqueue_attrs(struct Model0_workqueue_attrs *Model0_attrs);
int Model0_apply_workqueue_attrs(struct Model0_workqueue_struct *Model0_wq,
     const struct Model0_workqueue_attrs *Model0_attrs);
int Model0_workqueue_set_unbound_cpumask(Model0_cpumask_var_t Model0_cpumask);

extern bool Model0_queue_work_on(int Model0_cpu, struct Model0_workqueue_struct *Model0_wq,
   struct Model0_work_struct *Model0_work);
extern bool Model0_queue_delayed_work_on(int Model0_cpu, struct Model0_workqueue_struct *Model0_wq,
   struct Model0_delayed_work *Model0_work, unsigned long Model0_delay);
extern bool Model0_mod_delayed_work_on(int Model0_cpu, struct Model0_workqueue_struct *Model0_wq,
   struct Model0_delayed_work *Model0_dwork, unsigned long Model0_delay);

extern void Model0_flush_workqueue(struct Model0_workqueue_struct *Model0_wq);
extern void Model0_drain_workqueue(struct Model0_workqueue_struct *Model0_wq);

extern int Model0_schedule_on_each_cpu(Model0_work_func_t func);

int Model0_execute_in_process_context(Model0_work_func_t Model0_fn, struct Model0_execute_work *);

extern bool Model0_flush_work(struct Model0_work_struct *Model0_work);
extern bool Model0_cancel_work_sync(struct Model0_work_struct *Model0_work);

extern bool Model0_flush_delayed_work(struct Model0_delayed_work *Model0_dwork);
extern bool Model0_cancel_delayed_work(struct Model0_delayed_work *Model0_dwork);
extern bool Model0_cancel_delayed_work_sync(struct Model0_delayed_work *Model0_dwork);

extern void Model0_workqueue_set_max_active(struct Model0_workqueue_struct *Model0_wq,
         int Model0_max_active);
extern bool Model0_current_is_workqueue_rescuer(void);
extern bool Model0_workqueue_congested(int Model0_cpu, struct Model0_workqueue_struct *Model0_wq);
extern unsigned int Model0_work_busy(struct Model0_work_struct *Model0_work);
extern __attribute__((format(printf, 1, 2))) void Model0_set_worker_desc(const char *Model0_fmt, ...);
extern void Model0_print_worker_info(const char *Model0_log_lvl, struct Model0_task_struct *Model0_task);
extern void Model0_show_workqueue_state(void);

/**
 * queue_work - queue work on a workqueue
 * @wq: workqueue to use
 * @work: work to queue
 *
 * Returns %false if @work was already on a queue, %true otherwise.
 *
 * We queue the work to the CPU on which it was submitted, but if the CPU dies
 * it can be processed by another CPU.
 */
static inline __attribute__((no_instrument_function)) bool Model0_queue_work(struct Model0_workqueue_struct *Model0_wq,
         struct Model0_work_struct *Model0_work)
{
 return Model0_queue_work_on(Model0_WORK_CPU_UNBOUND, Model0_wq, Model0_work);
}

/**
 * queue_delayed_work - queue work on a workqueue after delay
 * @wq: workqueue to use
 * @dwork: delayable work to queue
 * @delay: number of jiffies to wait before queueing
 *
 * Equivalent to queue_delayed_work_on() but tries to use the local CPU.
 */
static inline __attribute__((no_instrument_function)) bool Model0_queue_delayed_work(struct Model0_workqueue_struct *Model0_wq,
          struct Model0_delayed_work *Model0_dwork,
          unsigned long Model0_delay)
{
 return Model0_queue_delayed_work_on(Model0_WORK_CPU_UNBOUND, Model0_wq, Model0_dwork, Model0_delay);
}

/**
 * mod_delayed_work - modify delay of or queue a delayed work
 * @wq: workqueue to use
 * @dwork: work to queue
 * @delay: number of jiffies to wait before queueing
 *
 * mod_delayed_work_on() on local CPU.
 */
static inline __attribute__((no_instrument_function)) bool Model0_mod_delayed_work(struct Model0_workqueue_struct *Model0_wq,
        struct Model0_delayed_work *Model0_dwork,
        unsigned long Model0_delay)
{
 return Model0_mod_delayed_work_on(Model0_WORK_CPU_UNBOUND, Model0_wq, Model0_dwork, Model0_delay);
}

/**
 * schedule_work_on - put work task on a specific cpu
 * @cpu: cpu to put the work task on
 * @work: job to be done
 *
 * This puts a job on a specific cpu
 */
static inline __attribute__((no_instrument_function)) bool Model0_schedule_work_on(int Model0_cpu, struct Model0_work_struct *Model0_work)
{
 return Model0_queue_work_on(Model0_cpu, Model0_system_wq, Model0_work);
}

/**
 * schedule_work - put work task in global workqueue
 * @work: job to be done
 *
 * Returns %false if @work was already on the kernel-global workqueue and
 * %true otherwise.
 *
 * This puts a job in the kernel-global workqueue if it was not already
 * queued and leaves it in the same position on the kernel-global
 * workqueue otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_schedule_work(struct Model0_work_struct *Model0_work)
{
 return Model0_queue_work(Model0_system_wq, Model0_work);
}

/**
 * flush_scheduled_work - ensure that any scheduled work has run to completion.
 *
 * Forces execution of the kernel-global workqueue and blocks until its
 * completion.
 *
 * Think twice before calling this function!  It's very easy to get into
 * trouble if you don't take great care.  Either of the following situations
 * will lead to deadlock:
 *
 *	One of the work items currently on the workqueue needs to acquire
 *	a lock held by your code or its caller.
 *
 *	Your code is running in the context of a work routine.
 *
 * They will be detected by lockdep when they occur, but the first might not
 * occur very often.  It depends on what work items are on the workqueue and
 * what locks they need, which you have no control over.
 *
 * In most situations flushing the entire workqueue is overkill; you merely
 * need to know that a particular work item isn't queued and isn't running.
 * In such cases you should use cancel_delayed_work_sync() or
 * cancel_work_sync() instead.
 */
static inline __attribute__((no_instrument_function)) void Model0_flush_scheduled_work(void)
{
 Model0_flush_workqueue(Model0_system_wq);
}

/**
 * schedule_delayed_work_on - queue work in global workqueue on CPU after delay
 * @cpu: cpu to use
 * @dwork: job to be done
 * @delay: number of jiffies to wait
 *
 * After waiting for a given time this puts a job in the kernel-global
 * workqueue on the specified CPU.
 */
static inline __attribute__((no_instrument_function)) bool Model0_schedule_delayed_work_on(int Model0_cpu, struct Model0_delayed_work *Model0_dwork,
         unsigned long Model0_delay)
{
 return Model0_queue_delayed_work_on(Model0_cpu, Model0_system_wq, Model0_dwork, Model0_delay);
}

/**
 * schedule_delayed_work - put work task in global workqueue after delay
 * @dwork: job to be done
 * @delay: number of jiffies to wait or 0 for immediate execution
 *
 * After waiting for a given time this puts a job in the kernel-global
 * workqueue.
 */
static inline __attribute__((no_instrument_function)) bool Model0_schedule_delayed_work(struct Model0_delayed_work *Model0_dwork,
      unsigned long Model0_delay)
{
 return Model0_queue_delayed_work(Model0_system_wq, Model0_dwork, Model0_delay);
}

/**
 * keventd_up - is workqueue initialized yet?
 */
static inline __attribute__((no_instrument_function)) bool Model0_keventd_up(void)
{
 return Model0_system_wq != ((void *)0);
}







long Model0_work_on_cpu(int Model0_cpu, long (*Model0_fn)(void *), void *Model0_arg);



extern void Model0_freeze_workqueues_begin(void);
extern bool Model0_freeze_workqueues_busy(void);
extern void Model0_thaw_workqueues(void);



int Model0_workqueue_sysfs_register(struct Model0_workqueue_struct *Model0_wq);
static inline __attribute__((no_instrument_function)) void Model0_wq_watchdog_touch(int Model0_cpu) { }



int Model0_workqueue_prepare_cpu(unsigned int Model0_cpu);
int Model0_workqueue_online_cpu(unsigned int Model0_cpu);
int Model0_workqueue_offline_cpu(unsigned int Model0_cpu);

struct Model0_srcu_struct_array {
 unsigned long Model0_c[2];
 unsigned long Model0_seq[2];
};

struct Model0_rcu_batch {
 struct Model0_callback_head *Model0_head, **Model0_tail;
};



struct Model0_srcu_struct {
 unsigned long Model0_completed;
 struct Model0_srcu_struct_array *Model0_per_cpu_ref;
 Model0_spinlock_t Model0_queue_lock; /* protect ->batch_queue, ->running */
 bool Model0_running;
 /* callbacks just queued */
 struct Model0_rcu_batch Model0_batch_queue;
 /* callbacks try to do the first check_zero */
 struct Model0_rcu_batch Model0_batch_check0;
 /* callbacks done with the first check_zero and the flip */
 struct Model0_rcu_batch Model0_batch_check1;
 struct Model0_rcu_batch Model0_batch_done;
 struct Model0_delayed_work Model0_work;



};
int Model0_init_srcu_struct(struct Model0_srcu_struct *Model0_sp);




void Model0_process_srcu(struct Model0_work_struct *Model0_work);
/*
 * Define and initialize a srcu struct at build time.
 * Do -not- call init_srcu_struct() nor cleanup_srcu_struct() on it.
 *
 * Note that although DEFINE_STATIC_SRCU() hides the name from other
 * files, the per-CPU variable rules nevertheless require that the
 * chosen name be globally unique.  These rules also prohibit use of
 * DEFINE_STATIC_SRCU() within a function.  If these rules are too
 * restrictive, declare the srcu_struct manually.  For example, in
 * each file:
 *
 *	static struct srcu_struct my_srcu;
 *
 * Then, before the first use of each my_srcu, manually initialize it:
 *
 *	init_srcu_struct(&my_srcu);
 *
 * See include/linux/percpu-defs.h for the rules on per-CPU variables.
 */






/**
 * call_srcu() - Queue a callback for invocation after an SRCU grace period
 * @sp: srcu_struct in queue the callback
 * @head: structure to be used for queueing the SRCU callback.
 * @func: function to be invoked after the SRCU grace period
 *
 * The callback function will be invoked some time after a full SRCU
 * grace period elapses, in other words after all pre-existing SRCU
 * read-side critical sections have completed.  However, the callback
 * function might well execute concurrently with other SRCU read-side
 * critical sections that started after call_srcu() was invoked.  SRCU
 * read-side critical sections are delimited by srcu_read_lock() and
 * srcu_read_unlock(), and may be nested.
 *
 * The callback will be invoked from process context, but must nevertheless
 * be fast and must not block.
 */
void Model0_call_srcu(struct Model0_srcu_struct *Model0_sp, struct Model0_callback_head *Model0_head,
  void (*func)(struct Model0_callback_head *Model0_head));

void Model0_cleanup_srcu_struct(struct Model0_srcu_struct *Model0_sp);
int Model0___srcu_read_lock(struct Model0_srcu_struct *Model0_sp) ;
void Model0___srcu_read_unlock(struct Model0_srcu_struct *Model0_sp, int Model0_idx) ;
void Model0_synchronize_srcu(struct Model0_srcu_struct *Model0_sp);
void Model0_synchronize_srcu_expedited(struct Model0_srcu_struct *Model0_sp);
unsigned long Model0_srcu_batches_completed(struct Model0_srcu_struct *Model0_sp);
void Model0_srcu_barrier(struct Model0_srcu_struct *Model0_sp);
static inline __attribute__((no_instrument_function)) int Model0_srcu_read_lock_held(struct Model0_srcu_struct *Model0_sp)
{
 return 1;
}



/**
 * srcu_dereference_check - fetch SRCU-protected pointer for later dereferencing
 * @p: the pointer to fetch and protect for later dereferencing
 * @sp: pointer to the srcu_struct, which is used to check that we
 *	really are in an SRCU read-side critical section.
 * @c: condition to check for update-side use
 *
 * If PROVE_RCU is enabled, invoking this outside of an RCU read-side
 * critical section will result in an RCU-lockdep splat, unless @c evaluates
 * to 1.  The @c argument will normally be a logical expression containing
 * lockdep_is_held() calls.
 */



/**
 * srcu_dereference - fetch SRCU-protected pointer for later dereferencing
 * @p: the pointer to fetch and protect for later dereferencing
 * @sp: pointer to the srcu_struct, which is used to check that we
 *	really are in an SRCU read-side critical section.
 *
 * Makes rcu_dereference_check() do the dirty work.  If PROVE_RCU
 * is enabled, invoking this outside of an RCU read-side critical
 * section will result in an RCU-lockdep splat.
 */


/**
 * srcu_read_lock - register a new reader for an SRCU-protected structure.
 * @sp: srcu_struct in which to register the new reader.
 *
 * Enter an SRCU read-side critical section.  Note that SRCU read-side
 * critical sections may be nested.  However, it is illegal to
 * call anything that waits on an SRCU grace period for the same
 * srcu_struct, whether directly or indirectly.  Please note that
 * one way to indirectly wait on an SRCU grace period is to acquire
 * a mutex that is held elsewhere while calling synchronize_srcu() or
 * synchronize_srcu_expedited().
 *
 * Note that srcu_read_lock() and the matching srcu_read_unlock() must
 * occur in the same context, for example, it is illegal to invoke
 * srcu_read_unlock() in an irq handler if the matching srcu_read_lock()
 * was invoked in process context.
 */
static inline __attribute__((no_instrument_function)) int Model0_srcu_read_lock(struct Model0_srcu_struct *Model0_sp)
{
 int Model0_retval;

 __asm__ __volatile__("": : :"memory");
 Model0_retval = Model0___srcu_read_lock(Model0_sp);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 return Model0_retval;
}

/**
 * srcu_read_unlock - unregister a old reader from an SRCU-protected structure.
 * @sp: srcu_struct in which to unregister the old reader.
 * @idx: return value from corresponding srcu_read_lock().
 *
 * Exit an SRCU read-side critical section.
 */
static inline __attribute__((no_instrument_function)) void Model0_srcu_read_unlock(struct Model0_srcu_struct *Model0_sp, int Model0_idx)

{
 do { } while (0);
 Model0___srcu_read_unlock(Model0_sp, Model0_idx);
}

/**
 * smp_mb__after_srcu_read_unlock - ensure full ordering after srcu_read_unlock
 *
 * Converts the preceding srcu_read_unlock into a two-way memory barrier.
 *
 * Call this after srcu_read_unlock, to guarantee that all memory operations
 * that occur after smp_mb__after_srcu_read_unlock will appear to happen after
 * the preceding srcu_read_unlock.
 */
static inline __attribute__((no_instrument_function)) void Model0_smp_mb__after_srcu_read_unlock(void)
{
 /* __srcu_read_unlock has smp_mb() internally so nothing to do here. */
}

/*
 * Notifier chains are of four types:
 *
 *	Atomic notifier chains: Chain callbacks run in interrupt/atomic
 *		context. Callouts are not allowed to block.
 *	Blocking notifier chains: Chain callbacks run in process context.
 *		Callouts are allowed to block.
 *	Raw notifier chains: There are no restrictions on callbacks,
 *		registration, or unregistration.  All locking and protection
 *		must be provided by the caller.
 *	SRCU notifier chains: A variant of blocking notifier chains, with
 *		the same restrictions.
 *
 * atomic_notifier_chain_register() may be called from an atomic context,
 * but blocking_notifier_chain_register() and srcu_notifier_chain_register()
 * must be called from a process context.  Ditto for the corresponding
 * _unregister() routines.
 *
 * atomic_notifier_chain_unregister(), blocking_notifier_chain_unregister(),
 * and srcu_notifier_chain_unregister() _must not_ be called from within
 * the call chain.
 *
 * SRCU notifier chains are an alternative form of blocking notifier chains.
 * They use SRCU (Sleepable Read-Copy Update) instead of rw-semaphores for
 * protection of the chain links.  This means there is _very_ low overhead
 * in srcu_notifier_call_chain(): no cache bounces and no memory barriers.
 * As compensation, srcu_notifier_chain_unregister() is rather expensive.
 * SRCU notifier chains should be used when the chain will be called very
 * often but notifier_blocks will seldom be removed.  Also, SRCU notifier
 * chains are slightly more difficult to use because they require special
 * runtime initialization.
 */

struct Model0_notifier_block;

typedef int (*Model0_notifier_fn_t)(struct Model0_notifier_block *Model0_nb,
   unsigned long Model0_action, void *Model0_data);

struct Model0_notifier_block {
 Model0_notifier_fn_t Model0_notifier_call;
 struct Model0_notifier_block *Model0_next;
 int Model0_priority;
};

struct Model0_atomic_notifier_head {
 Model0_spinlock_t Model0_lock;
 struct Model0_notifier_block *Model0_head;
};

struct Model0_blocking_notifier_head {
 struct Model0_rw_semaphore Model0_rwsem;
 struct Model0_notifier_block *Model0_head;
};

struct Model0_raw_notifier_head {
 struct Model0_notifier_block *Model0_head;
};

struct Model0_srcu_notifier_head {
 struct Model0_mutex Model0_mutex;
 struct Model0_srcu_struct Model0_srcu;
 struct Model0_notifier_block *Model0_head;
};
/* srcu_notifier_heads must be initialized and cleaned up dynamically */
extern void Model0_srcu_init_notifier_head(struct Model0_srcu_notifier_head *Model0_nh);
/* srcu_notifier_heads cannot be initialized statically */
extern int Model0_atomic_notifier_chain_register(struct Model0_atomic_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);
extern int Model0_blocking_notifier_chain_register(struct Model0_blocking_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);
extern int Model0_raw_notifier_chain_register(struct Model0_raw_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);
extern int Model0_srcu_notifier_chain_register(struct Model0_srcu_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);

extern int Model0_blocking_notifier_chain_cond_register(
  struct Model0_blocking_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);

extern int Model0_atomic_notifier_chain_unregister(struct Model0_atomic_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);
extern int Model0_blocking_notifier_chain_unregister(struct Model0_blocking_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);
extern int Model0_raw_notifier_chain_unregister(struct Model0_raw_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);
extern int Model0_srcu_notifier_chain_unregister(struct Model0_srcu_notifier_head *Model0_nh,
  struct Model0_notifier_block *Model0_nb);

extern int Model0_atomic_notifier_call_chain(struct Model0_atomic_notifier_head *Model0_nh,
  unsigned long Model0_val, void *Model0_v);
extern int Model0___atomic_notifier_call_chain(struct Model0_atomic_notifier_head *Model0_nh,
 unsigned long Model0_val, void *Model0_v, int Model0_nr_to_call, int *Model0_nr_calls);
extern int Model0_blocking_notifier_call_chain(struct Model0_blocking_notifier_head *Model0_nh,
  unsigned long Model0_val, void *Model0_v);
extern int Model0___blocking_notifier_call_chain(struct Model0_blocking_notifier_head *Model0_nh,
 unsigned long Model0_val, void *Model0_v, int Model0_nr_to_call, int *Model0_nr_calls);
extern int Model0_raw_notifier_call_chain(struct Model0_raw_notifier_head *Model0_nh,
  unsigned long Model0_val, void *Model0_v);
extern int Model0___raw_notifier_call_chain(struct Model0_raw_notifier_head *Model0_nh,
 unsigned long Model0_val, void *Model0_v, int Model0_nr_to_call, int *Model0_nr_calls);
extern int Model0_srcu_notifier_call_chain(struct Model0_srcu_notifier_head *Model0_nh,
  unsigned long Model0_val, void *Model0_v);
extern int Model0___srcu_notifier_call_chain(struct Model0_srcu_notifier_head *Model0_nh,
 unsigned long Model0_val, void *Model0_v, int Model0_nr_to_call, int *Model0_nr_calls);





      /* Bad/Veto action */
/*
 * Clean way to return from the notifier and stop further calls.
 */


/* Encapsulate (negative) errno value (in particular, NOTIFY_BAD <=> EPERM). */
static inline __attribute__((no_instrument_function)) int Model0_notifier_from_errno(int err)
{
 if (err)
  return 0x8000 | (0x0001 - err);

 return 0x0001;
}

/* Restore (negative) errno value from notify return value. */
static inline __attribute__((no_instrument_function)) int Model0_notifier_to_errno(int Model0_ret)
{
 Model0_ret &= ~0x8000;
 return Model0_ret > 0x0001 ? 0x0001 - Model0_ret : 0;
}

/*
 *	Declared notifiers so far. I can imagine quite a few more chains
 *	over time (eg laptop power reset chains, reboot chain (to clean 
 *	device units up), device [un]mount chain, module load/unload chain,
 *	low memory chain, screenblank chain (for plug in modular screenblankers) 
 *	VC switch chains (for loadable kernel svgalib VC switch helpers) etc...
 */

/* CPU notfiers are defined in include/linux/cpu.h. */

/* netdevice notifiers are defined in include/linux/netdevice.h */

/* reboot notifiers are defined in include/linux/reboot.h. */

/* Hibernation and suspend events are defined in include/linux/suspend.h. */

/* Virtual Terminal events are defined in include/linux/vt.h. */



/* Console keyboard events.
 * Note: KBD_KEYCODE is always sent before KBD_UNBOUND_KEYCODE, KBD_UNICODE and
 * KBD_KEYSYM. */






extern struct Model0_blocking_notifier_head Model0_reboot_notifier_list;


struct Model0_page;
struct Model0_zone;
struct Model0_pglist_data;
struct Model0_mem_section;
struct Model0_memory_block;
struct Model0_resource;
/*
 * Stub functions for when hotplug is off
 */
static inline __attribute__((no_instrument_function)) void Model0_pgdat_resize_lock(struct Model0_pglist_data *Model0_p, unsigned long *Model0_f) {}
static inline __attribute__((no_instrument_function)) void Model0_pgdat_resize_unlock(struct Model0_pglist_data *Model0_p, unsigned long *Model0_f) {}
static inline __attribute__((no_instrument_function)) void Model0_pgdat_resize_init(struct Model0_pglist_data *Model0_pgdat) {}

static inline __attribute__((no_instrument_function)) unsigned Model0_zone_span_seqbegin(struct Model0_zone *Model0_zone)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model0_zone_span_seqretry(struct Model0_zone *Model0_zone, unsigned Model0_iv)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) void Model0_zone_span_writelock(struct Model0_zone *Model0_zone) {}
static inline __attribute__((no_instrument_function)) void Model0_zone_span_writeunlock(struct Model0_zone *Model0_zone) {}
static inline __attribute__((no_instrument_function)) void Model0_zone_seqlock_init(struct Model0_zone *Model0_zone) {}

static inline __attribute__((no_instrument_function)) int Model0_mhp_notimplemented(const char *func)
{
 Model0_printk("\001" "4" "%s() called, with CONFIG_MEMORY_HOTPLUG disabled\n", func);
 Model0_dump_stack();
 return -38;
}

static inline __attribute__((no_instrument_function)) void Model0_register_page_bootmem_info_node(struct Model0_pglist_data *Model0_pgdat)
{
}

static inline __attribute__((no_instrument_function)) int Model0_try_online_node(int Model0_nid)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_get_online_mems(void) {}
static inline __attribute__((no_instrument_function)) void Model0_put_online_mems(void) {}

static inline __attribute__((no_instrument_function)) void Model0_mem_hotplug_begin(void) {}
static inline __attribute__((no_instrument_function)) void Model0_mem_hotplug_done(void) {}
static inline __attribute__((no_instrument_function)) bool Model0_is_mem_section_removable(unsigned long Model0_pfn,
     unsigned long Model0_nr_pages)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model0_try_offline_node(int Model0_nid) {}

static inline __attribute__((no_instrument_function)) int Model0_offline_pages(unsigned long Model0_start_pfn, unsigned long Model0_nr_pages)
{
 return -22;
}

static inline __attribute__((no_instrument_function)) void Model0_remove_memory(int Model0_nid, Model0_u64 Model0_start, Model0_u64 Model0_size) {}


extern int Model0_walk_memory_range(unsigned long Model0_start_pfn, unsigned long Model0_end_pfn,
  void *Model0_arg, int (*func)(struct Model0_memory_block *, void *));
extern int Model0_add_memory(int Model0_nid, Model0_u64 Model0_start, Model0_u64 Model0_size);
extern int Model0_add_memory_resource(int Model0_nid, struct Model0_resource *Model0_resource, bool Model0_online);
extern int Model0_zone_for_memory(int Model0_nid, Model0_u64 Model0_start, Model0_u64 Model0_size, int Model0_zone_default,
  bool Model0_for_device);
extern int Model0_arch_add_memory(int Model0_nid, Model0_u64 Model0_start, Model0_u64 Model0_size, bool Model0_for_device);
extern int Model0_offline_pages(unsigned long Model0_start_pfn, unsigned long Model0_nr_pages);
extern bool Model0_is_memblock_offlined(struct Model0_memory_block *Model0_mem);
extern void Model0_remove_memory(int Model0_nid, Model0_u64 Model0_start, Model0_u64 Model0_size);
extern int Model0_sparse_add_one_section(struct Model0_zone *Model0_zone, unsigned long Model0_start_pfn);
extern void Model0_sparse_remove_one_section(struct Model0_zone *Model0_zone, struct Model0_mem_section *Model0_ms,
  unsigned long Model0_map_offset);
extern struct Model0_page *Model0_sparse_decode_mem_map(unsigned long Model0_coded_mem_map,
       unsigned long Model0_pnum);
extern int Model0_zone_can_shift(unsigned long Model0_pfn, unsigned long Model0_nr_pages,
     enum Model0_zone_type Model0_target);

extern struct Model0_mutex Model0_zonelists_mutex;
void Model0_build_all_zonelists(Model0_pg_data_t *Model0_pgdat, struct Model0_zone *Model0_zone);
void Model0_wakeup_kswapd(struct Model0_zone *Model0_zone, int Model0_order, enum Model0_zone_type Model0_classzone_idx);
bool Model0___zone_watermark_ok(struct Model0_zone *Model0_z, unsigned int Model0_order, unsigned long Model0_mark,
    int Model0_classzone_idx, unsigned int Model0_alloc_flags,
    long Model0_free_pages);
bool Model0_zone_watermark_ok(struct Model0_zone *Model0_z, unsigned int Model0_order,
  unsigned long Model0_mark, int Model0_classzone_idx,
  unsigned int Model0_alloc_flags);
bool Model0_zone_watermark_ok_safe(struct Model0_zone *Model0_z, unsigned int Model0_order,
  unsigned long Model0_mark, int Model0_classzone_idx);
enum Model0_memmap_context {
 Model0_MEMMAP_EARLY,
 Model0_MEMMAP_HOTPLUG,
};
extern int Model0_init_currently_empty_zone(struct Model0_zone *Model0_zone, unsigned long Model0_start_pfn,
         unsigned long Model0_size);

extern void Model0_lruvec_init(struct Model0_lruvec *Model0_lruvec);

static inline __attribute__((no_instrument_function)) struct Model0_pglist_data *Model0_lruvec_pgdat(struct Model0_lruvec *Model0_lruvec)
{



 return ({ const typeof( ((struct Model0_pglist_data *)0)->Model0_lruvec ) *Model0___mptr = (Model0_lruvec); (struct Model0_pglist_data *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_pglist_data, Model0_lruvec) );});

}

extern unsigned long Model0_lruvec_lru_size(struct Model0_lruvec *Model0_lruvec, enum Model0_lru_list Model0_lru);


void Model0_memory_present(int Model0_nid, unsigned long Model0_start, unsigned long Model0_end);







static inline __attribute__((no_instrument_function)) int Model0_local_memory_node(int Model0_node_id) { return Model0_node_id; };






/*
 * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.
 */


/*
 * Returns true if a zone has pages managed by the buddy allocator.
 * All the reclaim decisions have to use this function rather than
 * populated_zone(). If the whole zone is reserved then we can easily
 * end up with populated_zone() && !managed_zone().
 */
static inline __attribute__((no_instrument_function)) bool Model0_managed_zone(struct Model0_zone *Model0_zone)
{
 return Model0_zone->Model0_managed_pages;
}

/* Returns true if a zone has memory */
static inline __attribute__((no_instrument_function)) bool Model0_populated_zone(struct Model0_zone *Model0_zone)
{
 return Model0_zone->Model0_present_pages;
}

extern int Model0_movable_zone;
static inline __attribute__((no_instrument_function)) int Model0_is_highmem_idx(enum Model0_zone_type Model0_idx)
{




 return 0;

}

/**
 * is_highmem - helper function to quickly check if a struct zone is a 
 *              highmem zone or not.  This is an attempt to keep references
 *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.
 * @zone - pointer to struct zone variable
 */
static inline __attribute__((no_instrument_function)) int Model0_is_highmem(struct Model0_zone *Model0_zone)
{



 return 0;

}

/* These two functions are used to setup the per zone pages min values */
struct Model0_ctl_table;
int Model0_min_free_kbytes_sysctl_handler(struct Model0_ctl_table *, int,
     void *, Model0_size_t *, Model0_loff_t *);
int Model0_watermark_scale_factor_sysctl_handler(struct Model0_ctl_table *, int,
     void *, Model0_size_t *, Model0_loff_t *);
extern int Model0_sysctl_lowmem_reserve_ratio[4 -1];
int Model0_lowmem_reserve_ratio_sysctl_handler(struct Model0_ctl_table *, int,
     void *, Model0_size_t *, Model0_loff_t *);
int Model0_percpu_pagelist_fraction_sysctl_handler(struct Model0_ctl_table *, int,
     void *, Model0_size_t *, Model0_loff_t *);
int Model0_sysctl_min_unmapped_ratio_sysctl_handler(struct Model0_ctl_table *, int,
   void *, Model0_size_t *, Model0_loff_t *);
int Model0_sysctl_min_slab_ratio_sysctl_handler(struct Model0_ctl_table *, int,
   void *, Model0_size_t *, Model0_loff_t *);

extern int Model0_numa_zonelist_order_handler(struct Model0_ctl_table *, int,
   void *, Model0_size_t *, Model0_loff_t *);
extern char Model0_numa_zonelist_order[];



/* K8 NUMA support */
/* Copyright 2002,2003 by Andi Kleen, SuSE Labs */
/* 2.5 Version loosely based on the NUMAQ Code by Pat Gaughen. */













/*
 * We need the APIC definitions automatically as part of 'smp.h'
 */









/*
 * Structure definitions for SMP machines following the
 * Intel Multiprocessing Specification 1.1 and 1.4.
 */

/*
 * This tag identifies where the SMP configuration
 * information is.
 */







/* Intel MP Floating Pointer Structure */
struct Model0_mpf_intel {
 char Model0_signature[4]; /* "_MP_"			*/
 unsigned int Model0_physptr; /* Configuration table address	*/
 unsigned char Model0_length; /* Our length (paragraphs)	*/
 unsigned char Model0_specification; /* Specification version	*/
 unsigned char Model0_checksum; /* Checksum (makes sum 0)	*/
 unsigned char Model0_feature1; /* Standard or configuration ?	*/
 unsigned char Model0_feature2; /* Bit7 set for IMCR|PIC	*/
 unsigned char Model0_feature3; /* Unused (0)			*/
 unsigned char Model0_feature4; /* Unused (0)			*/
 unsigned char Model0_feature5; /* Unused (0)			*/
};



struct Model0_mpc_table {
 char Model0_signature[4];
 unsigned short Model0_length; /* Size of table */
 char Model0_spec; /* 0x01 */
 char Model0_checksum;
 char Model0_oem[8];
 char Model0_productid[12];
 unsigned int Model0_oemptr; /* 0 if not present */
 unsigned short Model0_oemsize; /* 0 if not present */
 unsigned short Model0_oemcount;
 unsigned int Model0_lapic; /* APIC address */
 unsigned int Model0_reserved;
};

/* Followed by entries */






/* Used by IBM NUMA-Q to describe node locality */
struct Model0_mpc_cpu {
 unsigned char Model0_type;
 unsigned char Model0_apicid; /* Local APIC number */
 unsigned char Model0_apicver; /* Its versions */
 unsigned char Model0_cpuflag;
 unsigned int Model0_cpufeature;
 unsigned int Model0_featureflag; /* CPUID feature value */
 unsigned int Model0_reserved[2];
};

struct Model0_mpc_bus {
 unsigned char Model0_type;
 unsigned char Model0_busid;
 unsigned char Model0_bustype[6];
};

/* List of Bus Type string values, Intel MP Spec. */
struct Model0_mpc_ioapic {
 unsigned char Model0_type;
 unsigned char Model0_apicid;
 unsigned char Model0_apicver;
 unsigned char Model0_flags;
 unsigned int Model0_apicaddr;
};

struct Model0_mpc_intsrc {
 unsigned char Model0_type;
 unsigned char Model0_irqtype;
 unsigned short Model0_irqflag;
 unsigned char Model0_srcbus;
 unsigned char Model0_srcbusirq;
 unsigned char Model0_dstapic;
 unsigned char Model0_dstirq;
};

enum Model0_mp_irq_source_types {
 Model0_mp_INT = 0,
 Model0_mp_NMI = 1,
 Model0_mp_SMI = 2,
 Model0_mp_ExtINT = 3
};







struct Model0_mpc_lintsrc {
 unsigned char Model0_type;
 unsigned char Model0_irqtype;
 unsigned short Model0_irqflag;
 unsigned char Model0_srcbusid;
 unsigned char Model0_srcbusirq;
 unsigned char Model0_destapic;
 unsigned char Model0_destapiclint;
};



struct Model0_mpc_oemtable {
 char Model0_signature[4];
 unsigned short Model0_length; /* Size of table */
 char Model0_rev; /* 0x01 */
 char Model0_checksum;
 char Model0_mpc[8];
};

/*
 *	Default configurations
 *
 *	1	2 CPU ISA 82489DX
 *	2	2 CPU EISA 82489DX neither IRQ 0 timer nor IRQ 13 DMA chaining
 *	3	2 CPU EISA 82489DX
 *	4	2 CPU MCA 82489DX
 *	5	2 CPU ISA+PCI
 *	6	2 CPU EISA+PCI
 *	7	2 CPU MCA+PCI
 */

enum Model0_mp_bustype {
 Model0_MP_BUS_ISA = 1,
 Model0_MP_BUS_EISA,
 Model0_MP_BUS_PCI,
};






/* setup_data types */






/* ram_size flags */




/* loadflags */






/* xloadflags */








/*
 * These are set up by the setup-routine at boot-time:
 */

struct Model0_screen_info {
 __u8 Model0_orig_x; /* 0x00 */
 __u8 Model0_orig_y; /* 0x01 */
 Model0___u16 Model0_ext_mem_k; /* 0x02 */
 Model0___u16 Model0_orig_video_page; /* 0x04 */
 __u8 Model0_orig_video_mode; /* 0x06 */
 __u8 Model0_orig_video_cols; /* 0x07 */
 __u8 Model0_flags; /* 0x08 */
 __u8 Model0_unused2; /* 0x09 */
 Model0___u16 Model0_orig_video_ega_bx;/* 0x0a */
 Model0___u16 Model0_unused3; /* 0x0c */
 __u8 Model0_orig_video_lines; /* 0x0e */
 __u8 Model0_orig_video_isVGA; /* 0x0f */
 Model0___u16 Model0_orig_video_points;/* 0x10 */

 /* VESA graphic mode -- linear frame buffer */
 Model0___u16 Model0_lfb_width; /* 0x12 */
 Model0___u16 Model0_lfb_height; /* 0x14 */
 Model0___u16 Model0_lfb_depth; /* 0x16 */
 __u32 Model0_lfb_base; /* 0x18 */
 __u32 Model0_lfb_size; /* 0x1c */
 Model0___u16 Model0_cl_magic, Model0_cl_offset; /* 0x20 */
 Model0___u16 Model0_lfb_linelength; /* 0x24 */
 __u8 Model0_red_size; /* 0x26 */
 __u8 Model0_red_pos; /* 0x27 */
 __u8 Model0_green_size; /* 0x28 */
 __u8 Model0_green_pos; /* 0x29 */
 __u8 Model0_blue_size; /* 0x2a */
 __u8 Model0_blue_pos; /* 0x2b */
 __u8 Model0_rsvd_size; /* 0x2c */
 __u8 Model0_rsvd_pos; /* 0x2d */
 Model0___u16 Model0_vesapm_seg; /* 0x2e */
 Model0___u16 Model0_vesapm_off; /* 0x30 */
 Model0___u16 Model0_pages; /* 0x32 */
 Model0___u16 Model0_vesa_attributes; /* 0x34 */
 __u32 Model0_capabilities; /* 0x36 */
 __u32 Model0_ext_lfb_base; /* 0x3a */
 __u8 Model0__reserved[2]; /* 0x3e */
} __attribute__((packed));

extern struct Model0_screen_info Model0_screen_info;
/*
 * Include file for the interface to an APM BIOS
 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */




/*
 * Include file for the interface to an APM BIOS
 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */






typedef unsigned short Model0_apm_event_t;
typedef unsigned short Model0_apm_eventinfo_t;

struct Model0_apm_bios_info {
 Model0___u16 Model0_version;
 Model0___u16 Model0_cseg;
 __u32 Model0_offset;
 Model0___u16 Model0_cseg_16;
 Model0___u16 Model0_dseg;
 Model0___u16 Model0_flags;
 Model0___u16 Model0_cseg_len;
 Model0___u16 Model0_cseg_16_len;
 Model0___u16 Model0_dseg_len;
};


/*
 * Power states
 */
/*
 * Events (results of Get PM Event)
 */
/*
 * Error codes
 */
/*
 * APM Device IDs
 */
/*
 * Battery status
 */


/*
 * APM defined capability bit flags
 */
/*
 * ioctl operations
 */






/* Results of APM Installation Check */






/*
 * Data for APM that is persistent across module unload/load
 */
struct Model0_apm_info {
 struct Model0_apm_bios_info Model0_bios;
 unsigned short Model0_connection_version;
 int Model0_get_power_status_broken;
 int Model0_get_power_status_swabinminutes;
 int Model0_allow_ints;
 int Model0_forbid_idle;
 int Model0_realmode_power_off;
 int Model0_disabled;
};

/*
 * The APM function codes
 */
/*
 * Function code for APM_FUNC_RESUME_TIMER
 */




/*
 * Function code for APM_FUNC_RESUME_ON_RING
 */




/*
 * Function code for APM_FUNC_TIMER_STATUS
 */




/*
 * in arch/i386/kernel/setup.c
 */
extern struct Model0_apm_info Model0_apm_info;

/*
 * This is the "All Devices" ID communicated to the BIOS
 */
/*
 * linux/include/linux/edd.h
 *  Copyright (C) 2002, 2003, 2004 Dell Inc.
 *  by Matt Domsch <Matt_Domsch@dell.com>
 *
 * structures and definitions for the int 13h, ax={41,48}h
 * BIOS Enhanced Disk Drive Services
 * This is based on the T13 group document D1572 Revision 0 (August 14 2002)
 * available at http://www.t13.org/docs2002/d1572r0.pdf.  It is
 * very similar to D1484 Revision 3 http://www.t13.org/docs2002/d1484r3.pdf
 *
 * In a nutshell, arch/{i386,x86_64}/boot/setup.S populates a scratch
 * table in the boot_params that contains a list of BIOS-enumerated
 * boot devices.
 * In arch/{i386,x86_64}/kernel/setup.c, this information is
 * transferred into the edd structure, and in drivers/firmware/edd.c, that
 * information is used to identify BIOS boot disk.  The code in setup.S
 * is very sensitive to the size of these structures.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License v2.0 as published by
 * the Free Software Foundation
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */




/*
 * linux/include/linux/edd.h
 *  Copyright (C) 2002, 2003, 2004 Dell Inc.
 *  by Matt Domsch <Matt_Domsch@dell.com>
 *
 * structures and definitions for the int 13h, ax={41,48}h
 * BIOS Enhanced Disk Drive Services
 * This is based on the T13 group document D1572 Revision 0 (August 14 2002)
 * available at http://www.t13.org/docs2002/d1572r0.pdf.  It is
 * very similar to D1484 Revision 3 http://www.t13.org/docs2002/d1484r3.pdf
 *
 * In a nutshell, arch/{i386,x86_64}/boot/setup.S populates a scratch
 * table in the boot_params that contains a list of BIOS-enumerated
 * boot devices.
 * In arch/{i386,x86_64}/kernel/setup.c, this information is
 * transferred into the edd structure, and in drivers/firmware/edd.c, that
 * information is used to identify BIOS boot disk.  The code in setup.S
 * is very sensitive to the size of these structures.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License v2.0 as published by
 * the Free Software Foundation
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */
struct Model0_edd_device_params {
 Model0___u16 Model0_length;
 Model0___u16 Model0_info_flags;
 __u32 Model0_num_default_cylinders;
 __u32 Model0_num_default_heads;
 __u32 Model0_sectors_per_track;
 __u64 Model0_number_of_sectors;
 Model0___u16 Model0_bytes_per_sector;
 __u32 Model0_dpte_ptr; /* 0xFFFFFFFF for our purposes */
 Model0___u16 Model0_key; /* = 0xBEDD */
 __u8 Model0_device_path_info_length; /* = 44 */
 __u8 Model0_reserved2;
 Model0___u16 Model0_reserved3;
 __u8 Model0_host_bus_type[4];
 __u8 Model0_interface_type[8];
 union {
  struct {
   Model0___u16 Model0_base_address;
   Model0___u16 Model0_reserved1;
   __u32 Model0_reserved2;
  } __attribute__ ((packed)) Model0_isa;
  struct {
   __u8 Model0_bus;
   __u8 Model0_slot;
   __u8 Model0_function;
   __u8 Model0_channel;
   __u32 Model0_reserved;
  } __attribute__ ((packed)) Model0_pci;
  /* pcix is same as pci */
  struct {
   __u64 Model0_reserved;
  } __attribute__ ((packed)) Model0_ibnd;
  struct {
   __u64 Model0_reserved;
  } __attribute__ ((packed)) Model0_xprs;
  struct {
   __u64 Model0_reserved;
  } __attribute__ ((packed)) Model0_htpt;
  struct {
   __u64 Model0_reserved;
  } __attribute__ ((packed)) Model0_unknown;
 } Model0_interface_path;
 union {
  struct {
   __u8 Model0_device;
   __u8 Model0_reserved1;
   Model0___u16 Model0_reserved2;
   __u32 Model0_reserved3;
   __u64 Model0_reserved4;
  } __attribute__ ((packed)) Model0_ata;
  struct {
   __u8 Model0_device;
   __u8 Model0_lun;
   __u8 Model0_reserved1;
   __u8 Model0_reserved2;
   __u32 Model0_reserved3;
   __u64 Model0_reserved4;
  } __attribute__ ((packed)) Model0_atapi;
  struct {
   Model0___u16 Model0_id;
   __u64 Model0_lun;
   Model0___u16 Model0_reserved1;
   __u32 Model0_reserved2;
  } __attribute__ ((packed)) Model0_scsi;
  struct {
   __u64 Model0_serial_number;
   __u64 Model0_reserved;
  } __attribute__ ((packed)) Model0_usb;
  struct {
   __u64 Model0_eui;
   __u64 Model0_reserved;
  } __attribute__ ((packed)) Model0_i1394;
  struct {
   __u64 Model0_wwid;
   __u64 Model0_lun;
  } __attribute__ ((packed)) Model0_fibre;
  struct {
   __u64 Model0_identity_tag;
   __u64 Model0_reserved;
  } __attribute__ ((packed)) Model0_i2o;
  struct {
   __u32 Model0_array_number;
   __u32 Model0_reserved1;
   __u64 Model0_reserved2;
  } __attribute__ ((packed)) Model0_raid;
  struct {
   __u8 Model0_device;
   __u8 Model0_reserved1;
   Model0___u16 Model0_reserved2;
   __u32 Model0_reserved3;
   __u64 Model0_reserved4;
  } __attribute__ ((packed)) Model0_sata;
  struct {
   __u64 Model0_reserved1;
   __u64 Model0_reserved2;
  } __attribute__ ((packed)) Model0_unknown;
 } Model0_device_path;
 __u8 Model0_reserved4;
 __u8 Model0_checksum;
} __attribute__ ((packed));

struct Model0_edd_info {
 __u8 Model0_device;
 __u8 Model0_version;
 Model0___u16 Model0_interface_support;
 Model0___u16 Model0_legacy_max_cylinder;
 __u8 Model0_legacy_max_head;
 __u8 Model0_legacy_sectors_per_track;
 struct Model0_edd_device_params Model0_params;
} __attribute__ ((packed));

struct Model0_edd {
 unsigned int Model0_mbr_signature[16];
 struct Model0_edd_info Model0_edd_info[6];
 unsigned char Model0_mbr_signature_nr;
 unsigned char Model0_edd_info_nr;
};


extern struct Model0_edd Model0_edd;





/*
 * Legacy E820 BIOS limits us to 128 (E820MAX) nodes due to the
 * constrained space in the zeropage.  If we have more nodes than
 * that, and if we've booted off EFI firmware, then the EFI tables
 * passed us from the EFI firmware can list more nodes.  Size our
 * internal memory map tables to have room for these additional
 * nodes, based on up to three entries per node for which the
 * kernel was built: MAX_NUMNODES == (1 << CONFIG_NODES_SHIFT),
 * plus E820MAX, allowing space for the possible duplicate E820
 * entries that might need room in the same arrays, prior to the
 * call to sanitize_e820_map() to remove duplicates.  The allowance
 * of three memory map entries per node is "enough" entries for
 * the initial hardware platform motivating this mechanism to make
 * use of additional EFI map entries.  Future platforms may want
 * to allow more than three entries per node or otherwise refine
 * this size.
 */
/*
 * This is a non-standardized way to represent ADR or NVDIMM regions that
 * persist over a reboot.  The kernel will ignore their special capabilities
 * unless the CONFIG_X86_PMEM_LEGACY option is set.
 *
 * ( Note that older platforms also used 6 for the same type of memory,
 *   but newer versions switched to 12 as 6 was assigned differently.  Some
 *   time they will learn... )
 */


/*
 * reserved RAM used by kernel itself
 * if CONFIG_INTEL_TXT is enabled, memory of this type will be
 * included in the S3 integrity calculation and so should not include
 * any memory that BIOS might alter over the S3 transition
 */




struct Model0_e820entry {
 __u64 Model0_addr; /* start of memory segment */
 __u64 Model0_size; /* size of memory segment */
 __u32 Model0_type; /* type of memory segment */
} __attribute__((packed));

struct Model0_e820map {
 __u32 Model0_nr_map;
 struct Model0_e820entry Model0_map[(128 + 3 * (1 << 6))];
};

/* see comment in arch/x86/kernel/e820.c */
extern struct Model0_e820map Model0_e820;
extern struct Model0_e820map Model0_e820_saved;

extern unsigned long Model0_pci_mem_start;
extern int Model0_e820_any_mapped(Model0_u64 Model0_start, Model0_u64 Model0_end, unsigned Model0_type);
extern int Model0_e820_all_mapped(Model0_u64 Model0_start, Model0_u64 Model0_end, unsigned Model0_type);
extern void Model0_e820_add_region(Model0_u64 Model0_start, Model0_u64 Model0_size, int Model0_type);
extern void Model0_e820_print_map(char *Model0_who);
extern int
Model0_sanitize_e820_map(struct Model0_e820entry *Model0_biosmap, int Model0_max_nr_map, Model0_u32 *Model0_pnr_map);
extern Model0_u64 Model0_e820_update_range(Model0_u64 Model0_start, Model0_u64 Model0_size, unsigned Model0_old_type,
          unsigned Model0_new_type);
extern Model0_u64 Model0_e820_remove_range(Model0_u64 Model0_start, Model0_u64 Model0_size, unsigned Model0_old_type,
        int Model0_checktype);
extern void Model0_update_e820(void);
extern void Model0_e820_setup_gap(void);
extern int Model0_e820_search_gap(unsigned long *Model0_gapstart, unsigned long *Model0_gapsize,
   unsigned long Model0_start_addr, unsigned long long Model0_end_addr);
struct Model0_setup_data;
extern void Model0_parse_e820_ext(Model0_u64 Model0_phys_addr, Model0_u32 Model0_data_len);



extern void Model0_e820_mark_nosave_regions(unsigned long Model0_limit_pfn);






extern unsigned long Model0_e820_end_of_ram_pfn(void);
extern unsigned long Model0_e820_end_of_low_ram_pfn(void);
extern Model0_u64 Model0_early_reserve_e820(Model0_u64 Model0_sizet, Model0_u64 Model0_align);

void Model0_memblock_x86_fill(void);
void Model0_memblock_find_dma_reserve(void);

extern void Model0_finish_e820_parsing(void);
extern void Model0_e820_reserve_resources(void);
extern void Model0_e820_reserve_resources_late(void);
extern void Model0_setup_memory_map(void);
extern char *Model0_default_machine_specific_memory_setup(void);

/*
 * Returns true iff the specified range [s,e) is completely contained inside
 * the ISA region.
 */
static inline __attribute__((no_instrument_function)) bool Model0_is_ISA_range(Model0_u64 Model0_s, Model0_u64 Model0_e)
{
 return Model0_s >= 0xa0000 && Model0_e <= 0x100000;
}



/*
 * ioport.h	Definitions of routines for detecting, reserving and
 *		allocating system resources.
 *
 * Authors:	Linus Torvalds
 */







/*
 * Resources are tree-like, allowing
 * nesting etc..
 */
struct Model0_resource {
 Model0_resource_size_t Model0_start;
 Model0_resource_size_t Model0_end;
 const char *Model0_name;
 unsigned long Model0_flags;
 unsigned long Model0_desc;
 struct Model0_resource *Model0_parent, *Model0_sibling, *Model0_child;
};

/*
 * IO resources have these defined flags.
 *
 * PCI devices expose these flags to userspace in the "resource" sysfs file,
 * so don't move them.
 */
/* I/O resource extended types */


/* PnP IRQ specific bits (IORESOURCE_BITS) */







/* PnP DMA specific bits (IORESOURCE_BITS) */
/* PnP memory I/O specific bits (IORESOURCE_BITS) */
/* PnP I/O specific bits (IORESOURCE_BITS) */




/* PCI ROM control bits (IORESOURCE_BITS) */



/* PCI control bits.  Shares IORESOURCE_BITS with above PCI ROM.  */



/*
 * I/O Resource Descriptors
 *
 * Descriptors are used by walk_iomem_res_desc() and region_intersects()
 * for searching a specific resource range in the iomem table.  Assign
 * a new descriptor when a resource range supports the search interfaces.
 * Otherwise, resource.desc must be set to IORES_DESC_NONE (0).
 */
enum {
 Model0_IORES_DESC_NONE = 0,
 Model0_IORES_DESC_CRASH_KERNEL = 1,
 Model0_IORES_DESC_ACPI_TABLES = 2,
 Model0_IORES_DESC_ACPI_NV_STORAGE = 3,
 Model0_IORES_DESC_PERSISTENT_MEMORY = 4,
 Model0_IORES_DESC_PERSISTENT_MEMORY_LEGACY = 5,
};

/* helpers to define resources */
/* PC/ISA/whatever - the normal PC address spaces: IO and memory */
extern struct Model0_resource Model0_ioport_resource;
extern struct Model0_resource Model0_iomem_resource;

extern struct Model0_resource *Model0_request_resource_conflict(struct Model0_resource *Model0_root, struct Model0_resource *Model0_new);
extern int Model0_request_resource(struct Model0_resource *Model0_root, struct Model0_resource *Model0_new);
extern int Model0_release_resource(struct Model0_resource *Model0_new);
void Model0_release_child_resources(struct Model0_resource *Model0_new);
extern void Model0_reserve_region_with_split(struct Model0_resource *Model0_root,
        Model0_resource_size_t Model0_start, Model0_resource_size_t Model0_end,
        const char *Model0_name);
extern struct Model0_resource *Model0_insert_resource_conflict(struct Model0_resource *Model0_parent, struct Model0_resource *Model0_new);
extern int Model0_insert_resource(struct Model0_resource *Model0_parent, struct Model0_resource *Model0_new);
extern void Model0_insert_resource_expand_to_fit(struct Model0_resource *Model0_root, struct Model0_resource *Model0_new);
extern int Model0_remove_resource(struct Model0_resource *old);
extern void Model0_arch_remove_reservations(struct Model0_resource *Model0_avail);
extern int Model0_allocate_resource(struct Model0_resource *Model0_root, struct Model0_resource *Model0_new,
        Model0_resource_size_t Model0_size, Model0_resource_size_t Model0_min,
        Model0_resource_size_t Model0_max, Model0_resource_size_t Model0_align,
        Model0_resource_size_t (*Model0_alignf)(void *,
             const struct Model0_resource *,
             Model0_resource_size_t,
             Model0_resource_size_t),
        void *Model0_alignf_data);
struct Model0_resource *Model0_lookup_resource(struct Model0_resource *Model0_root, Model0_resource_size_t Model0_start);
int Model0_adjust_resource(struct Model0_resource *Model0_res, Model0_resource_size_t Model0_start,
      Model0_resource_size_t Model0_size);
Model0_resource_size_t Model0_resource_alignment(struct Model0_resource *Model0_res);
static inline __attribute__((no_instrument_function)) Model0_resource_size_t Model0_resource_size(const struct Model0_resource *Model0_res)
{
 return Model0_res->Model0_end - Model0_res->Model0_start + 1;
}
static inline __attribute__((no_instrument_function)) unsigned long Model0_resource_type(const struct Model0_resource *Model0_res)
{
 return Model0_res->Model0_flags & 0x00001f00;
}
static inline __attribute__((no_instrument_function)) unsigned long Model0_resource_ext_type(const struct Model0_resource *Model0_res)
{
 return Model0_res->Model0_flags & 0x01000000;
}
/* True iff r1 completely contains r2 */
static inline __attribute__((no_instrument_function)) bool Model0_resource_contains(struct Model0_resource *Model0_r1, struct Model0_resource *Model0_r2)
{
 if (Model0_resource_type(Model0_r1) != Model0_resource_type(Model0_r2))
  return false;
 if (Model0_r1->Model0_flags & 0x20000000 || Model0_r2->Model0_flags & 0x20000000)
  return false;
 return Model0_r1->Model0_start <= Model0_r2->Model0_start && Model0_r1->Model0_end >= Model0_r2->Model0_end;
}


/* Convenience shorthand with allocation */
extern struct Model0_resource * Model0___request_region(struct Model0_resource *,
     Model0_resource_size_t Model0_start,
     Model0_resource_size_t Model0_n,
     const char *Model0_name, int Model0_flags);

/* Compatibility cruft */



extern void Model0___release_region(struct Model0_resource *, Model0_resource_size_t,
    Model0_resource_size_t);





/* Wrappers for managed devices */
struct Model0_device;

extern int Model0_devm_request_resource(struct Model0_device *Model0_dev, struct Model0_resource *Model0_root,
     struct Model0_resource *Model0_new);
extern void Model0_devm_release_resource(struct Model0_device *Model0_dev, struct Model0_resource *Model0_new);






extern struct Model0_resource * Model0___devm_request_region(struct Model0_device *Model0_dev,
    struct Model0_resource *Model0_parent, Model0_resource_size_t Model0_start,
    Model0_resource_size_t Model0_n, const char *Model0_name);






extern void Model0___devm_release_region(struct Model0_device *Model0_dev, struct Model0_resource *Model0_parent,
      Model0_resource_size_t Model0_start, Model0_resource_size_t Model0_n);
extern int Model0_iomem_map_sanity_check(Model0_resource_size_t Model0_addr, unsigned long Model0_size);
extern int Model0_iomem_is_exclusive(Model0_u64 Model0_addr);

extern int
Model0_walk_system_ram_range(unsigned long Model0_start_pfn, unsigned long Model0_nr_pages,
  void *Model0_arg, int (*func)(unsigned long, unsigned long, void *));
extern int
Model0_walk_system_ram_res(Model0_u64 Model0_start, Model0_u64 Model0_end, void *Model0_arg,
      int (*func)(Model0_u64, Model0_u64, void *));
extern int
Model0_walk_iomem_res_desc(unsigned long Model0_desc, unsigned long Model0_flags, Model0_u64 Model0_start, Model0_u64 Model0_end,
      void *Model0_arg, int (*func)(Model0_u64, Model0_u64, void *));

/* True if any part of r1 overlaps r2 */
static inline __attribute__((no_instrument_function)) bool Model0_resource_overlaps(struct Model0_resource *Model0_r1, struct Model0_resource *Model0_r2)
{
       return (Model0_r1->Model0_start <= Model0_r2->Model0_end && Model0_r1->Model0_end >= Model0_r2->Model0_start);
}
/*
 * Include file for the interface to IST BIOS
 * Copyright 2002 Andy Grover <andrew.grover@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */




/*
 * Include file for the interface to IST BIOS
 * Copyright 2002 Andy Grover <andrew.grover@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */







struct Model0_ist_info {
 __u32 Model0_signature;
 __u32 Model0_command;
 __u32 Model0_event;
 __u32 Model0_perf_level;
};


extern struct Model0_ist_info Model0_ist_info;






struct Model0_edid_info {
 unsigned char Model0_dummy[128];
};


extern struct Model0_edid_info Model0_edid_info;

/* extensible setup data list node */
struct Model0_setup_data {
 __u64 Model0_next;
 __u32 Model0_type;
 __u32 Model0_len;
 __u8 Model0_data[0];
};

struct Model0_setup_header {
 __u8 Model0_setup_sects;
 Model0___u16 Model0_root_flags;
 __u32 Model0_syssize;
 Model0___u16 Model0_ram_size;
 Model0___u16 Model0_vid_mode;
 Model0___u16 Model0_root_dev;
 Model0___u16 Model0_boot_flag;
 Model0___u16 Model0_jump;
 __u32 Model0_header;
 Model0___u16 Model0_version;
 __u32 Model0_realmode_swtch;
 Model0___u16 Model0_start_sys;
 Model0___u16 Model0_kernel_version;
 __u8 Model0_type_of_loader;
 __u8 Model0_loadflags;
 Model0___u16 Model0_setup_move_size;
 __u32 Model0_code32_start;
 __u32 Model0_ramdisk_image;
 __u32 Model0_ramdisk_size;
 __u32 Model0_bootsect_kludge;
 Model0___u16 Model0_heap_end_ptr;
 __u8 Model0_ext_loader_ver;
 __u8 Model0_ext_loader_type;
 __u32 Model0_cmd_line_ptr;
 __u32 Model0_initrd_addr_max;
 __u32 Model0_kernel_alignment;
 __u8 Model0_relocatable_kernel;
 __u8 Model0_min_alignment;
 Model0___u16 Model0_xloadflags;
 __u32 Model0_cmdline_size;
 __u32 Model0_hardware_subarch;
 __u64 Model0_hardware_subarch_data;
 __u32 Model0_payload_offset;
 __u32 Model0_payload_length;
 __u64 Model0_setup_data;
 __u64 Model0_pref_address;
 __u32 Model0_init_size;
 __u32 Model0_handover_offset;
} __attribute__((packed));

struct Model0_sys_desc_table {
 Model0___u16 Model0_length;
 __u8 Model0_table[14];
};

/* Gleaned from OFW's set-parameters in cpu/x86/pc/linux.fth */
struct Model0_olpc_ofw_header {
 __u32 Model0_ofw_magic; /* OFW signature */
 __u32 Model0_ofw_version;
 __u32 Model0_cif_handler; /* callback into OFW */
 __u32 Model0_irq_desc_table;
} __attribute__((packed));

struct Model0_efi_info {
 __u32 Model0_efi_loader_signature;
 __u32 Model0_efi_systab;
 __u32 Model0_efi_memdesc_size;
 __u32 Model0_efi_memdesc_version;
 __u32 Model0_efi_memmap;
 __u32 Model0_efi_memmap_size;
 __u32 Model0_efi_systab_hi;
 __u32 Model0_efi_memmap_hi;
};

/* The so-called "zeropage" */
struct Model0_boot_params {
 struct Model0_screen_info Model0_screen_info; /* 0x000 */
 struct Model0_apm_bios_info Model0_apm_bios_info; /* 0x040 */
 __u8 Model0__pad2[4]; /* 0x054 */
 __u64 Model0_tboot_addr; /* 0x058 */
 struct Model0_ist_info Model0_ist_info; /* 0x060 */
 __u8 Model0__pad3[16]; /* 0x070 */
 __u8 Model0_hd0_info[16]; /* obsolete! */ /* 0x080 */
 __u8 Model0_hd1_info[16]; /* obsolete! */ /* 0x090 */
 struct Model0_sys_desc_table Model0_sys_desc_table; /* obsolete! */ /* 0x0a0 */
 struct Model0_olpc_ofw_header Model0_olpc_ofw_header; /* 0x0b0 */
 __u32 Model0_ext_ramdisk_image; /* 0x0c0 */
 __u32 Model0_ext_ramdisk_size; /* 0x0c4 */
 __u32 Model0_ext_cmd_line_ptr; /* 0x0c8 */
 __u8 Model0__pad4[116]; /* 0x0cc */
 struct Model0_edid_info Model0_edid_info; /* 0x140 */
 struct Model0_efi_info Model0_efi_info; /* 0x1c0 */
 __u32 Model0_alt_mem_k; /* 0x1e0 */
 __u32 Model0_scratch; /* Scratch field! */ /* 0x1e4 */
 __u8 Model0_e820_entries; /* 0x1e8 */
 __u8 Model0_eddbuf_entries; /* 0x1e9 */
 __u8 Model0_edd_mbr_sig_buf_entries; /* 0x1ea */
 __u8 Model0_kbd_status; /* 0x1eb */
 __u8 Model0__pad5[3]; /* 0x1ec */
 /*
	 * The sentinel is set to a nonzero value (0xff) in header.S.
	 *
	 * A bootloader is supposed to only take setup_header and put
	 * it into a clean boot_params buffer. If it turns out that
	 * it is clumsy or too generous with the buffer, it most
	 * probably will pick up the sentinel variable too. The fact
	 * that this variable then is still 0xff will let kernel
	 * know that some variables in boot_params are invalid and
	 * kernel should zero out certain portions of boot_params.
	 */
 __u8 Model0_sentinel; /* 0x1ef */
 __u8 Model0__pad6[1]; /* 0x1f0 */
 struct Model0_setup_header Model0_hdr; /* setup header */ /* 0x1f1 */
 __u8 Model0__pad7[0x290-0x1f1-sizeof(struct Model0_setup_header)];
 __u32 Model0_edd_mbr_sig_buffer[16]; /* 0x290 */
 struct Model0_e820entry Model0_e820_map[128]; /* 0x2d0 */
 __u8 Model0__pad8[48]; /* 0xcd0 */
 struct Model0_edd_info Model0_eddbuf[6]; /* 0xd00 */
 __u8 Model0__pad9[276]; /* 0xeec */
} __attribute__((packed));

/**
 * enum x86_hardware_subarch - x86 hardware subarchitecture
 *
 * The x86 hardware_subarch and hardware_subarch_data were added as of the x86
 * boot protocol 2.07 to help distinguish and support custom x86 boot
 * sequences. This enum represents accepted values for the x86
 * hardware_subarch.  Custom x86 boot sequences (not X86_SUBARCH_PC) do not
 * have or simply *cannot* make use of natural stubs like BIOS or EFI, the
 * hardware_subarch can be used on the Linux entry path to revector to a
 * subarchitecture stub when needed. This subarchitecture stub can be used to
 * set up Linux boot parameters or for special care to account for nonstandard
 * handling of page tables.
 *
 * These enums should only ever be used by x86 code, and the code that uses
 * it should be well contained and compartamentalized.
 *
 * KVM and Xen HVM do not have a subarch as these are expected to follow
 * standard x86 boot entries. If there is a genuine need for "hypervisor" type
 * that should be considered separately in the future. Future guest types
 * should seriously consider working with standard x86 boot stubs such as
 * the BIOS or EFI boot stubs.
 *
 * WARNING: this enum is only used for legacy hacks, for platform features that
 *	    are not easily enumerated or discoverable. You should not ever use
 *	    this for new features.
 *
 * @X86_SUBARCH_PC: Should be used if the hardware is enumerable using standard
 *	PC mechanisms (PCI, ACPI) and doesn't need a special boot flow.
 * @X86_SUBARCH_LGUEST: Used for x86 hypervisor demo, lguest
 * @X86_SUBARCH_XEN: Used for Xen guest types which follow the PV boot path,
 * 	which start at asm startup_xen() entry point and later jump to the C
 * 	xen_start_kernel() entry point. Both domU and dom0 type of guests are
 * 	currently supportd through this PV boot path.
 * @X86_SUBARCH_INTEL_MID: Used for Intel MID (Mobile Internet Device) platform
 *	systems which do not have the PCI legacy interfaces.
 * @X86_SUBARCH_CE4100: Used for Intel CE media processor (CE4100) SoC for
 * 	for settop boxes and media devices, the use of a subarch for CE4100
 * 	is more of a hack...
 */
enum Model0_x86_hardware_subarch {
 Model0_X86_SUBARCH_PC = 0,
 Model0_X86_SUBARCH_LGUEST,
 Model0_X86_SUBARCH_XEN,
 Model0_X86_SUBARCH_INTEL_MID,
 Model0_X86_SUBARCH_CE4100,
 Model0_X86_NR_SUBARCHS,
};

struct Model0_mpc_bus;
struct Model0_mpc_cpu;
struct Model0_mpc_table;
struct Model0_cpuinfo_x86;

/**
 * struct x86_init_mpparse - platform specific mpparse ops
 * @mpc_record:			platform specific mpc record accounting
 * @setup_ioapic_ids:		platform specific ioapic id override
 * @mpc_apic_id:		platform specific mpc apic id assignment
 * @smp_read_mpc_oem:		platform specific oem mpc table setup
 * @mpc_oem_pci_bus:		platform specific pci bus setup (default NULL)
 * @mpc_oem_bus_info:		platform specific mpc bus info
 * @find_smp_config:		find the smp configuration
 * @get_smp_config:		get the smp configuration
 */
struct Model0_x86_init_mpparse {
 void (*Model0_mpc_record)(unsigned int Model0_mode);
 void (*Model0_setup_ioapic_ids)(void);
 int (*Model0_mpc_apic_id)(struct Model0_mpc_cpu *Model0_m);
 void (*Model0_smp_read_mpc_oem)(struct Model0_mpc_table *Model0_mpc);
 void (*Model0_mpc_oem_pci_bus)(struct Model0_mpc_bus *Model0_m);
 void (*Model0_mpc_oem_bus_info)(struct Model0_mpc_bus *Model0_m, char *Model0_name);
 void (*Model0_find_smp_config)(void);
 void (*Model0_get_smp_config)(unsigned int Model0_early);
};

/**
 * struct x86_init_resources - platform specific resource related ops
 * @probe_roms:			probe BIOS roms
 * @reserve_resources:		reserve the standard resources for the
 *				platform
 * @memory_setup:		platform specific memory setup
 *
 */
struct Model0_x86_init_resources {
 void (*Model0_probe_roms)(void);
 void (*Model0_reserve_resources)(void);
 char *(*Model0_memory_setup)(void);
};

/**
 * struct x86_init_irqs - platform specific interrupt setup
 * @pre_vector_init:		init code to run before interrupt vectors
 *				are set up.
 * @intr_init:			interrupt init code
 * @trap_init:			platform specific trap setup
 */
struct Model0_x86_init_irqs {
 void (*Model0_pre_vector_init)(void);
 void (*Model0_intr_init)(void);
 void (*Model0_trap_init)(void);
};

/**
 * struct x86_init_oem - oem platform specific customizing functions
 * @arch_setup:			platform specific architecure setup
 * @banner:			print a platform specific banner
 */
struct Model0_x86_init_oem {
 void (*Model0_arch_setup)(void);
 void (*Model0_banner)(void);
};

/**
 * struct x86_init_paging - platform specific paging functions
 * @pagetable_init:	platform specific paging initialization call to setup
 *			the kernel pagetables and prepare accessors functions.
 *			Callback must call paging_init(). Called once after the
 *			direct mapping for phys memory is available.
 */
struct Model0_x86_init_paging {
 void (*Model0_pagetable_init)(void);
};

/**
 * struct x86_init_timers - platform specific timer setup
 * @setup_perpcu_clockev:	set up the per cpu clock event device for the
 *				boot cpu
 * @timer_init:			initialize the platform timer (default PIT/HPET)
 * @wallclock_init:		init the wallclock device
 */
struct Model0_x86_init_timers {
 void (*Model0_setup_percpu_clockev)(void);
 void (*Model0_timer_init)(void);
 void (*Model0_wallclock_init)(void);
};

/**
 * struct x86_init_iommu - platform specific iommu setup
 * @iommu_init:			platform specific iommu setup
 */
struct Model0_x86_init_iommu {
 int (*Model0_iommu_init)(void);
};

/**
 * struct x86_init_pci - platform specific pci init functions
 * @arch_init:			platform specific pci arch init call
 * @init:			platform specific pci subsystem init
 * @init_irq:			platform specific pci irq init
 * @fixup_irqs:			platform specific pci irq fixup
 */
struct Model0_x86_init_pci {
 int (*Model0_arch_init)(void);
 int (*Model0_init)(void);
 void (*Model0_init_irq)(void);
 void (*Model0_fixup_irqs)(void);
};

/**
 * struct x86_init_ops - functions for platform specific setup
 *
 */
struct Model0_x86_init_ops {
 struct Model0_x86_init_resources Model0_resources;
 struct Model0_x86_init_mpparse Model0_mpparse;
 struct Model0_x86_init_irqs Model0_irqs;
 struct Model0_x86_init_oem Model0_oem;
 struct Model0_x86_init_paging Model0_paging;
 struct Model0_x86_init_timers Model0_timers;
 struct Model0_x86_init_iommu Model0_iommu;
 struct Model0_x86_init_pci Model0_pci;
};

/**
 * struct x86_cpuinit_ops - platform specific cpu hotplug setups
 * @setup_percpu_clockev:	set up the per cpu clock event device
 * @early_percpu_clock_init:	early init of the per cpu clock event device
 */
struct Model0_x86_cpuinit_ops {
 void (*Model0_setup_percpu_clockev)(void);
 void (*Model0_early_percpu_clock_init)(void);
 void (*Model0_fixup_cpu_id)(struct Model0_cpuinfo_x86 *Model0_c, int Model0_node);
};

struct Model0_timespec;

/**
 * struct x86_legacy_devices - legacy x86 devices
 *
 * @pnpbios: this platform can have a PNPBIOS. If this is disabled the platform
 * 	is known to never have a PNPBIOS.
 *
 * These are devices known to require LPC or ISA bus. The definition of legacy
 * devices adheres to the ACPI 5.2.9.3 IA-PC Boot Architecture flag
 * ACPI_FADT_LEGACY_DEVICES. These devices consist of user visible devices on
 * the LPC or ISA bus. User visible devices are devices that have end-user
 * accessible connectors (for example, LPT parallel port). Legacy devices on
 * the LPC bus consist for example of serial and parallel ports, PS/2 keyboard
 * / mouse, and the floppy disk controller. A system that lacks all known
 * legacy devices can assume all devices can be detected exclusively via
 * standard device enumeration mechanisms including the ACPI namespace.
 *
 * A system which has does not have ACPI_FADT_LEGACY_DEVICES enabled must not
 * have any of the legacy devices enumerated below present.
 */
struct Model0_x86_legacy_devices {
 int Model0_pnpbios;
};

/**
 * struct x86_legacy_features - legacy x86 features
 *
 * @rtc: this device has a CMOS real-time clock present
 * @reserve_bios_regions: boot code will search for the EBDA address and the
 * 	start of the 640k - 1M BIOS region.  If false, the platform must
 * 	ensure that its memory map correctly reserves sub-1MB regions as needed.
 * @devices: legacy x86 devices, refer to struct x86_legacy_devices
 * 	documentation for further details.
 */
struct Model0_x86_legacy_features {
 int Model0_rtc;
 int Model0_reserve_bios_regions;
 struct Model0_x86_legacy_devices Model0_devices;
};

/**
 * struct x86_platform_ops - platform specific runtime functions
 * @calibrate_cpu:		calibrate CPU
 * @calibrate_tsc:		calibrate TSC, if different from CPU
 * @get_wallclock:		get time from HW clock like RTC etc.
 * @set_wallclock:		set time back to HW clock
 * @is_untracked_pat_range	exclude from PAT logic
 * @nmi_init			enable NMI on cpus
 * @i8042_detect		pre-detect if i8042 controller exists
 * @save_sched_clock_state:	save state for sched_clock() on suspend
 * @restore_sched_clock_state:	restore state for sched_clock() on resume
 * @apic_post_init:		adjust apic if neeeded
 * @legacy:			legacy features
 * @set_legacy_features:	override legacy features. Use of this callback
 * 				is highly discouraged. You should only need
 * 				this if your hardware platform requires further
 * 				custom fine tuning far beyong what may be
 * 				possible in x86_early_init_platform_quirks() by
 * 				only using the current x86_hardware_subarch
 * 				semantics.
 */
struct Model0_x86_platform_ops {
 unsigned long (*Model0_calibrate_cpu)(void);
 unsigned long (*Model0_calibrate_tsc)(void);
 void (*Model0_get_wallclock)(struct Model0_timespec *Model0_ts);
 int (*Model0_set_wallclock)(const struct Model0_timespec *Model0_ts);
 void (*Model0_iommu_shutdown)(void);
 bool (*Model0_is_untracked_pat_range)(Model0_u64 Model0_start, Model0_u64 Model0_end);
 void (*Model0_nmi_init)(void);
 unsigned char (*Model0_get_nmi_reason)(void);
 int (*Model0_i8042_detect)(void);
 void (*Model0_save_sched_clock_state)(void);
 void (*Model0_restore_sched_clock_state)(void);
 void (*Model0_apic_post_init)(void);
 struct Model0_x86_legacy_features Model0_legacy;
 void (*Model0_set_legacy_features)(void);
};

struct Model0_pci_dev;

struct Model0_x86_msi_ops {
 int (*Model0_setup_msi_irqs)(struct Model0_pci_dev *Model0_dev, int Model0_nvec, int Model0_type);
 void (*Model0_teardown_msi_irq)(unsigned int Model0_irq);
 void (*Model0_teardown_msi_irqs)(struct Model0_pci_dev *Model0_dev);
 void (*Model0_restore_msi_irqs)(struct Model0_pci_dev *Model0_dev);
};

struct Model0_x86_io_apic_ops {
 unsigned int (*Model0_read) (unsigned int Model0_apic, unsigned int Model0_reg);
 void (*Model0_disable)(void);
};

extern struct Model0_x86_init_ops Model0_x86_init;
extern struct Model0_x86_cpuinit_ops Model0_x86_cpuinit;
extern struct Model0_x86_platform_ops Model0_x86_platform;
extern struct Model0_x86_msi_ops Model0_x86_msi;
extern struct Model0_x86_io_apic_ops Model0_x86_io_apic_ops;

extern void Model0_x86_early_init_platform_quirks(void);
extern void Model0_x86_init_noop(void);
extern void Model0_x86_init_uint_noop(unsigned int unused);



/*
 * Constants for various Intel APICs. (local APIC, IOAPIC, etc.)
 *
 * Alan Cox <Alan.Cox@linux.org>, 1995.
 * Ingo Molnar <mingo@redhat.com>, 1999, 2000
 */




/*
 * This is the IO-APIC register space as specified
 * by Intel docs:
 */
/*
 * All x86-64 systems are xAPIC compatible.
 * In the following, "apicid" is a physical APIC ID.
 */
/*
 * the local APIC register structure, memory mapped. Not terribly well
 * tested, but we might eventually use this one in the future - the
 * problem why we cannot use it right now is the P5 APIC, it has an
 * errata which cannot take 8-bit reads and writes, only 32-bit ones ...
 */


struct Model0_local_apic {

/*000*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_01;

/*010*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_02;

/*020*/ struct { /* APIC ID Register */
  unsigned int Model0___reserved_1 : 24,
   Model0_phys_apic_id : 4,
   Model0___reserved_2 : 4;
  unsigned int Model0___reserved[3];
 } Model0_id;

/*030*/ const
 struct { /* APIC Version Register */
  unsigned int Model0_version : 8,
   Model0___reserved_1 : 8,
   Model0_max_lvt : 8,
   Model0___reserved_2 : 8;
  unsigned int Model0___reserved[3];
 } Model0_version;

/*040*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_03;

/*050*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_04;

/*060*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_05;

/*070*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_06;

/*080*/ struct { /* Task Priority Register */
  unsigned int Model0_priority : 8,
   Model0___reserved_1 : 24;
  unsigned int Model0___reserved_2[3];
 } Model0_tpr;

/*090*/ const
 struct { /* Arbitration Priority Register */
  unsigned int Model0_priority : 8,
   Model0___reserved_1 : 24;
  unsigned int Model0___reserved_2[3];
 } Model0_apr;

/*0A0*/ const
 struct { /* Processor Priority Register */
  unsigned int Model0_priority : 8,
   Model0___reserved_1 : 24;
  unsigned int Model0___reserved_2[3];
 } Model0_ppr;

/*0B0*/ struct { /* End Of Interrupt Register */
  unsigned int Model0_eoi;
  unsigned int Model0___reserved[3];
 } Model0_eoi;

/*0C0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_07;

/*0D0*/ struct { /* Logical Destination Register */
  unsigned int Model0___reserved_1 : 24,
   Model0_logical_dest : 8;
  unsigned int Model0___reserved_2[3];
 } Model0_ldr;

/*0E0*/ struct { /* Destination Format Register */
  unsigned int Model0___reserved_1 : 28,
   Model0_model : 4;
  unsigned int Model0___reserved_2[3];
 } Model0_dfr;

/*0F0*/ struct { /* Spurious Interrupt Vector Register */
  unsigned int Model0_spurious_vector : 8,
   Model0_apic_enabled : 1,
   Model0_focus_cpu : 1,
   Model0___reserved_2 : 22;
  unsigned int Model0___reserved_3[3];
 } Model0_svr;

/*100*/ struct { /* In Service Register */
/*170*/ unsigned int Model0_bitfield;
  unsigned int Model0___reserved[3];
 } Model0_isr [8];

/*180*/ struct { /* Trigger Mode Register */
/*1F0*/ unsigned int Model0_bitfield;
  unsigned int Model0___reserved[3];
 } Model0_tmr [8];

/*200*/ struct { /* Interrupt Request Register */
/*270*/ unsigned int Model0_bitfield;
  unsigned int Model0___reserved[3];
 } Model0_irr [8];

/*280*/ union { /* Error Status Register */
  struct {
   unsigned int Model0_send_cs_error : 1,
    Model0_receive_cs_error : 1,
    Model0_send_accept_error : 1,
    Model0_receive_accept_error : 1,
    Model0___reserved_1 : 1,
    Model0_send_illegal_vector : 1,
    Model0_receive_illegal_vector : 1,
    Model0_illegal_register_address : 1,
    Model0___reserved_2 : 24;
   unsigned int Model0___reserved_3[3];
  } Model0_error_bits;
  struct {
   unsigned int Model0_errors;
   unsigned int Model0___reserved_3[3];
  } Model0_all_errors;
 } Model0_esr;

/*290*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_08;

/*2A0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_09;

/*2B0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_10;

/*2C0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_11;

/*2D0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_12;

/*2E0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_13;

/*2F0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_14;

/*300*/ struct { /* Interrupt Command Register 1 */
  unsigned int Model0_vector : 8,
   Model0_delivery_mode : 3,
   Model0_destination_mode : 1,
   Model0_delivery_status : 1,
   Model0___reserved_1 : 1,
   Model0_level : 1,
   Model0_trigger : 1,
   Model0___reserved_2 : 2,
   Model0_shorthand : 2,
   Model0___reserved_3 : 12;
  unsigned int Model0___reserved_4[3];
 } Model0_icr1;

/*310*/ struct { /* Interrupt Command Register 2 */
  union {
   unsigned int Model0___reserved_1 : 24,
    Model0_phys_dest : 4,
    Model0___reserved_2 : 4;
   unsigned int Model0___reserved_3 : 24,
    Model0_logical_dest : 8;
  } Model0_dest;
  unsigned int Model0___reserved_4[3];
 } Model0_icr2;

/*320*/ struct { /* LVT - Timer */
  unsigned int Model0_vector : 8,
   Model0___reserved_1 : 4,
   Model0_delivery_status : 1,
   Model0___reserved_2 : 3,
   Model0_mask : 1,
   Model0_timer_mode : 1,
   Model0___reserved_3 : 14;
  unsigned int Model0___reserved_4[3];
 } Model0_lvt_timer;

/*330*/ struct { /* LVT - Thermal Sensor */
  unsigned int Model0_vector : 8,
   Model0_delivery_mode : 3,
   Model0___reserved_1 : 1,
   Model0_delivery_status : 1,
   Model0___reserved_2 : 3,
   Model0_mask : 1,
   Model0___reserved_3 : 15;
  unsigned int Model0___reserved_4[3];
 } Model0_lvt_thermal;

/*340*/ struct { /* LVT - Performance Counter */
  unsigned int Model0_vector : 8,
   Model0_delivery_mode : 3,
   Model0___reserved_1 : 1,
   Model0_delivery_status : 1,
   Model0___reserved_2 : 3,
   Model0_mask : 1,
   Model0___reserved_3 : 15;
  unsigned int Model0___reserved_4[3];
 } Model0_lvt_pc;

/*350*/ struct { /* LVT - LINT0 */
  unsigned int Model0_vector : 8,
   Model0_delivery_mode : 3,
   Model0___reserved_1 : 1,
   Model0_delivery_status : 1,
   Model0_polarity : 1,
   Model0_remote_irr : 1,
   Model0_trigger : 1,
   Model0_mask : 1,
   Model0___reserved_2 : 15;
  unsigned int Model0___reserved_3[3];
 } Model0_lvt_lint0;

/*360*/ struct { /* LVT - LINT1 */
  unsigned int Model0_vector : 8,
   Model0_delivery_mode : 3,
   Model0___reserved_1 : 1,
   Model0_delivery_status : 1,
   Model0_polarity : 1,
   Model0_remote_irr : 1,
   Model0_trigger : 1,
   Model0_mask : 1,
   Model0___reserved_2 : 15;
  unsigned int Model0___reserved_3[3];
 } Model0_lvt_lint1;

/*370*/ struct { /* LVT - Error */
  unsigned int Model0_vector : 8,
   Model0___reserved_1 : 4,
   Model0_delivery_status : 1,
   Model0___reserved_2 : 3,
   Model0_mask : 1,
   Model0___reserved_3 : 15;
  unsigned int Model0___reserved_4[3];
 } Model0_lvt_error;

/*380*/ struct { /* Timer Initial Count Register */
  unsigned int Model0_initial_count;
  unsigned int Model0___reserved_2[3];
 } Model0_timer_icr;

/*390*/ const
 struct { /* Timer Current Count Register */
  unsigned int Model0_curr_count;
  unsigned int Model0___reserved_2[3];
 } Model0_timer_ccr;

/*3A0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_16;

/*3B0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_17;

/*3C0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_18;

/*3D0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_19;

/*3E0*/ struct { /* Timer Divide Configuration Register */
  unsigned int Model0_divisor : 4,
   Model0___reserved_1 : 28;
  unsigned int Model0___reserved_2[3];
 } Model0_timer_dcr;

/*3F0*/ struct { unsigned int Model0___reserved[4]; } Model0___reserved_20;

} __attribute__ ((packed));
enum Model0_ioapic_irq_destination_types {
 Model0_dest_Fixed = 0,
 Model0_dest_LowestPrio = 1,
 Model0_dest_SMI = 2,
 Model0_dest__reserved_1 = 3,
 Model0_dest_NMI = 4,
 Model0_dest_INIT = 5,
 Model0_dest__reserved_2 = 6,
 Model0_dest_ExtINT = 7
};

extern int Model0_apic_version[];
extern int Model0_pic_mode;
/* Each PCI slot may be a combo card with its own bus.  4 IRQ pins per slot. */
extern unsigned long Model0_mp_bus_not_pci[(((256) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];

extern unsigned int Model0_boot_cpu_physical_apicid;
extern unsigned long Model0_mp_lapic_addr;


extern int Model0_smp_found_config;




static inline __attribute__((no_instrument_function)) void Model0_get_smp_config(void)
{
 Model0_x86_init.Model0_mpparse.Model0_get_smp_config(0);
}

static inline __attribute__((no_instrument_function)) void Model0_early_get_smp_config(void)
{
 Model0_x86_init.Model0_mpparse.Model0_get_smp_config(1);
}

static inline __attribute__((no_instrument_function)) void Model0_find_smp_config(void)
{
 Model0_x86_init.Model0_mpparse.Model0_find_smp_config();
}


extern void Model0_early_reserve_e820_mpc_new(void);
extern int Model0_enable_update_mptable;
extern int Model0_default_mpc_apic_id(struct Model0_mpc_cpu *Model0_m);
extern void Model0_default_smp_read_mpc_oem(struct Model0_mpc_table *Model0_mpc);

extern void Model0_default_mpc_oem_bus_info(struct Model0_mpc_bus *Model0_m, char *Model0_str);



extern void Model0_default_find_smp_config(void);
extern void Model0_default_get_smp_config(unsigned int Model0_early);
int Model0_generic_processor_info(int Model0_apicid, int Model0_version);



struct Model0_physid_mask {
 unsigned long Model0_mask[(((32768) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
};

typedef struct Model0_physid_mask Model0_physid_mask_t;
static inline __attribute__((no_instrument_function)) unsigned long Model0_physids_coerce(Model0_physid_mask_t *Model0_map)
{
 return Model0_map->Model0_mask[0];
}

static inline __attribute__((no_instrument_function)) void Model0_physids_promote(unsigned long Model0_physids, Model0_physid_mask_t *Model0_map)
{
 Model0_bitmap_zero((*Model0_map).Model0_mask, 32768);
 Model0_map->Model0_mask[0] = Model0_physids;
}

static inline __attribute__((no_instrument_function)) void Model0_physid_set_mask_of_physid(int Model0_physid, Model0_physid_mask_t *Model0_map)
{
 Model0_bitmap_zero((*Model0_map).Model0_mask, 32768);
 Model0_set_bit(Model0_physid, (*Model0_map).Model0_mask);
}




extern Model0_physid_mask_t Model0_phys_cpu_present_map;




/*
 *  pm.h - Power management interface
 *
 *  Copyright (C) 2000 Andrew Henroid
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */
/*
 * Callbacks for platform drivers to implement.
 */
extern void (*Model0_pm_power_off)(void);
extern void (*Model0_pm_power_off_prepare)(void);

struct Model0_device; /* we have a circular dep with device.h */

extern void Model0_pm_vt_switch_required(struct Model0_device *Model0_dev, bool Model0_required);
extern void Model0_pm_vt_switch_unregister(struct Model0_device *Model0_dev);
/*
 * Device power management
 */

struct Model0_device;


extern const char Model0_power_group_name[]; /* = "power" */




typedef struct Model0_pm_message {
 int Model0_event;
} Model0_pm_message_t;

/**
 * struct dev_pm_ops - device PM callbacks
 *
 * Several device power state transitions are externally visible, affecting
 * the state of pending I/O queues and (for drivers that touch hardware)
 * interrupts, wakeups, DMA, and other hardware state.  There may also be
 * internal transitions to various low-power modes which are transparent
 * to the rest of the driver stack (such as a driver that's ON gating off
 * clocks which are not in active use).
 *
 * The externally visible transitions are handled with the help of callbacks
 * included in this structure in such a way that two levels of callbacks are
 * involved.  First, the PM core executes callbacks provided by PM domains,
 * device types, classes and bus types.  They are the subsystem-level callbacks
 * supposed to execute callbacks provided by device drivers, although they may
 * choose not to do that.  If the driver callbacks are executed, they have to
 * collaborate with the subsystem-level callbacks to achieve the goals
 * appropriate for the given system transition, given transition phase and the
 * subsystem the device belongs to.
 *
 * @prepare: The principal role of this callback is to prevent new children of
 *	the device from being registered after it has returned (the driver's
 *	subsystem and generally the rest of the kernel is supposed to prevent
 *	new calls to the probe method from being made too once @prepare() has
 *	succeeded).  If @prepare() detects a situation it cannot handle (e.g.
 *	registration of a child already in progress), it may return -EAGAIN, so
 *	that the PM core can execute it once again (e.g. after a new child has
 *	been registered) to recover from the race condition.
 *	This method is executed for all kinds of suspend transitions and is
 *	followed by one of the suspend callbacks: @suspend(), @freeze(), or
 *	@poweroff().  If the transition is a suspend to memory or standby (that
 *	is, not related to hibernation), the return value of @prepare() may be
 *	used to indicate to the PM core to leave the device in runtime suspend
 *	if applicable.  Namely, if @prepare() returns a positive number, the PM
 *	core will understand that as a declaration that the device appears to be
 *	runtime-suspended and it may be left in that state during the entire
 *	transition and during the subsequent resume if all of its descendants
 *	are left in runtime suspend too.  If that happens, @complete() will be
 *	executed directly after @prepare() and it must ensure the proper
 *	functioning of the device after the system resume.
 *	The PM core executes subsystem-level @prepare() for all devices before
 *	starting to invoke suspend callbacks for any of them, so generally
 *	devices may be assumed to be functional or to respond to runtime resume
 *	requests while @prepare() is being executed.  However, device drivers
 *	may NOT assume anything about the availability of user space at that
 *	time and it is NOT valid to request firmware from within @prepare()
 *	(it's too late to do that).  It also is NOT valid to allocate
 *	substantial amounts of memory from @prepare() in the GFP_KERNEL mode.
 *	[To work around these limitations, drivers may register suspend and
 *	hibernation notifiers to be executed before the freezing of tasks.]
 *
 * @complete: Undo the changes made by @prepare().  This method is executed for
 *	all kinds of resume transitions, following one of the resume callbacks:
 *	@resume(), @thaw(), @restore().  Also called if the state transition
 *	fails before the driver's suspend callback: @suspend(), @freeze() or
 *	@poweroff(), can be executed (e.g. if the suspend callback fails for one
 *	of the other devices that the PM core has unsuccessfully attempted to
 *	suspend earlier).
 *	The PM core executes subsystem-level @complete() after it has executed
 *	the appropriate resume callbacks for all devices.  If the corresponding
 *	@prepare() at the beginning of the suspend transition returned a
 *	positive number and the device was left in runtime suspend (without
 *	executing any suspend and resume callbacks for it), @complete() will be
 *	the only callback executed for the device during resume.  In that case,
 *	@complete() must be prepared to do whatever is necessary to ensure the
 *	proper functioning of the device after the system resume.  To this end,
 *	@complete() can check the power.direct_complete flag of the device to
 *	learn whether (unset) or not (set) the previous suspend and resume
 *	callbacks have been executed for it.
 *
 * @suspend: Executed before putting the system into a sleep state in which the
 *	contents of main memory are preserved.  The exact action to perform
 *	depends on the device's subsystem (PM domain, device type, class or bus
 *	type), but generally the device must be quiescent after subsystem-level
 *	@suspend() has returned, so that it doesn't do any I/O or DMA.
 *	Subsystem-level @suspend() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @suspend_late: Continue operations started by @suspend().  For a number of
 *	devices @suspend_late() may point to the same callback routine as the
 *	runtime suspend callback.
 *
 * @resume: Executed after waking the system up from a sleep state in which the
 *	contents of main memory were preserved.  The exact action to perform
 *	depends on the device's subsystem, but generally the driver is expected
 *	to start working again, responding to hardware events and software
 *	requests (the device itself may be left in a low-power state, waiting
 *	for a runtime resume to occur).  The state of the device at the time its
 *	driver's @resume() callback is run depends on the platform and subsystem
 *	the device belongs to.  On most platforms, there are no restrictions on
 *	availability of resources like clocks during @resume().
 *	Subsystem-level @resume() is executed for all devices after invoking
 *	subsystem-level @resume_noirq() for all of them.
 *
 * @resume_early: Prepare to execute @resume().  For a number of devices
 *	@resume_early() may point to the same callback routine as the runtime
 *	resume callback.
 *
 * @freeze: Hibernation-specific, executed before creating a hibernation image.
 *	Analogous to @suspend(), but it should not enable the device to signal
 *	wakeup events or change its power state.  The majority of subsystems
 *	(with the notable exception of the PCI bus type) expect the driver-level
 *	@freeze() to save the device settings in memory to be used by @restore()
 *	during the subsequent resume from hibernation.
 *	Subsystem-level @freeze() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @freeze_late: Continue operations started by @freeze().  Analogous to
 *	@suspend_late(), but it should not enable the device to signal wakeup
 *	events or change its power state.
 *
 * @thaw: Hibernation-specific, executed after creating a hibernation image OR
 *	if the creation of an image has failed.  Also executed after a failing
 *	attempt to restore the contents of main memory from such an image.
 *	Undo the changes made by the preceding @freeze(), so the device can be
 *	operated in the same way as immediately before the call to @freeze().
 *	Subsystem-level @thaw() is executed for all devices after invoking
 *	subsystem-level @thaw_noirq() for all of them.  It also may be executed
 *	directly after @freeze() in case of a transition error.
 *
 * @thaw_early: Prepare to execute @thaw().  Undo the changes made by the
 *	preceding @freeze_late().
 *
 * @poweroff: Hibernation-specific, executed after saving a hibernation image.
 *	Analogous to @suspend(), but it need not save the device's settings in
 *	memory.
 *	Subsystem-level @poweroff() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @poweroff_late: Continue operations started by @poweroff().  Analogous to
 *	@suspend_late(), but it need not save the device's settings in memory.
 *
 * @restore: Hibernation-specific, executed after restoring the contents of main
 *	memory from a hibernation image, analogous to @resume().
 *
 * @restore_early: Prepare to execute @restore(), analogous to @resume_early().
 *
 * @suspend_noirq: Complete the actions started by @suspend().  Carry out any
 *	additional operations required for suspending the device that might be
 *	racing with its driver's interrupt handler, which is guaranteed not to
 *	run while @suspend_noirq() is being executed.
 *	It generally is expected that the device will be in a low-power state
 *	(appropriate for the target system sleep state) after subsystem-level
 *	@suspend_noirq() has returned successfully.  If the device can generate
 *	system wakeup signals and is enabled to wake up the system, it should be
 *	configured to do so at that time.  However, depending on the platform
 *	and device's subsystem, @suspend() or @suspend_late() may be allowed to
 *	put the device into the low-power state and configure it to generate
 *	wakeup signals, in which case it generally is not necessary to define
 *	@suspend_noirq().
 *
 * @resume_noirq: Prepare for the execution of @resume() by carrying out any
 *	operations required for resuming the device that might be racing with
 *	its driver's interrupt handler, which is guaranteed not to run while
 *	@resume_noirq() is being executed.
 *
 * @freeze_noirq: Complete the actions started by @freeze().  Carry out any
 *	additional operations required for freezing the device that might be
 *	racing with its driver's interrupt handler, which is guaranteed not to
 *	run while @freeze_noirq() is being executed.
 *	The power state of the device should not be changed by either @freeze(),
 *	or @freeze_late(), or @freeze_noirq() and it should not be configured to
 *	signal system wakeup by any of these callbacks.
 *
 * @thaw_noirq: Prepare for the execution of @thaw() by carrying out any
 *	operations required for thawing the device that might be racing with its
 *	driver's interrupt handler, which is guaranteed not to run while
 *	@thaw_noirq() is being executed.
 *
 * @poweroff_noirq: Complete the actions started by @poweroff().  Analogous to
 *	@suspend_noirq(), but it need not save the device's settings in memory.
 *
 * @restore_noirq: Prepare for the execution of @restore() by carrying out any
 *	operations required for thawing the device that might be racing with its
 *	driver's interrupt handler, which is guaranteed not to run while
 *	@restore_noirq() is being executed.  Analogous to @resume_noirq().
 *
 * All of the above callbacks, except for @complete(), return error codes.
 * However, the error codes returned by the resume operations, @resume(),
 * @thaw(), @restore(), @resume_noirq(), @thaw_noirq(), and @restore_noirq(), do
 * not cause the PM core to abort the resume transition during which they are
 * returned.  The error codes returned in those cases are only printed by the PM
 * core to the system logs for debugging purposes.  Still, it is recommended
 * that drivers only return error codes from their resume methods in case of an
 * unrecoverable failure (i.e. when the device being handled refuses to resume
 * and becomes unusable) to allow us to modify the PM core in the future, so
 * that it can avoid attempting to handle devices that failed to resume and
 * their children.
 *
 * It is allowed to unregister devices while the above callbacks are being
 * executed.  However, a callback routine must NOT try to unregister the device
 * it was called for, although it may unregister children of that device (for
 * example, if it detects that a child was unplugged while the system was
 * asleep).
 *
 * Refer to Documentation/power/devices.txt for more information about the role
 * of the above callbacks in the system suspend process.
 *
 * There also are callbacks related to runtime power management of devices.
 * Again, these callbacks are executed by the PM core only for subsystems
 * (PM domains, device types, classes and bus types) and the subsystem-level
 * callbacks are supposed to invoke the driver callbacks.  Moreover, the exact
 * actions to be performed by a device driver's callbacks generally depend on
 * the platform and subsystem the device belongs to.
 *
 * @runtime_suspend: Prepare the device for a condition in which it won't be
 *	able to communicate with the CPU(s) and RAM due to power management.
 *	This need not mean that the device should be put into a low-power state.
 *	For example, if the device is behind a link which is about to be turned
 *	off, the device may remain at full power.  If the device does go to low
 *	power and is capable of generating runtime wakeup events, remote wakeup
 *	(i.e., a hardware mechanism allowing the device to request a change of
 *	its power state via an interrupt) should be enabled for it.
 *
 * @runtime_resume: Put the device into the fully active state in response to a
 *	wakeup event generated by hardware or at the request of software.  If
 *	necessary, put the device into the full-power state and restore its
 *	registers, so that it is fully operational.
 *
 * @runtime_idle: Device appears to be inactive and it might be put into a
 *	low-power state if all of the necessary conditions are satisfied.
 *	Check these conditions, and return 0 if it's appropriate to let the PM
 *	core queue a suspend request for the device.
 *
 * Refer to Documentation/power/runtime_pm.txt for more information about the
 * role of the above callbacks in device runtime power management.
 *
 */

struct Model0_dev_pm_ops {
 int (*Model0_prepare)(struct Model0_device *Model0_dev);
 void (*Model0_complete)(struct Model0_device *Model0_dev);
 int (*Model0_suspend)(struct Model0_device *Model0_dev);
 int (*Model0_resume)(struct Model0_device *Model0_dev);
 int (*Model0_freeze)(struct Model0_device *Model0_dev);
 int (*Model0_thaw)(struct Model0_device *Model0_dev);
 int (*Model0_poweroff)(struct Model0_device *Model0_dev);
 int (*Model0_restore)(struct Model0_device *Model0_dev);
 int (*Model0_suspend_late)(struct Model0_device *Model0_dev);
 int (*Model0_resume_early)(struct Model0_device *Model0_dev);
 int (*Model0_freeze_late)(struct Model0_device *Model0_dev);
 int (*Model0_thaw_early)(struct Model0_device *Model0_dev);
 int (*Model0_poweroff_late)(struct Model0_device *Model0_dev);
 int (*Model0_restore_early)(struct Model0_device *Model0_dev);
 int (*Model0_suspend_noirq)(struct Model0_device *Model0_dev);
 int (*Model0_resume_noirq)(struct Model0_device *Model0_dev);
 int (*Model0_freeze_noirq)(struct Model0_device *Model0_dev);
 int (*Model0_thaw_noirq)(struct Model0_device *Model0_dev);
 int (*Model0_poweroff_noirq)(struct Model0_device *Model0_dev);
 int (*Model0_restore_noirq)(struct Model0_device *Model0_dev);
 int (*Model0_runtime_suspend)(struct Model0_device *Model0_dev);
 int (*Model0_runtime_resume)(struct Model0_device *Model0_dev);
 int (*Model0_runtime_idle)(struct Model0_device *Model0_dev);
};
/*
 * Use this if you want to use the same suspend and resume callbacks for suspend
 * to RAM and hibernation.
 */





/*
 * Use this for defining a set of PM operations to be used in all situations
 * (system suspend, hibernation or runtime PM).
 * NOTE: In general, system suspend callbacks, .suspend() and .resume(), should
 * be different from the corresponding runtime PM callbacks, .runtime_suspend(),
 * and .runtime_resume(), because .runtime_suspend() always works on an already
 * quiescent device, while .suspend() should assume that the device may be doing
 * something when it is called (it should ensure that the device will be
 * quiescent after it has returned).  Therefore it's better to point the "late"
 * suspend and "early" resume callback pointers, .suspend_late() and
 * .resume_early(), to the same routines as .runtime_suspend() and
 * .runtime_resume(), respectively (and analogously for hibernation).
 */






/**
 * PM_EVENT_ messages
 *
 * The following PM_EVENT_ messages are defined for the internal use of the PM
 * core, in order to provide a mechanism allowing the high level suspend and
 * hibernation code to convey the necessary information to the device PM core
 * code:
 *
 * ON		No transition.
 *
 * FREEZE	System is going to hibernate, call ->prepare() and ->freeze()
 *		for all devices.
 *
 * SUSPEND	System is going to suspend, call ->prepare() and ->suspend()
 *		for all devices.
 *
 * HIBERNATE	Hibernation image has been saved, call ->prepare() and
 *		->poweroff() for all devices.
 *
 * QUIESCE	Contents of main memory are going to be restored from a (loaded)
 *		hibernation image, call ->prepare() and ->freeze() for all
 *		devices.
 *
 * RESUME	System is resuming, call ->resume() and ->complete() for all
 *		devices.
 *
 * THAW		Hibernation image has been created, call ->thaw() and
 *		->complete() for all devices.
 *
 * RESTORE	Contents of main memory have been restored from a hibernation
 *		image, call ->restore() and ->complete() for all devices.
 *
 * RECOVER	Creation of a hibernation image or restoration of the main
 *		memory contents from a hibernation image has failed, call
 *		->thaw() and ->complete() for all devices.
 *
 * The following PM_EVENT_ messages are defined for internal use by
 * kernel subsystems.  They are never issued by the PM core.
 *
 * USER_SUSPEND		Manual selective suspend was issued by userspace.
 *
 * USER_RESUME		Manual selective resume was issued by userspace.
 *
 * REMOTE_WAKEUP	Remote-wakeup request was received from the device.
 *
 * AUTO_SUSPEND		Automatic (device idle) runtime suspend was
 *			initiated by the subsystem.
 *
 * AUTO_RESUME		Automatic (device needed) runtime resume was
 *			requested by a driver.
 */
/**
 * Device run-time power management status.
 *
 * These status labels are used internally by the PM core to indicate the
 * current status of a device with respect to the PM core operations.  They do
 * not reflect the actual power state of the device or its status as seen by the
 * driver.
 *
 * RPM_ACTIVE		Device is fully operational.  Indicates that the device
 *			bus type's ->runtime_resume() callback has completed
 *			successfully.
 *
 * RPM_SUSPENDED	Device bus type's ->runtime_suspend() callback has
 *			completed successfully.  The device is regarded as
 *			suspended.
 *
 * RPM_RESUMING		Device bus type's ->runtime_resume() callback is being
 *			executed.
 *
 * RPM_SUSPENDING	Device bus type's ->runtime_suspend() callback is being
 *			executed.
 */

enum Model0_rpm_status {
 Model0_RPM_ACTIVE = 0,
 Model0_RPM_RESUMING,
 Model0_RPM_SUSPENDED,
 Model0_RPM_SUSPENDING,
};

/**
 * Device run-time power management request types.
 *
 * RPM_REQ_NONE		Do nothing.
 *
 * RPM_REQ_IDLE		Run the device bus type's ->runtime_idle() callback
 *
 * RPM_REQ_SUSPEND	Run the device bus type's ->runtime_suspend() callback
 *
 * RPM_REQ_AUTOSUSPEND	Same as RPM_REQ_SUSPEND, but not until the device has
 *			been inactive for as long as power.autosuspend_delay
 *
 * RPM_REQ_RESUME	Run the device bus type's ->runtime_resume() callback
 */

enum Model0_rpm_request {
 Model0_RPM_REQ_NONE = 0,
 Model0_RPM_REQ_IDLE,
 Model0_RPM_REQ_SUSPEND,
 Model0_RPM_REQ_AUTOSUSPEND,
 Model0_RPM_REQ_RESUME,
};

struct Model0_wakeup_source;
struct Model0_wake_irq;
struct Model0_pm_domain_data;

struct Model0_pm_subsys_data {
 Model0_spinlock_t Model0_lock;
 unsigned int Model0_refcount;






};

struct Model0_dev_pm_info {
 Model0_pm_message_t Model0_power_state;
 unsigned int Model0_can_wakeup:1;
 unsigned int Model0_async_suspend:1;
 bool Model0_is_prepared:1; /* Owned by the PM core */
 bool Model0_is_suspended:1; /* Ditto */
 bool Model0_is_noirq_suspended:1;
 bool Model0_is_late_suspended:1;
 bool Model0_early_init:1; /* Owned by the PM core */
 bool Model0_direct_complete:1; /* Owned by the PM core */
 Model0_spinlock_t Model0_lock;

 struct Model0_list_head Model0_entry;
 struct Model0_completion Model0_completion;
 struct Model0_wakeup_source *Model0_wakeup;
 bool Model0_wakeup_path:1;
 bool Model0_syscore:1;
 bool Model0_no_pm_callbacks:1; /* Owned by the PM core */




 struct Model0_timer_list Model0_suspend_timer;
 unsigned long Model0_timer_expires;
 struct Model0_work_struct Model0_work;
 Model0_wait_queue_head_t Model0_wait_queue;
 struct Model0_wake_irq *Model0_wakeirq;
 Model0_atomic_t Model0_usage_count;
 Model0_atomic_t Model0_child_count;
 unsigned int Model0_disable_depth:3;
 unsigned int Model0_idle_notification:1;
 unsigned int Model0_request_pending:1;
 unsigned int Model0_deferred_resume:1;
 unsigned int Model0_run_wake:1;
 unsigned int Model0_runtime_auto:1;
 bool Model0_ignore_children:1;
 unsigned int Model0_no_callbacks:1;
 unsigned int Model0_irq_safe:1;
 unsigned int Model0_use_autosuspend:1;
 unsigned int Model0_timer_autosuspends:1;
 unsigned int Model0_memalloc_noio:1;
 enum Model0_rpm_request Model0_request;
 enum Model0_rpm_status Model0_runtime_status;
 int Model0_runtime_error;
 int Model0_autosuspend_delay;
 unsigned long Model0_last_busy;
 unsigned long Model0_active_jiffies;
 unsigned long Model0_suspended_jiffies;
 unsigned long Model0_accounting_timestamp;

 struct Model0_pm_subsys_data *Model0_subsys_data; /* Owned by the subsystem. */
 void (*Model0_set_latency_tolerance)(struct Model0_device *, Model0_s32);
 struct Model0_dev_pm_qos *Model0_qos;
};

extern void Model0_update_pm_runtime_accounting(struct Model0_device *Model0_dev);
extern int Model0_dev_pm_get_subsys_data(struct Model0_device *Model0_dev);
extern void Model0_dev_pm_put_subsys_data(struct Model0_device *Model0_dev);

/*
 * Power domains provide callbacks that are executed during system suspend,
 * hibernation, system resume and during runtime PM transitions along with
 * subsystem-level and driver-level callbacks.
 *
 * @detach: Called when removing a device from the domain.
 * @activate: Called before executing probe routines for bus types and drivers.
 * @sync: Called after successful driver probe.
 * @dismiss: Called after unsuccessful driver probe and after driver removal.
 */
struct Model0_dev_pm_domain {
 struct Model0_dev_pm_ops Model0_ops;
 void (*Model0_detach)(struct Model0_device *Model0_dev, bool Model0_power_off);
 int (*Model0_activate)(struct Model0_device *Model0_dev);
 void (*Model0_sync)(struct Model0_device *Model0_dev);
 void (*Model0_dismiss)(struct Model0_device *Model0_dev);
};

/*
 * The PM_EVENT_ messages are also used by drivers implementing the legacy
 * suspend framework, based on the ->suspend() and ->resume() callbacks common
 * for suspend and hibernation transitions, according to the rules below.
 */

/* Necessary, because several drivers use PM_EVENT_PRETHAW */


/*
 * One transition is triggered by resume(), after a suspend() call; the
 * message is implicit:
 *
 * ON		Driver starts working again, responding to hardware events
 *		and software requests.  The hardware may have gone through
 *		a power-off reset, or it may have maintained state from the
 *		previous suspend() which the driver will rely on while
 *		resuming.  On most platforms, there are no restrictions on
 *		availability of resources like clocks during resume().
 *
 * Other transitions are triggered by messages sent using suspend().  All
 * these transitions quiesce the driver, so that I/O queues are inactive.
 * That commonly entails turning off IRQs and DMA; there may be rules
 * about how to quiesce that are specific to the bus or the device's type.
 * (For example, network drivers mark the link state.)  Other details may
 * differ according to the message:
 *
 * SUSPEND	Quiesce, enter a low power device state appropriate for
 *		the upcoming system state (such as PCI_D3hot), and enable
 *		wakeup events as appropriate.
 *
 * HIBERNATE	Enter a low power device state appropriate for the hibernation
 *		state (eg. ACPI S4) and enable wakeup events as appropriate.
 *
 * FREEZE	Quiesce operations so that a consistent image can be saved;
 *		but do NOT otherwise enter a low power device state, and do
 *		NOT emit system wakeup events.
 *
 * PRETHAW	Quiesce as if for FREEZE; additionally, prepare for restoring
 *		the system from a snapshot taken after an earlier FREEZE.
 *		Some drivers will need to reset their hardware state instead
 *		of preserving it, to ensure that it's never mistaken for the
 *		state which that earlier snapshot had set up.
 *
 * A minimally power-aware driver treats all messages as SUSPEND, fully
 * reinitializes its device during resume() -- whether or not it was reset
 * during the suspend/resume cycle -- and can't issue wakeup events.
 *
 * More power-aware drivers may also use low power states at runtime as
 * well as during system sleep states like PM_SUSPEND_STANDBY.  They may
 * be able to use wakeup events to exit from runtime low-power states,
 * or from system low-power states such as standby or suspend-to-RAM.
 */


extern void Model0_device_pm_lock(void);
extern void Model0_dpm_resume_start(Model0_pm_message_t Model0_state);
extern void Model0_dpm_resume_end(Model0_pm_message_t Model0_state);
extern void Model0_dpm_resume_noirq(Model0_pm_message_t Model0_state);
extern void Model0_dpm_resume_early(Model0_pm_message_t Model0_state);
extern void Model0_dpm_resume(Model0_pm_message_t Model0_state);
extern void Model0_dpm_complete(Model0_pm_message_t Model0_state);

extern void Model0_device_pm_unlock(void);
extern int Model0_dpm_suspend_end(Model0_pm_message_t Model0_state);
extern int Model0_dpm_suspend_start(Model0_pm_message_t Model0_state);
extern int Model0_dpm_suspend_noirq(Model0_pm_message_t Model0_state);
extern int Model0_dpm_suspend_late(Model0_pm_message_t Model0_state);
extern int Model0_dpm_suspend(Model0_pm_message_t Model0_state);
extern int Model0_dpm_prepare(Model0_pm_message_t Model0_state);

extern void Model0___suspend_report_result(const char *Model0_function, void *Model0_fn, int Model0_ret);






extern int Model0_device_pm_wait_for_dev(struct Model0_device *Model0_sub, struct Model0_device *Model0_dev);
extern void Model0_dpm_for_each_dev(void *Model0_data, void (*Model0_fn)(struct Model0_device *, void *));

extern int Model0_pm_generic_prepare(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_suspend_late(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_suspend_noirq(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_suspend(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_resume_early(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_resume_noirq(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_resume(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_freeze_noirq(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_freeze_late(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_freeze(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_thaw_noirq(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_thaw_early(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_thaw(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_restore_noirq(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_restore_early(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_restore(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_poweroff_noirq(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_poweroff_late(struct Model0_device *Model0_dev);
extern int Model0_pm_generic_poweroff(struct Model0_device *Model0_dev);
extern void Model0_pm_generic_complete(struct Model0_device *Model0_dev);
extern void Model0_pm_complete_with_resume_check(struct Model0_device *Model0_dev);
/* How to reorder dpm_list after device_move() */
enum Model0_dpm_order {
 Model0_DPM_ORDER_NONE,
 Model0_DPM_ORDER_DEV_AFTER_PARENT,
 Model0_DPM_ORDER_PARENT_BEFORE_DEV,
 Model0_DPM_ORDER_DEV_LAST,
};





/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 */










/*
 *  Copyright (C) 2001 Paul Diefenbaugh <paul.s.diefenbaugh@intel.com>
 *  Copyright (C) 2001 Patrick Mochel <mochel@osdl.org>
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 */


/* _PDC bit definition for Intel processors */






/*
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */



/*
 * to preserve the visibility of NUMA_NO_NODE definition,
 * moved to there from here.  May be used independent of
 * CONFIG_NUMA.
 */
/* Mappings between logical cpu number and node number */
extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model0_x86_cpu_to_node_map; extern __typeof__(int) *Model0_x86_cpu_to_node_map_early_ptr; extern __typeof__(int) Model0_x86_cpu_to_node_map_early_map[];
/* Same function but used if called before per_cpu areas are setup */
static inline __attribute__((no_instrument_function)) int Model0_early_cpu_to_node(int Model0_cpu)
{
 return *((Model0_x86_cpu_to_node_map_early_ptr) ? &(Model0_x86_cpu_to_node_map_early_ptr)[Model0_cpu] : &(*({ do { const void *Model0___vpp_verify = (typeof((&(Model0_x86_cpu_to_node_map)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((&(Model0_x86_cpu_to_node_map)))) *)((&(Model0_x86_cpu_to_node_map))))); (typeof((typeof(*((&(Model0_x86_cpu_to_node_map)))) *)((&(Model0_x86_cpu_to_node_map))))) (Model0___ptr + (((Model0___per_cpu_offset[(Model0_cpu)])))); }); })));
}



/* Mappings between node number and cpus on that node. */
extern Model0_cpumask_var_t Model0_node_to_cpumask_map[(1 << 6)];




/* Returns a pointer to the cpumask of CPUs on Node 'node'. */
static inline __attribute__((no_instrument_function)) const struct Model0_cpumask *Model0_cpumask_of_node(int Model0_node)
{
 return Model0_node_to_cpumask_map[Model0_node];
}


extern void Model0_setup_node_to_cpumask_map(void);

/*
 * Returns the number of the node containing Node 'node'. This
 * architecture is flat, so it is a pretty simple function!
 */




extern int Model0___node_distance(int, int);
/*
 * linux/include/asm-generic/topology.h
 *
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */

extern const struct Model0_cpumask *Model0_cpu_coregroup_mask(int Model0_cpu);
extern unsigned int Model0___max_logical_packages;


extern int Model0___max_smt_threads;

static inline __attribute__((no_instrument_function)) int Model0_topology_max_smt_threads(void)
{
 return Model0___max_smt_threads;
}

int Model0_topology_update_package_map(unsigned int Model0_apicid, unsigned int Model0_cpu);
extern int Model0_topology_phys_to_logical_pkg(unsigned int Model0_pkg);
static inline __attribute__((no_instrument_function)) void Model0_arch_fix_phys_package_id(int Model0_num, Model0_u32 Model0_slot)
{
}

struct Model0_pci_bus;
int Model0_x86_pci_root_bus_node(int Model0_bus);
void Model0_x86_pci_root_bus_resources(int Model0_bus, struct Model0_list_head *Model0_resources);






/*
 * Too small node sizes may confuse the VM badly. Usually they
 * result from BIOS bugs. So dont recognize nodes as standalone
 * NUMA entities that have less than this amount of RAM listed:
 */


extern int Model0_numa_off;

/*
 * __apicid_to_node[] stores the raw mapping between physical apicid and
 * node and is used to initialize cpu_to_node mapping.
 *
 * The mapping may be overridden by apic->numa_cpu_node() on 32bit and thus
 * should be accessed by the accessors - set_apicid_to_node() and
 * numa_cpu_node().
 */
extern Model0_s16 Model0___apicid_to_node[32768];
extern Model0_nodemask_t Model0_numa_nodes_parsed __attribute__ ((__section__(".init.data")));

extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_numa_add_memblk(int Model0_nodeid, Model0_u64 Model0_start, Model0_u64 Model0_end);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_numa_set_distance(int Model0_from, int Model0_to, int Model0_distance);

static inline __attribute__((no_instrument_function)) void Model0_set_apicid_to_node(int Model0_apicid, Model0_s16 Model0_node)
{
 Model0___apicid_to_node[Model0_apicid] = Model0_node;
}

extern int Model0_numa_cpu_node(int Model0_cpu);
extern void Model0_numa_set_node(int Model0_cpu, int Model0_node);
extern void Model0_numa_clear_node(int Model0_cpu);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_init_cpu_to_node(void);
extern void Model0_numa_add_cpu(int Model0_cpu);
extern void Model0_numa_remove_cpu(int Model0_cpu);
/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 */







/*
 * The x86 doesn't have a mmu context, but
 * we put the segment information here.
 */
typedef struct {

 struct Model0_ldt_struct *Model0_ldt;



 /* True if mm supports a task running in 32 bit compatibility mode. */
 unsigned short Model0_ia32_compat;


 struct Model0_mutex Model0_lock;
 void *Model0_vdso; /* vdso base address */
 const struct Model0_vdso_image *Model0_vdso_image; /* vdso image in use */

 Model0_atomic_t Model0_perf_rdpmc_allowed; /* nonzero if rdpmc is allowed */
} Model0_mm_context_t;


void Model0_leave_mm(int Model0_cpu);








/*
 * This file contains the definitions for the x86 IO instructions
 * inb/inw/inl/outb/outw/outl and the "string versions" of the same
 * (insb/insw/insl/outsb/outsw/outsl). You can also use "pausing"
 * versions of the single-IO instructions (inb_p/inw_p/..).
 *
 * This file is not meant to be obfuscating: it's just complicated
 * to (a) handle it all in a way that makes gcc able to optimize it
 * as well as possible and (b) trying to avoid writing the same thing
 * over and over again with slight variations and possibly making a
 * mistake somewhere.
 */

/*
 * Thanks to James van Artsdalen for a better timing-fix than
 * the two short jumps: using outb's to a nonexistent port seems
 * to guarantee better timings even on fast machines.
 *
 * On the other hand, I'd like to be sure of a non-existent port:
 * I feel a bit unsafe about using 0x80 (should be safe, though)
 *
 *		Linus
 */

 /*
  *  Bit simplified and optimized by Jan Hubicka
  *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999.
  *
  *  isa_memset_io, isa_memcpy_fromio, isa_memcpy_toio added,
  *  isa_read[wl] and isa_write[wl] fixed
  *  - Arnaldo Carvalho de Melo <acme@conectiva.com.br>
  */













/*
 * early_ioremap() and early_iounmap() are for temporary early boot-time
 * mappings, before the real ioremap() is functional.
 */
extern void *Model0_early_ioremap(Model0_resource_size_t Model0_phys_addr,
       unsigned long Model0_size);
extern void *Model0_early_memremap(Model0_resource_size_t Model0_phys_addr,
       unsigned long Model0_size);
extern void *Model0_early_memremap_ro(Model0_resource_size_t Model0_phys_addr,
          unsigned long Model0_size);
extern void Model0_early_iounmap(void *Model0_addr, unsigned long Model0_size);
extern void Model0_early_memunmap(void *Model0_addr, unsigned long Model0_size);

/*
 * Weak function called by early_ioremap_reset(). It does nothing, but
 * architectures may provide their own version to do any needed cleanups.
 */
extern void Model0_early_ioremap_shutdown(void);


/* Arch-specific initialization */
extern void Model0_early_ioremap_init(void);

/* Generic initialization called by architecture code */
extern void Model0_early_ioremap_setup(void);

/*
 * Called as last step in paging_init() so library can act
 * accordingly for subsequent map/unmap requests.
 */
extern void Model0_early_ioremap_reset(void);

/*
 * Early copy from unmapped memory to kernel mapped memory.
 */
extern void Model0_copy_from_early_mem(void *Model0_dest, Model0_phys_addr_t Model0_src,
    unsigned long Model0_size);
static inline __attribute__((no_instrument_function)) unsigned char Model0_readb(const volatile void *Model0_addr) { unsigned char Model0_ret; asm volatile("mov" "b" " %1,%0":"=q" (Model0_ret) :"m" (*(volatile unsigned char *)Model0_addr) :"memory"); return Model0_ret; }
static inline __attribute__((no_instrument_function)) unsigned short Model0_readw(const volatile void *Model0_addr) { unsigned short Model0_ret; asm volatile("mov" "w" " %1,%0":"=r" (Model0_ret) :"m" (*(volatile unsigned short *)Model0_addr) :"memory"); return Model0_ret; }
static inline __attribute__((no_instrument_function)) unsigned int Model0_readl(const volatile void *Model0_addr) { unsigned int Model0_ret; asm volatile("mov" "l" " %1,%0":"=r" (Model0_ret) :"m" (*(volatile unsigned int *)Model0_addr) :"memory"); return Model0_ret; }

static inline __attribute__((no_instrument_function)) unsigned char Model0___readb(const volatile void *Model0_addr) { unsigned char Model0_ret; asm volatile("mov" "b" " %1,%0":"=q" (Model0_ret) :"m" (*(volatile unsigned char *)Model0_addr) ); return Model0_ret; }
static inline __attribute__((no_instrument_function)) unsigned short Model0___readw(const volatile void *Model0_addr) { unsigned short Model0_ret; asm volatile("mov" "w" " %1,%0":"=r" (Model0_ret) :"m" (*(volatile unsigned short *)Model0_addr) ); return Model0_ret; }
static inline __attribute__((no_instrument_function)) unsigned int Model0___readl(const volatile void *Model0_addr) { unsigned int Model0_ret; asm volatile("mov" "l" " %1,%0":"=r" (Model0_ret) :"m" (*(volatile unsigned int *)Model0_addr) ); return Model0_ret; }

static inline __attribute__((no_instrument_function)) void Model0_writeb(unsigned char Model0_val, volatile void *Model0_addr) { asm volatile("mov" "b" " %0,%1": :"q" (Model0_val), "m" (*(volatile unsigned char *)Model0_addr) :"memory"); }
static inline __attribute__((no_instrument_function)) void Model0_writew(unsigned short Model0_val, volatile void *Model0_addr) { asm volatile("mov" "w" " %0,%1": :"r" (Model0_val), "m" (*(volatile unsigned short *)Model0_addr) :"memory"); }
static inline __attribute__((no_instrument_function)) void Model0_writel(unsigned int Model0_val, volatile void *Model0_addr) { asm volatile("mov" "l" " %0,%1": :"r" (Model0_val), "m" (*(volatile unsigned int *)Model0_addr) :"memory"); }

static inline __attribute__((no_instrument_function)) void Model0___writeb(unsigned char Model0_val, volatile void *Model0_addr) { asm volatile("mov" "b" " %0,%1": :"q" (Model0_val), "m" (*(volatile unsigned char *)Model0_addr) ); }
static inline __attribute__((no_instrument_function)) void Model0___writew(unsigned short Model0_val, volatile void *Model0_addr) { asm volatile("mov" "w" " %0,%1": :"r" (Model0_val), "m" (*(volatile unsigned short *)Model0_addr) ); }
static inline __attribute__((no_instrument_function)) void Model0___writel(unsigned int Model0_val, volatile void *Model0_addr) { asm volatile("mov" "l" " %0,%1": :"r" (Model0_val), "m" (*(volatile unsigned int *)Model0_addr) ); }
static inline __attribute__((no_instrument_function)) unsigned long Model0_readq(const volatile void *Model0_addr) { unsigned long Model0_ret; asm volatile("mov" "q" " %1,%0":"=r" (Model0_ret) :"m" (*(volatile unsigned long *)Model0_addr) :"memory"); return Model0_ret; }
static inline __attribute__((no_instrument_function)) void Model0_writeq(unsigned long Model0_val, volatile void *Model0_addr) { asm volatile("mov" "q" " %0,%1": :"r" (Model0_val), "m" (*(volatile unsigned long *)Model0_addr) :"memory"); }







/* Let people know that we have them */





/**
 *	virt_to_phys	-	map virtual addresses to physical
 *	@address: address to remap
 *
 *	The returned physical address is the physical (CPU) mapping for
 *	the memory address given. It is only valid to use this function on
 *	addresses directly mapped or allocated via kmalloc.
 *
 *	This function does not give bus mappings for DMA transfers. In
 *	almost all conceivable cases a device driver should not be using
 *	this function
 */

static inline __attribute__((no_instrument_function)) Model0_phys_addr_t Model0_virt_to_phys(volatile void *Model0_address)
{
 return Model0___phys_addr_nodebug((unsigned long)(Model0_address));
}

/**
 *	phys_to_virt	-	map physical address to virtual
 *	@address: address to remap
 *
 *	The returned virtual address is a current CPU mapping for
 *	the memory address given. It is only valid to use this function on
 *	addresses that have a kernel mapping
 *
 *	This function does not handle bus mappings for DMA transfers. In
 *	almost all conceivable cases a device driver should not be using
 *	this function
 */

static inline __attribute__((no_instrument_function)) void *Model0_phys_to_virt(Model0_phys_addr_t Model0_address)
{
 return ((void *)((unsigned long)(Model0_address)+((unsigned long)(0xffff880000000000UL))));
}

/*
 * Change "struct page" to physical address.
 */


/*
 * ISA I/O bus memory addresses are 1:1 with the physical address.
 * However, we truncate the address to unsigned int to avoid undesirable
 * promitions in legacy drivers.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_isa_virt_to_bus(volatile void *Model0_address)
{
 return (unsigned int)Model0_virt_to_phys(Model0_address);
}



/*
 * However PCI ones are not necessarily 1:1 and therefore these interfaces
 * are forbidden in portable PCI drivers.
 *
 * Allow them on x86 for legacy drivers, though.
 */



/**
 * ioremap     -   map bus memory into CPU space
 * @offset:    bus address of the memory
 * @size:      size of the resource to map
 *
 * ioremap performs a platform specific sequence of operations to
 * make bus memory CPU accessible via the readb/readw/readl/writeb/
 * writew/writel functions and the other mmio helpers. The returned
 * address is not guaranteed to be usable directly as a virtual
 * address.
 *
 * If the area you are trying to map is a PCI BAR you should have a
 * look at pci_iomap().
 */
extern void *Model0_ioremap_nocache(Model0_resource_size_t Model0_offset, unsigned long Model0_size);
extern void *Model0_ioremap_uc(Model0_resource_size_t Model0_offset, unsigned long Model0_size);


extern void *Model0_ioremap_cache(Model0_resource_size_t Model0_offset, unsigned long Model0_size);
extern void *Model0_ioremap_prot(Model0_resource_size_t Model0_offset, unsigned long Model0_size,
    unsigned long Model0_prot_val);

/*
 * The default ioremap() behavior is non-cached:
 */
static inline __attribute__((no_instrument_function)) void *Model0_ioremap(Model0_resource_size_t Model0_offset, unsigned long Model0_size)
{
 return Model0_ioremap_nocache(Model0_offset, Model0_size);
}

extern void Model0_iounmap(volatile void *Model0_addr);

extern void Model0_set_iounmap_nonlazy(void);










/*
 * These are the "generic" interfaces for doing new-style
 * memory-mapped or PIO accesses. Architectures may do
 * their own arch-optimized versions, these just act as
 * wrappers around the old-style IO register access functions:
 * read[bwl]/write[bwl]/in[bwl]/out[bwl]
 *
 * Don't include this directly, include it from <asm/io.h>.
 */

/*
 * Read/write from/to an (offsettable) iomem cookie. It might be a PIO
 * access or a MMIO access, these functions don't care. The info is
 * encoded in the hardware mapping set up by the mapping functions
 * (or the cookie itself, depending on implementation and hw).
 *
 * The generic routines just encode the PIO/MMIO as part of the
 * cookie, and coldly assume that the MMIO IO mappings are not
 * in the low address range. Architectures for which this is not
 * true can't use this generic implementation.
 */
extern unsigned int Model0_ioread8(void *);
extern unsigned int Model0_ioread16(void *);
extern unsigned int Model0_ioread16be(void *);
extern unsigned int Model0_ioread32(void *);
extern unsigned int Model0_ioread32be(void *);

extern Model0_u64 Model0_ioread64(void *);
extern Model0_u64 Model0_ioread64be(void *);


extern void Model0_iowrite8(Model0_u8, void *);
extern void Model0_iowrite16(Model0_u16, void *);
extern void Model0_iowrite16be(Model0_u16, void *);
extern void Model0_iowrite32(Model0_u32, void *);
extern void Model0_iowrite32be(Model0_u32, void *);

extern void Model0_iowrite64(Model0_u64, void *);
extern void Model0_iowrite64be(Model0_u64, void *);


/*
 * "string" versions of the above. Note that they
 * use native byte ordering for the accesses (on
 * the assumption that IO and memory agree on a
 * byte order, and CPU byteorder is irrelevant).
 *
 * They do _not_ update the port address. If you
 * want MMIO that copies stuff laid out in MMIO
 * memory across multiple ports, use "memcpy_toio()"
 * and friends.
 */
extern void Model0_ioread8_rep(void *Model0_port, void *Model0_buf, unsigned long Model0_count);
extern void Model0_ioread16_rep(void *Model0_port, void *Model0_buf, unsigned long Model0_count);
extern void Model0_ioread32_rep(void *Model0_port, void *Model0_buf, unsigned long Model0_count);

extern void Model0_iowrite8_rep(void *Model0_port, const void *Model0_buf, unsigned long Model0_count);
extern void Model0_iowrite16_rep(void *Model0_port, const void *Model0_buf, unsigned long Model0_count);
extern void Model0_iowrite32_rep(void *Model0_port, const void *Model0_buf, unsigned long Model0_count);


/* Create a virtual mapping cookie for an IO port range */
extern void *Model0_ioport_map(unsigned long Model0_port, unsigned int Model0_nr);
extern void Model0_ioport_unmap(void *);
/* Destroy a virtual mapping cookie for a PCI BAR (memory or IO) */
struct Model0_pci_dev;
extern void Model0_pci_iounmap(struct Model0_pci_dev *Model0_dev, void *);







/* Generic I/O port emulation, based on MN10300 code
 *
 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public Licence
 * as published by the Free Software Foundation; either version
 * 2 of the Licence, or (at your option) any later version.
 */



struct Model0_pci_dev;

/* Create a virtual mapping cookie for a PCI BAR (memory or IO) */
extern void *Model0_pci_iomap(struct Model0_pci_dev *Model0_dev, int Model0_bar, unsigned long Model0_max);
extern void *Model0_pci_iomap_wc(struct Model0_pci_dev *Model0_dev, int Model0_bar, unsigned long Model0_max);
extern void *Model0_pci_iomap_range(struct Model0_pci_dev *Model0_dev, int Model0_bar,
         unsigned long Model0_offset,
         unsigned long Model0_maxlen);
extern void *Model0_pci_iomap_wc_range(struct Model0_pci_dev *Model0_dev, int Model0_bar,
     unsigned long Model0_offset,
     unsigned long Model0_maxlen);
/* Create a virtual mapping cookie for a port on a given PCI device.
 * Do not call this directly, it exists to make it easier for architectures
 * to override */

/*
 * Convert a virtual cached pointer to an uncached pointer
 */


static inline __attribute__((no_instrument_function)) void
Model0_memset_io(volatile void *Model0_addr, unsigned char Model0_val, Model0_size_t Model0_count)
{
 memset((void *)Model0_addr, Model0_val, Model0_count);
}

static inline __attribute__((no_instrument_function)) void
Model0_memcpy_fromio(void *Model0_dst, const volatile void *Model0_src, Model0_size_t Model0_count)
{
 ({ Model0_size_t Model0___len = (Model0_count); void *Model0___ret; if (__builtin_constant_p(Model0_count) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_dst), ((const void *)Model0_src), Model0___len); else Model0___ret = __builtin_memcpy((Model0_dst), ((const void *)Model0_src), Model0___len); Model0___ret; });
}

static inline __attribute__((no_instrument_function)) void
Model0_memcpy_toio(volatile void *Model0_dst, const void *Model0_src, Model0_size_t Model0_count)
{
 ({ Model0_size_t Model0___len = (Model0_count); void *Model0___ret; if (__builtin_constant_p(Model0_count) && Model0___len >= 64) Model0___ret = Model0___memcpy(((void *)Model0_dst), (Model0_src), Model0___len); else Model0___ret = __builtin_memcpy(((void *)Model0_dst), (Model0_src), Model0___len); Model0___ret; });
}

/*
 * ISA space is 'always mapped' on a typical x86 system, no need to
 * explicitly ioremap() it. The fact that the ISA IO space is mapped
 * to PAGE_OFFSET is pure coincidence - it does not mean ISA values
 * are physical addresses. The following constant pointer can be
 * used as the IO-area pointer (it can be iounmapped as well, so the
 * analogy with PCI is quite large):
 */


/*
 *	Cache management
 *
 *	This needed for two cases
 *	1. Out of order aware processors
 *	2. Accidentally out of order processors (PPro errata #51)
 */

static inline __attribute__((no_instrument_function)) void Model0_flush_write_buffers(void)
{



}



extern void Model0_native_io_delay(void);

extern int Model0_io_delay_type;
extern void Model0_io_delay_init(void);





static inline __attribute__((no_instrument_function)) void Model0_slow_down_io(void)
{
 Model0_native_io_delay();





}
static inline __attribute__((no_instrument_function)) void Model0_outb(unsigned char Model0_value, int Model0_port) { asm volatile("out" "b" " %" "b" "0, %w1" : : "a"(Model0_value), "Nd"(Model0_port)); } static inline __attribute__((no_instrument_function)) unsigned char Model0_inb(int Model0_port) { unsigned char Model0_value; asm volatile("in" "b" " %w1, %" "b" "0" : "=a"(Model0_value) : "Nd"(Model0_port)); return Model0_value; } static inline __attribute__((no_instrument_function)) void Model0_outb_p(unsigned char Model0_value, int Model0_port) { Model0_outb(Model0_value, Model0_port); Model0_slow_down_io(); } static inline __attribute__((no_instrument_function)) unsigned char Model0_inb_p(int Model0_port) { unsigned char Model0_value = Model0_inb(Model0_port); Model0_slow_down_io(); return Model0_value; } static inline __attribute__((no_instrument_function)) void Model0_outsb(int Model0_port, const void *Model0_addr, unsigned long Model0_count) { asm volatile("rep; outs" "b" : "+S"(Model0_addr), "+c"(Model0_count) : "d"(Model0_port)); } static inline __attribute__((no_instrument_function)) void Model0_insb(int Model0_port, void *Model0_addr, unsigned long Model0_count) { asm volatile("rep; ins" "b" : "+D"(Model0_addr), "+c"(Model0_count) : "d"(Model0_port)); }
static inline __attribute__((no_instrument_function)) void Model0_outw(unsigned short Model0_value, int Model0_port) { asm volatile("out" "w" " %" "w" "0, %w1" : : "a"(Model0_value), "Nd"(Model0_port)); } static inline __attribute__((no_instrument_function)) unsigned short Model0_inw(int Model0_port) { unsigned short Model0_value; asm volatile("in" "w" " %w1, %" "w" "0" : "=a"(Model0_value) : "Nd"(Model0_port)); return Model0_value; } static inline __attribute__((no_instrument_function)) void Model0_outw_p(unsigned short Model0_value, int Model0_port) { Model0_outw(Model0_value, Model0_port); Model0_slow_down_io(); } static inline __attribute__((no_instrument_function)) unsigned short Model0_inw_p(int Model0_port) { unsigned short Model0_value = Model0_inw(Model0_port); Model0_slow_down_io(); return Model0_value; } static inline __attribute__((no_instrument_function)) void Model0_outsw(int Model0_port, const void *Model0_addr, unsigned long Model0_count) { asm volatile("rep; outs" "w" : "+S"(Model0_addr), "+c"(Model0_count) : "d"(Model0_port)); } static inline __attribute__((no_instrument_function)) void Model0_insw(int Model0_port, void *Model0_addr, unsigned long Model0_count) { asm volatile("rep; ins" "w" : "+D"(Model0_addr), "+c"(Model0_count) : "d"(Model0_port)); }
static inline __attribute__((no_instrument_function)) void Model0_outl(unsigned int Model0_value, int Model0_port) { asm volatile("out" "l" " %" "" "0, %w1" : : "a"(Model0_value), "Nd"(Model0_port)); } static inline __attribute__((no_instrument_function)) unsigned int Model0_inl(int Model0_port) { unsigned int Model0_value; asm volatile("in" "l" " %w1, %" "" "0" : "=a"(Model0_value) : "Nd"(Model0_port)); return Model0_value; } static inline __attribute__((no_instrument_function)) void Model0_outl_p(unsigned int Model0_value, int Model0_port) { Model0_outl(Model0_value, Model0_port); Model0_slow_down_io(); } static inline __attribute__((no_instrument_function)) unsigned int Model0_inl_p(int Model0_port) { unsigned int Model0_value = Model0_inl(Model0_port); Model0_slow_down_io(); return Model0_value; } static inline __attribute__((no_instrument_function)) void Model0_outsl(int Model0_port, const void *Model0_addr, unsigned long Model0_count) { asm volatile("rep; outs" "l" : "+S"(Model0_addr), "+c"(Model0_count) : "d"(Model0_port)); } static inline __attribute__((no_instrument_function)) void Model0_insl(int Model0_port, void *Model0_addr, unsigned long Model0_count) { asm volatile("rep; ins" "l" : "+D"(Model0_addr), "+c"(Model0_count) : "d"(Model0_port)); }

extern void *Model0_xlate_dev_mem_ptr(Model0_phys_addr_t Model0_phys);
extern void Model0_unxlate_dev_mem_ptr(Model0_phys_addr_t Model0_phys, void *Model0_addr);

extern int Model0_ioremap_change_attr(unsigned long Model0_vaddr, unsigned long Model0_size,
    enum Model0_page_cache_mode Model0_pcm);
extern void *Model0_ioremap_wc(Model0_resource_size_t Model0_offset, unsigned long Model0_size);
extern void *Model0_ioremap_wt(Model0_resource_size_t Model0_offset, unsigned long Model0_size);

extern bool Model0_is_early_ioremap_ptep(Model0_pte_t *Model0_ptep);
extern int __attribute__((warn_unused_result)) Model0_arch_phys_wc_index(int Model0_handle);


extern int __attribute__((warn_unused_result)) Model0_arch_phys_wc_add(unsigned long Model0_base,
      unsigned long Model0_size);
extern void Model0_arch_phys_wc_del(int Model0_handle);

/* This must match data at realmode.S */
struct Model0_real_mode_header {
 Model0_u32 Model0_text_start;
 Model0_u32 Model0_ro_end;
 /* SMP trampoline */
 Model0_u32 Model0_trampoline_start;
 Model0_u32 Model0_trampoline_status;
 Model0_u32 Model0_trampoline_header;

 Model0_u32 Model0_trampoline_pgd;

 /* ACPI S3 wakeup */

 Model0_u32 Model0_wakeup_start;
 Model0_u32 Model0_wakeup_header;

 /* APM/BIOS reboot */
 Model0_u32 Model0_machine_real_restart_asm;

 Model0_u32 Model0_machine_real_restart_seg;

};

/* This must match data at trampoline_32/64.S */
struct Model0_trampoline_header {






 Model0_u64 Model0_start;
 Model0_u64 Model0_efer;
 Model0_u32 Model0_cr4;

};

extern struct Model0_real_mode_header *Model0_real_mode_header;
extern unsigned char Model0_real_mode_blob_end[];

extern unsigned long Model0_init_rsp;
extern unsigned long Model0_initial_code;
extern unsigned long Model0_initial_gs;

extern unsigned char Model0_real_mode_blob[];
extern unsigned char Model0_real_mode_relocs[];





extern unsigned char Model0_secondary_startup_64[];


static inline __attribute__((no_instrument_function)) Model0_size_t Model0_real_mode_size_needed(void)
{
 if (Model0_real_mode_header)
  return 0; /* already allocated. */

 return ((((Model0_real_mode_blob_end - Model0_real_mode_blob)) + ((typeof((Model0_real_mode_blob_end - Model0_real_mode_blob)))((((1UL) << 12))) - 1)) & ~((typeof((Model0_real_mode_blob_end - Model0_real_mode_blob)))((((1UL) << 12))) - 1));
}

void Model0_set_real_mode_mem(Model0_phys_addr_t Model0_mem, Model0_size_t Model0_size);
void Model0_reserve_real_mode(void);






extern int Model0_acpi_lapic;
extern int Model0_acpi_ioapic;
extern int Model0_acpi_noirq;
extern int Model0_acpi_strict;
extern int Model0_acpi_disabled;
extern int Model0_acpi_pci_disabled;
extern int Model0_acpi_skip_timer_override;
extern int Model0_acpi_use_timer_override;
extern int Model0_acpi_fix_pin2_polarity;
extern int Model0_acpi_disable_cmcff;

extern Model0_u8 Model0_acpi_sci_flags;
extern int Model0_acpi_sci_override_gsi;
void Model0_acpi_pic_sci_set_trigger(unsigned int, Model0_u16);

extern int (*Model0___acpi_register_gsi)(struct Model0_device *Model0_dev, Model0_u32 Model0_gsi,
      int Model0_trigger, int Model0_polarity);
extern void (*Model0___acpi_unregister_gsi)(Model0_u32 Model0_gsi);

static inline __attribute__((no_instrument_function)) void Model0_disable_acpi(void)
{
 Model0_acpi_disabled = 1;
 Model0_acpi_pci_disabled = 1;
 Model0_acpi_noirq = 1;
}

extern int Model0_acpi_gsi_to_irq(Model0_u32 Model0_gsi, unsigned int *Model0_irq);

static inline __attribute__((no_instrument_function)) void Model0_acpi_noirq_set(void) { Model0_acpi_noirq = 1; }
static inline __attribute__((no_instrument_function)) void Model0_acpi_disable_pci(void)
{
 Model0_acpi_pci_disabled = 1;
 Model0_acpi_noirq_set();
}

/* Low-level suspend routine. */
extern int (*Model0_acpi_suspend_lowlevel)(void);

/* Physical address to resume after wakeup */


/*
 * Check if the CPU can handle C2 and deeper
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_acpi_processor_cstate_check(unsigned int Model0_max_cstate)
{
 /*
	 * Early models (<=5) of AMD Opterons are not supposed to go into
	 * C2 state.
	 *
	 * Steppings 0x0A and later are good
	 */
 if (Model0_boot_cpu_data.Model0_x86 == 0x0F &&
     Model0_boot_cpu_data.Model0_x86_vendor == 2 &&
     Model0_boot_cpu_data.Model0_x86_model <= 0x05 &&
     Model0_boot_cpu_data.Model0_x86_mask < 0x0A)
  return 1;
 else if (Model0_amd_e400_c1e_detected)
  return 1;
 else
  return Model0_max_cstate;
}

static inline __attribute__((no_instrument_function)) bool Model0_arch_has_acpi_pdc(void)
{
 struct Model0_cpuinfo_x86 *Model0_c = &(*({ do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_info)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((&(Model0_cpu_info)))) *)((&(Model0_cpu_info))))); (typeof((typeof(*((&(Model0_cpu_info)))) *)((&(Model0_cpu_info))))) (Model0___ptr + (((Model0___per_cpu_offset[(0)])))); }); }));
 return (Model0_c->Model0_x86_vendor == 0 ||
  Model0_c->Model0_x86_vendor == 5);
}

static inline __attribute__((no_instrument_function)) void Model0_arch_acpi_set_pdc_bits(Model0_u32 *Model0_buf)
{
 struct Model0_cpuinfo_x86 *Model0_c = &(*({ do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_info)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((&(Model0_cpu_info)))) *)((&(Model0_cpu_info))))); (typeof((typeof(*((&(Model0_cpu_info)))) *)((&(Model0_cpu_info))))) (Model0___ptr + (((Model0___per_cpu_offset[(0)])))); }); }));

 Model0_buf[2] |= ((0x0010) | (0x0008) | (0x0002) | (0x0100) | (0x0200));

 if ((__builtin_constant_p(( 4*32+ 7)) && ( (((( 4*32+ 7))>>5)==(0) && (1UL<<((( 4*32+ 7))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 4*32+ 7))>>5)==(1) && (1UL<<((( 4*32+ 7))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 4*32+ 7))>>5)==(2) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(3) && (1UL<<((( 4*32+ 7))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 4*32+ 7))>>5)==(4) && (1UL<<((( 4*32+ 7))&31) & (0) )) || (((( 4*32+ 7))>>5)==(5) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(6) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(7) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(8) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(9) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(10) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(11) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(12) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(13) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(14) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(15) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(16) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(17) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 4*32+ 7))) ? Model0_constant_test_bit((( 4*32+ 7)), ((unsigned long *)((Model0_c)->Model0_x86_capability))) : Model0_variable_test_bit((( 4*32+ 7)), ((unsigned long *)((Model0_c)->Model0_x86_capability))))))
  Model0_buf[2] |= ((0x0008) | (0x0002) | (0x0020) | (0x0800) | (0x0001));

 if ((__builtin_constant_p(( 0*32+22)) && ( (((( 0*32+22))>>5)==(0) && (1UL<<((( 0*32+22))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 0*32+22))>>5)==(1) && (1UL<<((( 0*32+22))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 0*32+22))>>5)==(2) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(3) && (1UL<<((( 0*32+22))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 0*32+22))>>5)==(4) && (1UL<<((( 0*32+22))&31) & (0) )) || (((( 0*32+22))>>5)==(5) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(6) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(7) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(8) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(9) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(10) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(11) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(12) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(13) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(14) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(15) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(16) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(17) && (1UL<<((( 0*32+22))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 0*32+22))) ? Model0_constant_test_bit((( 0*32+22)), ((unsigned long *)((Model0_c)->Model0_x86_capability))) : Model0_variable_test_bit((( 0*32+22)), ((unsigned long *)((Model0_c)->Model0_x86_capability))))))
  Model0_buf[2] |= (0x0004);

 /*
	 * If mwait/monitor is unsupported, C2/C3_FFH will be disabled
	 */
 if (!(__builtin_constant_p(( 4*32+ 3)) && ( (((( 4*32+ 3))>>5)==(0) && (1UL<<((( 4*32+ 3))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 4*32+ 3))>>5)==(1) && (1UL<<((( 4*32+ 3))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 4*32+ 3))>>5)==(2) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(3) && (1UL<<((( 4*32+ 3))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 4*32+ 3))>>5)==(4) && (1UL<<((( 4*32+ 3))&31) & (0) )) || (((( 4*32+ 3))>>5)==(5) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(6) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(7) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(8) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(9) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(10) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(11) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(12) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(13) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(14) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(15) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(16) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(17) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 4*32+ 3))) ? Model0_constant_test_bit((( 4*32+ 3)), ((unsigned long *)((Model0_c)->Model0_x86_capability))) : Model0_variable_test_bit((( 4*32+ 3)), ((unsigned long *)((Model0_c)->Model0_x86_capability))))))
  Model0_buf[2] &= ~((0x0200));
}

static inline __attribute__((no_instrument_function)) bool Model0_acpi_has_cpu_in_madt(void)
{
 return !!Model0_acpi_lapic;
}
extern int Model0_x86_acpi_numa_init(void);









enum Model0_vsyscall_num {
 Model0___NR_vgettimeofday,
 Model0___NR_vtime,
 Model0___NR_vgetcpu,
};


/*
 * We can't declare FIXADDR_TOP as variable for x86_64 because vsyscall
 * uses fixmaps that relies on FIXADDR_TOP for proper address calculation.
 * Because of this, FIXADDR_TOP x86 integration was left as later work.
 */
/*
 * Here we define all the compile-time 'special' virtual
 * addresses. The point is to have a constant address at
 * compile time, but to set the physical address only
 * in the boot process.
 * for x86_32: We allocate these special addresses
 * from the end of virtual memory (0xfffff000) backwards.
 * Also this lets us do fail-safe vmalloc(), we
 * can guarantee that these special addresses and
 * vmalloc()-ed addresses never overlap.
 *
 * These 'compile-time allocated' memory buffers are
 * fixed-size 4k pages (or larger if used with an increment
 * higher than 1). Use set_fixmap(idx,phys) to associate
 * physical memory with fixmap indices.
 *
 * TLB entries of such buffers will not be flushed across
 * task switches.
 */
enum Model0_fixed_addresses {




 Model0_VSYSCALL_PAGE = (((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) - (-10UL << 20)) >> 12,


 Model0_FIX_DBGP_BASE,
 Model0_FIX_EARLYCON_MEM_BASE,

 Model0_FIX_OHCI1394_BASE,


 Model0_FIX_APIC_BASE, /* local (CPU) APIC) -- required for SMP or not */


 Model0_FIX_IO_APIC_BASE_0,
 Model0_FIX_IO_APIC_BASE_END = Model0_FIX_IO_APIC_BASE_0 + 128 - 1,

 Model0_FIX_RO_IDT, /* Virtual mapping for read-only IDT */
 Model0_FIX_TEXT_POKE1, /* reserve 2 pages for text_poke() */
 Model0_FIX_TEXT_POKE0, /* first page is last, because allocation is backward */



 Model0___end_of_permanent_fixed_addresses,

 /*
	 * 512 temporary boot-time mappings, used by early_ioremap(),
	 * before ioremap() is functional.
	 *
	 * If necessary we round it up to the next 512 pages boundary so
	 * that we can have a single pgd entry and a single pte table:
	 */



 Model0_FIX_BTMAP_END =
  (Model0___end_of_permanent_fixed_addresses ^
   (Model0___end_of_permanent_fixed_addresses + (64 * 8) - 1)) &
  -512
  ? Model0___end_of_permanent_fixed_addresses + (64 * 8) -
    (Model0___end_of_permanent_fixed_addresses & ((64 * 8) - 1))
  : Model0___end_of_permanent_fixed_addresses,
 Model0_FIX_BTMAP_BEGIN = Model0_FIX_BTMAP_END + (64 * 8) - 1,






 Model0___end_of_fixed_addresses
};


extern void Model0_reserve_top_address(unsigned long Model0_reserve);




extern int Model0_fixmaps_set;

extern Model0_pte_t *Model0_kmap_pte;

extern Model0_pte_t *Model0_pkmap_page_table;

void Model0___native_set_fixmap(enum Model0_fixed_addresses Model0_idx, Model0_pte_t Model0_pte);
void Model0_native_set_fixmap(enum Model0_fixed_addresses Model0_idx,
         Model0_phys_addr_t Model0_phys, Model0_pgprot_t Model0_flags);


static inline __attribute__((no_instrument_function)) void Model0___set_fixmap(enum Model0_fixed_addresses Model0_idx,
    Model0_phys_addr_t Model0_phys, Model0_pgprot_t Model0_flags)
{
 Model0_native_set_fixmap(Model0_idx, Model0_phys, Model0_flags);
}



/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 * Break out common bits to asm-generic by Mark Salter, November 2013
 */
/*
 * 'index to address' translation. If anyone tries to use the idx
 * directly without translation, we catch the bug with a NULL-deference
 * kernel oops. Illegal ranges of incoming indices are caught too.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model0_fix_to_virt(const unsigned int Model0_idx)
{
 do { bool Model0___cond = !(!(Model0_idx >= Model0___end_of_fixed_addresses)); extern void Model0___compiletime_assert_31(void) ; if (Model0___cond) Model0___compiletime_assert_31(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 return (((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) - ((Model0_idx) << 12));
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_virt_to_fix(const unsigned long Model0_vaddr)
{
 do { if (__builtin_expect(!!(Model0_vaddr >= ((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) || Model0_vaddr < (((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) - (Model0___end_of_permanent_fixed_addresses << 12))), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/asm-generic/fixmap.h"), "i" (37), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 return ((((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) - ((Model0_vaddr)&(~(((1UL) << 12)-1)))) >> 12);
}

/*
 * Provide some reasonable defaults for page flags.
 * Not all architectures use all of these different types and some
 * architectures use different names.
 */
/* Return a pointer with offset calculated */
/*
 * Some hardware wants to get fixmapped without caching.
 */






/*
 * Some fixmaps are for IO
 */




void Model0___early_set_fixmap(enum Model0_fixed_addresses Model0_idx,
   Model0_phys_addr_t Model0_phys, Model0_pgprot_t Model0_flags);








struct Model0_notifier_block;
void Model0_idle_notifier_register(struct Model0_notifier_block *Model0_n);
void Model0_idle_notifier_unregister(struct Model0_notifier_block *Model0_n);


void Model0_enter_idle(void);
void Model0_exit_idle(void);






void Model0_amd_e400_remove_cpu(int Model0_cpu);



/*
 * Debugging macros
 */




/* Macros for apic_extnmi which controls external NMI masking */




/*
 * Define the default level of output to be very little
 * This can be turned up by using apic=verbose for more
 * information and apic=debug for _lots_ of information.
 * apic_verbosity is defined in apic.c
 */
static inline __attribute__((no_instrument_function)) void Model0_generic_apic_probe(void)
{
}




extern unsigned int Model0_apic_verbosity;
extern int Model0_local_apic_timer_c2_ok;

extern int Model0_disable_apic;
extern unsigned int Model0_lapic_timer_frequency;


extern void Model0___inquire_remote_apic(int Model0_apicid);






static inline __attribute__((no_instrument_function)) void Model0_default_inquire_remote_apic(int Model0_apicid)
{
 if (Model0_apic_verbosity >= 2)
  Model0___inquire_remote_apic(Model0_apicid);
}

/*
 * With 82489DX we can't rely on apic feature bit
 * retrieved via cpuid but still have to deal with
 * such an apic chip so we assume that SMP configuration
 * is found from MP table (64bit case uses ACPI mostly
 * which set smp presence flag as well so we are safe
 * to use this helper too).
 */
static inline __attribute__((no_instrument_function)) bool Model0_apic_from_smp_config(void)
{
 return Model0_smp_found_config && !Model0_disable_apic;
}

/*
 * Basic functions accessing APICs.
 */




extern int Model0_setup_profiling_timer(unsigned int);

static inline __attribute__((no_instrument_function)) void Model0_native_apic_mem_write(Model0_u32 Model0_reg, Model0_u32 Model0_v)
{
 volatile Model0_u32 *Model0_addr = (volatile Model0_u32 *)((Model0_fix_to_virt(Model0_FIX_APIC_BASE)) + Model0_reg);

 asm volatile ("661:\n\t" "movl %0, %P1" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "(18*32 + (5))" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "xchgl %0, %P1" "\n" "665""1" ":\n\t" ".popsection" : "=r" (Model0_v), "=m" (*Model0_addr) : "i" (0), "0" (Model0_v), "m" (*Model0_addr));


}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_native_apic_mem_read(Model0_u32 Model0_reg)
{
 return *((volatile Model0_u32 *)((Model0_fix_to_virt(Model0_FIX_APIC_BASE)) + Model0_reg));
}

extern void Model0_native_apic_wait_icr_idle(void);
extern Model0_u32 Model0_native_safe_apic_wait_icr_idle(void);
extern void Model0_native_apic_icr_write(Model0_u32 Model0_low, Model0_u32 Model0_id);
extern Model0_u64 Model0_native_apic_icr_read(void);

static inline __attribute__((no_instrument_function)) bool Model0_apic_is_x2apic_enabled(void)
{
 Model0_u64 Model0_msr;

 if (Model0_rdmsrl_safe(0x0000001b, &Model0_msr))
  return false;
 return Model0_msr & (1UL << 10);
}

extern void Model0_enable_IR_x2apic(void);

extern int Model0_get_physical_broadcast(void);

extern int Model0_lapic_get_maxlvt(void);
extern void Model0_clear_local_APIC(void);
extern void Model0_disconnect_bsp_APIC(int Model0_virt_wire_setup);
extern void Model0_disable_local_APIC(void);
extern void Model0_lapic_shutdown(void);
extern void Model0_sync_Arb_IDs(void);
extern void Model0_init_bsp_APIC(void);
extern void Model0_setup_local_APIC(void);
extern void Model0_init_apic_mappings(void);
void Model0_register_lapic_address(unsigned long Model0_address);
extern void Model0_setup_boot_APIC_clock(void);
extern void Model0_setup_secondary_APIC_clock(void);
extern void Model0_lapic_update_tsc_freq(void);
extern int Model0_APIC_init_uniprocessor(void);


static inline __attribute__((no_instrument_function)) int Model0_apic_force_enable(unsigned long Model0_addr)
{
 return -1;
}




extern int Model0_apic_bsp_setup(bool Model0_upmode);
extern void Model0_apic_ap_setup(void);

/*
 * On 32bit this is mach-xxx local
 */

extern int Model0_apic_is_clustered_box(void);







extern int Model0_setup_APIC_eilvt(Model0_u8 Model0_lvt_off, Model0_u8 Model0_vector, Model0_u8 Model0_msg_type, Model0_u8 Model0_mask);
static inline __attribute__((no_instrument_function)) void Model0_check_x2apic(void) { }
static inline __attribute__((no_instrument_function)) void Model0_x2apic_setup(void) { }
static inline __attribute__((no_instrument_function)) int Model0_x2apic_enabled(void) { return 0; }
/*
 * Copyright 2004 James Cleverdon, IBM.
 * Subject to the GNU Public License, v.2
 *
 * Generic APIC sub-arch data struct.
 *
 * Hacked for x86-64 by James Cleverdon from i386 architecture code by
 * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
 * James Cleverdon.
 */
struct Model0_apic {
 char *Model0_name;

 int (*Model0_probe)(void);
 int (*Model0_acpi_madt_oem_check)(char *Model0_oem_id, char *Model0_oem_table_id);
 int (*Model0_apic_id_valid)(int Model0_apicid);
 int (*Model0_apic_id_registered)(void);

 Model0_u32 Model0_irq_delivery_mode;
 Model0_u32 Model0_irq_dest_mode;

 const struct Model0_cpumask *(*Model0_target_cpus)(void);

 int Model0_disable_esr;

 int Model0_dest_logical;
 unsigned long (*Model0_check_apicid_used)(Model0_physid_mask_t *Model0_map, int Model0_apicid);

 void (*Model0_vector_allocation_domain)(int Model0_cpu, struct Model0_cpumask *Model0_retmask,
      const struct Model0_cpumask *Model0_mask);
 void (*Model0_init_apic_ldr)(void);

 void (*Model0_ioapic_phys_id_map)(Model0_physid_mask_t *Model0_phys_map, Model0_physid_mask_t *Model0_retmap);

 void (*Model0_setup_apic_routing)(void);
 int (*Model0_cpu_present_to_apicid)(int Model0_mps_cpu);
 void (*Model0_apicid_to_cpu_present)(int Model0_phys_apicid, Model0_physid_mask_t *Model0_retmap);
 int (*Model0_check_phys_apicid_present)(int Model0_phys_apicid);
 int (*Model0_phys_pkg_id)(int Model0_cpuid_apic, int Model0_index_msb);

 unsigned int (*Model0_get_apic_id)(unsigned long Model0_x);
 unsigned long (*Model0_set_apic_id)(unsigned int Model0_id);

 int (*Model0_cpu_mask_to_apicid_and)(const struct Model0_cpumask *Model0_cpumask,
          const struct Model0_cpumask *Model0_andmask,
          unsigned int *Model0_apicid);

 /* ipi */
 void (*Model0_send_IPI)(int Model0_cpu, int Model0_vector);
 void (*Model0_send_IPI_mask)(const struct Model0_cpumask *Model0_mask, int Model0_vector);
 void (*Model0_send_IPI_mask_allbutself)(const struct Model0_cpumask *Model0_mask,
      int Model0_vector);
 void (*Model0_send_IPI_allbutself)(int Model0_vector);
 void (*Model0_send_IPI_all)(int Model0_vector);
 void (*Model0_send_IPI_self)(int Model0_vector);

 /* wakeup_secondary_cpu */
 int (*Model0_wakeup_secondary_cpu)(int Model0_apicid, unsigned long Model0_start_eip);

 void (*Model0_inquire_remote_apic)(int Model0_apicid);

 /* apic ops */
 Model0_u32 (*Model0_read)(Model0_u32 Model0_reg);
 void (*Model0_write)(Model0_u32 Model0_reg, Model0_u32 Model0_v);
 /*
	 * ->eoi_write() has the same signature as ->write().
	 *
	 * Drivers can support both ->eoi_write() and ->write() by passing the same
	 * callback value. Kernel can override ->eoi_write() and fall back
	 * on write for EOI.
	 */
 void (*Model0_eoi_write)(Model0_u32 Model0_reg, Model0_u32 Model0_v);
 Model0_u64 (*Model0_icr_read)(void);
 void (*Model0_icr_write)(Model0_u32 Model0_low, Model0_u32 Model0_high);
 void (*Model0_wait_icr_idle)(void);
 Model0_u32 (*Model0_safe_wait_icr_idle)(void);
};

/*
 * Pointer to the local APIC driver in use on this system (there's
 * always just one such driver in use - the kernel decides via an
 * early probing process which one it picks - and then sticks to it):
 */
extern struct Model0_apic *Model0_apic;

/*
 * APIC drivers are probed based on how they are listed in the .apicdrivers
 * section. So the order is important and enforced by the ordering
 * of different apic driver files in the Makefile.
 *
 * For the files having two apic drivers, we use apic_drivers()
 * to enforce the order with in them.
 */
extern struct Model0_apic *Model0___apicdrivers[], *Model0___apicdrivers_end[];

/*
 * APIC functionality to boot other CPUs - only used on SMP:
 */

extern int Model0_wakeup_secondary_cpu_via_nmi(int Model0_apicid, unsigned long Model0_start_eip);




static inline __attribute__((no_instrument_function)) Model0_u32 Model0_apic_read(Model0_u32 Model0_reg)
{
 return Model0_apic->Model0_read(Model0_reg);
}

static inline __attribute__((no_instrument_function)) void Model0_apic_write(Model0_u32 Model0_reg, Model0_u32 Model0_val)
{
 Model0_apic->Model0_write(Model0_reg, Model0_val);
}

static inline __attribute__((no_instrument_function)) void Model0_apic_eoi(void)
{
 Model0_apic->Model0_eoi_write(0xB0, 0x0);
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_apic_icr_read(void)
{
 return Model0_apic->Model0_icr_read();
}

static inline __attribute__((no_instrument_function)) void Model0_apic_icr_write(Model0_u32 Model0_low, Model0_u32 Model0_high)
{
 Model0_apic->Model0_icr_write(Model0_low, Model0_high);
}

static inline __attribute__((no_instrument_function)) void Model0_apic_wait_icr_idle(void)
{
 Model0_apic->Model0_wait_icr_idle();
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_safe_apic_wait_icr_idle(void)
{
 return Model0_apic->Model0_safe_wait_icr_idle();
}

extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_apic_set_eoi_write(void (*Model0_eoi_write)(Model0_u32 Model0_reg, Model0_u32 Model0_v));
static inline __attribute__((no_instrument_function)) void Model0_ack_APIC_irq(void)
{
 /*
	 * ack_APIC_irq() actually gets compiled as a single instruction
	 * ... yummie.
	 */
 Model0_apic_eoi();
}

static inline __attribute__((no_instrument_function)) unsigned Model0_default_get_apic_id(unsigned long Model0_x)
{
 unsigned int Model0_ver = ((Model0_apic_read(0x30)) & 0xFFu);

 if (((Model0_ver) >= 0x14) || (__builtin_constant_p(( 3*32+26)) && ( (((( 3*32+26))>>5)==(0) && (1UL<<((( 3*32+26))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 3*32+26))>>5)==(1) && (1UL<<((( 3*32+26))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 3*32+26))>>5)==(2) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(3) && (1UL<<((( 3*32+26))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 3*32+26))>>5)==(4) && (1UL<<((( 3*32+26))&31) & (0) )) || (((( 3*32+26))>>5)==(5) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(6) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(7) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(8) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(9) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(10) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(11) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(12) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(13) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(14) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(15) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(16) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(17) && (1UL<<((( 3*32+26))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 3*32+26))) ? Model0_constant_test_bit((( 3*32+26)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))) : Model0_variable_test_bit((( 3*32+26)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))))))
  return (Model0_x >> 24) & 0xFF;
 else
  return (Model0_x >> 24) & 0x0F;
}

/*
 * Warm reset vector position:
 */




extern void Model0_apic_send_IPI_self(int Model0_vector);

extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model0_x2apic_extra_bits;

extern int Model0_default_cpu_present_to_apicid(int Model0_mps_cpu);
extern int Model0_default_check_phys_apicid_present(int Model0_phys_apicid);


extern void Model0_generic_bigsmp_probe(void);








static inline __attribute__((no_instrument_function)) const struct Model0_cpumask *Model0_default_target_cpus(void)
{

 return ((const struct Model0_cpumask *)&Model0___cpu_online_mask);



}

static inline __attribute__((no_instrument_function)) const struct Model0_cpumask *Model0_online_target_cpus(void)
{
 return ((const struct Model0_cpumask *)&Model0___cpu_online_mask);
}

extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model0_u16) Model0_x86_bios_cpu_apicid; extern __typeof__(Model0_u16) *Model0_x86_bios_cpu_apicid_early_ptr; extern __typeof__(Model0_u16) Model0_x86_bios_cpu_apicid_early_map[];


static inline __attribute__((no_instrument_function)) unsigned int Model0_read_apic_id(void)
{
 unsigned int Model0_reg;

 Model0_reg = Model0_apic_read(0x20);

 return Model0_apic->Model0_get_apic_id(Model0_reg);
}

static inline __attribute__((no_instrument_function)) int Model0_default_apic_id_valid(int Model0_apicid)
{
 return (Model0_apicid < 255);
}

extern int Model0_default_acpi_madt_oem_check(char *, char *);

extern void Model0_default_setup_apic_routing(void);

extern struct Model0_apic Model0_apic_noop;
static inline __attribute__((no_instrument_function)) int
Model0_flat_cpu_mask_to_apicid_and(const struct Model0_cpumask *Model0_cpumask,
       const struct Model0_cpumask *Model0_andmask,
       unsigned int *Model0_apicid)
{
 unsigned long Model0_cpu_mask = ((Model0_cpumask)->Model0_bits)[0] &
     ((Model0_andmask)->Model0_bits)[0] &
     ((((const struct Model0_cpumask *)&Model0___cpu_online_mask))->Model0_bits)[0] &
     0xFFu;

 if (__builtin_expect(!!(Model0_cpu_mask), 1)) {
  *Model0_apicid = (unsigned int)Model0_cpu_mask;
  return 0;
 } else {
  return -22;
 }
}

extern int
Model0_default_cpu_mask_to_apicid_and(const struct Model0_cpumask *Model0_cpumask,
          const struct Model0_cpumask *Model0_andmask,
          unsigned int *Model0_apicid);

static inline __attribute__((no_instrument_function)) void
Model0_flat_vector_allocation_domain(int Model0_cpu, struct Model0_cpumask *Model0_retmask,
         const struct Model0_cpumask *Model0_mask)
{
 /* Careful. Some cpus do not strictly honor the set of cpus
	 * specified in the interrupt destination when using lowest
	 * priority interrupt delivery mode.
	 *
	 * In particular there was a hyperthreading cpu observed to
	 * deliver interrupts to the wrong hyperthread when only one
	 * hyperthread was specified in the interrupt desitination.
	 */
 Model0_cpumask_clear(Model0_retmask);
 ((Model0_retmask)->Model0_bits)[0] = 0xFFu;
}

static inline __attribute__((no_instrument_function)) void
Model0_default_vector_allocation_domain(int Model0_cpu, struct Model0_cpumask *Model0_retmask,
     const struct Model0_cpumask *Model0_mask)
{
 Model0_cpumask_copy(Model0_retmask, (Model0_get_cpu_mask(Model0_cpu)));
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_default_check_apicid_used(Model0_physid_mask_t *Model0_map, int Model0_apicid)
{
 return (__builtin_constant_p((Model0_apicid)) ? Model0_constant_test_bit((Model0_apicid), ((*Model0_map).Model0_mask)) : Model0_variable_test_bit((Model0_apicid), ((*Model0_map).Model0_mask)));
}

static inline __attribute__((no_instrument_function)) void Model0_default_ioapic_phys_id_map(Model0_physid_mask_t *Model0_phys_map, Model0_physid_mask_t *Model0_retmap)
{
 *Model0_retmap = *Model0_phys_map;
}

static inline __attribute__((no_instrument_function)) int Model0___default_cpu_present_to_apicid(int Model0_mps_cpu)
{
 if (Model0_mps_cpu < Model0_nr_cpu_ids && Model0_cpumask_test_cpu((Model0_mps_cpu), ((const struct Model0_cpumask *)&Model0___cpu_present_mask)))
  return (int)(*({ do { const void *Model0___vpp_verify = (typeof((&(Model0_x86_bios_cpu_apicid)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((&(Model0_x86_bios_cpu_apicid)))) *)((&(Model0_x86_bios_cpu_apicid))))); (typeof((typeof(*((&(Model0_x86_bios_cpu_apicid)))) *)((&(Model0_x86_bios_cpu_apicid))))) (Model0___ptr + (((Model0___per_cpu_offset[(Model0_mps_cpu)])))); }); }));
 else
  return 0xFFFFu;
}

static inline __attribute__((no_instrument_function)) int
Model0___default_check_phys_apicid_present(int Model0_phys_apicid)
{
 return (__builtin_constant_p((Model0_phys_apicid)) ? Model0_constant_test_bit((Model0_phys_apicid), ((Model0_phys_cpu_present_map).Model0_mask)) : Model0_variable_test_bit((Model0_phys_apicid), ((Model0_phys_cpu_present_map).Model0_mask)));
}
extern int Model0_default_cpu_present_to_apicid(int Model0_mps_cpu);
extern int Model0_default_check_phys_apicid_present(int Model0_phys_apicid);



extern void Model0_irq_enter(void);
extern void Model0_irq_exit(void);

static inline __attribute__((no_instrument_function)) void Model0_entering_irq(void)
{
 Model0_irq_enter();
 Model0_exit_idle();
}

static inline __attribute__((no_instrument_function)) void Model0_entering_ack_irq(void)
{
 Model0_entering_irq();
 Model0_ack_APIC_irq();
}

static inline __attribute__((no_instrument_function)) void Model0_ipi_entering_ack_irq(void)
{
 Model0_ack_APIC_irq();
 Model0_irq_enter();
}

static inline __attribute__((no_instrument_function)) void Model0_exiting_irq(void)
{
 Model0_irq_exit();
}

static inline __attribute__((no_instrument_function)) void Model0_exiting_ack_irq(void)
{
 Model0_irq_exit();
 /* Ack only at the end to avoid potential reentry */
 Model0_ack_APIC_irq();
}

extern void Model0_ioapic_zap_locks(void);











/*
 * Linux IRQ vector layout.
 *
 * There are 256 IDT entries (per CPU - each entry is 8 bytes) which can
 * be defined by Linux. They are used as a jump table by the CPU when a
 * given vector is triggered - by a CPU-external, CPU-internal or
 * software-triggered event.
 *
 * Linux sets the kernel code address each entry jumps to early during
 * bootup, and never changes them. This is the general layout of the
 * IDT entries:
 *
 *  Vectors   0 ...  31 : system traps and exceptions - hardcoded events
 *  Vectors  32 ... 127 : device interrupts
 *  Vector  128         : legacy int80 syscall interface
 *  Vectors 129 ... INVALIDATE_TLB_VECTOR_START-1 except 204 : device interrupts
 *  Vectors INVALIDATE_TLB_VECTOR_START ... 255 : special interrupts
 *
 * 64-bit x86 has per CPU IDT tables, 32-bit has one shared IDT table.
 *
 * This file enumerates the exact layout of them:
 */




/*
 * IDT vectors usable for external interrupt sources start at 0x20.
 * (0x80 is the syscall vector, 0x30-0x3f are for ISA)
 */

/*
 * We start allocating at 0x21 to spread out vectors evenly between
 * priority levels. (0x80 is the syscall vector)
 */


/*
 * Reserve the lowest usable vector (and hence lowest priority)  0x20 for
 * triggering cleanup after irq migration. 0x21-0x2f will still be used
 * for device interrupts.
 */




/*
 * Vectors 0x30-0x3f are used for ISA interrupts.
 *   round up to the next 16-vector boundary
 */


/*
 * Special IRQ vectors used by the SMP architecture, 0xf0-0xff
 *
 *  some of the following vectors are 'rare', they are merged
 *  into a single vector (CALL_FUNCTION_VECTOR) to save vector space.
 *  TLB, reschedule and local APIC vectors are performance-critical.
 */


/*
 * Sanity check
 */
/*
 * Generic system vector for platform specific use
 */



/*
 * IRQ work vector:
 */





/* Vector on which hypervisor callbacks will be delivered */


/* Vector for KVM to deliver posted interrupt IPI */




/*
 * Local APIC timer IRQ vector is on a different priority level,
 * to work around the 'lost local interrupt if more than 2 IRQ
 * sources per level' errata.
 */
/*
 * Size the maximum number of interrupts.
 *
 * If the irq_desc[] array has a sparse layout, we can size things
 * generously - it scales up linearly with the maximum number of CPUs,
 * and the maximum number of IO-APICs, whichever is higher.
 *
 * In other cases we size more conservatively, to not create too large
 * static arrays.
 */

/*
 * Intel IO-APIC support for SMP and UP systems.
 *
 * Copyright (C) 1997, 1998, 1999, 2000 Ingo Molnar
 */

/* I/O Unit Redirection Table */
/*
 * The structure of the IO-APIC:
 */
union Model0_IO_APIC_reg_00 {
 Model0_u32 Model0_raw;
 struct {
  Model0_u32 Model0___reserved_2 : 14,
   Model0_LTS : 1,
   Model0_delivery_type : 1,
   Model0___reserved_1 : 8,
   Model0_ID : 8;
 } __attribute__ ((packed)) Model0_bits;
};

union Model0_IO_APIC_reg_01 {
 Model0_u32 Model0_raw;
 struct {
  Model0_u32 Model0_version : 8,
   Model0___reserved_2 : 7,
   Model0_PRQ : 1,
   Model0_entries : 8,
   Model0___reserved_1 : 8;
 } __attribute__ ((packed)) Model0_bits;
};

union Model0_IO_APIC_reg_02 {
 Model0_u32 Model0_raw;
 struct {
  Model0_u32 Model0___reserved_2 : 24,
   Model0_arbitration : 4,
   Model0___reserved_1 : 4;
 } __attribute__ ((packed)) Model0_bits;
};

union Model0_IO_APIC_reg_03 {
 Model0_u32 Model0_raw;
 struct {
  Model0_u32 Model0_boot_DT : 1,
   Model0___reserved_1 : 31;
 } __attribute__ ((packed)) Model0_bits;
};

struct Model0_IO_APIC_route_entry {
 __u32 Model0_vector : 8,
  Model0_delivery_mode : 3, /* 000: FIXED
					 * 001: lowest prio
					 * 111: ExtINT
					 */
  Model0_dest_mode : 1, /* 0: physical, 1: logical */
  Model0_delivery_status : 1,
  Model0_polarity : 1,
  Model0_irr : 1,
  Model0_trigger : 1, /* 0: edge, 1: level */
  Model0_mask : 1, /* 0: enabled, 1: disabled */
  Model0___reserved_2 : 15;

 __u32 Model0___reserved_3 : 24,
  Model0_dest : 8;
} __attribute__ ((packed));

struct Model0_IR_IO_APIC_route_entry {
 __u64 Model0_vector : 8,
  Model0_zero : 3,
  Model0_index2 : 1,
  Model0_delivery_status : 1,
  Model0_polarity : 1,
  Model0_irr : 1,
  Model0_trigger : 1,
  Model0_mask : 1,
  Model0_reserved : 31,
  format : 1,
  Model0_index : 15;
} __attribute__ ((packed));

struct Model0_irq_alloc_info;
struct Model0_ioapic_domain_cfg;
/*
 * # of IO-APICs and # of IRQ routing registers
 */
extern int Model0_nr_ioapics;

extern int Model0_mpc_ioapic_id(int Model0_ioapic);
extern unsigned int Model0_mpc_ioapic_addr(int Model0_ioapic);

/* # of MP IRQ source entries */
extern int Model0_mp_irq_entries;

/* MP IRQ source entries */
extern struct Model0_mpc_intsrc Model0_mp_irqs[(256 * 4)];

/* 1 if "noapic" boot option passed */
extern int Model0_skip_ioapic_setup;

/* 1 if "noapic" boot option passed */
extern int Model0_noioapicquirk;

/* -1 if "noapic" boot option passed */
extern int Model0_noioapicreroute;

extern Model0_u32 Model0_gsi_top;

extern unsigned long Model0_io_apic_irqs;



/*
 * If we use the IO-APIC for IRQ routing, disable automatic
 * assignment of PCI IRQ's.
 */



struct Model0_irq_cfg;
extern void Model0_ioapic_insert_resources(void);
extern int Model0_arch_early_ioapic_init(void);

extern int Model0_save_ioapic_entries(void);
extern void Model0_mask_ioapic_entries(void);
extern int Model0_restore_ioapic_entries(void);

extern void Model0_setup_ioapic_ids_from_mpc(void);
extern void Model0_setup_ioapic_ids_from_mpc_nocheck(void);

extern int Model0_mp_find_ioapic(Model0_u32 Model0_gsi);
extern int Model0_mp_find_ioapic_pin(int Model0_ioapic, Model0_u32 Model0_gsi);
extern int Model0_mp_map_gsi_to_irq(Model0_u32 Model0_gsi, unsigned int Model0_flags,
        struct Model0_irq_alloc_info *Model0_info);
extern void Model0_mp_unmap_irq(int Model0_irq);
extern int Model0_mp_register_ioapic(int Model0_id, Model0_u32 Model0_address, Model0_u32 Model0_gsi_base,
         struct Model0_ioapic_domain_cfg *Model0_cfg);
extern int Model0_mp_unregister_ioapic(Model0_u32 Model0_gsi_base);
extern int Model0_mp_ioapic_registered(Model0_u32 Model0_gsi_base);

extern void Model0_ioapic_set_alloc_attr(struct Model0_irq_alloc_info *Model0_info,
      int Model0_node, int Model0_trigger, int Model0_polarity);

extern void Model0_mp_save_irq(struct Model0_mpc_intsrc *Model0_m);

extern void Model0_disable_ioapic_support(void);

extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_io_apic_init_mappings(void);
extern unsigned int Model0_native_io_apic_read(unsigned int Model0_apic, unsigned int Model0_reg);
extern void Model0_native_disable_io_apic(void);

static inline __attribute__((no_instrument_function)) unsigned int Model0_io_apic_read(unsigned int Model0_apic, unsigned int Model0_reg)
{
 return Model0_x86_io_apic_ops.Model0_read(Model0_apic, Model0_reg);
}

extern void Model0_setup_IO_APIC(void);
extern void Model0_enable_IO_APIC(void);
extern void Model0_disable_IO_APIC(void);
extern void Model0_setup_ioapic_dest(void);
extern int Model0_IO_APIC_get_PCI_irq_vector(int Model0_bus, int Model0_devfn, int Model0_pin);
extern void Model0_print_IO_APICs(void);





extern int Model0_smp_num_siblings;
extern unsigned int Model0_num_processors;

extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model0_cpumask_var_t) Model0_cpu_sibling_map;
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model0_cpumask_var_t) Model0_cpu_core_map;
/* cpus sharing the last level cache: */
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model0_cpumask_var_t) Model0_cpu_llc_shared_map;
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model0_u16) Model0_cpu_llc_id;
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(int) Model0_cpu_number;

static inline __attribute__((no_instrument_function)) struct Model0_cpumask *Model0_cpu_llc_shared_mask(int Model0_cpu)
{
 return (*({ do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_llc_shared_map)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((&(Model0_cpu_llc_shared_map)))) *)((&(Model0_cpu_llc_shared_map))))); (typeof((typeof(*((&(Model0_cpu_llc_shared_map)))) *)((&(Model0_cpu_llc_shared_map))))) (Model0___ptr + (((Model0___per_cpu_offset[(Model0_cpu)])))); }); }));
}

extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model0_u16) Model0_x86_cpu_to_apicid; extern __typeof__(Model0_u16) *Model0_x86_cpu_to_apicid_early_ptr; extern __typeof__(Model0_u16) Model0_x86_cpu_to_apicid_early_map[];
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model0_u32) Model0_x86_cpu_to_acpiid; extern __typeof__(Model0_u32) *Model0_x86_cpu_to_acpiid_early_ptr; extern __typeof__(Model0_u32) Model0_x86_cpu_to_acpiid_early_map[];
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model0_u16) Model0_x86_bios_cpu_apicid; extern __typeof__(Model0_u16) *Model0_x86_bios_cpu_apicid_early_ptr; extern __typeof__(Model0_u16) Model0_x86_bios_cpu_apicid_early_map[];




/* Static state in head.S used to set up a CPU */
extern unsigned long Model0_stack_start; /* Initial stack pointer address */

struct Model0_task_struct;

struct Model0_smp_ops {
 void (*Model0_smp_prepare_boot_cpu)(void);
 void (*Model0_smp_prepare_cpus)(unsigned Model0_max_cpus);
 void (*Model0_smp_cpus_done)(unsigned Model0_max_cpus);

 void (*Model0_stop_other_cpus)(int Model0_wait);
 void (*Model0_smp_send_reschedule)(int Model0_cpu);

 int (*Model0_cpu_up)(unsigned Model0_cpu, struct Model0_task_struct *Model0_tidle);
 int (*Model0_cpu_disable)(void);
 void (*Model0_cpu_die)(unsigned int Model0_cpu);
 void (*Model0_play_dead)(void);

 void (*Model0_send_call_func_ipi)(const struct Model0_cpumask *Model0_mask);
 void (*Model0_send_call_func_single_ipi)(int Model0_cpu);
};

/* Globals due to paravirt */
extern void Model0_set_cpu_sibling_map(int Model0_cpu);


extern struct Model0_smp_ops Model0_smp_ops;

static inline __attribute__((no_instrument_function)) void Model0_smp_send_stop(void)
{
 Model0_smp_ops.Model0_stop_other_cpus(0);
}

static inline __attribute__((no_instrument_function)) void Model0_stop_other_cpus(void)
{
 Model0_smp_ops.Model0_stop_other_cpus(1);
}

static inline __attribute__((no_instrument_function)) void Model0_smp_prepare_boot_cpu(void)
{
 Model0_smp_ops.Model0_smp_prepare_boot_cpu();
}

static inline __attribute__((no_instrument_function)) void Model0_smp_prepare_cpus(unsigned int Model0_max_cpus)
{
 Model0_smp_ops.Model0_smp_prepare_cpus(Model0_max_cpus);
}

static inline __attribute__((no_instrument_function)) void Model0_smp_cpus_done(unsigned int Model0_max_cpus)
{
 Model0_smp_ops.Model0_smp_cpus_done(Model0_max_cpus);
}

static inline __attribute__((no_instrument_function)) int Model0___cpu_up(unsigned int Model0_cpu, struct Model0_task_struct *Model0_tidle)
{
 return Model0_smp_ops.Model0_cpu_up(Model0_cpu, Model0_tidle);
}

static inline __attribute__((no_instrument_function)) int Model0___cpu_disable(void)
{
 return Model0_smp_ops.Model0_cpu_disable();
}

static inline __attribute__((no_instrument_function)) void Model0___cpu_die(unsigned int Model0_cpu)
{
 Model0_smp_ops.Model0_cpu_die(Model0_cpu);
}

static inline __attribute__((no_instrument_function)) void Model0_play_dead(void)
{
 Model0_smp_ops.Model0_play_dead();
}

static inline __attribute__((no_instrument_function)) void Model0_smp_send_reschedule(int Model0_cpu)
{
 Model0_smp_ops.Model0_smp_send_reschedule(Model0_cpu);
}

static inline __attribute__((no_instrument_function)) void Model0_arch_send_call_function_single_ipi(int Model0_cpu)
{
 Model0_smp_ops.Model0_send_call_func_single_ipi(Model0_cpu);
}

static inline __attribute__((no_instrument_function)) void Model0_arch_send_call_function_ipi_mask(const struct Model0_cpumask *Model0_mask)
{
 Model0_smp_ops.Model0_send_call_func_ipi(Model0_mask);
}

void Model0_cpu_disable_common(void);
void Model0_native_smp_prepare_boot_cpu(void);
void Model0_native_smp_prepare_cpus(unsigned int Model0_max_cpus);
void Model0_native_smp_cpus_done(unsigned int Model0_max_cpus);
void Model0_common_cpu_up(unsigned int Model0_cpunum, struct Model0_task_struct *Model0_tidle);
int Model0_native_cpu_up(unsigned int Model0_cpunum, struct Model0_task_struct *Model0_tidle);
int Model0_native_cpu_disable(void);
int Model0_common_cpu_die(unsigned int Model0_cpu);
void Model0_native_cpu_die(unsigned int Model0_cpu);
void Model0_hlt_play_dead(void);
void Model0_native_play_dead(void);
void Model0_play_dead_common(void);
void Model0_wbinvd_on_cpu(int Model0_cpu);
int Model0_wbinvd_on_all_cpus(void);

void Model0_native_send_call_func_ipi(const struct Model0_cpumask *Model0_mask);
void Model0_native_send_call_func_single_ipi(int Model0_cpu);
void Model0_x86_idle_thread_init(unsigned int Model0_cpu, struct Model0_task_struct *Model0_idle);

void Model0_smp_store_boot_cpu_info(void);
void Model0_smp_store_cpu_info(int Model0_id);
extern unsigned Model0_disabled_cpus;
extern int Model0_hard_smp_processor_id(void);

extern struct Model0_pglist_data *Model0_node_data[];



extern struct Model0_pglist_data *Model0_first_online_pgdat(void);
extern struct Model0_pglist_data *Model0_next_online_pgdat(struct Model0_pglist_data *Model0_pgdat);
extern struct Model0_zone *Model0_next_zone(struct Model0_zone *Model0_zone);

/**
 * for_each_online_pgdat - helper macro to iterate over all online nodes
 * @pgdat - pointer to a pg_data_t variable
 */




/**
 * for_each_zone - helper macro to iterate over all memory zones
 * @zone - pointer to struct zone variable
 *
 * The user only needs to declare the zone variable, for_each_zone
 * fills it in.
 */
static inline __attribute__((no_instrument_function)) struct Model0_zone *Model0_zonelist_zone(struct Model0_zoneref *Model0_zoneref)
{
 return Model0_zoneref->Model0_zone;
}

static inline __attribute__((no_instrument_function)) int Model0_zonelist_zone_idx(struct Model0_zoneref *Model0_zoneref)
{
 return Model0_zoneref->Model0_zone_idx;
}

static inline __attribute__((no_instrument_function)) int Model0_zonelist_node_idx(struct Model0_zoneref *Model0_zoneref)
{

 /* zone_to_nid not available in this context */
 return Model0_zoneref->Model0_zone->Model0_node;



}

struct Model0_zoneref *Model0___next_zones_zonelist(struct Model0_zoneref *Model0_z,
     enum Model0_zone_type Model0_highest_zoneidx,
     Model0_nodemask_t *Model0_nodes);

/**
 * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point
 * @z - The cursor used as a starting point for the search
 * @highest_zoneidx - The zone index of the highest zone to return
 * @nodes - An optional nodemask to filter the zonelist with
 *
 * This function returns the next zone at or below a given zone index that is
 * within the allowed nodemask using a cursor as the starting point for the
 * search. The zoneref returned is a cursor that represents the current zone
 * being examined. It should be advanced by one before calling
 * next_zones_zonelist again.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model0_zoneref *Model0_next_zones_zonelist(struct Model0_zoneref *Model0_z,
     enum Model0_zone_type Model0_highest_zoneidx,
     Model0_nodemask_t *Model0_nodes)
{
 if (__builtin_expect(!!(!Model0_nodes && Model0_zonelist_zone_idx(Model0_z) <= Model0_highest_zoneidx), 1))
  return Model0_z;
 return Model0___next_zones_zonelist(Model0_z, Model0_highest_zoneidx, Model0_nodes);
}

/**
 * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist
 * @zonelist - The zonelist to search for a suitable zone
 * @highest_zoneidx - The zone index of the highest zone to return
 * @nodes - An optional nodemask to filter the zonelist with
 * @zone - The first suitable zone found is returned via this parameter
 *
 * This function returns the first zone at or below a given zone index that is
 * within the allowed nodemask. The zoneref returned is a cursor that can be
 * used to iterate the zonelist with next_zones_zonelist by advancing it by
 * one before calling.
 */
static inline __attribute__((no_instrument_function)) struct Model0_zoneref *Model0_first_zones_zonelist(struct Model0_zonelist *Model0_zonelist,
     enum Model0_zone_type Model0_highest_zoneidx,
     Model0_nodemask_t *Model0_nodes)
{
 return Model0_next_zones_zonelist(Model0_zonelist->Model0__zonerefs,
       Model0_highest_zoneidx, Model0_nodes);
}

/**
 * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask
 * @zone - The current zone in the iterator
 * @z - The current pointer within zonelist->zones being iterated
 * @zlist - The zonelist being iterated
 * @highidx - The zone index of the highest zone to return
 * @nodemask - Nodemask allowed by the allocator
 *
 * This iterator iterates though all zones at or below a given zone index and
 * within a given nodemask
 */
/**
 * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index
 * @zone - The current zone in the iterator
 * @z - The current pointer within zonelist->zones being iterated
 * @zlist - The zonelist being iterated
 * @highidx - The zone index of the highest zone to return
 *
 * This iterator iterates though all zones at or below a given zone index.
 */
/*
 * SECTION_SHIFT    		#bits space required to store a section #
 *
 * PA_SECTION_SHIFT		physical address to/from section number
 * PFN_SECTION_SHIFT		pfn to/from section number
 */
struct Model0_page;
struct Model0_page_ext;
struct Model0_mem_section {
 /*
	 * This is, logically, a pointer to an array of struct
	 * pages.  However, it is stored with some other magic.
	 * (see sparse.c::sparse_init_one_section())
	 *
	 * Additionally during early boot we encode node id of
	 * the location of the section here to guide allocation.
	 * (see sparse.c::memory_present())
	 *
	 * Making it a UL at least makes someone do a cast
	 * before using it wrong.
	 */
 unsigned long Model0_section_mem_map;

 /* See declaration of similar field in struct zone */
 unsigned long *Model0_pageblock_flags;
 /*
	 * WARNING: mem_section must be a power-of-2 in size for the
	 * calculation and use of SECTION_ROOT_MASK to make sense.
	 */
};
extern struct Model0_mem_section *Model0_mem_section[((((1UL << (46 - 27))) + ((((1UL) << 12) / sizeof (struct Model0_mem_section))) - 1) / ((((1UL) << 12) / sizeof (struct Model0_mem_section))))];




static inline __attribute__((no_instrument_function)) struct Model0_mem_section *Model0___nr_to_section(unsigned long Model0_nr)
{
 if (!Model0_mem_section[((Model0_nr) / (((1UL) << 12) / sizeof (struct Model0_mem_section)))])
  return ((void *)0);
 return &Model0_mem_section[((Model0_nr) / (((1UL) << 12) / sizeof (struct Model0_mem_section)))][Model0_nr & ((((1UL) << 12) / sizeof (struct Model0_mem_section)) - 1)];
}
extern int Model0___section_nr(struct Model0_mem_section* Model0_ms);
extern unsigned long Model0_usemap_size(void);

/*
 * We use the lower bits of the mem_map pointer to store
 * a little bit of information.  There should be at least
 * 3 bits here due to 32-bit alignment.
 */






static inline __attribute__((no_instrument_function)) struct Model0_page *Model0___section_mem_map_addr(struct Model0_mem_section *section)
{
 unsigned long Model0_map = section->Model0_section_mem_map;
 Model0_map &= (~((1UL<<2)-1));
 return (struct Model0_page *)Model0_map;
}

static inline __attribute__((no_instrument_function)) int Model0_present_section(struct Model0_mem_section *section)
{
 return (section && (section->Model0_section_mem_map & (1UL<<0)));
}

static inline __attribute__((no_instrument_function)) int Model0_present_section_nr(unsigned long Model0_nr)
{
 return Model0_present_section(Model0___nr_to_section(Model0_nr));
}

static inline __attribute__((no_instrument_function)) int Model0_valid_section(struct Model0_mem_section *section)
{
 return (section && (section->Model0_section_mem_map & (1UL<<1)));
}

static inline __attribute__((no_instrument_function)) int Model0_valid_section_nr(unsigned long Model0_nr)
{
 return Model0_valid_section(Model0___nr_to_section(Model0_nr));
}

static inline __attribute__((no_instrument_function)) struct Model0_mem_section *Model0___pfn_to_section(unsigned long Model0_pfn)
{
 return Model0___nr_to_section(((Model0_pfn) >> (27 - 12)));
}


static inline __attribute__((no_instrument_function)) int Model0_pfn_valid(unsigned long Model0_pfn)
{
 if (((Model0_pfn) >> (27 - 12)) >= (1UL << (46 - 27)))
  return 0;
 return Model0_valid_section(Model0___nr_to_section(((Model0_pfn) >> (27 - 12))));
}


static inline __attribute__((no_instrument_function)) int Model0_pfn_present(unsigned long Model0_pfn)
{
 if (((Model0_pfn) >> (27 - 12)) >= (1UL << (46 - 27)))
  return 0;
 return Model0_present_section(Model0___nr_to_section(((Model0_pfn) >> (27 - 12))));
}

/*
 * These are _only_ used during initialisation, therefore they
 * can use __initdata ...  They could have names to indicate
 * this restriction.
 */
void Model0_sparse_init(void);





/*
 * During memory init memblocks map pfns to nids. The search is expensive and
 * this caches recent lookups. The implementation of __early_pfn_to_nid
 * may treat start/end as pfns or sections.
 */
struct Model0_mminit_pfnnid_cache {
 unsigned long Model0_last_start;
 unsigned long Model0_last_end;
 int Model0_last_nid;
};





void Model0_memory_present(int Model0_nid, unsigned long Model0_start, unsigned long Model0_end);
unsigned long __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_node_memmap_size_bytes(int, unsigned long, unsigned long);

/*
 * If it is possible to have holes within a MAX_ORDER_NR_PAGES, then we
 * need to check pfn validility within that MAX_ORDER_NR_PAGES block.
 * pfn_valid_within() should be used in this case; we optimise this away
 * when we have no holes within a MAX_ORDER_NR_PAGES block.
 */
static inline __attribute__((no_instrument_function)) bool Model0_memmap_valid_within(unsigned long Model0_pfn,
     struct Model0_page *Model0_page, struct Model0_zone *Model0_zone)
{
 return true;
}


/*
 * include/linux/topology.h
 *
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */










/*
 *	Generic SMP support
 *		Alan Cox. <alan@redhat.com>
 */









/*
 * Lock-less NULL terminated single linked list
 *
 * If there are multiple producers and multiple consumers, llist_add
 * can be used in producers and llist_del_all can be used in
 * consumers.  They can work simultaneously without lock.  But
 * llist_del_first can not be used here.  Because llist_del_first
 * depends on list->first->next does not changed if list->first is not
 * changed during its operation, but llist_del_first, llist_add,
 * llist_add (or llist_del_all, llist_add, llist_add) sequence in
 * another consumer may violate that.
 *
 * If there are multiple producers and one consumer, llist_add can be
 * used in producers and llist_del_all or llist_del_first can be used
 * in the consumer.
 *
 * This can be summarized as follow:
 *
 *           |   add    | del_first |  del_all
 * add       |    -     |     -     |     -
 * del_first |          |     L     |     L
 * del_all   |          |           |     -
 *
 * Where "-" stands for no lock is needed, while "L" stands for lock
 * is needed.
 *
 * The list entries deleted via llist_del_all can be traversed with
 * traversing function such as llist_for_each etc.  But the list
 * entries can not be traversed safely before deleted from the list.
 * The order of deleted entries is from the newest to the oldest added
 * one.  If you want to traverse from the oldest to the newest, you
 * must reverse the order by yourself before traversing.
 *
 * The basic atomic operation of this list is cmpxchg on long.  On
 * architectures that don't have NMI-safe cmpxchg implementation, the
 * list can NOT be used in NMI handlers.  So code that uses the list in
 * an NMI handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
 *
 * Copyright 2010,2011 Intel Corp.
 *   Author: Huang Ying <ying.huang@intel.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License version
 * 2 as published by the Free Software Foundation;
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */




struct Model0_llist_head {
 struct Model0_llist_node *Model0_first;
};

struct Model0_llist_node {
 struct Model0_llist_node *Model0_next;
};




/**
 * init_llist_head - initialize lock-less list head
 * @head:	the head for your lock-less list
 */
static inline __attribute__((no_instrument_function)) void Model0_init_llist_head(struct Model0_llist_head *Model0_list)
{
 Model0_list->Model0_first = ((void *)0);
}

/**
 * llist_entry - get the struct of this entry
 * @ptr:	the &struct llist_node pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the llist_node within the struct.
 */



/**
 * llist_for_each - iterate over some deleted entries of a lock-less list
 * @pos:	the &struct llist_node to use as a loop cursor
 * @node:	the first entry of deleted list entries
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being deleted from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */



/**
 * llist_for_each_entry - iterate over some deleted entries of lock-less list of given type
 * @pos:	the type * to use as a loop cursor.
 * @node:	the fist entry of deleted list entries.
 * @member:	the name of the llist_node with the struct.
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being removed from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */





/**
 * llist_for_each_entry_safe - iterate over some deleted entries of lock-less list of given type
 *			       safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @node:	the first entry of deleted list entries.
 * @member:	the name of the llist_node with the struct.
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being removed from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */






/**
 * llist_empty - tests whether a lock-less list is empty
 * @head:	the list to test
 *
 * Not guaranteed to be accurate or up to date.  Just a quick way to
 * test whether the list is empty without deleting something from the
 * list.
 */
static inline __attribute__((no_instrument_function)) bool Model0_llist_empty(const struct Model0_llist_head *Model0_head)
{
 return (*({ __attribute__((unused)) typeof(Model0_head->Model0_first) Model0___var = ( typeof(Model0_head->Model0_first)) 0; (volatile typeof(Model0_head->Model0_first) *)&(Model0_head->Model0_first); })) == ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_llist_node *Model0_llist_next(struct Model0_llist_node *Model0_node)
{
 return Model0_node->Model0_next;
}

extern bool Model0_llist_add_batch(struct Model0_llist_node *Model0_new_first,
       struct Model0_llist_node *Model0_new_last,
       struct Model0_llist_head *Model0_head);
/**
 * llist_add - add a new entry
 * @new:	new entry to be added
 * @head:	the head for your lock-less list
 *
 * Returns true if the list was empty prior to adding this entry.
 */
static inline __attribute__((no_instrument_function)) bool Model0_llist_add(struct Model0_llist_node *Model0_new, struct Model0_llist_head *Model0_head)
{
 return Model0_llist_add_batch(Model0_new, Model0_new, Model0_head);
}

/**
 * llist_del_all - delete all entries from lock-less list
 * @head:	the head of lock-less list to delete all entries
 *
 * If list is empty, return NULL, otherwise, delete all entries and
 * return the pointer to the first entry.  The order of entries
 * deleted is from the newest to the oldest added one.
 */
static inline __attribute__((no_instrument_function)) struct Model0_llist_node *Model0_llist_del_all(struct Model0_llist_head *Model0_head)
{
 return ({ __typeof__ (*((&Model0_head->Model0_first))) Model0___ret = ((((void *)0))); switch (sizeof(*((&Model0_head->Model0_first)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*((&Model0_head->Model0_first))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_head->Model0_first))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_head->Model0_first))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_head->Model0_first))) : : "memory", "cc"); break; default: Model0___xchg_wrong_size(); } Model0___ret; });
}

extern struct Model0_llist_node *Model0_llist_del_first(struct Model0_llist_head *Model0_head);

struct Model0_llist_node *Model0_llist_reverse_order(struct Model0_llist_node *Model0_head);

typedef void (*Model0_smp_call_func_t)(void *Model0_info);
struct Model0_call_single_data {
 struct Model0_llist_node Model0_llist;
 Model0_smp_call_func_t func;
 void *Model0_info;
 unsigned int Model0_flags;
};

/* total number of cpus in this system (may exceed NR_CPUS) */
extern unsigned int Model0_total_cpus;

int Model0_smp_call_function_single(int Model0_cpuid, Model0_smp_call_func_t func, void *Model0_info,
        int Model0_wait);

/*
 * Call a function on all processors
 */
int Model0_on_each_cpu(Model0_smp_call_func_t func, void *Model0_info, int Model0_wait);

/*
 * Call a function on processors specified by mask, which might include
 * the local one.
 */
void Model0_on_each_cpu_mask(const struct Model0_cpumask *Model0_mask, Model0_smp_call_func_t func,
  void *Model0_info, bool Model0_wait);

/*
 * Call a function on each processor for which the supplied function
 * cond_func returns a positive value. This may include the local
 * processor.
 */
void Model0_on_each_cpu_cond(bool (*Model0_cond_func)(int Model0_cpu, void *Model0_info),
  Model0_smp_call_func_t func, void *Model0_info, bool Model0_wait,
  Model0_gfp_t Model0_gfp_flags);

int Model0_smp_call_function_single_async(int Model0_cpu, struct Model0_call_single_data *Model0_csd);
/*
 * main cross-CPU interfaces, handles INIT, TLB flush, STOP, etc.
 * (defined in asm header):
 */

/*
 * stops all CPUs but the current one:
 */
extern void Model0_smp_send_stop(void);

/*
 * sends a 'reschedule' event to another CPU:
 */
extern void Model0_smp_send_reschedule(int Model0_cpu);


/*
 * Prepare machine for booting other CPUs.
 */
extern void Model0_smp_prepare_cpus(unsigned int Model0_max_cpus);

/*
 * Bring a CPU up
 */
extern int Model0___cpu_up(unsigned int Model0_cpunum, struct Model0_task_struct *Model0_tidle);

/*
 * Final polishing of CPUs
 */
extern void Model0_smp_cpus_done(unsigned int Model0_max_cpus);

/*
 * Call a function on all other processors
 */
int Model0_smp_call_function(Model0_smp_call_func_t func, void *Model0_info, int Model0_wait);
void Model0_smp_call_function_many(const struct Model0_cpumask *Model0_mask,
       Model0_smp_call_func_t func, void *Model0_info, bool Model0_wait);

int Model0_smp_call_function_any(const struct Model0_cpumask *Model0_mask,
     Model0_smp_call_func_t func, void *Model0_info, int Model0_wait);

void Model0_kick_all_cpus_sync(void);
void Model0_wake_up_all_idle_cpus(void);

/*
 * Generic and arch helpers
 */
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_call_function_init(void);
void Model0_generic_smp_call_function_single_interrupt(void);



/*
 * Mark the boot cpu "online" so that it can call console drivers in
 * printk() and can access its per-cpu storage.
 */
void Model0_smp_prepare_boot_cpu(void);

extern unsigned int Model0_setup_max_cpus;
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_setup_nr_cpu_ids(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_smp_init(void);
/*
 * smp_processor_id(): get the current CPU ID.
 *
 * if DEBUG_PREEMPT is enabled then we check whether it is
 * used in a preemption-safe way. (smp_processor_id() is safe
 * if it's used in a preemption-off critical section, or in
 * a thread that is bound to the current CPU.)
 *
 * NOTE: raw_smp_processor_id() is for internal use only
 * (smp_processor_id() is the preferred variant), but in rare
 * instances it might also be used to turn off false positives
 * (i.e. smp_processor_id() use that the debugging code reports but
 * which use for some reason is legal). Don't use this to hack around
 * the warning message, as your code might not work under PREEMPT.
 */
/*
 * Callback to arch code if there's nosmp or maxcpus=0 on the
 * boot command line:
 */
extern void Model0_arch_disable_smp_support(void);

extern void Model0_arch_enable_nonboot_cpus_begin(void);
extern void Model0_arch_enable_nonboot_cpus_end(void);

void Model0_smp_setup_processor_id(void);

/* SMP core functions */
int Model0_smpcfd_prepare_cpu(unsigned int Model0_cpu);
int Model0_smpcfd_dead_cpu(unsigned int Model0_cpu);
int Model0_smpcfd_dying_cpu(unsigned int Model0_cpu);
/* enough to cover all DEFINE_PER_CPUs in modules */






/* minimum unit size, also is the maximum supported allocation size */


/*
 * Percpu allocator can serve percpu allocations before slab is
 * initialized which allows slab to depend on the percpu allocator.
 * The following two parameters decide how much resource to
 * preallocate for this.  Keep PERCPU_DYNAMIC_RESERVE equal to or
 * larger than PERCPU_DYNAMIC_EARLY_SIZE.
 */



/*
 * PERCPU_DYNAMIC_RESERVE indicates the amount of free area to piggy
 * back on the first chunk for dynamic percpu allocation if arch is
 * manually allocating and mapping it for faster access (as a part of
 * large page mapping for example).
 *
 * The following values give between one and two pages of free space
 * after typical minimal boot (2-way SMP, single disk and NIC) with
 * both defconfig and a distro config on x86_64 and 32.  More
 * intelligent way to determine this would be nice.
 */






extern void *Model0_pcpu_base_addr;
extern const unsigned long *Model0_pcpu_unit_offsets;

struct Model0_pcpu_group_info {
 int Model0_nr_units; /* aligned # of units */
 unsigned long Model0_base_offset; /* base address offset */
 unsigned int *Model0_cpu_map; /* unit->cpu map, empty
						 * entries contain NR_CPUS */
};

struct Model0_pcpu_alloc_info {
 Model0_size_t Model0_static_size;
 Model0_size_t Model0_reserved_size;
 Model0_size_t Model0_dyn_size;
 Model0_size_t Model0_unit_size;
 Model0_size_t Model0_atom_size;
 Model0_size_t Model0_alloc_size;
 Model0_size_t Model0___ai_size; /* internal, don't use */
 int Model0_nr_groups; /* 0 if grouping unnecessary */
 struct Model0_pcpu_group_info Model0_groups[];
};

enum Model0_pcpu_fc {
 Model0_PCPU_FC_AUTO,
 Model0_PCPU_FC_EMBED,
 Model0_PCPU_FC_PAGE,

 Model0_PCPU_FC_NR,
};
extern const char * const Model0_pcpu_fc_names[Model0_PCPU_FC_NR];

extern enum Model0_pcpu_fc Model0_pcpu_chosen_fc;

typedef void * (*Model0_pcpu_fc_alloc_fn_t)(unsigned int Model0_cpu, Model0_size_t Model0_size,
         Model0_size_t Model0_align);
typedef void (*Model0_pcpu_fc_free_fn_t)(void *Model0_ptr, Model0_size_t Model0_size);
typedef void (*Model0_pcpu_fc_populate_pte_fn_t)(unsigned long Model0_addr);
typedef int (Model0_pcpu_fc_cpu_distance_fn_t)(unsigned int Model0_from, unsigned int Model0_to);

extern struct Model0_pcpu_alloc_info * __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pcpu_alloc_alloc_info(int Model0_nr_groups,
            int Model0_nr_units);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pcpu_free_alloc_info(struct Model0_pcpu_alloc_info *Model0_ai);

extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pcpu_setup_first_chunk(const struct Model0_pcpu_alloc_info *Model0_ai,
      void *Model0_base_addr);


extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pcpu_embed_first_chunk(Model0_size_t Model0_reserved_size, Model0_size_t Model0_dyn_size,
    Model0_size_t Model0_atom_size,
    Model0_pcpu_fc_cpu_distance_fn_t Model0_cpu_distance_fn,
    Model0_pcpu_fc_alloc_fn_t Model0_alloc_fn,
    Model0_pcpu_fc_free_fn_t Model0_free_fn);



extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pcpu_page_first_chunk(Model0_size_t Model0_reserved_size,
    Model0_pcpu_fc_alloc_fn_t Model0_alloc_fn,
    Model0_pcpu_fc_free_fn_t Model0_free_fn,
    Model0_pcpu_fc_populate_pte_fn_t Model0_populate_pte_fn);


extern void *Model0___alloc_reserved_percpu(Model0_size_t Model0_size, Model0_size_t Model0_align);
extern bool Model0_is_kernel_percpu_address(unsigned long Model0_addr);




extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_percpu_init_late(void);

extern void *Model0___alloc_percpu_gfp(Model0_size_t Model0_size, Model0_size_t Model0_align, Model0_gfp_t Model0_gfp);
extern void *Model0___alloc_percpu(Model0_size_t Model0_size, Model0_size_t Model0_align);
extern void Model0_free_percpu(void *Model0___pdata);
extern Model0_phys_addr_t Model0_per_cpu_ptr_to_phys(void *Model0_addr);
int Model0_arch_update_cpu_topology(void);

/* Conform to ACPI 2.0 SLIT distance definitions */






/*
 * If the distance between nodes in a system is larger than RECLAIM_DISTANCE
 * (in whatever arch specific measurement units returned by node_distance())
 * and node_reclaim_mode is enabled then the VM will only call node_reclaim()
 * on nodes within this distance.
 */







extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model0_numa_node;


/* Returns the number of the current Node. */
static inline __attribute__((no_instrument_function)) int Model0_numa_node_id(void)
{
 return ({ typeof(Model0_numa_node) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_numa_node)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_numa_node)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_numa_node) Model0_pfo_ret__; switch (sizeof(Model0_numa_node)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_numa_node) Model0_pfo_ret__; switch (sizeof(Model0_numa_node)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_numa_node) Model0_pfo_ret__; switch (sizeof(Model0_numa_node)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_numa_node) Model0_pfo_ret__; switch (sizeof(Model0_numa_node)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_numa_node)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; });
}



static inline __attribute__((no_instrument_function)) int Model0_cpu_to_node(int Model0_cpu)
{
 return (*({ do { const void *Model0___vpp_verify = (typeof((&(Model0_numa_node)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((&(Model0_numa_node)))) *)((&(Model0_numa_node))))); (typeof((typeof(*((&(Model0_numa_node)))) *)((&(Model0_numa_node))))) (Model0___ptr + (((Model0___per_cpu_offset[(Model0_cpu)])))); }); }));
}



static inline __attribute__((no_instrument_function)) void Model0_set_numa_node(int Model0_node)
{
 do { do { const void *Model0___vpp_verify = (typeof((&(Model0_numa_node)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_numa_node)) { case 1: do { typedef typeof((Model0_numa_node)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_node); (void)Model0_pto_tmp__; } switch (sizeof((Model0_numa_node))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "qi" ((Model0_pto_T__)(Model0_node))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "ri" ((Model0_pto_T__)(Model0_node))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "ri" ((Model0_pto_T__)(Model0_node))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "re" ((Model0_pto_T__)(Model0_node))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model0_numa_node)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_node); (void)Model0_pto_tmp__; } switch (sizeof((Model0_numa_node))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "qi" ((Model0_pto_T__)(Model0_node))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "ri" ((Model0_pto_T__)(Model0_node))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "ri" ((Model0_pto_T__)(Model0_node))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "re" ((Model0_pto_T__)(Model0_node))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model0_numa_node)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_node); (void)Model0_pto_tmp__; } switch (sizeof((Model0_numa_node))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "qi" ((Model0_pto_T__)(Model0_node))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "ri" ((Model0_pto_T__)(Model0_node))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "ri" ((Model0_pto_T__)(Model0_node))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "re" ((Model0_pto_T__)(Model0_node))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model0_numa_node)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_node); (void)Model0_pto_tmp__; } switch (sizeof((Model0_numa_node))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "qi" ((Model0_pto_T__)(Model0_node))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "ri" ((Model0_pto_T__)(Model0_node))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "ri" ((Model0_pto_T__)(Model0_node))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0_numa_node)) : "re" ((Model0_pto_T__)(Model0_node))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
}



static inline __attribute__((no_instrument_function)) void Model0_set_cpu_numa_node(int Model0_cpu, int Model0_node)
{
 (*({ do { const void *Model0___vpp_verify = (typeof((&(Model0_numa_node)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((&(Model0_numa_node)))) *)((&(Model0_numa_node))))); (typeof((typeof(*((&(Model0_numa_node)))) *)((&(Model0_numa_node))))) (Model0___ptr + (((Model0___per_cpu_offset[(Model0_cpu)])))); }); })) = Model0_node;
}
/* Returns the number of the nearest Node with memory */
static inline __attribute__((no_instrument_function)) int Model0_numa_mem_id(void)
{
 return Model0_numa_node_id();
}



static inline __attribute__((no_instrument_function)) int Model0_node_to_mem_node(int Model0_node)
{
 return Model0_node;
}



static inline __attribute__((no_instrument_function)) int Model0_cpu_to_mem(int Model0_cpu)
{
 return Model0_cpu_to_node(Model0_cpu);
}
static inline __attribute__((no_instrument_function)) const struct Model0_cpumask *Model0_cpu_smt_mask(int Model0_cpu)
{
 return ((*({ do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_sibling_map)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((&(Model0_cpu_sibling_map)))) *)((&(Model0_cpu_sibling_map))))); (typeof((typeof(*((&(Model0_cpu_sibling_map)))) *)((&(Model0_cpu_sibling_map))))) (Model0___ptr + (((Model0___per_cpu_offset[(Model0_cpu)])))); }); })));
}


static inline __attribute__((no_instrument_function)) const struct Model0_cpumask *Model0_cpu_cpu_mask(int Model0_cpu)
{
 return Model0_cpumask_of_node(Model0_cpu_to_node(Model0_cpu));
}

struct Model0_vm_area_struct;

/*
 * In case of changes, please don't forget to update
 * include/trace/events/mmflags.h and tools/perf/builtin-kmem.c
 */

/* Plain integer GFP bitmasks. Do not use this directly. */
/* If the above are modified, __GFP_BITS_SHIFT may need updating */

/*
 * Physical address zone modifiers (see linux/mmzone.h - low four bits)
 *
 * Do not put any conditional on these. If necessary modify the definitions
 * without the underscores and use them consistently. The definitions here may
 * be used in bit comparisons.
 */






/*
 * Page mobility and placement hints
 *
 * These flags provide hints about how mobile the page is. Pages with similar
 * mobility are placed within the same pageblocks to minimise problems due
 * to external fragmentation.
 *
 * __GFP_MOVABLE (also a zone modifier) indicates that the page can be
 *   moved by page migration during memory compaction or can be reclaimed.
 *
 * __GFP_RECLAIMABLE is used for slab allocations that specify
 *   SLAB_RECLAIM_ACCOUNT and whose pages can be freed via shrinkers.
 *
 * __GFP_WRITE indicates the caller intends to dirty the page. Where possible,
 *   these pages will be spread between local zones to avoid all the dirty
 *   pages being in one zone (fair zone allocation policy).
 *
 * __GFP_HARDWALL enforces the cpuset memory allocation policy.
 *
 * __GFP_THISNODE forces the allocation to be satisified from the requested
 *   node with no fallbacks or placement policy enforcements.
 *
 * __GFP_ACCOUNT causes the allocation to be accounted to kmemcg.
 */






/*
 * Watermark modifiers -- controls access to emergency reserves
 *
 * __GFP_HIGH indicates that the caller is high-priority and that granting
 *   the request is necessary before the system can make forward progress.
 *   For example, creating an IO context to clean pages.
 *
 * __GFP_ATOMIC indicates that the caller cannot reclaim or sleep and is
 *   high priority. Users are typically interrupt handlers. This may be
 *   used in conjunction with __GFP_HIGH
 *
 * __GFP_MEMALLOC allows access to all memory. This should only be used when
 *   the caller guarantees the allocation will allow more memory to be freed
 *   very shortly e.g. process exiting or swapping. Users either should
 *   be the MM or co-ordinating closely with the VM (e.g. swap over NFS).
 *
 * __GFP_NOMEMALLOC is used to explicitly forbid access to emergency reserves.
 *   This takes precedence over the __GFP_MEMALLOC flag if both are set.
 */





/*
 * Reclaim modifiers
 *
 * __GFP_IO can start physical IO.
 *
 * __GFP_FS can call down to the low-level FS. Clearing the flag avoids the
 *   allocator recursing into the filesystem which might already be holding
 *   locks.
 *
 * __GFP_DIRECT_RECLAIM indicates that the caller may enter direct reclaim.
 *   This flag can be cleared to avoid unnecessary delays when a fallback
 *   option is available.
 *
 * __GFP_KSWAPD_RECLAIM indicates that the caller wants to wake kswapd when
 *   the low watermark is reached and have it reclaim pages until the high
 *   watermark is reached. A caller may wish to clear this flag when fallback
 *   options are available and the reclaim is likely to disrupt the system. The
 *   canonical example is THP allocation where a fallback is cheap but
 *   reclaim/compaction may cause indirect stalls.
 *
 * __GFP_RECLAIM is shorthand to allow/forbid both direct and kswapd reclaim.
 *
 * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt
 *   _might_ fail.  This depends upon the particular VM implementation.
 *
 * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller
 *   cannot handle allocation failures. New users should be evaluated carefully
 *   (and the flag should be used only when there is no reasonable failure
 *   policy) but it is definitely preferable to use the flag rather than
 *   opencode endless loop around allocator.
 *
 * __GFP_NORETRY: The VM implementation must not retry indefinitely and will
 *   return NULL when direct reclaim and memory compaction have failed to allow
 *   the allocation to succeed.  The OOM killer is not called with the current
 *   implementation.
 */
/*
 * Action modifiers
 *
 * __GFP_COLD indicates that the caller does not expect to be used in the near
 *   future. Where possible, a cache-cold page will be returned.
 *
 * __GFP_NOWARN suppresses allocation failure reports.
 *
 * __GFP_COMP address compound page metadata.
 *
 * __GFP_ZERO returns a zeroed page on success.
 *
 * __GFP_NOTRACK avoids tracking with kmemcheck.
 *
 * __GFP_NOTRACK_FALSE_POSITIVE is an alias of __GFP_NOTRACK. It's a means of
 *   distinguishing in the source between false positives and allocations that
 *   cannot be supported (e.g. page tables).
 *
 * __GFP_OTHER_NODE is for allocations that are on a remote node but that
 *   should not be accounted for as a remote allocation in vmstat. A
 *   typical user would be khugepaged collapsing a huge page on a remote
 *   node.
 */
/* Room for N __GFP_FOO bits */



/*
 * Useful GFP flag combinations that are commonly used. It is recommended
 * that subsystems start with one of these combinations and then set/clear
 * __GFP_FOO flags as necessary.
 *
 * GFP_ATOMIC users can not sleep and need the allocation to succeed. A lower
 *   watermark is applied to allow access to "atomic reserves"
 *
 * GFP_KERNEL is typical for kernel-internal allocations. The caller requires
 *   ZONE_NORMAL or a lower zone for direct access but can direct reclaim.
 *
 * GFP_KERNEL_ACCOUNT is the same as GFP_KERNEL, except the allocation is
 *   accounted to kmemcg.
 *
 * GFP_NOWAIT is for kernel allocations that should not stall for direct
 *   reclaim, start physical IO or use any filesystem callback.
 *
 * GFP_NOIO will use direct reclaim to discard clean pages or slab pages
 *   that do not require the starting of any physical IO.
 *
 * GFP_NOFS will use direct reclaim but will not use any filesystem interfaces.
 *
 * GFP_USER is for userspace allocations that also need to be directly
 *   accessibly by the kernel or hardware. It is typically used by hardware
 *   for buffers that are mapped to userspace (e.g. graphics) that hardware
 *   still must DMA to. cpuset limits are enforced for these allocations.
 *
 * GFP_DMA exists for historical reasons and should be avoided where possible.
 *   The flags indicates that the caller requires that the lowest zone be
 *   used (ZONE_DMA or 16M on x86-64). Ideally, this would be removed but
 *   it would require careful auditing as some users really require it and
 *   others use the flag to avoid lowmem reserves in ZONE_DMA and treat the
 *   lowest zone as a type of emergency reserve.
 *
 * GFP_DMA32 is similar to GFP_DMA except that the caller requires a 32-bit
 *   address.
 *
 * GFP_HIGHUSER is for userspace allocations that may be mapped to userspace,
 *   do not need to be directly accessible by the kernel but that cannot
 *   move once in use. An example may be a hardware allocation that maps
 *   data directly into userspace but has no addressing limitations.
 *
 * GFP_HIGHUSER_MOVABLE is for userspace allocations that the kernel does not
 *   need direct access to but can use kmap() when access is required. They
 *   are expected to be movable via page reclaim or page migration. Typically,
 *   pages on the LRU would also be allocated with GFP_HIGHUSER_MOVABLE.
 *
 * GFP_TRANSHUGE and GFP_TRANSHUGE_LIGHT are used for THP allocations. They are
 *   compound allocations that will generally fail quickly if memory is not
 *   available and will not wake kswapd/kcompactd on failure. The _LIGHT
 *   version does not attempt reclaim/compaction at all and is by default used
 *   in page fault path, while the non-light is used by khugepaged.
 */
/* Convert GFP flags to their corresponding migrate type */



static inline __attribute__((no_instrument_function)) int Model0_gfpflags_to_migratetype(const Model0_gfp_t Model0_gfp_flags)
{
 ((void)(sizeof(( long)((Model0_gfp_flags & ((( Model0_gfp_t)0x10u)|(( Model0_gfp_t)0x08u))) == ((( Model0_gfp_t)0x10u)|(( Model0_gfp_t)0x08u))))));
 do { bool Model0___cond = !(!((1UL << 3) != 0x08u)); extern void Model0___compiletime_assert_270(void) ; if (Model0___cond) Model0___compiletime_assert_270(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!((0x08u >> 3) != Model0_MIGRATE_MOVABLE)); extern void Model0___compiletime_assert_271(void) ; if (Model0___cond) Model0___compiletime_assert_271(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);

 if (__builtin_expect(!!(Model0_page_group_by_mobility_disabled), 0))
  return Model0_MIGRATE_UNMOVABLE;

 /* Group based on mobility */
 return (Model0_gfp_flags & ((( Model0_gfp_t)0x10u)|(( Model0_gfp_t)0x08u))) >> 3;
}



static inline __attribute__((no_instrument_function)) bool Model0_gfpflags_allow_blocking(const Model0_gfp_t Model0_gfp_flags)
{
 return !!(Model0_gfp_flags & (( Model0_gfp_t)0x400000u));
}
/*
 * GFP_ZONE_TABLE is a word size bitstring that is used for looking up the
 * zone to use given the lowest 4 bits of gfp_t. Entries are ZONE_SHIFT long
 * and there are 16 of them to cover all possible combinations of
 * __GFP_DMA, __GFP_DMA32, __GFP_MOVABLE and __GFP_HIGHMEM.
 *
 * The zone fallback order is MOVABLE=>HIGHMEM=>NORMAL=>DMA32=>DMA.
 * But GFP_MOVABLE is not only a zone specifier but also an allocation
 * policy. Therefore __GFP_MOVABLE plus another zone selector is valid.
 * Only 1 bit of the lowest 3 bits (DMA,DMA32,HIGHMEM) can be set to "1".
 *
 *       bit       result
 *       =================
 *       0x0    => NORMAL
 *       0x1    => DMA or NORMAL
 *       0x2    => HIGHMEM or NORMAL
 *       0x3    => BAD (DMA+HIGHMEM)
 *       0x4    => DMA32 or DMA or NORMAL
 *       0x5    => BAD (DMA+DMA32)
 *       0x6    => BAD (HIGHMEM+DMA32)
 *       0x7    => BAD (HIGHMEM+DMA32+DMA)
 *       0x8    => NORMAL (MOVABLE+0)
 *       0x9    => DMA or NORMAL (MOVABLE+DMA)
 *       0xa    => MOVABLE (Movable is valid only if HIGHMEM is set too)
 *       0xb    => BAD (MOVABLE+HIGHMEM+DMA)
 *       0xc    => DMA32 (MOVABLE+DMA32)
 *       0xd    => BAD (MOVABLE+DMA32+DMA)
 *       0xe    => BAD (MOVABLE+DMA32+HIGHMEM)
 *       0xf    => BAD (MOVABLE+DMA32+HIGHMEM+DMA)
 *
 * GFP_ZONES_SHIFT must be <= 2 on 32 bit platforms.
 */
/*
 * GFP_ZONE_BAD is a bitmap for all combinations of __GFP_DMA, __GFP_DMA32
 * __GFP_HIGHMEM and __GFP_MOVABLE that are not permitted. One flag per
 * entry starting with bit 0. Bit is set if the combination is not
 * allowed.
 */
static inline __attribute__((no_instrument_function)) enum Model0_zone_type Model0_gfp_zone(Model0_gfp_t Model0_flags)
{
 enum Model0_zone_type Model0_z;
 int Model0_bit = ( int) (Model0_flags & ((( Model0_gfp_t)0x01u)|(( Model0_gfp_t)0x02u)|(( Model0_gfp_t)0x04u)|(( Model0_gfp_t)0x08u)));

 Model0_z = (( (Model0_ZONE_NORMAL << 0 * 2) | (Model0_ZONE_DMA << 0x01u * 2) | (Model0_ZONE_NORMAL << 0x02u * 2) | (Model0_ZONE_DMA32 << 0x04u * 2) | (Model0_ZONE_NORMAL << 0x08u * 2) | (Model0_ZONE_DMA << (0x08u | 0x01u) * 2) | (Model0_ZONE_MOVABLE << (0x08u | 0x02u) * 2) | (Model0_ZONE_DMA32 << (0x08u | 0x04u) * 2)) >> (Model0_bit * 2)) &
      ((1 << 2) - 1);
 ((void)(sizeof(( long)((( 1 << (0x01u | 0x02u) | 1 << (0x01u | 0x04u) | 1 << (0x04u | 0x02u) | 1 << (0x01u | 0x04u | 0x02u) | 1 << (0x08u | 0x02u | 0x01u) | 1 << (0x08u | 0x04u | 0x01u) | 1 << (0x08u | 0x04u | 0x02u) | 1 << (0x08u | 0x04u | 0x01u | 0x02u) ) >> Model0_bit) & 1))));
 return Model0_z;
}

/*
 * There is only one page-allocator function, and two main namespaces to
 * it. The alloc_page*() variants return 'struct page *' and as such
 * can allocate highmem pages, the *get*page*() variants return
 * virtual kernel addresses to the allocated page(s).
 */

static inline __attribute__((no_instrument_function)) int Model0_gfp_zonelist(Model0_gfp_t Model0_flags)
{

 if (__builtin_expect(!!(Model0_flags & (( Model0_gfp_t)0x40000u)), 0))
  return Model0_ZONELIST_NOFALLBACK;

 return Model0_ZONELIST_FALLBACK;
}

/*
 * We get the zone list from the current node and the gfp_mask.
 * This zone list contains a maximum of MAXNODES*MAX_NR_ZONES zones.
 * There are two zonelists per node, one for all zones with memory and
 * one containing just zones from the node the zonelist belongs to.
 *
 * For the normal case of non-DISCONTIGMEM systems the NODE_DATA() gets
 * optimized to &contig_page_data at compile-time.
 */
static inline __attribute__((no_instrument_function)) struct Model0_zonelist *Model0_node_zonelist(int Model0_nid, Model0_gfp_t Model0_flags)
{
 return (Model0_node_data[Model0_nid])->Model0_node_zonelists + Model0_gfp_zonelist(Model0_flags);
}


static inline __attribute__((no_instrument_function)) void Model0_arch_free_page(struct Model0_page *Model0_page, int Model0_order) { }


static inline __attribute__((no_instrument_function)) void Model0_arch_alloc_page(struct Model0_page *Model0_page, int Model0_order) { }


struct Model0_page *
Model0___alloc_pages_nodemask(Model0_gfp_t Model0_gfp_mask, unsigned int Model0_order,
         struct Model0_zonelist *Model0_zonelist, Model0_nodemask_t *Model0_nodemask);

static inline __attribute__((no_instrument_function)) struct Model0_page *
Model0___alloc_pages(Model0_gfp_t Model0_gfp_mask, unsigned int Model0_order,
  struct Model0_zonelist *Model0_zonelist)
{
 return Model0___alloc_pages_nodemask(Model0_gfp_mask, Model0_order, Model0_zonelist, ((void *)0));
}

/*
 * Allocate pages, preferring the node given as nid. The node must be valid and
 * online. For more general interface, see alloc_pages_node().
 */
static inline __attribute__((no_instrument_function)) struct Model0_page *
Model0___alloc_pages_node(int Model0_nid, Model0_gfp_t Model0_gfp_mask, unsigned int Model0_order)
{
 ((void)(sizeof(( long)(Model0_nid < 0 || Model0_nid >= (1 << 6)))));
 ((void)(sizeof(( long)(!Model0_node_state((Model0_nid), Model0_N_ONLINE)))));

 return Model0___alloc_pages(Model0_gfp_mask, Model0_order, Model0_node_zonelist(Model0_nid, Model0_gfp_mask));
}

/*
 * Allocate pages, preferring the node given as nid. When nid == NUMA_NO_NODE,
 * prefer the current CPU's closest node. Otherwise node must be valid and
 * online.
 */
static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_alloc_pages_node(int Model0_nid, Model0_gfp_t Model0_gfp_mask,
      unsigned int Model0_order)
{
 if (Model0_nid == (-1))
  Model0_nid = Model0_numa_mem_id();

 return Model0___alloc_pages_node(Model0_nid, Model0_gfp_mask, Model0_order);
}


extern struct Model0_page *Model0_alloc_pages_current(Model0_gfp_t Model0_gfp_mask, unsigned Model0_order);

static inline __attribute__((no_instrument_function)) struct Model0_page *
Model0_alloc_pages(Model0_gfp_t Model0_gfp_mask, unsigned int Model0_order)
{
 return Model0_alloc_pages_current(Model0_gfp_mask, Model0_order);
}
extern struct Model0_page *Model0_alloc_pages_vma(Model0_gfp_t Model0_gfp_mask, int Model0_order,
   struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr,
   int Model0_node, bool Model0_hugepage);
extern unsigned long Model0___get_free_pages(Model0_gfp_t Model0_gfp_mask, unsigned int Model0_order);
extern unsigned long Model0_get_zeroed_page(Model0_gfp_t Model0_gfp_mask);

void *Model0_alloc_pages_exact(Model0_size_t Model0_size, Model0_gfp_t Model0_gfp_mask);
void Model0_free_pages_exact(void *Model0_virt, Model0_size_t Model0_size);
void * __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model0_alloc_pages_exact_nid(int Model0_nid, Model0_size_t Model0_size, Model0_gfp_t Model0_gfp_mask);







extern void Model0___free_pages(struct Model0_page *Model0_page, unsigned int Model0_order);
extern void Model0_free_pages(unsigned long Model0_addr, unsigned int Model0_order);
extern void Model0_free_hot_cold_page(struct Model0_page *Model0_page, bool Model0_cold);
extern void Model0_free_hot_cold_page_list(struct Model0_list_head *Model0_list, bool Model0_cold);

struct Model0_page_frag_cache;
extern void *Model0___alloc_page_frag(struct Model0_page_frag_cache *Model0_nc,
          unsigned int Model0_fragsz, Model0_gfp_t Model0_gfp_mask);
extern void Model0___free_page_frag(void *Model0_addr);




void Model0_page_alloc_init(void);
void Model0_drain_zone_pages(struct Model0_zone *Model0_zone, struct Model0_per_cpu_pages *Model0_pcp);
void Model0_drain_all_pages(struct Model0_zone *Model0_zone);
void Model0_drain_local_pages(struct Model0_zone *Model0_zone);

void Model0_page_alloc_init_late(void);

/*
 * gfp_allowed_mask is set to GFP_BOOT_MASK during early boot to restrict what
 * GFP flags are used before interrupts are enabled. Once interrupts are
 * enabled, it is set to __GFP_BITS_MASK while the system is running. During
 * hibernation, it is used by PM to avoid I/O during memory allocation while
 * devices are suspended.
 */
extern Model0_gfp_t Model0_gfp_allowed_mask;

/* Returns true if the gfp_mask allows use of ALLOC_NO_WATERMARK */
bool Model0_gfp_pfmemalloc_allowed(Model0_gfp_t Model0_gfp_mask);

extern void Model0_pm_restrict_gfp_mask(void);
extern void Model0_pm_restore_gfp_mask(void);


extern bool Model0_pm_suspended_storage(void);












struct Model0_task_struct;

extern int Model0_debug_locks;
extern int Model0_debug_locks_silent;


static inline __attribute__((no_instrument_function)) int Model0___debug_locks_off(void)
{
 return ({ __typeof__ (*((&Model0_debug_locks))) Model0___ret = ((0)); switch (sizeof(*((&Model0_debug_locks)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*((&Model0_debug_locks))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_debug_locks))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_debug_locks))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_debug_locks))) : : "memory", "cc"); break; default: Model0___xchg_wrong_size(); } Model0___ret; });
}

/*
 * Generic 'turn off all lock debugging' function:
 */
extern int Model0_debug_locks_off(void);
struct Model0_task_struct;







static inline __attribute__((no_instrument_function)) void Model0_debug_show_all_locks(void)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_show_held_locks(struct Model0_task_struct *Model0_task)
{
}

static inline __attribute__((no_instrument_function)) void
Model0_debug_check_no_locks_freed(const void *Model0_from, unsigned long Model0_len)
{
}

static inline __attribute__((no_instrument_function)) void
Model0_debug_check_no_locks_held(void)
{
}











/*
 * Architecture-neutral AT_ values in 0-17, leave some room
 * for more of them, start the x86-specific ones at 32.
 */





/* entries in ARCH_DLINFO: */

/* Symbolic values for the entries in the auxiliary table
   put on the initial stack */
/* AT_* values 18 through 22 are reserved */


  /* number of "#define AT_.*" above, minus {AT_NULL, AT_IGNORE, AT_NOTELF} */










/*
 * User-space Probes (UProbes)
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 *
 * Copyright (C) IBM Corporation, 2008-2012
 * Authors:
 *	Srikar Dronamraju
 *	Jim Keniston
 * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra
 */





struct Model0_vm_area_struct;
struct Model0_mm_struct;
struct Model0_inode;
struct Model0_notifier_block;
struct Model0_page;






enum Model0_uprobe_filter_ctx {
 Model0_UPROBE_FILTER_REGISTER,
 Model0_UPROBE_FILTER_UNREGISTER,
 Model0_UPROBE_FILTER_MMAP,
};

struct Model0_uprobe_consumer {
 int (*Model0_handler)(struct Model0_uprobe_consumer *Model0_self, struct Model0_pt_regs *Model0_regs);
 int (*Model0_ret_handler)(struct Model0_uprobe_consumer *Model0_self,
    unsigned long func,
    struct Model0_pt_regs *Model0_regs);
 bool (*Model0_filter)(struct Model0_uprobe_consumer *Model0_self,
    enum Model0_uprobe_filter_ctx Model0_ctx,
    struct Model0_mm_struct *Model0_mm);

 struct Model0_uprobe_consumer *Model0_next;
};
struct Model0_uprobes_state {
};



static inline __attribute__((no_instrument_function)) int
Model0_uprobe_register(struct Model0_inode *Model0_inode, Model0_loff_t Model0_offset, struct Model0_uprobe_consumer *Model0_uc)
{
 return -38;
}
static inline __attribute__((no_instrument_function)) int
Model0_uprobe_apply(struct Model0_inode *Model0_inode, Model0_loff_t Model0_offset, struct Model0_uprobe_consumer *Model0_uc, bool Model0_add)
{
 return -38;
}
static inline __attribute__((no_instrument_function)) void
Model0_uprobe_unregister(struct Model0_inode *Model0_inode, Model0_loff_t Model0_offset, struct Model0_uprobe_consumer *Model0_uc)
{
}
static inline __attribute__((no_instrument_function)) int Model0_uprobe_mmap(struct Model0_vm_area_struct *Model0_vma)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) void
Model0_uprobe_munmap(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_start, unsigned long Model0_end)
{
}
static inline __attribute__((no_instrument_function)) void Model0_uprobe_start_dup_mmap(void)
{
}
static inline __attribute__((no_instrument_function)) void Model0_uprobe_end_dup_mmap(void)
{
}
static inline __attribute__((no_instrument_function)) void
Model0_uprobe_dup_mmap(struct Model0_mm_struct *Model0_oldmm, struct Model0_mm_struct *Model0_newmm)
{
}
static inline __attribute__((no_instrument_function)) void Model0_uprobe_notify_resume(struct Model0_pt_regs *Model0_regs)
{
}
static inline __attribute__((no_instrument_function)) bool Model0_uprobe_deny_signal(void)
{
 return false;
}
static inline __attribute__((no_instrument_function)) void Model0_uprobe_free_utask(struct Model0_task_struct *Model0_t)
{
}
static inline __attribute__((no_instrument_function)) void Model0_uprobe_copy_process(struct Model0_task_struct *Model0_t, unsigned long Model0_flags)
{
}
static inline __attribute__((no_instrument_function)) void Model0_uprobe_clear_state(struct Model0_mm_struct *Model0_mm)
{
}
struct Model0_address_space;
struct Model0_mem_cgroup;






/*
 * Each physical page in the system has a struct page associated with
 * it to keep track of whatever it is we are using the page for at the
 * moment. Note that we have no way to track which tasks are using
 * a page, though if it is a pagecache page, rmap structures can tell us
 * who is mapping it.
 *
 * The objects in struct page are organized in double word blocks in
 * order to allows us to use atomic double word operations on portions
 * of struct page. That is currently only used by slub but the arrangement
 * allows the use of atomic double word operations on the flags/mapping
 * and lru list pointers also.
 */
struct Model0_page {
 /* First double word block */
 unsigned long Model0_flags; /* Atomic flags, some possibly
					 * updated asynchronously */
 union {
  struct Model0_address_space *Model0_mapping; /* If low bit clear, points to
						 * inode address_space, or NULL.
						 * If page mapped as anonymous
						 * memory, low bit is set, and
						 * it points to anon_vma object:
						 * see PAGE_MAPPING_ANON below.
						 */
  void *Model0_s_mem; /* slab first object */
  Model0_atomic_t Model0_compound_mapcount; /* first tail page */
  /* page_deferred_list().next	 -- second tail page */
 };

 /* Second double word */
 union {
  unsigned long Model0_index; /* Our offset within mapping. */
  void *Model0_freelist; /* sl[aou]b first free object */
  /* page_deferred_list().prev	-- second tail page */
 };

 union {


  /* Used for cmpxchg_double in slub */
  unsigned long Model0_counters;
  struct {

   union {
    /*
				 * Count of ptes mapped in mms, to show when
				 * page is mapped & limit reverse map searches.
				 *
				 * Extra information about page type may be
				 * stored here for pages that are never mapped,
				 * in which case the value MUST BE <= -2.
				 * See page-flags.h for more details.
				 */
    Model0_atomic_t Model0__mapcount;

    unsigned int Model0_active; /* SLAB */
    struct { /* SLUB */
     unsigned Model0_inuse:16;
     unsigned Model0_objects:15;
     unsigned Model0_frozen:1;
    };
    int Model0_units; /* SLOB */
   };
   /*
			 * Usage count, *USE WRAPPER FUNCTION* when manual
			 * accounting. See page_ref.h
			 */
   Model0_atomic_t Model0__refcount;
  };
 };

 /*
	 * Third double word block
	 *
	 * WARNING: bit 0 of the first word encode PageTail(). That means
	 * the rest users of the storage space MUST NOT use the bit to
	 * avoid collision and false-positive PageTail().
	 */
 union {
  struct Model0_list_head Model0_lru; /* Pageout list, eg. active_list
					 * protected by zone_lru_lock !
					 * Can be used as a generic list
					 * by the page owner.
					 */
  struct Model0_dev_pagemap *Model0_pgmap; /* ZONE_DEVICE pages are never on an
					    * lru or handled by a slab
					    * allocator, this points to the
					    * hosting device page map.
					    */
  struct { /* slub per cpu partial pages */
   struct Model0_page *Model0_next; /* Next partial slab */

   int Model0_pages; /* Nr of partial slabs left */
   int Model0_pobjects; /* Approximate # of objects */




  };

  struct Model0_callback_head Model0_callback_head; /* Used by SLAB
						 * when destroying via RCU
						 */
  /* Tail pages of compound page */
  struct {
   unsigned long Model0_compound_head; /* If bit zero is set */

   /* First tail page only */

   /*
			 * On 64 bit system we have enough space in struct page
			 * to encode compound_dtor and compound_order with
			 * unsigned int. It can help compiler generate better or
			 * smaller code on some archtectures.
			 */
   unsigned int Model0_compound_dtor;
   unsigned int Model0_compound_order;




  };
 };

 /* Remainder is not double word aligned */
 union {
  unsigned long Model0_private; /* Mapping-private opaque data:
					 	 * usually used for buffer_heads
						 * if PagePrivate set; used for
						 * swp_entry_t if PageSwapCache;
						 * indicates order in the buddy
						 * system if PG_buddy is set.
						 */




  Model0_spinlock_t Model0_ptl;


  struct Model0_kmem_cache *Model0_slab_cache; /* SL[AU]B: Pointer to slab */
 };





 /*
	 * On machines where all RAM is mapped into kernel address space,
	 * we can simply calculate the virtual address. On machines with
	 * highmem some memory is mapped into kernel virtual memory
	 * dynamically, so we need a place to store that address.
	 * Note that this field could be 16 bits on x86 ... ;)
	 *
	 * Architectures with slow multiplication can define
	 * WANT_PAGE_VIRTUAL in asm/page.h
	 */
}
/*
 * The struct page can be forced to be double word aligned so that atomic ops
 * on double words work. The SLUB allocator can make use of such a feature.
 */

 __attribute__((aligned(2 * sizeof(unsigned long))))

;

struct Model0_page_frag {
 struct Model0_page *Model0_page;

 __u32 Model0_offset;
 __u32 Model0_size;




};




struct Model0_page_frag_cache {
 void * Model0_va;

 Model0___u16 Model0_offset;
 Model0___u16 Model0_size;



 /* we maintain a pagecount bias, so that we dont dirty cache line
	 * containing page->_refcount every time we allocate a fragment.
	 */
 unsigned int Model0_pagecnt_bias;
 bool Model0_pfmemalloc;
};

typedef unsigned long Model0_vm_flags_t;

/*
 * A region containing a mapping of a non-memory backed file under NOMMU
 * conditions.  These are held in a global tree and are pinned by the VMAs that
 * map parts of them.
 */
struct Model0_vm_region {
 struct Model0_rb_node Model0_vm_rb; /* link in global region tree */
 Model0_vm_flags_t Model0_vm_flags; /* VMA vm_flags */
 unsigned long Model0_vm_start; /* start address of region */
 unsigned long Model0_vm_end; /* region initialised to here */
 unsigned long Model0_vm_top; /* region allocated to here */
 unsigned long Model0_vm_pgoff; /* the offset in vm_file corresponding to vm_start */
 struct Model0_file *Model0_vm_file; /* the backing file or NULL */

 int Model0_vm_usage; /* region usage count (access under nommu_region_sem) */
 bool Model0_vm_icache_flushed : 1; /* true if the icache has been flushed for
						* this region */
};
struct Model0_vm_userfaultfd_ctx {};


/*
 * This struct defines a memory VMM memory area. There is one of these
 * per VM-area/task.  A VM area is any part of the process virtual memory
 * space that has a special rule for the page-fault handlers (ie a shared
 * library, the executable area etc).
 */
struct Model0_vm_area_struct {
 /* The first cache line has the info for VMA tree walking. */

 unsigned long Model0_vm_start; /* Our start address within vm_mm. */
 unsigned long Model0_vm_end; /* The first byte after our end address
					   within vm_mm. */

 /* linked list of VM areas per task, sorted by address */
 struct Model0_vm_area_struct *Model0_vm_next, *Model0_vm_prev;

 struct Model0_rb_node Model0_vm_rb;

 /*
	 * Largest free memory gap in bytes to the left of this VMA.
	 * Either between this VMA and vma->vm_prev, or between one of the
	 * VMAs below us in the VMA rbtree and its ->vm_prev. This helps
	 * get_unmapped_area find a free area of the right size.
	 */
 unsigned long Model0_rb_subtree_gap;

 /* Second cache line starts here. */

 struct Model0_mm_struct *Model0_vm_mm; /* The address space we belong to. */
 Model0_pgprot_t Model0_vm_page_prot; /* Access permissions of this VMA. */
 unsigned long Model0_vm_flags; /* Flags, see mm.h. */

 /*
	 * For areas with an address space and backing store,
	 * linkage into the address_space->i_mmap interval tree.
	 */
 struct {
  struct Model0_rb_node Model0_rb;
  unsigned long Model0_rb_subtree_last;
 } Model0_shared;

 /*
	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
	 * list, after a COW of one of the file pages.	A MAP_SHARED vma
	 * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack
	 * or brk vma (with NULL file) can only be in an anon_vma list.
	 */
 struct Model0_list_head Model0_anon_vma_chain; /* Serialized by mmap_sem &
					  * page_table_lock */
 struct Model0_anon_vma *Model0_anon_vma; /* Serialized by page_table_lock */

 /* Function pointers to deal with this struct. */
 const struct Model0_vm_operations_struct *Model0_vm_ops;

 /* Information about our backing store: */
 unsigned long Model0_vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE
					   units */
 struct Model0_file * Model0_vm_file; /* File we map to (can be NULL). */
 void * Model0_vm_private_data; /* was vm_pte (shared mem) */





 struct Model0_mempolicy *Model0_vm_policy; /* NUMA policy for the VMA */

 struct Model0_vm_userfaultfd_ctx Model0_vm_userfaultfd_ctx;
};

struct Model0_core_thread {
 struct Model0_task_struct *Model0_task;
 struct Model0_core_thread *Model0_next;
};

struct Model0_core_state {
 Model0_atomic_t Model0_nr_threads;
 struct Model0_core_thread Model0_dumper;
 struct Model0_completion Model0_startup;
};

enum {
 Model0_MM_FILEPAGES, /* Resident file mapping pages */
 Model0_MM_ANONPAGES, /* Resident anonymous pages */
 Model0_MM_SWAPENTS, /* Anonymous swap entries */
 Model0_MM_SHMEMPAGES, /* Resident shared memory pages */
 Model0_NR_MM_COUNTERS
};



/* per-thread cached information, */
struct Model0_task_rss_stat {
 int Model0_events; /* for synchronization threshold */
 int Model0_count[Model0_NR_MM_COUNTERS];
};


struct Model0_mm_rss_stat {
 Model0_atomic_long_t Model0_count[Model0_NR_MM_COUNTERS];
};

struct Model0_kioctx_table;
struct Model0_mm_struct {
 struct Model0_vm_area_struct *Model0_mmap; /* list of VMAs */
 struct Model0_rb_root Model0_mm_rb;
 Model0_u32 Model0_vmacache_seqnum; /* per-thread vmacache */

 unsigned long (*Model0_get_unmapped_area) (struct Model0_file *Model0_filp,
    unsigned long Model0_addr, unsigned long Model0_len,
    unsigned long Model0_pgoff, unsigned long Model0_flags);

 unsigned long Model0_mmap_base; /* base of mmap area */
 unsigned long Model0_mmap_legacy_base; /* base of mmap area in bottom-up allocations */
 unsigned long Model0_task_size; /* size of task vm space */
 unsigned long Model0_highest_vm_end; /* highest vma end address */
 Model0_pgd_t * Model0_pgd;
 Model0_atomic_t Model0_mm_users; /* How many users with user space? */
 Model0_atomic_t Model0_mm_count; /* How many references to "struct mm_struct" (users count as 1) */
 Model0_atomic_long_t Model0_nr_ptes; /* PTE page table pages */

 Model0_atomic_long_t Model0_nr_pmds; /* PMD page table pages */

 int Model0_map_count; /* number of VMAs */

 Model0_spinlock_t Model0_page_table_lock; /* Protects page tables and some counters */
 struct Model0_rw_semaphore Model0_mmap_sem;

 struct Model0_list_head Model0_mmlist; /* List of maybe swapped mm's.	These are globally strung
						 * together off init_mm.mmlist, and are protected
						 * by mmlist_lock
						 */


 unsigned long Model0_hiwater_rss; /* High-watermark of RSS usage */
 unsigned long Model0_hiwater_vm; /* High-water virtual memory usage */

 unsigned long Model0_total_vm; /* Total pages mapped */
 unsigned long Model0_locked_vm; /* Pages that have PG_mlocked set */
 unsigned long Model0_pinned_vm; /* Refcount permanently increased */
 unsigned long Model0_data_vm; /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
 unsigned long Model0_exec_vm; /* VM_EXEC & ~VM_WRITE & ~VM_STACK */
 unsigned long Model0_stack_vm; /* VM_STACK */
 unsigned long Model0_def_flags;
 unsigned long Model0_start_code, Model0_end_code, Model0_start_data, Model0_end_data;
 unsigned long Model0_start_brk, Model0_brk, Model0_start_stack;
 unsigned long Model0_arg_start, Model0_arg_end, Model0_env_start, Model0_env_end;

 unsigned long Model0_saved_auxv[(2*(2 + 20 + 1))]; /* for /proc/PID/auxv */

 /*
	 * Special counters, in some configurations protected by the
	 * page_table_lock, in other configurations by being atomic.
	 */
 struct Model0_mm_rss_stat Model0_rss_stat;

 struct Model0_linux_binfmt *Model0_binfmt;

 Model0_cpumask_var_t Model0_cpu_vm_mask_var;

 /* Architecture-specific MM context */
 Model0_mm_context_t Model0_context;

 unsigned long Model0_flags; /* Must use atomic bitops to access the bits */

 struct Model0_core_state *Model0_core_state; /* coredumping support */

 Model0_spinlock_t Model0_ioctx_lock;
 struct Model0_kioctx_table *Model0_ioctx_table;
 /* store ref to file /proc/<pid>/exe symlink points to */
 struct Model0_file *Model0_exe_file;

 struct Model0_mmu_notifier_mm *Model0_mmu_notifier_mm;
 /*
	 * An operation with batched TLB flushing is going on. Anything that
	 * can move process memory needs to flush the TLB when moving a
	 * PROT_NONE or PROT_NUMA mapped page.
	 */
 bool Model0_tlb_flush_pending;

 struct Model0_uprobes_state Model0_uprobes_state;





 Model0_atomic_long_t Model0_hugetlb_usage;


 struct Model0_work_struct Model0_async_put_work;

};

static inline __attribute__((no_instrument_function)) void Model0_mm_init_cpumask(struct Model0_mm_struct *Model0_mm)
{



 Model0_cpumask_clear(Model0_mm->Model0_cpu_vm_mask_var);
}

/* Future-safe accessor for struct mm_struct's cpu_vm_mask. */
static inline __attribute__((no_instrument_function)) Model0_cpumask_t *Model0_mm_cpumask(struct Model0_mm_struct *Model0_mm)
{
 return Model0_mm->Model0_cpu_vm_mask_var;
}


/*
 * Memory barriers to keep this state in sync are graciously provided by
 * the page table locks, outside of which no page table modifications happen.
 * The barriers below prevent the compiler from re-ordering the instructions
 * around the memory barriers that are already present in the code.
 */
static inline __attribute__((no_instrument_function)) bool Model0_mm_tlb_flush_pending(struct Model0_mm_struct *Model0_mm)
{
 __asm__ __volatile__("": : :"memory");
 return Model0_mm->Model0_tlb_flush_pending;
}
static inline __attribute__((no_instrument_function)) void Model0_set_tlb_flush_pending(struct Model0_mm_struct *Model0_mm)
{
 Model0_mm->Model0_tlb_flush_pending = true;

 /*
	 * Guarantee that the tlb_flush_pending store does not leak into the
	 * critical section updating the page tables
	 */
 __asm__ __volatile__("": : :"memory");
}
/* Clearing is done after a TLB flush, which also provides a barrier. */
static inline __attribute__((no_instrument_function)) void Model0_clear_tlb_flush_pending(struct Model0_mm_struct *Model0_mm)
{
 __asm__ __volatile__("": : :"memory");
 Model0_mm->Model0_tlb_flush_pending = false;
}
struct Model0_vm_fault;

struct Model0_vm_special_mapping {
 const char *Model0_name; /* The name, e.g. "[vdso]". */

 /*
	 * If .fault is not provided, this points to a
	 * NULL-terminated array of pages that back the special mapping.
	 *
	 * This must not be NULL unless .fault is provided.
	 */
 struct Model0_page **Model0_pages;

 /*
	 * If non-NULL, then this is called to resolve page faults
	 * on the special mapping.  If used, .pages is not checked.
	 */
 int (*fault)(const struct Model0_vm_special_mapping *Model0_sm,
       struct Model0_vm_area_struct *Model0_vma,
       struct Model0_vm_fault *Model0_vmf);

 int (*Model0_mremap)(const struct Model0_vm_special_mapping *Model0_sm,
       struct Model0_vm_area_struct *Model0_new_vma);
};

enum Model0_tlb_flush_reason {
 Model0_TLB_FLUSH_ON_TASK_SWITCH,
 Model0_TLB_REMOTE_SHOOTDOWN,
 Model0_TLB_LOCAL_SHOOTDOWN,
 Model0_TLB_LOCAL_MM_SHOOTDOWN,
 Model0_TLB_REMOTE_SEND_IPI,
 Model0_NR_TLB_FLUSH_REASONS,
};

 /*
  * A swap entry has to fit into a "unsigned long", as the entry is hidden
  * in the "index" field of the swapper address space.
  */
typedef struct {
 unsigned long Model0_val;
} Model0_swp_entry_t;


/*
 * Percpu refcounts:
 * (C) 2012 Google, Inc.
 * Author: Kent Overstreet <koverstreet@google.com>
 *
 * This implements a refcount with similar semantics to atomic_t - atomic_inc(),
 * atomic_dec_and_test() - but percpu.
 *
 * There's one important difference between percpu refs and normal atomic_t
 * refcounts; you have to keep track of your initial refcount, and then when you
 * start shutting down you call percpu_ref_kill() _before_ dropping the initial
 * refcount.
 *
 * The refcount will have a range of 0 to ((1U << 31) - 1), i.e. one bit less
 * than an atomic_t - this is because of the way shutdown works, see
 * percpu_ref_kill()/PERCPU_COUNT_BIAS.
 *
 * Before you call percpu_ref_kill(), percpu_ref_put() does not check for the
 * refcount hitting 0 - it can't, if it was in percpu mode. percpu_ref_kill()
 * puts the ref back in single atomic_t mode, collecting the per cpu refs and
 * issuing the appropriate barriers, and then marks the ref as shutting down so
 * that percpu_ref_put() will check for the ref hitting 0.  After it returns,
 * it's safe to drop the initial ref.
 *
 * USAGE:
 *
 * See fs/aio.c for some example usage; it's used there for struct kioctx, which
 * is created when userspaces calls io_setup(), and destroyed when userspace
 * calls io_destroy() or the process exits.
 *
 * In the aio code, kill_ioctx() is called when we wish to destroy a kioctx; it
 * calls percpu_ref_kill(), then hlist_del_rcu() and synchronize_rcu() to remove
 * the kioctx from the proccess's list of kioctxs - after that, there can't be
 * any new users of the kioctx (from lookup_ioctx()) and it's then safe to drop
 * the initial ref with percpu_ref_put().
 *
 * Code that does a two stage shutdown like this often needs some kind of
 * explicit synchronization to ensure the initial refcount can only be dropped
 * once - percpu_ref_kill() does this for you, it returns true once and false if
 * someone else already called it. The aio code uses it this way, but it's not
 * necessary if the code has some other mechanism to synchronize teardown.
 * around.
 */
struct Model0_percpu_ref;
typedef void (Model0_percpu_ref_func_t)(struct Model0_percpu_ref *);

/* flags set in the lower bits of percpu_ref->percpu_count_ptr */
enum {
 Model0___PERCPU_REF_ATOMIC = 1LU << 0, /* operating in atomic mode */
 Model0___PERCPU_REF_DEAD = 1LU << 1, /* (being) killed */
 Model0___PERCPU_REF_ATOMIC_DEAD = Model0___PERCPU_REF_ATOMIC | Model0___PERCPU_REF_DEAD,

 Model0___PERCPU_REF_FLAG_BITS = 2,
};

/* @flags for percpu_ref_init() */
enum {
 /*
	 * Start w/ ref == 1 in atomic mode.  Can be switched to percpu
	 * operation using percpu_ref_switch_to_percpu().  If initialized
	 * with this flag, the ref will stay in atomic mode until
	 * percpu_ref_switch_to_percpu() is invoked on it.
	 */
 Model0_PERCPU_REF_INIT_ATOMIC = 1 << 0,

 /*
	 * Start dead w/ ref == 0 in atomic mode.  Must be revived with
	 * percpu_ref_reinit() before used.  Implies INIT_ATOMIC.
	 */
 Model0_PERCPU_REF_INIT_DEAD = 1 << 1,
};

struct Model0_percpu_ref {
 Model0_atomic_long_t Model0_count;
 /*
	 * The low bit of the pointer indicates whether the ref is in percpu
	 * mode; if set, then get/put will manipulate the atomic_t.
	 */
 unsigned long Model0_percpu_count_ptr;
 Model0_percpu_ref_func_t *Model0_release;
 Model0_percpu_ref_func_t *Model0_confirm_switch;
 bool Model0_force_atomic:1;
 struct Model0_callback_head Model0_rcu;
};

int __attribute__((warn_unused_result)) Model0_percpu_ref_init(struct Model0_percpu_ref *Model0_ref,
     Model0_percpu_ref_func_t *Model0_release, unsigned int Model0_flags,
     Model0_gfp_t Model0_gfp);
void Model0_percpu_ref_exit(struct Model0_percpu_ref *Model0_ref);
void Model0_percpu_ref_switch_to_atomic(struct Model0_percpu_ref *Model0_ref,
     Model0_percpu_ref_func_t *Model0_confirm_switch);
void Model0_percpu_ref_switch_to_percpu(struct Model0_percpu_ref *Model0_ref);
void Model0_percpu_ref_kill_and_confirm(struct Model0_percpu_ref *Model0_ref,
     Model0_percpu_ref_func_t *Model0_confirm_kill);
void Model0_percpu_ref_reinit(struct Model0_percpu_ref *Model0_ref);

/**
 * percpu_ref_kill - drop the initial ref
 * @ref: percpu_ref to kill
 *
 * Must be used to drop the initial ref on a percpu refcount; must be called
 * precisely once before shutdown.
 *
 * Puts @ref in non percpu mode, then does a call_rcu() before gathering up the
 * percpu counters and dropping the initial ref.
 */
static inline __attribute__((no_instrument_function)) void Model0_percpu_ref_kill(struct Model0_percpu_ref *Model0_ref)
{
 Model0_percpu_ref_kill_and_confirm(Model0_ref, ((void *)0));
}

/*
 * Internal helper.  Don't use outside percpu-refcount proper.  The
 * function doesn't return the pointer and let the caller test it for NULL
 * because doing so forces the compiler to generate two conditional
 * branches as it can't assume that @ref->percpu_count is not NULL.
 */
static inline __attribute__((no_instrument_function)) bool Model0___ref_is_percpu(struct Model0_percpu_ref *Model0_ref,
       unsigned long **Model0_percpu_countp)
{
 unsigned long Model0_percpu_ptr;

 /*
	 * The value of @ref->percpu_count_ptr is tested for
	 * !__PERCPU_REF_ATOMIC, which may be set asynchronously, and then
	 * used as a pointer.  If the compiler generates a separate fetch
	 * when using it as a pointer, __PERCPU_REF_ATOMIC may be set in
	 * between contaminating the pointer value, meaning that
	 * READ_ONCE() is required when fetching it.
	 */
 Model0_percpu_ptr = ({ union { typeof(Model0_ref->Model0_percpu_count_ptr) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_ref->Model0_percpu_count_ptr), Model0___u.Model0___c, sizeof(Model0_ref->Model0_percpu_count_ptr)); else Model0___read_once_size_nocheck(&(Model0_ref->Model0_percpu_count_ptr), Model0___u.Model0___c, sizeof(Model0_ref->Model0_percpu_count_ptr)); Model0___u.Model0___val; });

 /* paired with smp_store_release() in __percpu_ref_switch_to_percpu() */
 do { } while (0);

 /*
	 * Theoretically, the following could test just ATOMIC; however,
	 * then we'd have to mask off DEAD separately as DEAD may be
	 * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
	 * implies ATOMIC anyway.  Test them together.
	 */
 if (__builtin_expect(!!(Model0_percpu_ptr & Model0___PERCPU_REF_ATOMIC_DEAD), 0))
  return false;

 *Model0_percpu_countp = (unsigned long *)Model0_percpu_ptr;
 return true;
}

/**
 * percpu_ref_get_many - increment a percpu refcount
 * @ref: percpu_ref to get
 * @nr: number of references to get
 *
 * Analogous to atomic_long_add().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) void Model0_percpu_ref_get_many(struct Model0_percpu_ref *Model0_ref, unsigned long Model0_nr)
{
 unsigned long *Model0_percpu_count;

 Model0_rcu_read_lock_sched();

 if (Model0___ref_is_percpu(Model0_ref, &Model0_percpu_count))
  do { do { const void *Model0___vpp_verify = (typeof((&(*Model0_percpu_count)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(*Model0_percpu_count)) { case 1: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_nr) && ((Model0_nr) == 1 || (Model0_nr) == -1)) ? (int)(Model0_nr) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_nr); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(Model0_nr))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(Model0_nr))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(Model0_nr))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(Model0_nr))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_nr) && ((Model0_nr) == 1 || (Model0_nr) == -1)) ? (int)(Model0_nr) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_nr); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(Model0_nr))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(Model0_nr))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(Model0_nr))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(Model0_nr))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_nr) && ((Model0_nr) == 1 || (Model0_nr) == -1)) ? (int)(Model0_nr) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_nr); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(Model0_nr))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(Model0_nr))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(Model0_nr))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(Model0_nr))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_nr) && ((Model0_nr) == 1 || (Model0_nr) == -1)) ? (int)(Model0_nr) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_nr); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(Model0_nr))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(Model0_nr))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(Model0_nr))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(Model0_nr))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
 else
  Model0_atomic_long_add(Model0_nr, &Model0_ref->Model0_count);

 Model0_rcu_read_unlock_sched();
}

/**
 * percpu_ref_get - increment a percpu refcount
 * @ref: percpu_ref to get
 *
 * Analagous to atomic_long_inc().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) void Model0_percpu_ref_get(struct Model0_percpu_ref *Model0_ref)
{
 Model0_percpu_ref_get_many(Model0_ref, 1);
}

/**
 * percpu_ref_tryget - try to increment a percpu refcount
 * @ref: percpu_ref to try-get
 *
 * Increment a percpu refcount unless its count already reached zero.
 * Returns %true on success; %false on failure.
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) bool Model0_percpu_ref_tryget(struct Model0_percpu_ref *Model0_ref)
{
 unsigned long *Model0_percpu_count;
 int Model0_ret;

 Model0_rcu_read_lock_sched();

 if (Model0___ref_is_percpu(Model0_ref, &Model0_percpu_count)) {
  do { do { const void *Model0___vpp_verify = (typeof((&(*Model0_percpu_count)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(*Model0_percpu_count)) { case 1: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
  Model0_ret = true;
 } else {
  Model0_ret = Model0_atomic64_add_unless(((Model0_atomic64_t *)(&Model0_ref->Model0_count)), 1, 0);
 }

 Model0_rcu_read_unlock_sched();

 return Model0_ret;
}

/**
 * percpu_ref_tryget_live - try to increment a live percpu refcount
 * @ref: percpu_ref to try-get
 *
 * Increment a percpu refcount unless it has already been killed.  Returns
 * %true on success; %false on failure.
 *
 * Completion of percpu_ref_kill() in itself doesn't guarantee that this
 * function will fail.  For such guarantee, percpu_ref_kill_and_confirm()
 * should be used.  After the confirm_kill callback is invoked, it's
 * guaranteed that no new reference will be given out by
 * percpu_ref_tryget_live().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) bool Model0_percpu_ref_tryget_live(struct Model0_percpu_ref *Model0_ref)
{
 unsigned long *Model0_percpu_count;
 int Model0_ret = false;

 Model0_rcu_read_lock_sched();

 if (Model0___ref_is_percpu(Model0_ref, &Model0_percpu_count)) {
  do { do { const void *Model0___vpp_verify = (typeof((&(*Model0_percpu_count)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(*Model0_percpu_count)) { case 1: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
  Model0_ret = true;
 } else if (!(Model0_ref->Model0_percpu_count_ptr & Model0___PERCPU_REF_DEAD)) {
  Model0_ret = Model0_atomic64_add_unless(((Model0_atomic64_t *)(&Model0_ref->Model0_count)), 1, 0);
 }

 Model0_rcu_read_unlock_sched();

 return Model0_ret;
}

/**
 * percpu_ref_put_many - decrement a percpu refcount
 * @ref: percpu_ref to put
 * @nr: number of references to put
 *
 * Decrement the refcount, and if 0, call the release function (which was passed
 * to percpu_ref_init())
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) void Model0_percpu_ref_put_many(struct Model0_percpu_ref *Model0_ref, unsigned long Model0_nr)
{
 unsigned long *Model0_percpu_count;

 Model0_rcu_read_lock_sched();

 if (Model0___ref_is_percpu(Model0_ref, &Model0_percpu_count))
  do { do { const void *Model0___vpp_verify = (typeof((&(*Model0_percpu_count)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(*Model0_percpu_count)) { case 1: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-(typeof(*Model0_percpu_count))(Model0_nr)) && ((-(typeof(*Model0_percpu_count))(Model0_nr)) == 1 || (-(typeof(*Model0_percpu_count))(Model0_nr)) == -1)) ? (int)(-(typeof(*Model0_percpu_count))(Model0_nr)) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-(typeof(*Model0_percpu_count))(Model0_nr)); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-(typeof(*Model0_percpu_count))(Model0_nr)) && ((-(typeof(*Model0_percpu_count))(Model0_nr)) == 1 || (-(typeof(*Model0_percpu_count))(Model0_nr)) == -1)) ? (int)(-(typeof(*Model0_percpu_count))(Model0_nr)) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-(typeof(*Model0_percpu_count))(Model0_nr)); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-(typeof(*Model0_percpu_count))(Model0_nr)) && ((-(typeof(*Model0_percpu_count))(Model0_nr)) == 1 || (-(typeof(*Model0_percpu_count))(Model0_nr)) == -1)) ? (int)(-(typeof(*Model0_percpu_count))(Model0_nr)) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-(typeof(*Model0_percpu_count))(Model0_nr)); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model0_percpu_count)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-(typeof(*Model0_percpu_count))(Model0_nr)) && ((-(typeof(*Model0_percpu_count))(Model0_nr)) == 1 || (-(typeof(*Model0_percpu_count))(Model0_nr)) == -1)) ? (int)(-(typeof(*Model0_percpu_count))(Model0_nr)) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-(typeof(*Model0_percpu_count))(Model0_nr)); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_percpu_count))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "qi" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_percpu_count)) : "re" ((Model0_pao_T__)(-(typeof(*Model0_percpu_count))(Model0_nr)))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
 else if (__builtin_expect(!!(Model0_atomic_long_sub_and_test(Model0_nr, &Model0_ref->Model0_count)), 0))
  Model0_ref->Model0_release(Model0_ref);

 Model0_rcu_read_unlock_sched();
}

/**
 * percpu_ref_put - decrement a percpu refcount
 * @ref: percpu_ref to put
 *
 * Decrement the refcount, and if 0, call the release function (which was passed
 * to percpu_ref_init())
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) void Model0_percpu_ref_put(struct Model0_percpu_ref *Model0_ref)
{
 Model0_percpu_ref_put_many(Model0_ref, 1);
}

/**
 * percpu_ref_is_dying - test whether a percpu refcount is dying or dead
 * @ref: percpu_ref to test
 *
 * Returns %true if @ref is dying or dead.
 *
 * This function is safe to call as long as @ref is between init and exit
 * and the caller is responsible for synchronizing against state changes.
 */
static inline __attribute__((no_instrument_function)) bool Model0_percpu_ref_is_dying(struct Model0_percpu_ref *Model0_ref)
{
 return Model0_ref->Model0_percpu_count_ptr & Model0___PERCPU_REF_DEAD;
}

/**
 * percpu_ref_is_zero - test whether a percpu refcount reached zero
 * @ref: percpu_ref to test
 *
 * Returns %true if @ref reached zero.
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) bool Model0_percpu_ref_is_zero(struct Model0_percpu_ref *Model0_ref)
{
 unsigned long *Model0_percpu_count;

 if (Model0___ref_is_percpu(Model0_ref, &Model0_percpu_count))
  return false;
 return !Model0_atomic_long_read(&Model0_ref->Model0_count);
}








/*
 *  bit-based spin_lock()
 *
 * Don't use this unless you really need to: spin_lock() and spin_unlock()
 * are significantly faster.
 */
static inline __attribute__((no_instrument_function)) void Model0_bit_spin_lock(int bitnum, unsigned long *Model0_addr)
{
 /*
	 * Assuming the lock is uncontended, this never enters
	 * the body of the outer loop. If it is contended, then
	 * within the inner loop a non-atomic test is used to
	 * busywait with less bus contention for a good time to
	 * attempt to acquire the lock bit.
	 */
 __asm__ __volatile__("": : :"memory");

 while (__builtin_expect(!!(Model0_test_and_set_bit_lock(bitnum, Model0_addr)), 0)) {
  __asm__ __volatile__("": : :"memory");
  do {
   Model0_cpu_relax();
  } while ((__builtin_constant_p((bitnum)) ? Model0_constant_test_bit((bitnum), (Model0_addr)) : Model0_variable_test_bit((bitnum), (Model0_addr))));
  __asm__ __volatile__("": : :"memory");
 }

 (void)0;
}

/*
 * Return true if it was acquired
 */
static inline __attribute__((no_instrument_function)) int Model0_bit_spin_trylock(int bitnum, unsigned long *Model0_addr)
{
 __asm__ __volatile__("": : :"memory");

 if (__builtin_expect(!!(Model0_test_and_set_bit_lock(bitnum, Model0_addr)), 0)) {
  __asm__ __volatile__("": : :"memory");
  return 0;
 }

 (void)0;
 return 1;
}

/*
 *  bit-based spin_unlock()
 */
static inline __attribute__((no_instrument_function)) void Model0_bit_spin_unlock(int bitnum, unsigned long *Model0_addr)
{




 Model0_clear_bit_unlock(bitnum, Model0_addr);

 __asm__ __volatile__("": : :"memory");
 (void)0;
}

/*
 *  bit-based spin_unlock()
 *  non-atomic version, which can be used eg. if the bit lock itself is
 *  protecting the rest of the flags in the word.
 */
static inline __attribute__((no_instrument_function)) void Model0___bit_spin_unlock(int bitnum, unsigned long *Model0_addr)
{




 Model0___clear_bit_unlock(bitnum, Model0_addr);

 __asm__ __volatile__("": : :"memory");
 (void)0;
}

/*
 * Return true if the lock is held.
 */
static inline __attribute__((no_instrument_function)) int Model0_bit_spin_is_locked(int bitnum, unsigned long *Model0_addr)
{

 return (__builtin_constant_p((bitnum)) ? Model0_constant_test_bit((bitnum), (Model0_addr)) : Model0_variable_test_bit((bitnum), (Model0_addr)));





}



/*
 * This struct is used to pass information from page reclaim to the shrinkers.
 * We consolidate the values for easier extention later.
 *
 * The 'gfpmask' refers to the allocation we are currently trying to
 * fulfil.
 */
struct Model0_shrink_control {
 Model0_gfp_t Model0_gfp_mask;

 /*
	 * How many objects scan_objects should scan and try to reclaim.
	 * This is reset before every call, so it is safe for callees
	 * to modify.
	 */
 unsigned long Model0_nr_to_scan;

 /* current node being shrunk (for NUMA aware shrinkers) */
 int Model0_nid;

 /* current memcg being shrunk (for memcg aware shrinkers) */
 struct Model0_mem_cgroup *Model0_memcg;
};


/*
 * A callback you can register to apply pressure to ageable caches.
 *
 * @count_objects should return the number of freeable items in the cache. If
 * there are no objects to free or the number of freeable items cannot be
 * determined, it should return 0. No deadlock checks should be done during the
 * count callback - the shrinker relies on aggregating scan counts that couldn't
 * be executed due to potential deadlocks to be run at a later call when the
 * deadlock condition is no longer pending.
 *
 * @scan_objects will only be called if @count_objects returned a non-zero
 * value for the number of freeable objects. The callout should scan the cache
 * and attempt to free items from the cache. It should then return the number
 * of objects freed during the scan, or SHRINK_STOP if progress cannot be made
 * due to potential deadlocks. If SHRINK_STOP is returned, then no further
 * attempts to call the @scan_objects will be made from the current reclaim
 * context.
 *
 * @flags determine the shrinker abilities, like numa awareness
 */
struct Model0_shrinker {
 unsigned long (*Model0_count_objects)(struct Model0_shrinker *,
           struct Model0_shrink_control *Model0_sc);
 unsigned long (*Model0_scan_objects)(struct Model0_shrinker *,
          struct Model0_shrink_control *Model0_sc);

 int Model0_seeks; /* seeks to recreate an obj */
 long Model0_batch; /* reclaim batch size, 0 = default */
 unsigned long Model0_flags;

 /* These are for internal use */
 struct Model0_list_head Model0_list;
 /* objs pending delete, per node */
 Model0_atomic_long_t *Model0_nr_deferred;
};


/* Flags */



extern int Model0_register_shrinker(struct Model0_shrinker *);
extern void Model0_unregister_shrinker(struct Model0_shrinker *);









/*
 * Resource control/accounting header file for linux
 */

/*
 * Definition of struct rusage taken from BSD 4.3 Reno
 * 
 * We don't support all of these yet, but we might as well have them....
 * Otherwise, each time we add new items, programs which depend on this
 * structure will lose.  This reduces the chances of that happening.
 */





struct Model0_rusage {
 struct Model0_timeval Model0_ru_utime; /* user time used */
 struct Model0_timeval Model0_ru_stime; /* system time used */
 Model0___kernel_long_t Model0_ru_maxrss; /* maximum resident set size */
 Model0___kernel_long_t Model0_ru_ixrss; /* integral shared memory size */
 Model0___kernel_long_t Model0_ru_idrss; /* integral unshared data size */
 Model0___kernel_long_t Model0_ru_isrss; /* integral unshared stack size */
 Model0___kernel_long_t Model0_ru_minflt; /* page reclaims */
 Model0___kernel_long_t Model0_ru_majflt; /* page faults */
 Model0___kernel_long_t Model0_ru_nswap; /* swaps */
 Model0___kernel_long_t Model0_ru_inblock; /* block input operations */
 Model0___kernel_long_t Model0_ru_oublock; /* block output operations */
 Model0___kernel_long_t Model0_ru_msgsnd; /* messages sent */
 Model0___kernel_long_t Model0_ru_msgrcv; /* messages received */
 Model0___kernel_long_t Model0_ru_nsignals; /* signals received */
 Model0___kernel_long_t Model0_ru_nvcsw; /* voluntary context switches */
 Model0___kernel_long_t Model0_ru_nivcsw; /* involuntary " */
};

struct Model0_rlimit {
 Model0___kernel_ulong_t Model0_rlim_cur;
 Model0___kernel_ulong_t Model0_rlim_max;
};



struct Model0_rlimit64 {
 __u64 Model0_rlim_cur;
 __u64 Model0_rlim_max;
};
/*
 * Limit the stack by to some sane default: root can always
 * increase this limit if needed..  8MB seems reasonable.
 */


/*
 * GPG2 wants 64kB of mlocked memory, to make sure pass phrases
 * and other sensitive information are never written to disk.
 */


/*
 * Due to binary compatibility, the actual resource numbers
 * may be different for different linux versions..
 */







/*
 * Resource limit IDs
 *
 * ( Compatibility detail: there are architectures that have
 *   a different rlimit ID order in the 5-9 range and want
 *   to keep that order for binary compatibility. The reasons
 *   are historic and all new rlimits are identical across all
 *   arches. If an arch has such special order for some rlimits
 *   then it defines them prior including asm-generic/resource.h. )
 */
/*
 * SuS says limits have to be unsigned.
 * Which makes a ton more sense anyway.
 *
 * Some architectures override this (for compatibility reasons):
 */


/*
 * boot-time rlimit defaults for the init task:
 */


struct Model0_task_struct;

int Model0_getrusage(struct Model0_task_struct *Model0_p, int Model0_who, struct Model0_rusage *Model0_ru);
int Model0_do_prlimit(struct Model0_task_struct *Model0_tsk, unsigned int Model0_resource,
  struct Model0_rlimit *Model0_new_rlim, struct Model0_rlimit *Model0_old_rlim);









struct Model0_task_struct;
struct Model0_pt_regs;


struct Model0_stack_trace {
 unsigned int Model0_nr_entries, Model0_max_entries;
 unsigned long *Model0_entries;
 int Model0_skip; /* input argument: How many entries to skip */
};

extern void Model0_save_stack_trace(struct Model0_stack_trace *Model0_trace);
extern void Model0_save_stack_trace_regs(struct Model0_pt_regs *Model0_regs,
      struct Model0_stack_trace *Model0_trace);
extern void Model0_save_stack_trace_tsk(struct Model0_task_struct *Model0_tsk,
    struct Model0_stack_trace *Model0_trace);

extern void Model0_print_stack_trace(struct Model0_stack_trace *Model0_trace, int Model0_spaces);
extern int Model0_snprint_stack_trace(char *Model0_buf, Model0_size_t Model0_size,
   struct Model0_stack_trace *Model0_trace, int Model0_spaces);


extern void Model0_save_stack_trace_user(struct Model0_stack_trace *Model0_trace);
/*
 * A generic stack depot implementation
 *
 * Author: Alexander Potapenko <glider@google.com>
 * Copyright (C) 2016 Google, Inc.
 *
 * Based on code by Dmitry Chernenkov.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */




typedef Model0_u32 Model0_depot_stack_handle_t;

struct Model0_stack_trace;

Model0_depot_stack_handle_t Model0_depot_save_stack(struct Model0_stack_trace *Model0_trace, Model0_gfp_t Model0_flags);

void Model0_depot_fetch_stack(Model0_depot_stack_handle_t Model0_handle, struct Model0_stack_trace *Model0_trace);

struct Model0_pglist_data;
struct Model0_page_ext_operations {
 bool (*Model0_need)(void);
 void (*Model0_init)(void);
};
struct Model0_page_ext;

static inline __attribute__((no_instrument_function)) void Model0_pgdat_page_ext_init(struct Model0_pglist_data *Model0_pgdat)
{
}

static inline __attribute__((no_instrument_function)) struct Model0_page_ext *Model0_lookup_page_ext(struct Model0_page *Model0_page)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_page_ext_init(void)
{
}

static inline __attribute__((no_instrument_function)) void Model0_page_ext_init_flatmem(void)
{
}






/*
 * Macros for manipulating and testing page->flags
 */
/*
 * Various page->flags bits:
 *
 * PG_reserved is set for special pages, which can never be swapped out. Some
 * of them might not even exist (eg empty_bad_page)...
 *
 * The PG_private bitflag is set on pagecache pages if they contain filesystem
 * specific data (which is normally at page->private). It can be used by
 * private allocations for its own usage.
 *
 * During initiation of disk I/O, PG_locked is set. This bit is set before I/O
 * and cleared when writeback _starts_ or when read _completes_. PG_writeback
 * is set before writeback starts and cleared when it finishes.
 *
 * PG_locked also pins a page in pagecache, and blocks truncation of the file
 * while it is held.
 *
 * page_waitqueue(page) is a wait queue of all tasks waiting for the page
 * to become unlocked.
 *
 * PG_uptodate tells whether the page's contents is valid.  When a read
 * completes, the page becomes uptodate, unless a disk I/O error happened.
 *
 * PG_referenced, PG_reclaim are used for page reclaim for anonymous and
 * file-backed pagecache (see mm/vmscan.c).
 *
 * PG_error is set to indicate that an I/O error occurred on this page.
 *
 * PG_arch_1 is an architecture specific page state bit.  The generic code
 * guarantees that this bit is cleared for a page when it first is entered into
 * the page cache.
 *
 * PG_highmem pages are not permanently mapped into the kernel virtual address
 * space, they need to be kmapped separately for doing IO on the pages.  The
 * struct page (these bits with information) are always mapped into kernel
 * address space...
 *
 * PG_hwpoison indicates that a page got corrupted in hardware and contains
 * data with incorrect ECC bits that triggered a machine check. Accessing is
 * not safe since it may cause another machine check. Don't touch!
 */

/*
 * Don't use the *_dontuse flags.  Use the macros.  Otherwise you'll break
 * locked- and dirty-page accounting.
 *
 * The page flags field is split into two parts, the main flags area
 * which extends from the low bits upwards, and the fields area which
 * extends from the high bits downwards.
 *
 *  | FIELD | ... | FLAGS |
 *  N-1           ^       0
 *               (NR_PAGEFLAGS)
 *
 * The fields area is reserved for fields mapping zone, node (for NUMA) and
 * SPARSEMEM section (for variants of SPARSEMEM that require section ids like
 * SPARSEMEM_EXTREME with !SPARSEMEM_VMEMMAP).
 */
enum Model0_pageflags {
 Model0_PG_locked, /* Page is locked. Don't touch. */
 Model0_PG_error,
 Model0_PG_referenced,
 Model0_PG_uptodate,
 Model0_PG_dirty,
 Model0_PG_lru,
 Model0_PG_active,
 Model0_PG_slab,
 Model0_PG_owner_priv_1, /* Owner use. If pagecache, fs may use*/
 Model0_PG_arch_1,
 Model0_PG_reserved,
 Model0_PG_private, /* If pagecache, has fs-private data */
 Model0_PG_private_2, /* If pagecache, has fs aux data */
 Model0_PG_writeback, /* Page is under writeback */
 Model0_PG_head, /* A head page */
 Model0_PG_swapcache, /* Swap page: swp_entry_t in private */
 Model0_PG_mappedtodisk, /* Has blocks allocated on-disk */
 Model0_PG_reclaim, /* To be reclaimed asap */
 Model0_PG_swapbacked, /* Page is backed by RAM/swap */
 Model0_PG_unevictable, /* Page is "unevictable"  */

 Model0_PG_mlocked, /* Page is vma mlocked */


 Model0_PG_uncached, /* Page has been mapped as uncached */
 Model0___NR_PAGEFLAGS,

 /* Filesystems */
 Model0_PG_checked = Model0_PG_owner_priv_1,

 /* Two page bits are conscripted by FS-Cache to maintain local caching
	 * state.  These bits are set on pages belonging to the netfs's inodes
	 * when those inodes are being locally cached.
	 */
 Model0_PG_fscache = Model0_PG_private_2, /* page backed by cache */

 /* XEN */
 /* Pinned in Xen as a read-only pagetable page. */
 Model0_PG_pinned = Model0_PG_owner_priv_1,
 /* Pinned as part of domain save (see xen_mm_pin_all()). */
 Model0_PG_savepinned = Model0_PG_dirty,
 /* Has a grant mapping of another (foreign) domain's page. */
 Model0_PG_foreign = Model0_PG_owner_priv_1,

 /* SLOB */
 Model0_PG_slob_free = Model0_PG_private,

 /* Compound pages. Stored in first tail page's flags */
 Model0_PG_double_map = Model0_PG_private_2,

 /* non-lru isolated movable page */
 Model0_PG_isolated = Model0_PG_reclaim,
};



struct Model0_page; /* forward declaration */

static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_compound_head(struct Model0_page *Model0_page)
{
 unsigned long Model0_head = ({ union { typeof(Model0_page->Model0_compound_head) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_page->Model0_compound_head), Model0___u.Model0___c, sizeof(Model0_page->Model0_compound_head)); else Model0___read_once_size_nocheck(&(Model0_page->Model0_compound_head), Model0___u.Model0___c, sizeof(Model0_page->Model0_compound_head)); Model0___u.Model0___val; });

 if (__builtin_expect(!!(Model0_head & 1), 0))
  return (struct Model0_page *) (Model0_head - 1);
 return Model0_page;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageTail(struct Model0_page *Model0_page)
{
 return ({ union { typeof(Model0_page->Model0_compound_head) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_page->Model0_compound_head), Model0___u.Model0___c, sizeof(Model0_page->Model0_compound_head)); else Model0___read_once_size_nocheck(&(Model0_page->Model0_compound_head), Model0___u.Model0___c, sizeof(Model0_page->Model0_compound_head)); Model0___u.Model0___val; }) & 1;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageCompound(struct Model0_page *Model0_page)
{
 return (__builtin_constant_p((Model0_PG_head)) ? Model0_constant_test_bit((Model0_PG_head), (&Model0_page->Model0_flags)) : Model0_variable_test_bit((Model0_PG_head), (&Model0_page->Model0_flags))) || Model0_PageTail(Model0_page);
}

/*
 * Page flags policies wrt compound pages
 *
 * PF_ANY:
 *     the page flag is relevant for small, head and tail pages.
 *
 * PF_HEAD:
 *     for compound page all operations related to the page flag applied to
 *     head page.
 *
 * PF_NO_TAIL:
 *     modifications of the page flag must be done on small or head pages,
 *     checks can be done on tail pages too.
 *
 * PF_NO_COMPOUND:
 *     the page flag is not relevant for compound pages.
 */
/*
 * Macros to create function definitions for page flags
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageLocked(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_locked)) ? Model0_constant_test_bit((Model0_PG_locked), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_locked), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageLocked(struct Model0_page *Model0_page) { Model0___set_bit(Model0_PG_locked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageLocked(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_locked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageError(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_error)) ? Model0_constant_test_bit((Model0_PG_error), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_error), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageError(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_error, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageError(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_error, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageError(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_error, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageReferenced(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_referenced)) ? Model0_constant_test_bit((Model0_PG_referenced), (&Model0_compound_head(Model0_page)->Model0_flags)) : Model0_variable_test_bit((Model0_PG_referenced), (&Model0_compound_head(Model0_page)->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageReferenced(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_referenced, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageReferenced(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_referenced, &Model0_compound_head(Model0_page)->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageReferenced(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_referenced, &Model0_compound_head(Model0_page)->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageReferenced(struct Model0_page *Model0_page) { Model0___set_bit(Model0_PG_referenced, &Model0_compound_head(Model0_page)->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageDirty(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_dirty)) ? Model0_constant_test_bit((Model0_PG_dirty), (&Model0_compound_head(Model0_page)->Model0_flags)) : Model0_variable_test_bit((Model0_PG_dirty), (&Model0_compound_head(Model0_page)->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageDirty(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_dirty, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageDirty(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_dirty, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestSetPageDirty(struct Model0_page *Model0_page) { return Model0_test_and_set_bit(Model0_PG_dirty, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageDirty(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_dirty, &Model0_compound_head(Model0_page)->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageDirty(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_dirty, &Model0_compound_head(Model0_page)->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageLRU(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_lru)) ? Model0_constant_test_bit((Model0_PG_lru), (&Model0_compound_head(Model0_page)->Model0_flags)) : Model0_variable_test_bit((Model0_PG_lru), (&Model0_compound_head(Model0_page)->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageLRU(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_lru, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageLRU(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_lru, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageLRU(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_lru, &Model0_compound_head(Model0_page)->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageActive(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_active)) ? Model0_constant_test_bit((Model0_PG_active), (&Model0_compound_head(Model0_page)->Model0_flags)) : Model0_variable_test_bit((Model0_PG_active), (&Model0_compound_head(Model0_page)->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageActive(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_active, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageActive(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_active, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageActive(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_active, &Model0_compound_head(Model0_page)->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageActive(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_active, &Model0_compound_head(Model0_page)->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageSlab(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_slab)) ? Model0_constant_test_bit((Model0_PG_slab), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_slab), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageSlab(struct Model0_page *Model0_page) { Model0___set_bit(Model0_PG_slab, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageSlab(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_slab, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageSlobFree(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_slob_free)) ? Model0_constant_test_bit((Model0_PG_slob_free), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_slob_free), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageSlobFree(struct Model0_page *Model0_page) { Model0___set_bit(Model0_PG_slob_free, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageSlobFree(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_slob_free, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageChecked(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_checked)) ? Model0_constant_test_bit((Model0_PG_checked), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_checked), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageChecked(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_checked, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageChecked(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_checked, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } /* Used by some filesystems */

/* Xen */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PagePinned(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_pinned)) ? Model0_constant_test_bit((Model0_PG_pinned), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_pinned), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPagePinned(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_pinned, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPagePinned(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_pinned, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestSetPagePinned(struct Model0_page *Model0_page) { return Model0_test_and_set_bit(Model0_PG_pinned, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPagePinned(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_pinned, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageSavePinned(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_savepinned)) ? Model0_constant_test_bit((Model0_PG_savepinned), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_savepinned), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageSavePinned(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_savepinned, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageSavePinned(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_savepinned, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); };
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageForeign(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_foreign)) ? Model0_constant_test_bit((Model0_PG_foreign), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_foreign), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageForeign(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_foreign, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageForeign(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_foreign, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); };

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageReserved(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_reserved)) ? Model0_constant_test_bit((Model0_PG_reserved), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_reserved), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageReserved(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_reserved, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageReserved(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_reserved, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageReserved(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_reserved, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageSwapBacked(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_swapbacked)) ? Model0_constant_test_bit((Model0_PG_swapbacked), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_swapbacked), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageSwapBacked(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_swapbacked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageSwapBacked(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_swapbacked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageSwapBacked(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_swapbacked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageSwapBacked(struct Model0_page *Model0_page) { Model0___set_bit(Model0_PG_swapbacked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }

/*
 * Private page markings that may be used by the filesystem that owns the page
 * for its own purposes.
 * - PG_private and PG_private_2 cause releasepage() and co to be invoked
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PagePrivate(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_private)) ? Model0_constant_test_bit((Model0_PG_private), (&Model0_page->Model0_flags)) : Model0_variable_test_bit((Model0_PG_private), (&Model0_page->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPagePrivate(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_private, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPagePrivate(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_private, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPagePrivate(struct Model0_page *Model0_page) { Model0___set_bit(Model0_PG_private, &Model0_page->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPagePrivate(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_private, &Model0_page->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PagePrivate2(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_private_2)) ? Model0_constant_test_bit((Model0_PG_private_2), (&Model0_page->Model0_flags)) : Model0_variable_test_bit((Model0_PG_private_2), (&Model0_page->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPagePrivate2(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_private_2, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPagePrivate2(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_private_2, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestSetPagePrivate2(struct Model0_page *Model0_page) { return Model0_test_and_set_bit(Model0_PG_private_2, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPagePrivate2(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_private_2, &Model0_page->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageOwnerPriv1(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_owner_priv_1)) ? Model0_constant_test_bit((Model0_PG_owner_priv_1), (&Model0_page->Model0_flags)) : Model0_variable_test_bit((Model0_PG_owner_priv_1), (&Model0_page->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageOwnerPriv1(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_owner_priv_1, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageOwnerPriv1(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_owner_priv_1, &Model0_page->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageOwnerPriv1(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_owner_priv_1, &Model0_page->Model0_flags); }

/*
 * Only test-and-set exist for PG_writeback.  The unconditional operators are
 * risky: they bypass page accounting.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageWriteback(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_writeback)) ? Model0_constant_test_bit((Model0_PG_writeback), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_writeback), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestSetPageWriteback(struct Model0_page *Model0_page) { return Model0_test_and_set_bit(Model0_PG_writeback, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageWriteback(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_writeback, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageMappedToDisk(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_mappedtodisk)) ? Model0_constant_test_bit((Model0_PG_mappedtodisk), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_mappedtodisk), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageMappedToDisk(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_mappedtodisk, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageMappedToDisk(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_mappedtodisk, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }

/* PG_readahead is only used for reads; PG_reclaim is only for writes */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageReclaim(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_reclaim)) ? Model0_constant_test_bit((Model0_PG_reclaim), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_reclaim), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageReclaim(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageReclaim(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageReclaim(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageReadahead(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_reclaim)) ? Model0_constant_test_bit((Model0_PG_reclaim), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_reclaim), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageReadahead(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageReadahead(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageReadahead(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
static inline __attribute__((no_instrument_function)) int Model0_PageHighMem(const struct Model0_page *Model0_page) { return 0; } static inline __attribute__((no_instrument_function)) void Model0_SetPageHighMem(struct Model0_page *Model0_page) { } static inline __attribute__((no_instrument_function)) void Model0_ClearPageHighMem(struct Model0_page *Model0_page) { }



static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageSwapCache(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_swapcache)) ? Model0_constant_test_bit((Model0_PG_swapcache), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_swapcache), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageSwapCache(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_swapcache, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageSwapCache(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_swapcache, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }




static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageUnevictable(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_unevictable)) ? Model0_constant_test_bit((Model0_PG_unevictable), (&Model0_compound_head(Model0_page)->Model0_flags)) : Model0_variable_test_bit((Model0_PG_unevictable), (&Model0_compound_head(Model0_page)->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageUnevictable(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_unevictable, &Model0_compound_head(Model0_page)->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageUnevictable(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_unevictable, &Model0_compound_head(Model0_page)->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageUnevictable(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_unevictable, &Model0_compound_head(Model0_page)->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageUnevictable(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_unevictable, &Model0_compound_head(Model0_page)->Model0_flags); }


static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageMlocked(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_mlocked)) ? Model0_constant_test_bit((Model0_PG_mlocked), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_mlocked), (&({ ((void)(sizeof(( long)(0 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageMlocked(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageMlocked(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageMlocked(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestSetPageMlocked(struct Model0_page *Model0_page) { return Model0_test_and_set_bit(Model0_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_TestClearPageMlocked(struct Model0_page *Model0_page) { return Model0_test_and_clear_bit(Model0_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }






static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageUncached(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_uncached)) ? Model0_constant_test_bit((Model0_PG_uncached), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags)) : Model0_variable_test_bit((Model0_PG_uncached), (&({ ((void)(sizeof(( long)(0 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageUncached(struct Model0_page *Model0_page) { Model0_set_bit(Model0_PG_uncached, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageUncached(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_uncached, &({ ((void)(sizeof(( long)(1 && Model0_PageCompound(Model0_page))))); Model0_page;})->Model0_flags); }
static inline __attribute__((no_instrument_function)) int Model0_PageHWPoison(const struct Model0_page *Model0_page) { return 0; } static inline __attribute__((no_instrument_function)) void Model0_SetPageHWPoison(struct Model0_page *Model0_page) { } static inline __attribute__((no_instrument_function)) void Model0_ClearPageHWPoison(struct Model0_page *Model0_page) { }
/*
 * On an anonymous page mapped into a user virtual memory area,
 * page->mapping points to its anon_vma, not to a struct address_space;
 * with the PAGE_MAPPING_ANON bit set to distinguish it.  See rmap.h.
 *
 * On an anonymous page in a VM_MERGEABLE area, if CONFIG_KSM is enabled,
 * the PAGE_MAPPING_MOVABLE bit may be set along with the PAGE_MAPPING_ANON
 * bit; and then page->mapping points, not to an anon_vma, but to a private
 * structure which KSM associates with that merged page.  See ksm.h.
 *
 * PAGE_MAPPING_KSM without PAGE_MAPPING_ANON is used for non-lru movable
 * page and then page->mapping points a struct address_space.
 *
 * Please note that, confusingly, "page_mapping" refers to the inode
 * address_space which maps the page from disk; whereas "page_mapped"
 * refers to user virtual address space into which the page is mapped.
 */





static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageMappingFlags(struct Model0_page *Model0_page)
{
 return ((unsigned long)Model0_page->Model0_mapping & (0x1 | 0x2)) != 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageAnon(struct Model0_page *Model0_page)
{
 Model0_page = Model0_compound_head(Model0_page);
 return ((unsigned long)Model0_page->Model0_mapping & 0x1) != 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0___PageMovable(struct Model0_page *Model0_page)
{
 return ((unsigned long)Model0_page->Model0_mapping & (0x1 | 0x2)) ==
    0x2;
}
static inline __attribute__((no_instrument_function)) int Model0_PageKsm(const struct Model0_page *Model0_page) { return 0; }


Model0_u64 Model0_stable_page_flags(struct Model0_page *Model0_page);

static inline __attribute__((no_instrument_function)) int Model0_PageUptodate(struct Model0_page *Model0_page)
{
 int Model0_ret;
 Model0_page = Model0_compound_head(Model0_page);
 Model0_ret = (__builtin_constant_p((Model0_PG_uptodate)) ? Model0_constant_test_bit((Model0_PG_uptodate), (&(Model0_page)->Model0_flags)) : Model0_variable_test_bit((Model0_PG_uptodate), (&(Model0_page)->Model0_flags)));
 /*
	 * Must ensure that the data we read out of the page is loaded
	 * _after_ we've loaded page->flags to check for PageUptodate.
	 * We can skip the barrier if the page is not uptodate, because
	 * we wouldn't be reading anything from it.
	 *
	 * See SetPageUptodate() for the other side of the story.
	 */
 if (Model0_ret)
  __asm__ __volatile__("": : :"memory");

 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageUptodate(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(Model0_PageTail(Model0_page)))));
 __asm__ __volatile__("": : :"memory");
 Model0___set_bit(Model0_PG_uptodate, &Model0_page->Model0_flags);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_SetPageUptodate(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(Model0_PageTail(Model0_page)))));
 /*
	 * Memory barrier must be issued before setting the PG_uptodate bit,
	 * so that all previous stores issued in order to bring the page
	 * uptodate are actually visible before PageUptodate becomes true.
	 */
 __asm__ __volatile__("": : :"memory");
 Model0_set_bit(Model0_PG_uptodate, &Model0_page->Model0_flags);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageUptodate(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_uptodate, &({ ((void)(sizeof(( long)(1 && Model0_PageTail(Model0_page))))); Model0_compound_head(Model0_page);})->Model0_flags); }

int Model0_test_clear_page_writeback(struct Model0_page *Model0_page);
int Model0___test_set_page_writeback(struct Model0_page *Model0_page, bool Model0_keep_write);






static inline __attribute__((no_instrument_function)) void Model0_set_page_writeback(struct Model0_page *Model0_page)
{
 Model0___test_set_page_writeback(Model0_page, false);
}

static inline __attribute__((no_instrument_function)) void Model0_set_page_writeback_keepwrite(struct Model0_page *Model0_page)
{
 Model0___test_set_page_writeback(Model0_page, true);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageHead(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_head)) ? Model0_constant_test_bit((Model0_PG_head), (&Model0_page->Model0_flags)) : Model0_variable_test_bit((Model0_PG_head), (&Model0_page->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageHead(struct Model0_page *Model0_page) { Model0___set_bit(Model0_PG_head, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageHead(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_head, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_ClearPageHead(struct Model0_page *Model0_page) { Model0_clear_bit(Model0_PG_head, &Model0_page->Model0_flags); }

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_set_compound_head(struct Model0_page *Model0_page, struct Model0_page *Model0_head)
{
 ({ union { typeof(Model0_page->Model0_compound_head) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_page->Model0_compound_head)) ((unsigned long)Model0_head + 1) }; Model0___write_once_size(&(Model0_page->Model0_compound_head), Model0___u.Model0___c, sizeof(Model0_page->Model0_compound_head)); Model0___u.Model0___val; });
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_clear_compound_head(struct Model0_page *Model0_page)
{
 ({ union { typeof(Model0_page->Model0_compound_head) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_page->Model0_compound_head)) (0) }; Model0___write_once_size(&(Model0_page->Model0_compound_head), Model0___u.Model0___c, sizeof(Model0_page->Model0_compound_head)); Model0___u.Model0___val; });
}
int Model0_PageHuge(struct Model0_page *Model0_page);
int Model0_PageHeadHuge(struct Model0_page *Model0_page);
bool Model0_page_huge_active(struct Model0_page *Model0_page);
static inline __attribute__((no_instrument_function)) int Model0_PageTransHuge(const struct Model0_page *Model0_page) { return 0; }
static inline __attribute__((no_instrument_function)) int Model0_PageTransCompound(const struct Model0_page *Model0_page) { return 0; }
static inline __attribute__((no_instrument_function)) int Model0_PageTransCompoundMap(const struct Model0_page *Model0_page) { return 0; }
static inline __attribute__((no_instrument_function)) int Model0_PageTransTail(const struct Model0_page *Model0_page) { return 0; }
static inline __attribute__((no_instrument_function)) int Model0_PageDoubleMap(const struct Model0_page *Model0_page) { return 0; } static inline __attribute__((no_instrument_function)) void Model0_SetPageDoubleMap(struct Model0_page *Model0_page) { } static inline __attribute__((no_instrument_function)) void Model0_ClearPageDoubleMap(struct Model0_page *Model0_page) { }
 static inline __attribute__((no_instrument_function)) int Model0_TestSetPageDoubleMap(struct Model0_page *Model0_page) { return 0; }
 static inline __attribute__((no_instrument_function)) int Model0_TestClearPageDoubleMap(struct Model0_page *Model0_page) { return 0; }


/*
 * For pages that are never mapped to userspace, page->mapcount may be
 * used for storing extra information about page type. Any value used
 * for this purpose must be <= -2, but it's better start not too close
 * to -2 so that an underflow of the page_mapcount() won't be mistaken
 * for a special page.
 */
/*
 * PageBuddy() indicate that the page is free and in the buddy system
 * (see mm/page_alloc.c).
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageBuddy(struct Model0_page *Model0_page) { return Model0_atomic_read(&Model0_page->Model0__mapcount) == (-128); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageBuddy(struct Model0_page *Model0_page) { ((void)(sizeof(( long)(Model0_atomic_read(&Model0_page->Model0__mapcount) != -1)))); Model0_atomic_set(&Model0_page->Model0__mapcount, (-128)); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageBuddy(struct Model0_page *Model0_page) { ((void)(sizeof(( long)(!Model0_PageBuddy(Model0_page))))); Model0_atomic_set(&Model0_page->Model0__mapcount, -1); }

/*
 * PageBalloon() is set on pages that are on the balloon page list
 * (see mm/balloon_compaction.c).
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageBalloon(struct Model0_page *Model0_page) { return Model0_atomic_read(&Model0_page->Model0__mapcount) == (-256); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageBalloon(struct Model0_page *Model0_page) { ((void)(sizeof(( long)(Model0_atomic_read(&Model0_page->Model0__mapcount) != -1)))); Model0_atomic_set(&Model0_page->Model0__mapcount, (-256)); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageBalloon(struct Model0_page *Model0_page) { ((void)(sizeof(( long)(!Model0_PageBalloon(Model0_page))))); Model0_atomic_set(&Model0_page->Model0__mapcount, -1); }

/*
 * If kmemcg is enabled, the buddy allocator will set PageKmemcg() on
 * pages allocated with __GFP_ACCOUNT. It gets cleared on page free.
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageKmemcg(struct Model0_page *Model0_page) { return Model0_atomic_read(&Model0_page->Model0__mapcount) == (-512); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageKmemcg(struct Model0_page *Model0_page) { ((void)(sizeof(( long)(Model0_atomic_read(&Model0_page->Model0__mapcount) != -1)))); Model0_atomic_set(&Model0_page->Model0__mapcount, (-512)); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageKmemcg(struct Model0_page *Model0_page) { ((void)(sizeof(( long)(!Model0_PageKmemcg(Model0_page))))); Model0_atomic_set(&Model0_page->Model0__mapcount, -1); }

extern bool Model0_is_free_buddy_page(struct Model0_page *Model0_page);

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_PageIsolated(struct Model0_page *Model0_page) { return (__builtin_constant_p((Model0_PG_isolated)) ? Model0_constant_test_bit((Model0_PG_isolated), (&Model0_page->Model0_flags)) : Model0_variable_test_bit((Model0_PG_isolated), (&Model0_page->Model0_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___SetPageIsolated(struct Model0_page *Model0_page) { Model0___set_bit(Model0_PG_isolated, &Model0_page->Model0_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___ClearPageIsolated(struct Model0_page *Model0_page) { Model0___clear_bit(Model0_PG_isolated, &Model0_page->Model0_flags); };

/*
 * If network-based swap is enabled, sl*b must keep track of whether pages
 * were allocated from pfmemalloc reserves.
 */
static inline __attribute__((no_instrument_function)) int Model0_PageSlabPfmemalloc(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(!Model0_PageSlab(Model0_page)))));
 return Model0_PageActive(Model0_page);
}

static inline __attribute__((no_instrument_function)) void Model0_SetPageSlabPfmemalloc(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(!Model0_PageSlab(Model0_page)))));
 Model0_SetPageActive(Model0_page);
}

static inline __attribute__((no_instrument_function)) void Model0___ClearPageSlabPfmemalloc(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(!Model0_PageSlab(Model0_page)))));
 Model0___ClearPageActive(Model0_page);
}

static inline __attribute__((no_instrument_function)) void Model0_ClearPageSlabPfmemalloc(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(!Model0_PageSlab(Model0_page)))));
 Model0_ClearPageActive(Model0_page);
}







/*
 * Flags checked when a page is freed.  Pages being freed should not have
 * these flags set.  It they are, there is a problem.
 */







/*
 * Flags checked when a page is prepped for return by the page allocator.
 * Pages being prepped should not have these flags set.  It they are set,
 * there has been a kernel bug or struct page corruption.
 *
 * __PG_HWPOISON is exceptional because it needs to be kept beyond page's
 * alloc-free cycle to prevent from reusing the page.
 */





/**
 * page_has_private - Determine if page has private stuff
 * @page: The page to be checked
 *
 * Determine if a page has private stuff, indicating that release routines
 * should be invoked upon it.
 */
static inline __attribute__((no_instrument_function)) int Model0_page_has_private(struct Model0_page *Model0_page)
{
 return !!(Model0_page->Model0_flags & (1UL << Model0_PG_private | 1UL << Model0_PG_private_2));
}


extern struct Model0_tracepoint Model0___tracepoint_page_ref_set;
extern struct Model0_tracepoint Model0___tracepoint_page_ref_mod;
extern struct Model0_tracepoint Model0___tracepoint_page_ref_mod_and_test;
extern struct Model0_tracepoint Model0___tracepoint_page_ref_mod_and_return;
extern struct Model0_tracepoint Model0___tracepoint_page_ref_mod_unless;
extern struct Model0_tracepoint Model0___tracepoint_page_ref_freeze;
extern struct Model0_tracepoint Model0___tracepoint_page_ref_unfreeze;
static inline __attribute__((no_instrument_function)) void Model0___page_ref_set(struct Model0_page *Model0_page, int Model0_v)
{
}
static inline __attribute__((no_instrument_function)) void Model0___page_ref_mod(struct Model0_page *Model0_page, int Model0_v)
{
}
static inline __attribute__((no_instrument_function)) void Model0___page_ref_mod_and_test(struct Model0_page *Model0_page, int Model0_v, int Model0_ret)
{
}
static inline __attribute__((no_instrument_function)) void Model0___page_ref_mod_and_return(struct Model0_page *Model0_page, int Model0_v, int Model0_ret)
{
}
static inline __attribute__((no_instrument_function)) void Model0___page_ref_mod_unless(struct Model0_page *Model0_page, int Model0_v, int Model0_u)
{
}
static inline __attribute__((no_instrument_function)) void Model0___page_ref_freeze(struct Model0_page *Model0_page, int Model0_v, int Model0_ret)
{
}
static inline __attribute__((no_instrument_function)) void Model0___page_ref_unfreeze(struct Model0_page *Model0_page, int Model0_v)
{
}



static inline __attribute__((no_instrument_function)) int Model0_page_ref_count(struct Model0_page *Model0_page)
{
 return Model0_atomic_read(&Model0_page->Model0__refcount);
}

static inline __attribute__((no_instrument_function)) int Model0_page_count(struct Model0_page *Model0_page)
{
 return Model0_atomic_read(&Model0_compound_head(Model0_page)->Model0__refcount);
}

static inline __attribute__((no_instrument_function)) void Model0_set_page_count(struct Model0_page *Model0_page, int Model0_v)
{
 Model0_atomic_set(&Model0_page->Model0__refcount, Model0_v);
 if (false)
  Model0___page_ref_set(Model0_page, Model0_v);
}

/*
 * Setup the page count before being freed into the page allocator for
 * the first time (boot or memory hotplug)
 */
static inline __attribute__((no_instrument_function)) void Model0_init_page_count(struct Model0_page *Model0_page)
{
 Model0_set_page_count(Model0_page, 1);
}

static inline __attribute__((no_instrument_function)) void Model0_page_ref_add(struct Model0_page *Model0_page, int Model0_nr)
{
 Model0_atomic_add(Model0_nr, &Model0_page->Model0__refcount);
 if (false)
  Model0___page_ref_mod(Model0_page, Model0_nr);
}

static inline __attribute__((no_instrument_function)) void Model0_page_ref_sub(struct Model0_page *Model0_page, int Model0_nr)
{
 Model0_atomic_sub(Model0_nr, &Model0_page->Model0__refcount);
 if (false)
  Model0___page_ref_mod(Model0_page, -Model0_nr);
}

static inline __attribute__((no_instrument_function)) void Model0_page_ref_inc(struct Model0_page *Model0_page)
{
 Model0_atomic_inc(&Model0_page->Model0__refcount);
 if (false)
  Model0___page_ref_mod(Model0_page, 1);
}

static inline __attribute__((no_instrument_function)) void Model0_page_ref_dec(struct Model0_page *Model0_page)
{
 Model0_atomic_dec(&Model0_page->Model0__refcount);
 if (false)
  Model0___page_ref_mod(Model0_page, -1);
}

static inline __attribute__((no_instrument_function)) int Model0_page_ref_sub_and_test(struct Model0_page *Model0_page, int Model0_nr)
{
 int Model0_ret = Model0_atomic_sub_and_test(Model0_nr, &Model0_page->Model0__refcount);

 if (false)
  Model0___page_ref_mod_and_test(Model0_page, -Model0_nr, Model0_ret);
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) int Model0_page_ref_inc_return(struct Model0_page *Model0_page)
{
 int Model0_ret = (Model0_atomic_add_return(1, &Model0_page->Model0__refcount));

 if (false)
  Model0___page_ref_mod_and_return(Model0_page, 1, Model0_ret);
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) int Model0_page_ref_dec_and_test(struct Model0_page *Model0_page)
{
 int Model0_ret = Model0_atomic_dec_and_test(&Model0_page->Model0__refcount);

 if (false)
  Model0___page_ref_mod_and_test(Model0_page, -1, Model0_ret);
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) int Model0_page_ref_dec_return(struct Model0_page *Model0_page)
{
 int Model0_ret = (Model0_atomic_sub_return(1, &Model0_page->Model0__refcount));

 if (false)
  Model0___page_ref_mod_and_return(Model0_page, -1, Model0_ret);
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) int Model0_page_ref_add_unless(struct Model0_page *Model0_page, int Model0_nr, int Model0_u)
{
 int Model0_ret = Model0_atomic_add_unless(&Model0_page->Model0__refcount, Model0_nr, Model0_u);

 if (false)
  Model0___page_ref_mod_unless(Model0_page, Model0_nr, Model0_ret);
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) int Model0_page_ref_freeze(struct Model0_page *Model0_page, int Model0_count)
{
 int Model0_ret = __builtin_expect(!!(Model0_atomic_cmpxchg(&Model0_page->Model0__refcount, Model0_count, 0) == Model0_count), 1);

 if (false)
  Model0___page_ref_freeze(Model0_page, Model0_count, Model0_ret);
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) void Model0_page_ref_unfreeze(struct Model0_page *Model0_page, int Model0_count)
{
 ((void)(sizeof(( long)(Model0_page_count(Model0_page) != 0))));
 ((void)(sizeof(( long)(Model0_count == 0))));

 Model0_atomic_set(&Model0_page->Model0__refcount, Model0_count);
 if (false)
  Model0___page_ref_unfreeze(Model0_page, Model0_count);
}

struct Model0_mempolicy;
struct Model0_anon_vma;
struct Model0_anon_vma_chain;
struct Model0_file_ra_state;
struct Model0_user_struct;
struct Model0_writeback_control;
struct Model0_bdi_writeback;
static inline __attribute__((no_instrument_function)) void Model0_set_max_mapnr(unsigned long Model0_limit) { }


extern unsigned long Model0_totalram_pages;
extern void * Model0_high_memory;
extern int Model0_page_cluster;


extern int Model0_sysctl_legacy_va_layout;





extern const int Model0_mmap_rnd_bits_min;
extern const int Model0_mmap_rnd_bits_max;
extern int Model0_mmap_rnd_bits __attribute__((__section__(".data..read_mostly")));


extern const int Model0_mmap_rnd_compat_bits_min;
extern const int Model0_mmap_rnd_compat_bits_max;
extern int Model0_mmap_rnd_compat_bits __attribute__((__section__(".data..read_mostly")));












/*
 * Macro to mark a page protection value as UC-
 */
void Model0_ptdump_walk_pgd_level(struct Model0_seq_file *Model0_m, Model0_pgd_t *Model0_pgd);
void Model0_ptdump_walk_pgd_level_checkwx(void);







/*
 * ZERO_PAGE is a global shared page that is always zero: used
 * for zero-mapped memory areas etc..
 */
extern unsigned long Model0_empty_zero_page[((1UL) << 12) / sizeof(unsigned long)]
          ;


extern Model0_spinlock_t Model0_pgd_lock;
extern struct Model0_list_head Model0_pgd_list;

extern struct Model0_mm_struct *Model0_pgd_page_get_mm(struct Model0_page *Model0_page);
/*
 * The following only work if pte_present() is true.
 * Undefined behaviour if not..
 */
static inline __attribute__((no_instrument_function)) int Model0_pte_dirty(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(1)) << 6);
}


static inline __attribute__((no_instrument_function)) Model0_u32 Model0_read_pkru(void)
{
 if ((__builtin_constant_p((16*32+ 4)) && ( ((((16*32+ 4))>>5)==(0) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || ((((16*32+ 4))>>5)==(1) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 1*32+29) & 31))|0) )) || ((((16*32+ 4))>>5)==(2) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(3) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 3*32+20) & 31))) )) || ((((16*32+ 4))>>5)==(4) && (1UL<<(((16*32+ 4))&31) & (0) )) || ((((16*32+ 4))>>5)==(5) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(6) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(7) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(8) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(9) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(10) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(11) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(12) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(13) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(14) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(15) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(16) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(17) && (1UL<<(((16*32+ 4))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p(((16*32+ 4))) ? Model0_constant_test_bit(((16*32+ 4)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))) : Model0_variable_test_bit(((16*32+ 4)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))))))
  return Model0___read_pkru();
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_write_pkru(Model0_u32 Model0_pkru)
{
 if ((__builtin_constant_p((16*32+ 4)) && ( ((((16*32+ 4))>>5)==(0) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || ((((16*32+ 4))>>5)==(1) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 1*32+29) & 31))|0) )) || ((((16*32+ 4))>>5)==(2) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(3) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 3*32+20) & 31))) )) || ((((16*32+ 4))>>5)==(4) && (1UL<<(((16*32+ 4))&31) & (0) )) || ((((16*32+ 4))>>5)==(5) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(6) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(7) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(8) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(9) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(10) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(11) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(12) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(13) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(14) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(15) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(16) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(17) && (1UL<<(((16*32+ 4))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p(((16*32+ 4))) ? Model0_constant_test_bit(((16*32+ 4)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))) : Model0_variable_test_bit(((16*32+ 4)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))))))
  Model0___write_pkru(Model0_pkru);
}

static inline __attribute__((no_instrument_function)) int Model0_pte_young(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(1)) << 5);
}

static inline __attribute__((no_instrument_function)) int Model0_pmd_dirty(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_flags(Model0_pmd) & (((Model0_pteval_t)(1)) << 6);
}

static inline __attribute__((no_instrument_function)) int Model0_pmd_young(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_flags(Model0_pmd) & (((Model0_pteval_t)(1)) << 5);
}

static inline __attribute__((no_instrument_function)) int Model0_pte_write(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(1)) << 1);
}

static inline __attribute__((no_instrument_function)) int Model0_pte_huge(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(1)) << 7);
}

static inline __attribute__((no_instrument_function)) int Model0_pte_global(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(1)) << 8);
}

static inline __attribute__((no_instrument_function)) int Model0_pte_exec(Model0_pte_t Model0_pte)
{
 return !(Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(1)) << 63));
}

static inline __attribute__((no_instrument_function)) int Model0_pte_special(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(1)) << 9);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_pte_pfn(Model0_pte_t Model0_pte)
{
 return (Model0_native_pte_val(Model0_pte) & ((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1))))) >> 12;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_pmd_pfn(Model0_pmd_t Model0_pmd)
{
 return (Model0_native_pmd_val(Model0_pmd) & Model0_pmd_pfn_mask(Model0_pmd)) >> 12;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_pud_pfn(Model0_pud_t Model0_pud)
{
 return (Model0_native_pud_val(Model0_pud) & Model0_pud_pfn_mask(Model0_pud)) >> 12;
}



static inline __attribute__((no_instrument_function)) int Model0_pmd_large(Model0_pmd_t Model0_pte)
{
 return Model0_pmd_flags(Model0_pte) & (((Model0_pteval_t)(1)) << 7);
}
static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_set_flags(Model0_pte_t Model0_pte, Model0_pteval_t Model0_set)
{
 Model0_pteval_t Model0_v = Model0_native_pte_val(Model0_pte);

 return Model0_native_make_pte(Model0_v | Model0_set);
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_clear_flags(Model0_pte_t Model0_pte, Model0_pteval_t Model0_clear)
{
 Model0_pteval_t Model0_v = Model0_native_pte_val(Model0_pte);

 return Model0_native_make_pte(Model0_v & ~Model0_clear);
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkclean(Model0_pte_t Model0_pte)
{
 return Model0_pte_clear_flags(Model0_pte, (((Model0_pteval_t)(1)) << 6));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkold(Model0_pte_t Model0_pte)
{
 return Model0_pte_clear_flags(Model0_pte, (((Model0_pteval_t)(1)) << 5));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_wrprotect(Model0_pte_t Model0_pte)
{
 return Model0_pte_clear_flags(Model0_pte, (((Model0_pteval_t)(1)) << 1));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkexec(Model0_pte_t Model0_pte)
{
 return Model0_pte_clear_flags(Model0_pte, (((Model0_pteval_t)(1)) << 63));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkdirty(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkyoung(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(1)) << 5));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkwrite(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(1)) << 1));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkhuge(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(1)) << 7));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_clrhuge(Model0_pte_t Model0_pte)
{
 return Model0_pte_clear_flags(Model0_pte, (((Model0_pteval_t)(1)) << 7));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkglobal(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(1)) << 8));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_clrglobal(Model0_pte_t Model0_pte)
{
 return Model0_pte_clear_flags(Model0_pte, (((Model0_pteval_t)(1)) << 8));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkspecial(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(1)) << 9));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mkdevmap(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(1)) << 9)|(((Model0_u64)(1)) << 58));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_set_flags(Model0_pmd_t Model0_pmd, Model0_pmdval_t Model0_set)
{
 Model0_pmdval_t Model0_v = Model0_native_pmd_val(Model0_pmd);

 return Model0_native_make_pmd(Model0_v | Model0_set);
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_clear_flags(Model0_pmd_t Model0_pmd, Model0_pmdval_t Model0_clear)
{
 Model0_pmdval_t Model0_v = Model0_native_pmd_val(Model0_pmd);

 return Model0_native_make_pmd(Model0_v & ~Model0_clear);
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mkold(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_clear_flags(Model0_pmd, (((Model0_pteval_t)(1)) << 5));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mkclean(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_clear_flags(Model0_pmd, (((Model0_pteval_t)(1)) << 6));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_wrprotect(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_clear_flags(Model0_pmd, (((Model0_pteval_t)(1)) << 1));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mkdirty(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_set_flags(Model0_pmd, (((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mkdevmap(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_set_flags(Model0_pmd, (((Model0_u64)(1)) << 58));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mkhuge(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_set_flags(Model0_pmd, (((Model0_pteval_t)(1)) << 7));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mkyoung(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_set_flags(Model0_pmd, (((Model0_pteval_t)(1)) << 5));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mkwrite(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_set_flags(Model0_pmd, (((Model0_pteval_t)(1)) << 1));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mknotpresent(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_clear_flags(Model0_pmd, (((Model0_pteval_t)(1)) << 0) | (((Model0_pteval_t)(1)) << 8));
}


static inline __attribute__((no_instrument_function)) int Model0_pte_soft_dirty(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(0)));
}

static inline __attribute__((no_instrument_function)) int Model0_pmd_soft_dirty(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_flags(Model0_pmd) & (((Model0_pteval_t)(0)));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_mksoft_dirty(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_mksoft_dirty(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_set_flags(Model0_pmd, (((Model0_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_clear_soft_dirty(Model0_pte_t Model0_pte)
{
 return Model0_pte_clear_flags(Model0_pte, (((Model0_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_clear_soft_dirty(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_clear_flags(Model0_pmd, (((Model0_pteval_t)(0))));
}



/*
 * Mask out unsupported bits in a present pgprot.  Non-present pgprots
 * can use those bits for other purposes, so leave them be.
 */
static inline __attribute__((no_instrument_function)) Model0_pgprotval_t Model0_massage_pgprot(Model0_pgprot_t Model0_pgprot)
{
 Model0_pgprotval_t Model0_protval = ((Model0_pgprot).Model0_pgprot);

 if (Model0_protval & (((Model0_pteval_t)(1)) << 0))
  Model0_protval &= Model0___supported_pte_mask;

 return Model0_protval;
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pfn_pte(unsigned long Model0_page_nr, Model0_pgprot_t Model0_pgprot)
{
 return Model0_native_make_pte(((Model0_phys_addr_t)Model0_page_nr << 12) | Model0_massage_pgprot(Model0_pgprot));

}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pfn_pmd(unsigned long Model0_page_nr, Model0_pgprot_t Model0_pgprot)
{
 return Model0_native_make_pmd(((Model0_phys_addr_t)Model0_page_nr << 12) | Model0_massage_pgprot(Model0_pgprot));

}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_modify(Model0_pte_t Model0_pte, Model0_pgprot_t Model0_newprot)
{
 Model0_pteval_t Model0_val = Model0_native_pte_val(Model0_pte);

 /*
	 * Chop off the NX bit (if present), and add the NX portion of
	 * the newprot (if present):
	 */
 Model0_val &= (((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)))) | (((Model0_pteval_t)(1)) << 4) | (((Model0_pteval_t)(1)) << 3) | (((Model0_pteval_t)(1)) << 9) | (((Model0_pteval_t)(1)) << 5) | (((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(0))));
 Model0_val |= Model0_massage_pgprot(Model0_newprot) & ~(((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)))) | (((Model0_pteval_t)(1)) << 4) | (((Model0_pteval_t)(1)) << 3) | (((Model0_pteval_t)(1)) << 9) | (((Model0_pteval_t)(1)) << 5) | (((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(0))));

 return Model0_native_make_pte(Model0_val);
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_modify(Model0_pmd_t Model0_pmd, Model0_pgprot_t Model0_newprot)
{
 Model0_pmdval_t Model0_val = Model0_native_pmd_val(Model0_pmd);

 Model0_val &= ((((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)))) | (((Model0_pteval_t)(1)) << 4) | (((Model0_pteval_t)(1)) << 3) | (((Model0_pteval_t)(1)) << 9) | (((Model0_pteval_t)(1)) << 5) | (((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(0)))) | (((Model0_pteval_t)(1)) << 7));
 Model0_val |= Model0_massage_pgprot(Model0_newprot) & ~((((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)))) | (((Model0_pteval_t)(1)) << 4) | (((Model0_pteval_t)(1)) << 3) | (((Model0_pteval_t)(1)) << 9) | (((Model0_pteval_t)(1)) << 5) | (((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(0)))) | (((Model0_pteval_t)(1)) << 7));

 return Model0_native_make_pmd(Model0_val);
}

/* mprotect needs to preserve PAT bits when updating vm_page_prot */

static inline __attribute__((no_instrument_function)) Model0_pgprot_t Model0_pgprot_modify(Model0_pgprot_t Model0_oldprot, Model0_pgprot_t Model0_newprot)
{
 Model0_pgprotval_t Model0_preservebits = ((Model0_oldprot).Model0_pgprot) & (((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)))) | (((Model0_pteval_t)(1)) << 4) | (((Model0_pteval_t)(1)) << 3) | (((Model0_pteval_t)(1)) << 9) | (((Model0_pteval_t)(1)) << 5) | (((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(0))));
 Model0_pgprotval_t Model0_addbits = ((Model0_newprot).Model0_pgprot);
 return ((Model0_pgprot_t) { (Model0_preservebits | Model0_addbits) } );
}







static inline __attribute__((no_instrument_function)) int Model0_is_new_memtype_allowed(Model0_u64 Model0_paddr, unsigned long Model0_size,
      enum Model0_page_cache_mode Model0_pcm,
      enum Model0_page_cache_mode Model0_new_pcm)
{
 /*
	 * PAT type is always WB for untracked ranges, so no need to check.
	 */
 if (Model0_x86_platform.Model0_is_untracked_pat_range(Model0_paddr, Model0_paddr + Model0_size))
  return 1;

 /*
	 * Certain new memtypes are not allowed with certain
	 * requested memtype:
	 * - request is uncached, return cannot be write-back
	 * - request is write-combine, return cannot be write-back
	 * - request is write-through, return cannot be write-back
	 * - request is write-through, return cannot be write-combine
	 */
 if ((Model0_pcm == Model0__PAGE_CACHE_MODE_UC_MINUS &&
      Model0_new_pcm == Model0__PAGE_CACHE_MODE_WB) ||
     (Model0_pcm == Model0__PAGE_CACHE_MODE_WC &&
      Model0_new_pcm == Model0__PAGE_CACHE_MODE_WB) ||
     (Model0_pcm == Model0__PAGE_CACHE_MODE_WT &&
      Model0_new_pcm == Model0__PAGE_CACHE_MODE_WB) ||
     (Model0_pcm == Model0__PAGE_CACHE_MODE_WT &&
      Model0_new_pcm == Model0__PAGE_CACHE_MODE_WC)) {
  return 0;
 }

 return 1;
}

Model0_pmd_t *Model0_populate_extra_pmd(unsigned long Model0_vaddr);
Model0_pte_t *Model0_populate_extra_pte(unsigned long Model0_vaddr);














/*
 * This file contains the functions and defines necessary to modify and use
 * the x86-64 page table tree.
 */




extern Model0_pud_t Model0_level3_kernel_pgt[512];
extern Model0_pud_t Model0_level3_ident_pgt[512];
extern Model0_pmd_t Model0_level2_kernel_pgt[512];
extern Model0_pmd_t Model0_level2_fixmap_pgt[512];
extern Model0_pmd_t Model0_level2_ident_pgt[512];
extern Model0_pte_t Model0_level1_fixmap_pgt[512];
extern Model0_pgd_t Model0_init_level4_pgt[];



extern void Model0_paging_init(void);
struct Model0_mm_struct;

void Model0_set_pte_vaddr_pud(Model0_pud_t *Model0_pud_page, unsigned long Model0_vaddr, Model0_pte_t Model0_new_pte);


static inline __attribute__((no_instrument_function)) void Model0_native_pte_clear(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr,
        Model0_pte_t *Model0_ptep)
{
 *Model0_ptep = Model0_native_make_pte(0);
}

static inline __attribute__((no_instrument_function)) void Model0_native_set_pte(Model0_pte_t *Model0_ptep, Model0_pte_t Model0_pte)
{
 *Model0_ptep = Model0_pte;
}

static inline __attribute__((no_instrument_function)) void Model0_native_set_pte_atomic(Model0_pte_t *Model0_ptep, Model0_pte_t Model0_pte)
{
 Model0_native_set_pte(Model0_ptep, Model0_pte);
}

static inline __attribute__((no_instrument_function)) void Model0_native_set_pmd(Model0_pmd_t *Model0_pmdp, Model0_pmd_t Model0_pmd)
{
 *Model0_pmdp = Model0_pmd;
}

static inline __attribute__((no_instrument_function)) void Model0_native_pmd_clear(Model0_pmd_t *Model0_pmd)
{
 Model0_native_set_pmd(Model0_pmd, Model0_native_make_pmd(0));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_native_ptep_get_and_clear(Model0_pte_t *Model0_xp)
{

 return Model0_native_make_pte(({ __typeof__ (*((&Model0_xp->Model0_pte))) Model0___ret = ((0)); switch (sizeof(*((&Model0_xp->Model0_pte)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*((&Model0_xp->Model0_pte))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_xp->Model0_pte))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_xp->Model0_pte))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_xp->Model0_pte))) : : "memory", "cc"); break; default: Model0___xchg_wrong_size(); } Model0___ret; }));







}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_native_pmdp_get_and_clear(Model0_pmd_t *Model0_xp)
{

 return Model0_native_make_pmd(({ __typeof__ (*((&Model0_xp->Model0_pmd))) Model0___ret = ((0)); switch (sizeof(*((&Model0_xp->Model0_pmd)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*((&Model0_xp->Model0_pmd))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_xp->Model0_pmd))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_xp->Model0_pmd))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_xp->Model0_pmd))) : : "memory", "cc"); break; default: Model0___xchg_wrong_size(); } Model0___ret; }));







}

static inline __attribute__((no_instrument_function)) void Model0_native_set_pud(Model0_pud_t *Model0_pudp, Model0_pud_t Model0_pud)
{
 *Model0_pudp = Model0_pud;
}

static inline __attribute__((no_instrument_function)) void Model0_native_pud_clear(Model0_pud_t *Model0_pud)
{
 Model0_native_set_pud(Model0_pud, Model0_native_make_pud(0));
}

static inline __attribute__((no_instrument_function)) void Model0_native_set_pgd(Model0_pgd_t *Model0_pgdp, Model0_pgd_t Model0_pgd)
{
 *Model0_pgdp = Model0_pgd;
}

static inline __attribute__((no_instrument_function)) void Model0_native_pgd_clear(Model0_pgd_t *Model0_pgd)
{
 Model0_native_set_pgd(Model0_pgd, Model0_native_make_pgd(0));
}

extern void Model0_sync_global_pgds(unsigned long Model0_start, unsigned long Model0_end,
        int Model0_removed);

/*
 * Conversion functions: convert a page and protection to a page entry,
 * and a page entry and page directory to the page they refer to.
 */

/*
 * Level 4 access.
 */
static inline __attribute__((no_instrument_function)) int Model0_pgd_large(Model0_pgd_t Model0_pgd) { return 0; }


/* PUD - Level3 access */

/* PMD  - Level 2 access */

/* PTE - Level 1 access. */

/* x86-64 always has all page tables mapped. */



/*
 * Encode and de-code a swap entry
 *
 * |     ...            | 11| 10|  9|8|7|6|5| 4| 3|2|1|0| <- bit number
 * |     ...            |SW3|SW2|SW1|G|L|D|A|CD|WT|U|W|P| <- bit names
 * | OFFSET (14->63) | TYPE (9-13)  |0|X|X|X| X| X|X|X|0| <- swp entry
 *
 * G (8) is aliased and used as a PROT_NONE indicator for
 * !present ptes.  We need to start storing swap entries above
 * there.  We also need to avoid using A and D because of an
 * erratum where they can be incorrectly set by hardware on
 * non-present PTEs.
 */


/* Place the offset above the type: */
extern int Model0_kern_addr_valid(unsigned long Model0_addr);
extern void Model0_cleanup_highmap(void);
/* fs/proc/kcore.c */







extern void Model0_init_extra_mapping_uc(unsigned long Model0_phys, unsigned long Model0_size);
extern void Model0_init_extra_mapping_wb(unsigned long Model0_phys, unsigned long Model0_size);







static inline __attribute__((no_instrument_function)) int Model0_pte_none(Model0_pte_t Model0_pte)
{
 return !(Model0_pte.Model0_pte & ~(((((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(1)) << 5))));
}


static inline __attribute__((no_instrument_function)) int Model0_pte_same(Model0_pte_t Model0_a, Model0_pte_t Model0_b)
{
 return Model0_a.Model0_pte == Model0_b.Model0_pte;
}

static inline __attribute__((no_instrument_function)) int Model0_pte_present(Model0_pte_t Model0_a)
{
 return Model0_pte_flags(Model0_a) & ((((Model0_pteval_t)(1)) << 0) | (((Model0_pteval_t)(1)) << 8));
}


static inline __attribute__((no_instrument_function)) int Model0_pte_devmap(Model0_pte_t Model0_a)
{
 return (Model0_pte_flags(Model0_a) & (((Model0_u64)(1)) << 58)) == (((Model0_u64)(1)) << 58);
}



static inline __attribute__((no_instrument_function)) bool Model0_pte_accessible(struct Model0_mm_struct *Model0_mm, Model0_pte_t Model0_a)
{
 if (Model0_pte_flags(Model0_a) & (((Model0_pteval_t)(1)) << 0))
  return true;

 if ((Model0_pte_flags(Model0_a) & (((Model0_pteval_t)(1)) << 8)) &&
   Model0_mm_tlb_flush_pending(Model0_mm))
  return true;

 return false;
}

static inline __attribute__((no_instrument_function)) int Model0_pte_hidden(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(0)));
}

static inline __attribute__((no_instrument_function)) int Model0_pmd_present(Model0_pmd_t Model0_pmd)
{
 /*
	 * Checking for _PAGE_PSE is needed too because
	 * split_huge_page will temporarily clear the present bit (but
	 * the _PAGE_PSE flag will remain set at all times while the
	 * _PAGE_PRESENT bit is clear).
	 */
 return Model0_pmd_flags(Model0_pmd) & ((((Model0_pteval_t)(1)) << 0) | (((Model0_pteval_t)(1)) << 8) | (((Model0_pteval_t)(1)) << 7));
}
static inline __attribute__((no_instrument_function)) int Model0_pmd_none(Model0_pmd_t Model0_pmd)
{
 /* Only check low word on 32-bit platforms, since it might be
	   out of sync with upper half. */
 unsigned long Model0_val = Model0_native_pmd_val(Model0_pmd);
 return (Model0_val & ~((((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(1)) << 5))) == 0;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_pmd_page_vaddr(Model0_pmd_t Model0_pmd)
{
 return (unsigned long)((void *)((unsigned long)(Model0_native_pmd_val(Model0_pmd) & Model0_pmd_pfn_mask(Model0_pmd))+((unsigned long)(0xffff880000000000UL))));
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h's __section_mem_map_addr() definition:
 */



/*
 * the pmd page can be thought of an array like this: pmd_t[PTRS_PER_PMD]
 *
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_pmd_index(unsigned long Model0_address)
{
 return (Model0_address >> 21) & (512 - 1);
}

/*
 * Conversion functions: convert a page and protection to a page entry,
 * and a page entry and page directory to the page they refer to.
 *
 * (Currently stuck as a macro because of indirect forward reference
 * to linux/mm.h:page_to_nid())
 */


/*
 * the pte page can be thought of an array like this: pte_t[PTRS_PER_PTE]
 *
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_pte_index(unsigned long Model0_address)
{
 return (Model0_address >> 12) & (512 - 1);
}

static inline __attribute__((no_instrument_function)) Model0_pte_t *Model0_pte_offset_kernel(Model0_pmd_t *Model0_pmd, unsigned long Model0_address)
{
 return (Model0_pte_t *)Model0_pmd_page_vaddr(*Model0_pmd) + Model0_pte_index(Model0_address);
}

static inline __attribute__((no_instrument_function)) int Model0_pmd_bad(Model0_pmd_t Model0_pmd)
{
 return (Model0_pmd_flags(Model0_pmd) & ~(((Model0_pteval_t)(1)) << 2)) != ((((Model0_pteval_t)(1)) << 0) | (((Model0_pteval_t)(1)) << 1) | (((Model0_pteval_t)(1)) << 5) | (((Model0_pteval_t)(1)) << 6));
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_pages_to_mb(unsigned long Model0_npg)
{
 return Model0_npg >> (20 - 12);
}


static inline __attribute__((no_instrument_function)) int Model0_pud_none(Model0_pud_t Model0_pud)
{
 return (Model0_native_pud_val(Model0_pud) & ~(((((Model0_pteval_t)(1)) << 6) | (((Model0_pteval_t)(1)) << 5)))) == 0;
}

static inline __attribute__((no_instrument_function)) int Model0_pud_present(Model0_pud_t Model0_pud)
{
 return Model0_pud_flags(Model0_pud) & (((Model0_pteval_t)(1)) << 0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_pud_page_vaddr(Model0_pud_t Model0_pud)
{
 return (unsigned long)((void *)((unsigned long)(Model0_native_pud_val(Model0_pud) & Model0_pud_pfn_mask(Model0_pud))+((unsigned long)(0xffff880000000000UL))));
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h's __section_mem_map_addr() definition:
 */



/* Find an entry in the second-level page table.. */
static inline __attribute__((no_instrument_function)) Model0_pmd_t *Model0_pmd_offset(Model0_pud_t *Model0_pud, unsigned long Model0_address)
{
 return (Model0_pmd_t *)Model0_pud_page_vaddr(*Model0_pud) + Model0_pmd_index(Model0_address);
}

static inline __attribute__((no_instrument_function)) int Model0_pud_large(Model0_pud_t Model0_pud)
{
 return (Model0_native_pud_val(Model0_pud) & ((((Model0_pteval_t)(1)) << 7) | (((Model0_pteval_t)(1)) << 0))) ==
  ((((Model0_pteval_t)(1)) << 7) | (((Model0_pteval_t)(1)) << 0));
}

static inline __attribute__((no_instrument_function)) int Model0_pud_bad(Model0_pud_t Model0_pud)
{
 return (Model0_pud_flags(Model0_pud) & ~(((((Model0_pteval_t)(1)) << 0) | (((Model0_pteval_t)(1)) << 1) | (((Model0_pteval_t)(1)) << 5) | (((Model0_pteval_t)(1)) << 6)) | (((Model0_pteval_t)(1)) << 2))) != 0;
}
static inline __attribute__((no_instrument_function)) int Model0_pgd_present(Model0_pgd_t Model0_pgd)
{
 return Model0_pgd_flags(Model0_pgd) & (((Model0_pteval_t)(1)) << 0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_pgd_page_vaddr(Model0_pgd_t Model0_pgd)
{
 return (unsigned long)((void *)((unsigned long)((unsigned long)Model0_native_pgd_val(Model0_pgd) & ((Model0_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model0_phys_addr_t)((1ULL << 46) - 1)))))+((unsigned long)(0xffff880000000000UL))));
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h's __section_mem_map_addr() definition:
 */


/* to find an entry in a page-table-directory. */
static inline __attribute__((no_instrument_function)) unsigned long Model0_pud_index(unsigned long Model0_address)
{
 return (Model0_address >> 30) & (512 - 1);
}

static inline __attribute__((no_instrument_function)) Model0_pud_t *Model0_pud_offset(Model0_pgd_t *Model0_pgd, unsigned long Model0_address)
{
 return (Model0_pud_t *)Model0_pgd_page_vaddr(*Model0_pgd) + Model0_pud_index(Model0_address);
}

static inline __attribute__((no_instrument_function)) int Model0_pgd_bad(Model0_pgd_t Model0_pgd)
{
 return (Model0_pgd_flags(Model0_pgd) & ~(((Model0_pteval_t)(1)) << 2)) != ((((Model0_pteval_t)(1)) << 0) | (((Model0_pteval_t)(1)) << 1) | (((Model0_pteval_t)(1)) << 5) | (((Model0_pteval_t)(1)) << 6));
}

static inline __attribute__((no_instrument_function)) int Model0_pgd_none(Model0_pgd_t Model0_pgd)
{
 /*
	 * There is no need to do a workaround for the KNL stray
	 * A/D bit erratum here.  PGDs only point to page tables
	 * except on 32-bit non-PAE which is not supported on
	 * KNL.
	 */
 return !Model0_native_pgd_val(Model0_pgd);
}




/*
 * the pgd page can be thought of an array like this: pgd_t[PTRS_PER_PGD]
 *
 * this macro returns the index of the entry in the pgd page which would
 * control the given virtual address
 */


/*
 * pgd_offset() returns a (pgd_t *)
 * pgd_index() is used get the offset into the pgd page's array of pgd_t's;
 */

/*
 * a shortcut which implies the use of the kernel's pgd, instead
 * of a process's
 */
extern int Model0_direct_gbpages;
void Model0_init_mem_mapping(void);
void Model0_early_alloc_pgt_buf(void);


/* Realmode trampoline initialization. */
extern Model0_pgd_t Model0_trampoline_pgd_entry;
static inline __attribute__((no_instrument_function)) void __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model0_init_trampoline_default(void)
{
 /* Default trampoline pgd value */
 Model0_trampoline_pgd_entry = Model0_init_level4_pgt[((((0xffff880000000000UL)) >> 39) & (512 - 1))];
}
/* local pte updates need not use xchg for locking */
static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_native_local_ptep_get_and_clear(Model0_pte_t *Model0_ptep)
{
 Model0_pte_t Model0_res = *Model0_ptep;

 /* Pure native function needs no input for mm, addr */
 Model0_native_pte_clear(((void *)0), 0, Model0_ptep);
 return Model0_res;
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_native_local_pmdp_get_and_clear(Model0_pmd_t *Model0_pmdp)
{
 Model0_pmd_t Model0_res = *Model0_pmdp;

 Model0_native_pmd_clear(Model0_pmdp);
 return Model0_res;
}

static inline __attribute__((no_instrument_function)) void Model0_native_set_pte_at(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr,
         Model0_pte_t *Model0_ptep , Model0_pte_t Model0_pte)
{
 Model0_native_set_pte(Model0_ptep, Model0_pte);
}

static inline __attribute__((no_instrument_function)) void Model0_native_set_pmd_at(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr,
         Model0_pmd_t *Model0_pmdp , Model0_pmd_t Model0_pmd)
{
 Model0_native_set_pmd(Model0_pmdp, Model0_pmd);
}


/*
 * Rules for using pte_update - it must be called after any PTE update which
 * has not been done using the set_pte / clear_pte interfaces.  It is used by
 * shadow mode hypervisors to resynchronize the shadow page tables.  Kernel PTE
 * updates should either be sets, clears, or set_pte_atomic for P->P
 * transitions, which means this hook should only be called for user PTEs.
 * This hook implies a P->P protection or access change has taken place, which
 * requires a subsequent TLB flush.
 */



/*
 * We only update the dirty/accessed state if we set
 * the dirty bit by hand in the kernel, since the hardware
 * will do the accessed bit for us, and we don't want to
 * race with other CPU's that might be updating the dirty
 * bit at the same time.
 */
struct Model0_vm_area_struct;


extern int Model0_ptep_set_access_flags(struct Model0_vm_area_struct *Model0_vma,
     unsigned long Model0_address, Model0_pte_t *Model0_ptep,
     Model0_pte_t Model0_entry, int Model0_dirty);


extern int Model0_ptep_test_and_clear_young(struct Model0_vm_area_struct *Model0_vma,
         unsigned long Model0_addr, Model0_pte_t *Model0_ptep);


extern int Model0_ptep_clear_flush_young(struct Model0_vm_area_struct *Model0_vma,
      unsigned long Model0_address, Model0_pte_t *Model0_ptep);


static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_ptep_get_and_clear(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr,
           Model0_pte_t *Model0_ptep)
{
 Model0_pte_t Model0_pte = Model0_native_ptep_get_and_clear(Model0_ptep);
 do { } while (0);
 return Model0_pte;
}


static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_ptep_get_and_clear_full(struct Model0_mm_struct *Model0_mm,
         unsigned long Model0_addr, Model0_pte_t *Model0_ptep,
         int Model0_full)
{
 Model0_pte_t Model0_pte;
 if (Model0_full) {
  /*
		 * Full address destruction in progress; paravirt does not
		 * care about updates and native needs no locking
		 */
  Model0_pte = Model0_native_local_ptep_get_and_clear(Model0_ptep);
 } else {
  Model0_pte = Model0_ptep_get_and_clear(Model0_mm, Model0_addr, Model0_ptep);
 }
 return Model0_pte;
}


static inline __attribute__((no_instrument_function)) void Model0_ptep_set_wrprotect(struct Model0_mm_struct *Model0_mm,
          unsigned long Model0_addr, Model0_pte_t *Model0_ptep)
{
 Model0_clear_bit(1, (unsigned long *)&Model0_ptep->Model0_pte);
 do { } while (0);
}






extern int Model0_pmdp_set_access_flags(struct Model0_vm_area_struct *Model0_vma,
     unsigned long Model0_address, Model0_pmd_t *Model0_pmdp,
     Model0_pmd_t Model0_entry, int Model0_dirty);


extern int Model0_pmdp_test_and_clear_young(struct Model0_vm_area_struct *Model0_vma,
         unsigned long Model0_addr, Model0_pmd_t *Model0_pmdp);


extern int Model0_pmdp_clear_flush_young(struct Model0_vm_area_struct *Model0_vma,
      unsigned long Model0_address, Model0_pmd_t *Model0_pmdp);



static inline __attribute__((no_instrument_function)) int Model0_pmd_write(Model0_pmd_t Model0_pmd)
{
 return Model0_pmd_flags(Model0_pmd) & (((Model0_pteval_t)(1)) << 1);
}


static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmdp_huge_get_and_clear(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr,
           Model0_pmd_t *Model0_pmdp)
{
 return Model0_native_pmdp_get_and_clear(Model0_pmdp);
}


static inline __attribute__((no_instrument_function)) void Model0_pmdp_set_wrprotect(struct Model0_mm_struct *Model0_mm,
          unsigned long Model0_addr, Model0_pmd_t *Model0_pmdp)
{
 Model0_clear_bit(1, (unsigned long *)Model0_pmdp);
}

/*
 * clone_pgd_range(pgd_t *dst, pgd_t *src, int count);
 *
 *  dst - pointer to pgd range anwhere on a pgd page
 *  src - ""
 *  count - the number of pgds to copy.
 *
 * dst and src can be on the same page, but the range must not overlap,
 * and must not cross a page boundary.
 */
static inline __attribute__((no_instrument_function)) void Model0_clone_pgd_range(Model0_pgd_t *Model0_dst, Model0_pgd_t *Model0_src, int Model0_count)
{
       ({ Model0_size_t Model0___len = (Model0_count * sizeof(Model0_pgd_t)); void *Model0___ret; if (__builtin_constant_p(Model0_count * sizeof(Model0_pgd_t)) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_dst), (Model0_src), Model0___len); else Model0___ret = __builtin_memcpy((Model0_dst), (Model0_src), Model0___len); Model0___ret; });
}


static inline __attribute__((no_instrument_function)) int Model0_page_level_shift(enum Model0_pg_level Model0_level)
{
 return (12 - ( __builtin_constant_p(512) ? ( (512) < 1 ? Model0_____ilog2_NaN() : (512) & (1ULL << 63) ? 63 : (512) & (1ULL << 62) ? 62 : (512) & (1ULL << 61) ? 61 : (512) & (1ULL << 60) ? 60 : (512) & (1ULL << 59) ? 59 : (512) & (1ULL << 58) ? 58 : (512) & (1ULL << 57) ? 57 : (512) & (1ULL << 56) ? 56 : (512) & (1ULL << 55) ? 55 : (512) & (1ULL << 54) ? 54 : (512) & (1ULL << 53) ? 53 : (512) & (1ULL << 52) ? 52 : (512) & (1ULL << 51) ? 51 : (512) & (1ULL << 50) ? 50 : (512) & (1ULL << 49) ? 49 : (512) & (1ULL << 48) ? 48 : (512) & (1ULL << 47) ? 47 : (512) & (1ULL << 46) ? 46 : (512) & (1ULL << 45) ? 45 : (512) & (1ULL << 44) ? 44 : (512) & (1ULL << 43) ? 43 : (512) & (1ULL << 42) ? 42 : (512) & (1ULL << 41) ? 41 : (512) & (1ULL << 40) ? 40 : (512) & (1ULL << 39) ? 39 : (512) & (1ULL << 38) ? 38 : (512) & (1ULL << 37) ? 37 : (512) & (1ULL << 36) ? 36 : (512) & (1ULL << 35) ? 35 : (512) & (1ULL << 34) ? 34 : (512) & (1ULL << 33) ? 33 : (512) & (1ULL << 32) ? 32 : (512) & (1ULL << 31) ? 31 : (512) & (1ULL << 30) ? 30 : (512) & (1ULL << 29) ? 29 : (512) & (1ULL << 28) ? 28 : (512) & (1ULL << 27) ? 27 : (512) & (1ULL << 26) ? 26 : (512) & (1ULL << 25) ? 25 : (512) & (1ULL << 24) ? 24 : (512) & (1ULL << 23) ? 23 : (512) & (1ULL << 22) ? 22 : (512) & (1ULL << 21) ? 21 : (512) & (1ULL << 20) ? 20 : (512) & (1ULL << 19) ? 19 : (512) & (1ULL << 18) ? 18 : (512) & (1ULL << 17) ? 17 : (512) & (1ULL << 16) ? 16 : (512) & (1ULL << 15) ? 15 : (512) & (1ULL << 14) ? 14 : (512) & (1ULL << 13) ? 13 : (512) & (1ULL << 12) ? 12 : (512) & (1ULL << 11) ? 11 : (512) & (1ULL << 10) ? 10 : (512) & (1ULL << 9) ? 9 : (512) & (1ULL << 8) ? 8 : (512) & (1ULL << 7) ? 7 : (512) & (1ULL << 6) ? 6 : (512) & (1ULL << 5) ? 5 : (512) & (1ULL << 4) ? 4 : (512) & (1ULL << 3) ? 3 : (512) & (1ULL << 2) ? 2 : (512) & (1ULL << 1) ? 1 : (512) & (1ULL << 0) ? 0 : Model0_____ilog2_NaN() ) : (sizeof(512) <= 4) ? Model0___ilog2_u32(512) : Model0___ilog2_u64(512) )) + Model0_level * ( __builtin_constant_p(512) ? ( (512) < 1 ? Model0_____ilog2_NaN() : (512) & (1ULL << 63) ? 63 : (512) & (1ULL << 62) ? 62 : (512) & (1ULL << 61) ? 61 : (512) & (1ULL << 60) ? 60 : (512) & (1ULL << 59) ? 59 : (512) & (1ULL << 58) ? 58 : (512) & (1ULL << 57) ? 57 : (512) & (1ULL << 56) ? 56 : (512) & (1ULL << 55) ? 55 : (512) & (1ULL << 54) ? 54 : (512) & (1ULL << 53) ? 53 : (512) & (1ULL << 52) ? 52 : (512) & (1ULL << 51) ? 51 : (512) & (1ULL << 50) ? 50 : (512) & (1ULL << 49) ? 49 : (512) & (1ULL << 48) ? 48 : (512) & (1ULL << 47) ? 47 : (512) & (1ULL << 46) ? 46 : (512) & (1ULL << 45) ? 45 : (512) & (1ULL << 44) ? 44 : (512) & (1ULL << 43) ? 43 : (512) & (1ULL << 42) ? 42 : (512) & (1ULL << 41) ? 41 : (512) & (1ULL << 40) ? 40 : (512) & (1ULL << 39) ? 39 : (512) & (1ULL << 38) ? 38 : (512) & (1ULL << 37) ? 37 : (512) & (1ULL << 36) ? 36 : (512) & (1ULL << 35) ? 35 : (512) & (1ULL << 34) ? 34 : (512) & (1ULL << 33) ? 33 : (512) & (1ULL << 32) ? 32 : (512) & (1ULL << 31) ? 31 : (512) & (1ULL << 30) ? 30 : (512) & (1ULL << 29) ? 29 : (512) & (1ULL << 28) ? 28 : (512) & (1ULL << 27) ? 27 : (512) & (1ULL << 26) ? 26 : (512) & (1ULL << 25) ? 25 : (512) & (1ULL << 24) ? 24 : (512) & (1ULL << 23) ? 23 : (512) & (1ULL << 22) ? 22 : (512) & (1ULL << 21) ? 21 : (512) & (1ULL << 20) ? 20 : (512) & (1ULL << 19) ? 19 : (512) & (1ULL << 18) ? 18 : (512) & (1ULL << 17) ? 17 : (512) & (1ULL << 16) ? 16 : (512) & (1ULL << 15) ? 15 : (512) & (1ULL << 14) ? 14 : (512) & (1ULL << 13) ? 13 : (512) & (1ULL << 12) ? 12 : (512) & (1ULL << 11) ? 11 : (512) & (1ULL << 10) ? 10 : (512) & (1ULL << 9) ? 9 : (512) & (1ULL << 8) ? 8 : (512) & (1ULL << 7) ? 7 : (512) & (1ULL << 6) ? 6 : (512) & (1ULL << 5) ? 5 : (512) & (1ULL << 4) ? 4 : (512) & (1ULL << 3) ? 3 : (512) & (1ULL << 2) ? 2 : (512) & (1ULL << 1) ? 1 : (512) & (1ULL << 0) ? 0 : Model0_____ilog2_NaN() ) : (sizeof(512) <= 4) ? Model0___ilog2_u32(512) : Model0___ilog2_u64(512) );
}
static inline __attribute__((no_instrument_function)) unsigned long Model0_page_level_size(enum Model0_pg_level Model0_level)
{
 return 1UL << Model0_page_level_shift(Model0_level);
}
static inline __attribute__((no_instrument_function)) unsigned long Model0_page_level_mask(enum Model0_pg_level Model0_level)
{
 return ~(Model0_page_level_size(Model0_level) - 1);
}

/*
 * The x86 doesn't have any external MMU info: the kernel page
 * tables contain all the necessary information.
 */
static inline __attribute__((no_instrument_function)) void Model0_update_mmu_cache(struct Model0_vm_area_struct *Model0_vma,
  unsigned long Model0_addr, Model0_pte_t *Model0_ptep)
{
}
static inline __attribute__((no_instrument_function)) void Model0_update_mmu_cache_pmd(struct Model0_vm_area_struct *Model0_vma,
  unsigned long Model0_addr, Model0_pmd_t *Model0_pmd)
{
}


static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_swp_mksoft_dirty(Model0_pte_t Model0_pte)
{
 return Model0_pte_set_flags(Model0_pte, (((Model0_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) int Model0_pte_swp_soft_dirty(Model0_pte_t Model0_pte)
{
 return Model0_pte_flags(Model0_pte) & (((Model0_pteval_t)(0)));
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_pte_swp_clear_soft_dirty(Model0_pte_t Model0_pte)
{
 return Model0_pte_clear_flags(Model0_pte, (((Model0_pteval_t)(0))));
}






static inline __attribute__((no_instrument_function)) bool Model0___pkru_allows_read(Model0_u32 Model0_pkru, Model0_u16 Model0_pkey)
{
 int Model0_pkru_pkey_bits = Model0_pkey * 2;
 return !(Model0_pkru & (0x1 << Model0_pkru_pkey_bits));
}

static inline __attribute__((no_instrument_function)) bool Model0___pkru_allows_write(Model0_u32 Model0_pkru, Model0_u16 Model0_pkey)
{
 int Model0_pkru_pkey_bits = Model0_pkey * 2;
 /*
	 * Access-disable disables writes too so we need to check
	 * both bits here.
	 */
 return !(Model0_pkru & ((0x1|0x2) << Model0_pkru_pkey_bits));
}

static inline __attribute__((no_instrument_function)) Model0_u16 Model0_pte_flags_pkey(unsigned long Model0_pte_flags)
{

 /* ifdef to avoid doing 59-bit shift on 32-bit values */
 return (Model0_pte_flags & ((((Model0_pteval_t)(1)) << 59) | (((Model0_pteval_t)(1)) << 60) | (((Model0_pteval_t)(1)) << 61) | (((Model0_pteval_t)(1)) << 62))) >> 59;



}


/*
 * On almost all architectures and configurations, 0 can be used as the
 * upper ceiling to free_pgtables(): on many architectures it has the same
 * effect as using TASK_SIZE.  However, there is one configuration which
 * must impose a more careful limit, to avoid freeing kernel pgtables.
 */
/*
 * Some architectures may be able to avoid expensive synchronization
 * primitives when modifications are made to PTE's which are already
 * not present, or in the process of an address space destruction.
 */

static inline __attribute__((no_instrument_function)) void Model0_pte_clear_not_present_full(struct Model0_mm_struct *Model0_mm,
           unsigned long Model0_address,
           Model0_pte_t *Model0_ptep,
           int Model0_full)
{
 Model0_native_pte_clear(Model0_mm, Model0_address, Model0_ptep);
}



extern Model0_pte_t Model0_ptep_clear_flush(struct Model0_vm_area_struct *Model0_vma,
         unsigned long Model0_address,
         Model0_pte_t *Model0_ptep);



extern Model0_pmd_t Model0_pmdp_huge_clear_flush(struct Model0_vm_area_struct *Model0_vma,
         unsigned long Model0_address,
         Model0_pmd_t *Model0_pmdp);
static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmdp_collapse_flush(struct Model0_vm_area_struct *Model0_vma,
     unsigned long Model0_address,
     Model0_pmd_t *Model0_pmdp)
{
 do { bool Model0___cond = !(!(1)); extern void Model0___compiletime_assert_221(void) ; if (Model0___cond) Model0___compiletime_assert_221(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 return *Model0_pmdp;
}





extern void Model0_pgtable_trans_huge_deposit(struct Model0_mm_struct *Model0_mm, Model0_pmd_t *Model0_pmdp,
           Model0_pgtable_t Model0_pgtable);



extern Model0_pgtable_t Model0_pgtable_trans_huge_withdraw(struct Model0_mm_struct *Model0_mm, Model0_pmd_t *Model0_pmdp);



extern void Model0_pmdp_invalidate(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_address,
       Model0_pmd_t *Model0_pmdp);



static inline __attribute__((no_instrument_function)) void Model0_pmdp_huge_split_prepare(struct Model0_vm_area_struct *Model0_vma,
        unsigned long Model0_address, Model0_pmd_t *Model0_pmdp)
{

}
/*
 * Some architectures provide facilities to virtualization guests
 * so that they can flag allocated pages as unused. This allows the
 * host to transparently reclaim unused pages. This function returns
 * whether the pte's page is unused.
 */
static inline __attribute__((no_instrument_function)) int Model0_pte_unused(Model0_pte_t Model0_pte)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model0_pmd_same(Model0_pmd_t Model0_pmd_a, Model0_pmd_t Model0_pmd_b)
{
 do { bool Model0___cond = !(!(1)); extern void Model0___compiletime_assert_279(void) ; if (Model0___cond) Model0___compiletime_assert_279(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 return 0;
}
/*
 * When walking page tables, get the address of the next boundary,
 * or the end address of the range if that comes earlier.  Although no
 * vma end wraps to 0, rounded up __boundary may wrap to 0 throughout.
 */
/*
 * When walking page tables, we usually want to skip any p?d_none entries;
 * and any p?d_bad entries - reporting the error before resetting to none.
 * Do the tests inline, but report and clear the bad entry in mm/memory.c.
 */
void Model0_pgd_clear_bad(Model0_pgd_t *);
void Model0_pud_clear_bad(Model0_pud_t *);
void Model0_pmd_clear_bad(Model0_pmd_t *);

static inline __attribute__((no_instrument_function)) int Model0_pgd_none_or_clear_bad(Model0_pgd_t *Model0_pgd)
{
 if (Model0_pgd_none(*Model0_pgd))
  return 1;
 if (__builtin_expect(!!(Model0_pgd_bad(*Model0_pgd)), 0)) {
  Model0_pgd_clear_bad(Model0_pgd);
  return 1;
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_pud_none_or_clear_bad(Model0_pud_t *Model0_pud)
{
 if (Model0_pud_none(*Model0_pud))
  return 1;
 if (__builtin_expect(!!(Model0_pud_bad(*Model0_pud)), 0)) {
  Model0_pud_clear_bad(Model0_pud);
  return 1;
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_pmd_none_or_clear_bad(Model0_pmd_t *Model0_pmd)
{
 if (Model0_pmd_none(*Model0_pmd))
  return 1;
 if (__builtin_expect(!!(Model0_pmd_bad(*Model0_pmd)), 0)) {
  Model0_pmd_clear_bad(Model0_pmd);
  return 1;
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) Model0_pte_t Model0___ptep_modify_prot_start(struct Model0_mm_struct *Model0_mm,
          unsigned long Model0_addr,
          Model0_pte_t *Model0_ptep)
{
 /*
	 * Get the current pte state, but zero it out to make it
	 * non-present, preventing the hardware from asynchronously
	 * updating it.
	 */
 return Model0_ptep_get_and_clear(Model0_mm, Model0_addr, Model0_ptep);
}

static inline __attribute__((no_instrument_function)) void Model0___ptep_modify_prot_commit(struct Model0_mm_struct *Model0_mm,
          unsigned long Model0_addr,
          Model0_pte_t *Model0_ptep, Model0_pte_t Model0_pte)
{
 /*
	 * The pte is non-present, so there's no hardware state to
	 * preserve.
	 */
 Model0_native_set_pte_at(Model0_mm, Model0_addr, Model0_ptep, Model0_pte);
}


/*
 * Start a pte protection read-modify-write transaction, which
 * protects against asynchronous hardware modifications to the pte.
 * The intention is not to prevent the hardware from making pte
 * updates, but to prevent any updates it may make from being lost.
 *
 * This does not protect against other software modifications of the
 * pte; the appropriate pte lock must be held over the transation.
 *
 * Note that this interface is intended to be batchable, meaning that
 * ptep_modify_prot_commit may not actually update the pte, but merely
 * queue the update to be done at some later time.  The update must be
 * actually committed before the pte lock is released, however.
 */
static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_ptep_modify_prot_start(struct Model0_mm_struct *Model0_mm,
        unsigned long Model0_addr,
        Model0_pte_t *Model0_ptep)
{
 return Model0___ptep_modify_prot_start(Model0_mm, Model0_addr, Model0_ptep);
}

/*
 * Commit an update to a pte, leaving any hardware-controlled bits in
 * the PTE unmodified.
 */
static inline __attribute__((no_instrument_function)) void Model0_ptep_modify_prot_commit(struct Model0_mm_struct *Model0_mm,
        unsigned long Model0_addr,
        Model0_pte_t *Model0_ptep, Model0_pte_t Model0_pte)
{
 Model0___ptep_modify_prot_commit(Model0_mm, Model0_addr, Model0_ptep, Model0_pte);
}



/*
 * A facility to provide lazy MMU batching.  This allows PTE updates and
 * page invalidations to be delayed until a call to leave lazy MMU mode
 * is issued.  Some architectures may benefit from doing this, and it is
 * beneficial for both shadow and direct mode hypervisors, which may batch
 * the PTE updates which happen during this window.  Note that using this
 * interface requires that read hazards be removed from the code.  A read
 * hazard could result in the direct mode hypervisor case, since the actual
 * write to the page tables may not yet have taken place, so reads though
 * a raw PTE pointer after it has been modified are not guaranteed to be
 * up to date.  This mode can only be entered and left under the protection of
 * the page table locks for all page tables which may be modified.  In the UP
 * case, this is required so that preemption is disabled, and in the SMP case,
 * it must synchronize the delayed page table writes properly on other CPUs.
 */






/*
 * A facility to provide batching of the reload of page tables and
 * other process state with the actual context switch code for
 * paravirtualized guests.  By convention, only one of the batched
 * update (lazy) modes (CPU, MMU) should be active at any given time,
 * entry should never be nested, and entry and exits should always be
 * paired.  This is for sanity of maintaining and reasoning about the
 * kernel code.  In this case, the exit (end of the context switch) is
 * in architecture-specific code, and so doesn't need a generic
 * definition.
 */
extern int Model0_track_pfn_remap(struct Model0_vm_area_struct *Model0_vma, Model0_pgprot_t *Model0_prot,
      unsigned long Model0_pfn, unsigned long Model0_addr,
      unsigned long Model0_size);
extern int Model0_track_pfn_insert(struct Model0_vm_area_struct *Model0_vma, Model0_pgprot_t *Model0_prot,
       Model0_pfn_t Model0_pfn);
extern int Model0_track_pfn_copy(struct Model0_vm_area_struct *Model0_vma);
extern void Model0_untrack_pfn(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_pfn,
   unsigned long Model0_size);
extern void Model0_untrack_pfn_moved(struct Model0_vm_area_struct *Model0_vma);
static inline __attribute__((no_instrument_function)) int Model0_is_zero_pfn(unsigned long Model0_pfn)
{
 extern unsigned long Model0_zero_pfn;
 return Model0_pfn == Model0_zero_pfn;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_my_zero_pfn(unsigned long Model0_addr)
{
 extern unsigned long Model0_zero_pfn;
 return Model0_zero_pfn;
}





static inline __attribute__((no_instrument_function)) int Model0_pmd_trans_huge(Model0_pmd_t Model0_pmd)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) Model0_pmd_t Model0_pmd_read_atomic(Model0_pmd_t *Model0_pmdp)
{
 /*
	 * Depend on compiler for an atomic pmd read. NOTE: this is
	 * only going to work, if the pmdval_t isn't larger than
	 * an unsigned long.
	 */
 return *Model0_pmdp;
}



static inline __attribute__((no_instrument_function)) int Model0_pmd_move_must_withdraw(Model0_spinlock_t *Model0_new_pmd_ptl,
      Model0_spinlock_t *Model0_old_pmd_ptl)
{
 /*
	 * With split pmd lock we also need to move preallocated
	 * PTE page table if new_pmd is on different PMD page table.
	 */
 return Model0_new_pmd_ptl != Model0_old_pmd_ptl;
}


/*
 * This function is meant to be used by sites walking pagetables with
 * the mmap_sem hold in read mode to protect against MADV_DONTNEED and
 * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd
 * into a null pmd and the transhuge page fault can convert a null pmd
 * into an hugepmd or into a regular pmd (if the hugepage allocation
 * fails). While holding the mmap_sem in read mode the pmd becomes
 * stable and stops changing under us only if it's not null and not a
 * transhuge pmd. When those races occurs and this function makes a
 * difference vs the standard pmd_none_or_clear_bad, the result is
 * undefined so behaving like if the pmd was none is safe (because it
 * can return none anyway). The compiler level barrier() is critically
 * important to compute the two checks atomically on the same pmdval.
 *
 * For 32bit kernels with a 64bit large pmd_t this automatically takes
 * care of reading the pmd atomically to avoid SMP race conditions
 * against pmd_populate() when the mmap_sem is hold for reading by the
 * caller (a special atomic read not done by "gcc" as in the generic
 * version above, is also needed when THP is disabled because the page
 * fault can populate the pmd from under us).
 */
static inline __attribute__((no_instrument_function)) int Model0_pmd_none_or_trans_huge_or_clear_bad(Model0_pmd_t *Model0_pmd)
{
 Model0_pmd_t Model0_pmdval = Model0_pmd_read_atomic(Model0_pmd);
 /*
	 * The barrier will stabilize the pmdval in a register or on
	 * the stack so that it will stop changing under the code.
	 *
	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,
	 * pmd_read_atomic is allowed to return a not atomic pmdval
	 * (for example pointing to an hugepage that has never been
	 * mapped in the pmd). The below checks will only care about
	 * the low part of the pmd with 32bit PAE x86 anyway, with the
	 * exception of pmd_none(). So the important thing is that if
	 * the low part of the pmd is found null, the high part will
	 * be also null or the pmd_none() check below would be
	 * confused.
	 */



 if (Model0_pmd_none(Model0_pmdval) || Model0_pmd_trans_huge(Model0_pmdval))
  return 1;
 if (__builtin_expect(!!(Model0_pmd_bad(Model0_pmdval)), 0)) {
  Model0_pmd_clear_bad(Model0_pmd);
  return 1;
 }
 return 0;
}

/*
 * This is a noop if Transparent Hugepage Support is not built into
 * the kernel. Otherwise it is equivalent to
 * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in
 * places that already verified the pmd is not none and they want to
 * walk ptes while holding the mmap sem in read mode (write mode don't
 * need this). If THP is not enabled, the pmd can't go away under the
 * code even if MADV_DONTNEED runs, but if THP is enabled we need to
 * run a pmd_trans_unstable before walking the ptes after
 * split_huge_page_pmd returns (because it may have run when the pmd
 * become null, but then a page fault can map in a THP and not a
 * regular page).
 */
static inline __attribute__((no_instrument_function)) int Model0_pmd_trans_unstable(Model0_pmd_t *Model0_pmd)
{



 return 0;

}


/*
 * Technically a PTE can be PROTNONE even when not doing NUMA balancing but
 * the only case the kernel cares is for NUMA balancing and is only ever set
 * when the VMA is accessible. For PROT_NONE VMAs, the PTEs are not marked
 * _PAGE_PROTNONE so by by default, implement the helper as "always no". It
 * is the responsibility of the caller to distinguish between PROT_NONE
 * protections and NUMA hinting fault protections.
 */
static inline __attribute__((no_instrument_function)) int Model0_pte_protnone(Model0_pte_t Model0_pte)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_pmd_protnone(Model0_pmd_t Model0_pmd)
{
 return 0;
}





int Model0_pud_set_huge(Model0_pud_t *Model0_pud, Model0_phys_addr_t Model0_addr, Model0_pgprot_t Model0_prot);
int Model0_pmd_set_huge(Model0_pmd_t *Model0_pmd, Model0_phys_addr_t Model0_addr, Model0_pgprot_t Model0_prot);
int Model0_pud_clear_huge(Model0_pud_t *Model0_pud);
int Model0_pmd_clear_huge(Model0_pmd_t *Model0_pmd);
/*
 * To prevent common memory management code establishing
 * a zero page mapping on a read fault.
 * This macro should be defined within <asm/pgtable.h>.
 * s390 does this to prevent multiplexing of hardware bits
 * related to the physical page in case of virtualization.
 */




/*
 * Default maximum number of active map areas, this limits the number of vmas
 * per mm struct. Users can overwrite this number by sysctl but there is a
 * problem.
 *
 * When a program's coredump is generated as ELF format, a section is created
 * per a vma. In ELF, the number of sections is represented in unsigned short.
 * This means the number of sections should be smaller than 65535 at coredump.
 * Because the kernel adds some informative sections to a image of program at
 * generating coredump, we need some margin. The number of extra sections is
 * 1-3 now and depends on arch. We use "5" as safe margin, here.
 *
 * ELF extended numbering allows more than 65535 sections, so 16-bit bound is
 * not a hard limit any more. Although some userspace tools can be surprised by
 * that.
 */



extern int Model0_sysctl_max_map_count;

extern unsigned long Model0_sysctl_user_reserve_kbytes;
extern unsigned long Model0_sysctl_admin_reserve_kbytes;

extern int Model0_sysctl_overcommit_memory;
extern int Model0_sysctl_overcommit_ratio;
extern unsigned long Model0_sysctl_overcommit_kbytes;

extern int Model0_overcommit_ratio_handler(struct Model0_ctl_table *, int, void *,
        Model0_size_t *, Model0_loff_t *);
extern int Model0_overcommit_kbytes_handler(struct Model0_ctl_table *, int, void *,
        Model0_size_t *, Model0_loff_t *);



/* to align the pointer to the (next) page boundary */


/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */


/*
 * Linux kernel virtual memory manager primitives.
 * The idea being to have a "virtual" mm in the same way
 * we have a virtual fs - giving a cleaner interface to the
 * mm details, and allowing different kinds of memory mappings
 * (from shared memory to executable loading to arbitrary
 * mmap() functions).
 */

extern struct Model0_kmem_cache *Model0_vm_area_cachep;
/*
 * vm_flags in vm_area_struct, see mm_types.h.
 * When changing, update also include/trace/events/mmflags.h
 */







/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */
     /* Used by sys_madvise() */
/* MPX specific bounds table or bounds directory */







/* Bits set in the VMA until the stack is in its final location */
/*
 * Special vmas that are non-mergable, non-mlock()able.
 * Note: mm/huge_memory.c VM_NO_THP depends on this definition.
 */


/* This mask defines which mm->def_flags a process can inherit its parent */


/* This mask is used to clear all the VMA flags used by mlock */


/*
 * mapping from the currently active vm_flags protection bits (the
 * low four bits) to a page protection mask..
 */
extern Model0_pgprot_t Model0_protection_map[16];
/*
 * vm_fault is filled by the the pagefault handler and passed to the vma's
 * ->fault function. The vma's ->fault is responsible for returning a bitmask
 * of VM_FAULT_xxx flags that give details about how the fault was handled.
 *
 * MM layer fills up gfp_mask for page allocations but fault handler might
 * alter it if its implementation requires a different allocation context.
 *
 * pgoff should be used in favour of virtual_address, if possible.
 */
struct Model0_vm_fault {
 unsigned int Model0_flags; /* FAULT_FLAG_xxx flags */
 Model0_gfp_t Model0_gfp_mask; /* gfp mask to be used for allocations */
 unsigned long Model0_pgoff; /* Logical page offset based on vma */
 void *Model0_virtual_address; /* Faulting virtual address */

 struct Model0_page *Model0_cow_page; /* Handler may choose to COW */
 struct Model0_page *Model0_page; /* ->fault handlers should return a
					 * page here, unless VM_FAULT_NOPAGE
					 * is set (which is also implied by
					 * VM_FAULT_ERROR).
					 */
 void *Model0_entry; /* ->fault handler can alternatively
					 * return locked DAX entry. In that
					 * case handler should return
					 * VM_FAULT_DAX_LOCKED and fill in
					 * entry here.
					 */
};

/*
 * Page fault context: passes though page fault handler instead of endless list
 * of function arguments.
 */
struct Model0_fault_env {
 struct Model0_vm_area_struct *Model0_vma; /* Target VMA */
 unsigned long Model0_address; /* Faulting virtual address */
 unsigned int Model0_flags; /* FAULT_FLAG_xxx flags */
 Model0_pmd_t *Model0_pmd; /* Pointer to pmd entry matching
					 * the 'address'
					 */
 Model0_pte_t *Model0_pte; /* Pointer to pte entry matching
					 * the 'address'. NULL if the page
					 * table hasn't been allocated.
					 */
 Model0_spinlock_t *Model0_ptl; /* Page table lock.
					 * Protects pte page table if 'pte'
					 * is not NULL, otherwise pmd.
					 */
 Model0_pgtable_t Model0_prealloc_pte; /* Pre-allocated pte page table.
					 * vm_ops->map_pages() calls
					 * alloc_set_pte() from atomic context.
					 * do_fault_around() pre-allocates
					 * page table to avoid allocation from
					 * atomic context.
					 */
};

/*
 * These are the virtual MM functions - opening of an area, closing and
 * unmapping it (needed to keep files on disk up-to-date etc), pointer
 * to the functions called when a no-page or a wp-page exception occurs. 
 */
struct Model0_vm_operations_struct {
 void (*Model0_open)(struct Model0_vm_area_struct * Model0_area);
 void (*Model0_close)(struct Model0_vm_area_struct * Model0_area);
 int (*Model0_mremap)(struct Model0_vm_area_struct * Model0_area);
 int (*fault)(struct Model0_vm_area_struct *Model0_vma, struct Model0_vm_fault *Model0_vmf);
 int (*Model0_pmd_fault)(struct Model0_vm_area_struct *, unsigned long Model0_address,
      Model0_pmd_t *, unsigned int Model0_flags);
 void (*Model0_map_pages)(struct Model0_fault_env *Model0_fe,
   unsigned long Model0_start_pgoff, unsigned long Model0_end_pgoff);

 /* notification that a previously read-only page is about to become
	 * writable, if an error is returned it will cause a SIGBUS */
 int (*Model0_page_mkwrite)(struct Model0_vm_area_struct *Model0_vma, struct Model0_vm_fault *Model0_vmf);

 /* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */
 int (*Model0_pfn_mkwrite)(struct Model0_vm_area_struct *Model0_vma, struct Model0_vm_fault *Model0_vmf);

 /* called by access_process_vm when get_user_pages() fails, typically
	 * for use by special VMAs that can switch between memory and hardware
	 */
 int (*Model0_access)(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr,
        void *Model0_buf, int Model0_len, int Model0_write);

 /* Called by the /proc/PID/maps code to ask the vma whether it
	 * has a special name.  Returning non-NULL will also cause this
	 * vma to be dumped unconditionally. */
 const char *(*Model0_name)(struct Model0_vm_area_struct *Model0_vma);


 /*
	 * set_policy() op must add a reference to any non-NULL @new mempolicy
	 * to hold the policy upon return.  Caller should pass NULL @new to
	 * remove a policy and fall back to surrounding context--i.e. do not
	 * install a MPOL_DEFAULT policy, nor the task or system default
	 * mempolicy.
	 */
 int (*Model0_set_policy)(struct Model0_vm_area_struct *Model0_vma, struct Model0_mempolicy *Model0_new);

 /*
	 * get_policy() op must add reference [mpol_get()] to any policy at
	 * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure
	 * in mm/mempolicy.c will do this automatically.
	 * get_policy() must NOT add a ref if the policy at (vma,addr) is not
	 * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.
	 * If no [shared/vma] mempolicy exists at the addr, get_policy() op
	 * must return NULL--i.e., do not "fallback" to task or system default
	 * policy.
	 */
 struct Model0_mempolicy *(*Model0_get_policy)(struct Model0_vm_area_struct *Model0_vma,
     unsigned long Model0_addr);

 /*
	 * Called by vm_normal_page() for special PTEs to find the
	 * page for @addr.  This is useful if the default behavior
	 * (using pte_page()) would not find the correct page.
	 */
 struct Model0_page *(*Model0_find_special_page)(struct Model0_vm_area_struct *Model0_vma,
       unsigned long Model0_addr);
};

struct Model0_mmu_gather;
struct Model0_inode;





static inline __attribute__((no_instrument_function)) int Model0_pmd_devmap(Model0_pmd_t Model0_pmd)
{
 return 0;
}


/*
 * FIXME: take this include out, include page-flags.h in
 * files which need it (119 of them)
 */





extern int Model0_do_huge_pmd_anonymous_page(struct Model0_fault_env *Model0_fe);
extern int Model0_copy_huge_pmd(struct Model0_mm_struct *Model0_dst_mm, struct Model0_mm_struct *Model0_src_mm,
    Model0_pmd_t *Model0_dst_pmd, Model0_pmd_t *Model0_src_pmd, unsigned long Model0_addr,
    struct Model0_vm_area_struct *Model0_vma);
extern void Model0_huge_pmd_set_accessed(struct Model0_fault_env *Model0_fe, Model0_pmd_t Model0_orig_pmd);
extern int Model0_do_huge_pmd_wp_page(struct Model0_fault_env *Model0_fe, Model0_pmd_t Model0_orig_pmd);
extern struct Model0_page *Model0_follow_trans_huge_pmd(struct Model0_vm_area_struct *Model0_vma,
       unsigned long Model0_addr,
       Model0_pmd_t *Model0_pmd,
       unsigned int Model0_flags);
extern bool Model0_madvise_free_huge_pmd(struct Model0_mmu_gather *Model0_tlb,
   struct Model0_vm_area_struct *Model0_vma,
   Model0_pmd_t *Model0_pmd, unsigned long Model0_addr, unsigned long Model0_next);
extern int Model0_zap_huge_pmd(struct Model0_mmu_gather *Model0_tlb,
   struct Model0_vm_area_struct *Model0_vma,
   Model0_pmd_t *Model0_pmd, unsigned long Model0_addr);
extern int Model0_mincore_huge_pmd(struct Model0_vm_area_struct *Model0_vma, Model0_pmd_t *Model0_pmd,
   unsigned long Model0_addr, unsigned long Model0_end,
   unsigned char *Model0_vec);
extern bool Model0_move_huge_pmd(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_old_addr,
    unsigned long Model0_new_addr, unsigned long Model0_old_end,
    Model0_pmd_t *Model0_old_pmd, Model0_pmd_t *Model0_new_pmd);
extern int Model0_change_huge_pmd(struct Model0_vm_area_struct *Model0_vma, Model0_pmd_t *Model0_pmd,
   unsigned long Model0_addr, Model0_pgprot_t Model0_newprot,
   int Model0_prot_numa);
int Model0_vmf_insert_pfn_pmd(struct Model0_vm_area_struct *, unsigned long Model0_addr, Model0_pmd_t *,
   Model0_pfn_t Model0_pfn, bool Model0_write);
enum Model0_transparent_hugepage_flag {
 Model0_TRANSPARENT_HUGEPAGE_FLAG,
 Model0_TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,
 Model0_TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG,
 Model0_TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_FLAG,
 Model0_TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG,
 Model0_TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG,
 Model0_TRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG,



};

struct Model0_kobject;
struct Model0_kobj_attribute;

extern Model0_ssize_t Model0_single_hugepage_flag_store(struct Model0_kobject *Model0_kobj,
     struct Model0_kobj_attribute *Model0_attr,
     const char *Model0_buf, Model0_size_t Model0_count,
     enum Model0_transparent_hugepage_flag Model0_flag);
extern Model0_ssize_t Model0_single_hugepage_flag_show(struct Model0_kobject *Model0_kobj,
    struct Model0_kobj_attribute *Model0_attr, char *Model0_buf,
    enum Model0_transparent_hugepage_flag Model0_flag);
extern struct Model0_kobj_attribute Model0_shmem_enabled_attr;
static inline __attribute__((no_instrument_function)) void Model0_prep_transhuge_page(struct Model0_page *Model0_page) {}


static inline __attribute__((no_instrument_function)) int
Model0_split_huge_page_to_list(struct Model0_page *Model0_page, struct Model0_list_head *Model0_list)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model0_split_huge_page(struct Model0_page *Model0_page)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) void Model0_deferred_split_huge_page(struct Model0_page *Model0_page) {}



static inline __attribute__((no_instrument_function)) void Model0_split_huge_pmd_address(struct Model0_vm_area_struct *Model0_vma,
  unsigned long Model0_address, bool Model0_freeze, struct Model0_page *Model0_page) {}

static inline __attribute__((no_instrument_function)) int Model0_hugepage_madvise(struct Model0_vm_area_struct *Model0_vma,
       unsigned long *Model0_vm_flags, int Model0_advice)
{
 do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/huge_mm.h"), "i" (191), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0);
 return 0;
}
static inline __attribute__((no_instrument_function)) void Model0_vma_adjust_trans_huge(struct Model0_vm_area_struct *Model0_vma,
      unsigned long Model0_start,
      unsigned long Model0_end,
      long Model0_adjust_next)
{
}
static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_pmd_trans_huge_lock(Model0_pmd_t *Model0_pmd,
  struct Model0_vm_area_struct *Model0_vma)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_do_huge_pmd_numa_page(struct Model0_fault_env *Model0_fe, Model0_pmd_t Model0_orig_pmd)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) bool Model0_is_huge_zero_page(struct Model0_page *Model0_page)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model0_put_huge_zero_page(void)
{
 do { bool Model0___cond = !(!(1)); extern void Model0___compiletime_assert_218(void) ; if (Model0___cond) Model0___compiletime_assert_218(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
}

static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_follow_devmap_pmd(struct Model0_vm_area_struct *Model0_vma,
  unsigned long Model0_addr, Model0_pmd_t *Model0_pmd, int Model0_flags)
{
 return ((void *)0);
}

/*
 * Methods to modify the page usage count.
 *
 * What counts for a page usage:
 * - cache mapping   (page->mapping)
 * - private data    (page->private)
 * - page mapped in a task's page tables, each mapping
 *   is counted separately
 *
 * Also, many kernel routines increase the page count before a critical
 * routine so they can be sure the page doesn't go away from under them.
 */

/*
 * Drop a ref, return true if the refcount fell to zero (the page has no users)
 */
static inline __attribute__((no_instrument_function)) int Model0_put_page_testzero(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(Model0_page_ref_count(Model0_page) == 0))));
 return Model0_page_ref_dec_and_test(Model0_page);
}

/*
 * Try to grab a ref unless the page has a refcount of zero, return false if
 * that is the case.
 * This can be called when MMU is off so it must not access
 * any of the virtual mappings.
 */
static inline __attribute__((no_instrument_function)) int Model0_get_page_unless_zero(struct Model0_page *Model0_page)
{
 return Model0_page_ref_add_unless(Model0_page, 1, 0);
}

extern int Model0_page_is_ram(unsigned long Model0_pfn);

enum {
 Model0_REGION_INTERSECTS,
 Model0_REGION_DISJOINT,
 Model0_REGION_MIXED,
};

int Model0_region_intersects(Model0_resource_size_t Model0_offset, Model0_size_t Model0_size, unsigned long Model0_flags,
        unsigned long Model0_desc);

/* Support for virtually mapped pages */
struct Model0_page *Model0_vmalloc_to_page(const void *Model0_addr);
unsigned long Model0_vmalloc_to_pfn(const void *Model0_addr);

/*
 * Determine if an address is within the vmalloc range
 *
 * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there
 * is no special casing required.
 */
static inline __attribute__((no_instrument_function)) bool Model0_is_vmalloc_addr(const void *Model0_x)
{

 unsigned long Model0_addr = (unsigned long)Model0_x;

 return Model0_addr >= (0xffffc90000000000UL) && Model0_addr < ((0xffffc90000000000UL) + (((32UL) << 40) - 1UL));



}

extern int Model0_is_vmalloc_or_module_addr(const void *Model0_x);







extern void Model0_kvfree(const void *Model0_addr);

static inline __attribute__((no_instrument_function)) Model0_atomic_t *Model0_compound_mapcount_ptr(struct Model0_page *Model0_page)
{
 return &Model0_page[1].Model0_compound_mapcount;
}

static inline __attribute__((no_instrument_function)) int Model0_compound_mapcount(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(!Model0_PageCompound(Model0_page)))));
 Model0_page = Model0_compound_head(Model0_page);
 return Model0_atomic_read(Model0_compound_mapcount_ptr(Model0_page)) + 1;
}

/*
 * The atomic page->_mapcount, starts from -1: so that transitions
 * both from it and to it can be tracked, using atomic_inc_and_test
 * and atomic_add_negative(-1).
 */
static inline __attribute__((no_instrument_function)) void Model0_page_mapcount_reset(struct Model0_page *Model0_page)
{
 Model0_atomic_set(&(Model0_page)->Model0__mapcount, -1);
}

int Model0___page_mapcount(struct Model0_page *Model0_page);

static inline __attribute__((no_instrument_function)) int Model0_page_mapcount(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(Model0_PageSlab(Model0_page)))));

 if (__builtin_expect(!!(Model0_PageCompound(Model0_page)), 0))
  return Model0___page_mapcount(Model0_page);
 return Model0_atomic_read(&Model0_page->Model0__mapcount) + 1;
}





static inline __attribute__((no_instrument_function)) int Model0_total_mapcount(struct Model0_page *Model0_page)
{
 return Model0_page_mapcount(Model0_page);
}
static inline __attribute__((no_instrument_function)) int Model0_page_trans_huge_mapcount(struct Model0_page *Model0_page,
        int *Model0_total_mapcount)
{
 int Model0_mapcount = Model0_page_mapcount(Model0_page);
 if (Model0_total_mapcount)
  *Model0_total_mapcount = Model0_mapcount;
 return Model0_mapcount;
}


static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_virt_to_head_page(const void *Model0_x)
{
 struct Model0_page *Model0_page = (((struct Model0_page *)(0xffffea0000000000UL)) + (Model0___phys_addr_nodebug((unsigned long)(Model0_x)) >> 12));

 return Model0_compound_head(Model0_page);
}

void Model0___put_page(struct Model0_page *Model0_page);

void Model0_put_pages_list(struct Model0_list_head *Model0_pages);

void Model0_split_page(struct Model0_page *Model0_page, unsigned int Model0_order);

/*
 * Compound pages have a destructor function.  Provide a
 * prototype for that function and accessor functions.
 * These are _only_ valid on the head of a compound page.
 */
typedef void Model0_compound_page_dtor(struct Model0_page *);

/* Keep the enum in sync with compound_page_dtors array in mm/page_alloc.c */
enum Model0_compound_dtor_id {
 Model0_NULL_COMPOUND_DTOR,
 Model0_COMPOUND_PAGE_DTOR,

 Model0_HUGETLB_PAGE_DTOR,




 Model0_NR_COMPOUND_DTORS,
};
extern Model0_compound_page_dtor * const Model0_compound_page_dtors[];

static inline __attribute__((no_instrument_function)) void Model0_set_compound_page_dtor(struct Model0_page *Model0_page,
  enum Model0_compound_dtor_id Model0_compound_dtor)
{
 ((void)(sizeof(( long)(Model0_compound_dtor >= Model0_NR_COMPOUND_DTORS))));
 Model0_page[1].Model0_compound_dtor = Model0_compound_dtor;
}

static inline __attribute__((no_instrument_function)) Model0_compound_page_dtor *Model0_get_compound_page_dtor(struct Model0_page *Model0_page)
{
 ((void)(sizeof(( long)(Model0_page[1].Model0_compound_dtor >= Model0_NR_COMPOUND_DTORS))));
 return Model0_compound_page_dtors[Model0_page[1].Model0_compound_dtor];
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_compound_order(struct Model0_page *Model0_page)
{
 if (!Model0_PageHead(Model0_page))
  return 0;
 return Model0_page[1].Model0_compound_order;
}

static inline __attribute__((no_instrument_function)) void Model0_set_compound_order(struct Model0_page *Model0_page, unsigned int Model0_order)
{
 Model0_page[1].Model0_compound_order = Model0_order;
}

void Model0_free_compound_page(struct Model0_page *Model0_page);


/*
 * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when
 * servicing faults for write access.  In the normal case, do always want
 * pte_mkwrite.  But get_user_pages can cause write faults for mappings
 * that do not have writing enabled, when used by access_process_vm.
 */
static inline __attribute__((no_instrument_function)) Model0_pte_t Model0_maybe_mkwrite(Model0_pte_t Model0_pte, struct Model0_vm_area_struct *Model0_vma)
{
 if (__builtin_expect(!!(Model0_vma->Model0_vm_flags & 0x00000002), 1))
  Model0_pte = Model0_pte_mkwrite(Model0_pte);
 return Model0_pte;
}

int Model0_alloc_set_pte(struct Model0_fault_env *Model0_fe, struct Model0_mem_cgroup *Model0_memcg,
  struct Model0_page *Model0_page);


/*
 * Multiple processes may "see" the same page. E.g. for untouched
 * mappings of /dev/null, all processes see the same page full of
 * zeroes, and text pages of executables and shared libraries have
 * only one copy in memory, at most, normally.
 *
 * For the non-reserved pages, page_count(page) denotes a reference count.
 *   page_count() == 0 means the page is free. page->lru is then used for
 *   freelist management in the buddy allocator.
 *   page_count() > 0  means the page has been allocated.
 *
 * Pages are allocated by the slab allocator in order to provide memory
 * to kmalloc and kmem_cache_alloc. In this case, the management of the
 * page, and the fields in 'struct page' are the responsibility of mm/slab.c
 * unless a particular usage is carefully commented. (the responsibility of
 * freeing the kmalloc memory is the caller's, of course).
 *
 * A page may be used by anyone else who does a __get_free_page().
 * In this case, page_count still tracks the references, and should only
 * be used through the normal accessor functions. The top bits of page->flags
 * and page->virtual store page management information, but all other fields
 * are unused and could be used privately, carefully. The management of this
 * page is the responsibility of the one who allocated it, and those who have
 * subsequently been given references to it.
 *
 * The other pages (we may call them "pagecache pages") are completely
 * managed by the Linux memory manager: I/O, buffers, swapping etc.
 * The following discussion applies only to them.
 *
 * A pagecache page contains an opaque `private' member, which belongs to the
 * page's address_space. Usually, this is the address of a circular list of
 * the page's disk buffers. PG_private must be set to tell the VM to call
 * into the filesystem to release these pages.
 *
 * A page may belong to an inode's memory mapping. In this case, page->mapping
 * is the pointer to the inode, and page->index is the file offset of the page,
 * in units of PAGE_SIZE.
 *
 * If pagecache pages are not associated with an inode, they are said to be
 * anonymous pages. These may become associated with the swapcache, and in that
 * case PG_swapcache is set, and page->private is an offset into the swapcache.
 *
 * In either case (swapcache or inode backed), the pagecache itself holds one
 * reference to the page. Setting PG_private should also increment the
 * refcount. The each user mapping also has a reference to the page.
 *
 * The pagecache pages are stored in a per-mapping radix tree, which is
 * rooted at mapping->page_tree, and indexed by offset.
 * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space
 * lists, we instead now tag pages as dirty/writeback in the radix tree.
 *
 * All pagecache pages may be subject to I/O:
 * - inode pages may need to be read from disk,
 * - inode pages which have been modified and are MAP_SHARED may need
 *   to be written back to the inode on disk,
 * - anonymous pages (including MAP_PRIVATE file mappings) which have been
 *   modified may need to be swapped out to swap space and (later) to be read
 *   back into memory.
 */

/*
 * The zone field is never updated after free_area_init_core()
 * sets it, so none of the operations on it need to be atomic.
 */

/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */





/*
 * Define the bit shifts to access each section.  For non-existent
 * sections we define the shift as 0; that plus a 0 mask ensures
 * the compiler will optimise away reference to them.
 */





/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
static inline __attribute__((no_instrument_function)) enum Model0_zone_type Model0_page_zonenum(const struct Model0_page *Model0_page)
{
 return (Model0_page->Model0_flags >> (((((sizeof(unsigned long)*8) - 0) - 6) - 2) * (2 != 0))) & ((1UL << 2) - 1);
}
static inline __attribute__((no_instrument_function)) void Model0_get_zone_device_page(struct Model0_page *Model0_page)
{
}
static inline __attribute__((no_instrument_function)) void Model0_put_zone_device_page(struct Model0_page *Model0_page)
{
}
static inline __attribute__((no_instrument_function)) bool Model0_is_zone_device_page(const struct Model0_page *Model0_page)
{
 return false;
}


static inline __attribute__((no_instrument_function)) void Model0_get_page(struct Model0_page *Model0_page)
{
 Model0_page = Model0_compound_head(Model0_page);
 /*
	 * Getting a normal page or the head of a compound page
	 * requires to already have an elevated page->_refcount.
	 */
 ((void)(sizeof(( long)(Model0_page_ref_count(Model0_page) <= 0))));
 Model0_page_ref_inc(Model0_page);

 if (__builtin_expect(!!(Model0_is_zone_device_page(Model0_page)), 0))
  Model0_get_zone_device_page(Model0_page);
}

static inline __attribute__((no_instrument_function)) void Model0_put_page(struct Model0_page *Model0_page)
{
 Model0_page = Model0_compound_head(Model0_page);

 if (Model0_put_page_testzero(Model0_page))
  Model0___put_page(Model0_page);

 if (__builtin_expect(!!(Model0_is_zone_device_page(Model0_page)), 0))
  Model0_put_zone_device_page(Model0_page);
}





/*
 * The identification function is mainly used by the buddy allocator for
 * determining if two pages could be buddies. We are not really identifying
 * the zone since we could be using the section number id if we do not have
 * node id available in page flags.
 * We only guarantee that it will return the same value for two combinable
 * pages in a zone.
 */
static inline __attribute__((no_instrument_function)) int Model0_page_zone_id(struct Model0_page *Model0_page)
{
 return (Model0_page->Model0_flags >> ((((((sizeof(unsigned long)*8) - 0) - 6) < ((((sizeof(unsigned long)*8) - 0) - 6) - 2))? (((sizeof(unsigned long)*8) - 0) - 6) : ((((sizeof(unsigned long)*8) - 0) - 6) - 2)) * ((6 + 2) != 0))) & ((1UL << (6 + 2)) - 1);
}

static inline __attribute__((no_instrument_function)) int Model0_zone_to_nid(struct Model0_zone *Model0_zone)
{

 return Model0_zone->Model0_node;



}




static inline __attribute__((no_instrument_function)) int Model0_page_to_nid(const struct Model0_page *Model0_page)
{
 return (Model0_page->Model0_flags >> ((((sizeof(unsigned long)*8) - 0) - 6) * (6 != 0))) & ((1UL << 6) - 1);
}
static inline __attribute__((no_instrument_function)) int Model0_page_cpupid_xchg_last(struct Model0_page *Model0_page, int Model0_cpupid)
{
 return Model0_page_to_nid(Model0_page); /* XXX */
}

static inline __attribute__((no_instrument_function)) int Model0_page_cpupid_last(struct Model0_page *Model0_page)
{
 return Model0_page_to_nid(Model0_page); /* XXX */
}

static inline __attribute__((no_instrument_function)) int Model0_cpupid_to_nid(int Model0_cpupid)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) int Model0_cpupid_to_pid(int Model0_cpupid)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) int Model0_cpupid_to_cpu(int Model0_cpupid)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) int Model0_cpu_pid_to_cpupid(int Model0_nid, int Model0_pid)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) bool Model0_cpupid_pid_unset(int Model0_cpupid)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) void Model0_page_cpupid_reset_last(struct Model0_page *Model0_page)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_cpupid_match_pid(struct Model0_task_struct *Model0_task, int Model0_cpupid)
{
 return false;
}


static inline __attribute__((no_instrument_function)) struct Model0_zone *Model0_page_zone(const struct Model0_page *Model0_page)
{
 return &(Model0_node_data[Model0_page_to_nid(Model0_page)])->Model0_node_zones[Model0_page_zonenum(Model0_page)];
}

static inline __attribute__((no_instrument_function)) Model0_pg_data_t *Model0_page_pgdat(const struct Model0_page *Model0_page)
{
 return (Model0_node_data[Model0_page_to_nid(Model0_page)]);
}
static inline __attribute__((no_instrument_function)) void Model0_set_page_zone(struct Model0_page *Model0_page, enum Model0_zone_type Model0_zone)
{
 Model0_page->Model0_flags &= ~(((1UL << 2) - 1) << (((((sizeof(unsigned long)*8) - 0) - 6) - 2) * (2 != 0)));
 Model0_page->Model0_flags |= (Model0_zone & ((1UL << 2) - 1)) << (((((sizeof(unsigned long)*8) - 0) - 6) - 2) * (2 != 0));
}

static inline __attribute__((no_instrument_function)) void Model0_set_page_node(struct Model0_page *Model0_page, unsigned long Model0_node)
{
 Model0_page->Model0_flags &= ~(((1UL << 6) - 1) << ((((sizeof(unsigned long)*8) - 0) - 6) * (6 != 0)));
 Model0_page->Model0_flags |= (Model0_node & ((1UL << 6) - 1)) << ((((sizeof(unsigned long)*8) - 0) - 6) * (6 != 0));
}

static inline __attribute__((no_instrument_function)) void Model0_set_page_links(struct Model0_page *Model0_page, enum Model0_zone_type Model0_zone,
 unsigned long Model0_node, unsigned long Model0_pfn)
{
 Model0_set_page_zone(Model0_page, Model0_zone);
 Model0_set_page_node(Model0_page, Model0_node);



}
static inline __attribute__((no_instrument_function)) struct Model0_mem_cgroup *Model0_page_memcg(struct Model0_page *Model0_page)
{
 return ((void *)0);
}
static inline __attribute__((no_instrument_function)) struct Model0_mem_cgroup *Model0_page_memcg_rcu(struct Model0_page *Model0_page)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(!Model0_rcu_read_lock_held()); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/mm.h", 993); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });
 return ((void *)0);
}


/*
 * Some inline functions in vmstat.h depend on page_zone()
 */







enum Model0_vm_event_item { Model0_PGPGIN, Model0_PGPGOUT, Model0_PSWPIN, Model0_PSWPOUT,
  Model0_PGALLOC_DMA, Model0_PGALLOC_DMA32, Model0_PGALLOC_NORMAL, Model0_PGALLOC_MOVABLE,
  Model0_ALLOCSTALL_DMA, Model0_ALLOCSTALL_DMA32, Model0_ALLOCSTALL_NORMAL, Model0_ALLOCSTALL_MOVABLE,
  Model0_PGSCAN_SKIP_DMA, Model0_PGSCAN_SKIP_DMA32, Model0_PGSCAN_SKIP_NORMAL, Model0_PGSCAN_SKIP_MOVABLE,
  Model0_PGFREE, Model0_PGACTIVATE, Model0_PGDEACTIVATE,
  Model0_PGFAULT, Model0_PGMAJFAULT,
  Model0_PGLAZYFREED,
  Model0_PGREFILL,
  Model0_PGSTEAL_KSWAPD,
  Model0_PGSTEAL_DIRECT,
  Model0_PGSCAN_KSWAPD,
  Model0_PGSCAN_DIRECT,
  Model0_PGSCAN_DIRECT_THROTTLE,

  Model0_PGSCAN_ZONE_RECLAIM_FAILED,

  Model0_PGINODESTEAL, Model0_SLABS_SCANNED, Model0_KSWAPD_INODESTEAL,
  Model0_KSWAPD_LOW_WMARK_HIT_QUICKLY, Model0_KSWAPD_HIGH_WMARK_HIT_QUICKLY,
  Model0_PAGEOUTRUN, Model0_PGROTATED,
  Model0_DROP_PAGECACHE, Model0_DROP_SLAB,
  Model0_PGMIGRATE_SUCCESS, Model0_PGMIGRATE_FAIL,


  Model0_COMPACTMIGRATE_SCANNED, Model0_COMPACTFREE_SCANNED,
  Model0_COMPACTISOLATED,
  Model0_COMPACTSTALL, Model0_COMPACTFAIL, Model0_COMPACTSUCCESS,
  Model0_KCOMPACTD_WAKE,


  Model0_HTLB_BUDDY_PGALLOC, Model0_HTLB_BUDDY_PGALLOC_FAIL,

  Model0_UNEVICTABLE_PGCULLED, /* culled to noreclaim list */
  Model0_UNEVICTABLE_PGSCANNED, /* scanned for reclaimability */
  Model0_UNEVICTABLE_PGRESCUED, /* rescued from noreclaim list */
  Model0_UNEVICTABLE_PGMLOCKED,
  Model0_UNEVICTABLE_PGMUNLOCKED,
  Model0_UNEVICTABLE_PGCLEARED, /* on COW, page truncate */
  Model0_UNEVICTABLE_PGSTRANDED, /* unable to isolate on unlock */
  Model0_NR_VM_EVENT_ITEMS
};


extern int Model0_sysctl_stat_interval;


/*
 * Light weight per cpu counter implementation.
 *
 * Counters should only be incremented and no critical kernel component
 * should rely on the counter values.
 *
 * Counters are handled completely inline. On many platforms the code
 * generated will simply be the increment of a global address.
 */

struct Model0_vm_event_state {
 unsigned long Model0_event[Model0_NR_VM_EVENT_ITEMS];
};

extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model0_vm_event_state) Model0_vm_event_states;

/*
 * vm counters are allowed to be racy. Use raw_cpu_ops to avoid the
 * local_irq_disable overhead.
 */
static inline __attribute__((no_instrument_function)) void Model0___count_vm_event(enum Model0_vm_event_item Model0_item)
{
 do { do { const void *Model0___vpp_verify = (typeof((&(Model0_vm_event_states.Model0_event[Model0_item])) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_vm_event_states.Model0_event[Model0_item])) { case 1: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((no_instrument_function)) void Model0_count_vm_event(enum Model0_vm_event_item Model0_item)
{
 do { do { const void *Model0___vpp_verify = (typeof((&(Model0_vm_event_states.Model0_event[Model0_item])) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_vm_event_states.Model0_event[Model0_item])) { case 1: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((no_instrument_function)) void Model0___count_vm_events(enum Model0_vm_event_item Model0_item, long Model0_delta)
{
 do { do { const void *Model0___vpp_verify = (typeof((&(Model0_vm_event_states.Model0_event[Model0_item])) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_vm_event_states.Model0_event[Model0_item])) { case 1: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_delta) && ((Model0_delta) == 1 || (Model0_delta) == -1)) ? (int)(Model0_delta) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_delta); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(Model0_delta))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(Model0_delta))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_delta) && ((Model0_delta) == 1 || (Model0_delta) == -1)) ? (int)(Model0_delta) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_delta); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(Model0_delta))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(Model0_delta))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_delta) && ((Model0_delta) == 1 || (Model0_delta) == -1)) ? (int)(Model0_delta) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_delta); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(Model0_delta))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(Model0_delta))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_delta) && ((Model0_delta) == 1 || (Model0_delta) == -1)) ? (int)(Model0_delta) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_delta); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(Model0_delta))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(Model0_delta))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((no_instrument_function)) void Model0_count_vm_events(enum Model0_vm_event_item Model0_item, long Model0_delta)
{
 do { do { const void *Model0___vpp_verify = (typeof((&(Model0_vm_event_states.Model0_event[Model0_item])) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_vm_event_states.Model0_event[Model0_item])) { case 1: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_delta) && ((Model0_delta) == 1 || (Model0_delta) == -1)) ? (int)(Model0_delta) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_delta); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(Model0_delta))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(Model0_delta))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_delta) && ((Model0_delta) == 1 || (Model0_delta) == -1)) ? (int)(Model0_delta) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_delta); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(Model0_delta))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(Model0_delta))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_delta) && ((Model0_delta) == 1 || (Model0_delta) == -1)) ? (int)(Model0_delta) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_delta); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(Model0_delta))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(Model0_delta))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model0_vm_event_states.Model0_event[Model0_item])) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(Model0_delta) && ((Model0_delta) == 1 || (Model0_delta) == -1)) ? (int)(Model0_delta) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (Model0_delta); (void)Model0_pao_tmp__; } switch (sizeof((Model0_vm_event_states.Model0_event[Model0_item]))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "qi" ((Model0_pao_T__)(Model0_delta))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "ri" ((Model0_pao_T__)(Model0_delta))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model0_vm_event_states.Model0_event[Model0_item])) : "re" ((Model0_pao_T__)(Model0_delta))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
}

extern void Model0_all_vm_events(unsigned long *);

extern void Model0_vm_events_fold_cpu(int Model0_cpu);
/*
 * Zone and node-based page accounting with per cpu differentials.
 */
extern Model0_atomic_long_t Model0_vm_zone_stat[Model0_NR_VM_ZONE_STAT_ITEMS];
extern Model0_atomic_long_t Model0_vm_node_stat[Model0_NR_VM_NODE_STAT_ITEMS];

static inline __attribute__((no_instrument_function)) void Model0_zone_page_state_add(long Model0_x, struct Model0_zone *Model0_zone,
     enum Model0_zone_stat_item Model0_item)
{
 Model0_atomic_long_add(Model0_x, &Model0_zone->Model0_vm_stat[Model0_item]);
 Model0_atomic_long_add(Model0_x, &Model0_vm_zone_stat[Model0_item]);
}

static inline __attribute__((no_instrument_function)) void Model0_node_page_state_add(long Model0_x, struct Model0_pglist_data *Model0_pgdat,
     enum Model0_node_stat_item Model0_item)
{
 Model0_atomic_long_add(Model0_x, &Model0_pgdat->Model0_vm_stat[Model0_item]);
 Model0_atomic_long_add(Model0_x, &Model0_vm_node_stat[Model0_item]);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_global_page_state(enum Model0_zone_stat_item Model0_item)
{
 long Model0_x = Model0_atomic_long_read(&Model0_vm_zone_stat[Model0_item]);

 if (Model0_x < 0)
  Model0_x = 0;

 return Model0_x;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_global_node_page_state(enum Model0_node_stat_item Model0_item)
{
 long Model0_x = Model0_atomic_long_read(&Model0_vm_node_stat[Model0_item]);

 if (Model0_x < 0)
  Model0_x = 0;

 return Model0_x;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_zone_page_state(struct Model0_zone *Model0_zone,
     enum Model0_zone_stat_item Model0_item)
{
 long Model0_x = Model0_atomic_long_read(&Model0_zone->Model0_vm_stat[Model0_item]);

 if (Model0_x < 0)
  Model0_x = 0;

 return Model0_x;
}

/*
 * More accurate version that also considers the currently pending
 * deltas. For that we need to loop over all cpus to find the current
 * deltas. There is no synchronization so the result cannot be
 * exactly accurate either.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_zone_page_state_snapshot(struct Model0_zone *Model0_zone,
     enum Model0_zone_stat_item Model0_item)
{
 long Model0_x = Model0_atomic_long_read(&Model0_zone->Model0_vm_stat[Model0_item]);


 int Model0_cpu;
 for (((Model0_cpu)) = -1; ((Model0_cpu)) = Model0_cpumask_next(((Model0_cpu)), (((const struct Model0_cpumask *)&Model0___cpu_online_mask))), ((Model0_cpu)) < Model0_nr_cpu_ids;)
  Model0_x += ({ do { const void *Model0___vpp_verify = (typeof((Model0_zone->Model0_pageset) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((Model0_zone->Model0_pageset))) *)((Model0_zone->Model0_pageset)))); (typeof((typeof(*((Model0_zone->Model0_pageset))) *)((Model0_zone->Model0_pageset)))) (Model0___ptr + (((Model0___per_cpu_offset[(Model0_cpu)])))); }); })->Model0_vm_stat_diff[Model0_item];

 if (Model0_x < 0)
  Model0_x = 0;

 return Model0_x;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_node_page_state_snapshot(Model0_pg_data_t *Model0_pgdat,
     enum Model0_node_stat_item Model0_item)
{
 long Model0_x = Model0_atomic_long_read(&Model0_pgdat->Model0_vm_stat[Model0_item]);


 int Model0_cpu;
 for (((Model0_cpu)) = -1; ((Model0_cpu)) = Model0_cpumask_next(((Model0_cpu)), (((const struct Model0_cpumask *)&Model0___cpu_online_mask))), ((Model0_cpu)) < Model0_nr_cpu_ids;)
  Model0_x += ({ do { const void *Model0___vpp_verify = (typeof((Model0_pgdat->Model0_per_cpu_nodestats) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0___ptr; __asm__ ("" : "=r"(Model0___ptr) : "0"((typeof(*((Model0_pgdat->Model0_per_cpu_nodestats))) *)((Model0_pgdat->Model0_per_cpu_nodestats)))); (typeof((typeof(*((Model0_pgdat->Model0_per_cpu_nodestats))) *)((Model0_pgdat->Model0_per_cpu_nodestats)))) (Model0___ptr + (((Model0___per_cpu_offset[(Model0_cpu)])))); }); })->Model0_vm_node_stat_diff[Model0_item];

 if (Model0_x < 0)
  Model0_x = 0;

 return Model0_x;
}



extern unsigned long Model0_sum_zone_node_page_state(int Model0_node,
      enum Model0_zone_stat_item Model0_item);
extern unsigned long Model0_node_page_state(struct Model0_pglist_data *Model0_pgdat,
      enum Model0_node_stat_item Model0_item);
void Model0___mod_zone_page_state(struct Model0_zone *, enum Model0_zone_stat_item Model0_item, long);
void Model0___inc_zone_page_state(struct Model0_page *, enum Model0_zone_stat_item);
void Model0___dec_zone_page_state(struct Model0_page *, enum Model0_zone_stat_item);

void Model0___mod_node_page_state(struct Model0_pglist_data *, enum Model0_node_stat_item Model0_item, long);
void Model0___inc_node_page_state(struct Model0_page *, enum Model0_node_stat_item);
void Model0___dec_node_page_state(struct Model0_page *, enum Model0_node_stat_item);

void Model0_mod_zone_page_state(struct Model0_zone *, enum Model0_zone_stat_item, long);
void Model0_inc_zone_page_state(struct Model0_page *, enum Model0_zone_stat_item);
void Model0_dec_zone_page_state(struct Model0_page *, enum Model0_zone_stat_item);

void Model0_mod_node_page_state(struct Model0_pglist_data *, enum Model0_node_stat_item, long);
void Model0_inc_node_page_state(struct Model0_page *, enum Model0_node_stat_item);
void Model0_dec_node_page_state(struct Model0_page *, enum Model0_node_stat_item);

extern void Model0_inc_node_state(struct Model0_pglist_data *, enum Model0_node_stat_item);
extern void Model0___inc_zone_state(struct Model0_zone *, enum Model0_zone_stat_item);
extern void Model0___inc_node_state(struct Model0_pglist_data *, enum Model0_node_stat_item);
extern void Model0_dec_zone_state(struct Model0_zone *, enum Model0_zone_stat_item);
extern void Model0___dec_zone_state(struct Model0_zone *, enum Model0_zone_stat_item);
extern void Model0___dec_node_state(struct Model0_pglist_data *, enum Model0_node_stat_item);

void Model0_quiet_vmstat(void);
void Model0_cpu_vm_stats_fold(int Model0_cpu);
void Model0_refresh_zone_stat_thresholds(void);

struct Model0_ctl_table;
int Model0_vmstat_refresh(struct Model0_ctl_table *, int Model0_write,
     void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);

void Model0_drain_zonestat(struct Model0_zone *Model0_zone, struct Model0_per_cpu_pageset *);

int Model0_calculate_pressure_threshold(struct Model0_zone *Model0_zone);
int Model0_calculate_normal_threshold(struct Model0_zone *Model0_zone);
void Model0_set_pgdat_percpu_threshold(Model0_pg_data_t *Model0_pgdat,
    int (*Model0_calculate_pressure)(struct Model0_zone *));
static inline __attribute__((no_instrument_function)) void Model0___mod_zone_freepage_state(struct Model0_zone *Model0_zone, int Model0_nr_pages,
          int Model0_migratetype)
{
 Model0___mod_zone_page_state(Model0_zone, Model0_NR_FREE_PAGES, Model0_nr_pages);
 if (false)
  Model0___mod_zone_page_state(Model0_zone, Model0_NR_FREE_CMA_PAGES, Model0_nr_pages);
}

extern const char * const Model0_vmstat_text[];

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model0_lowmem_page_address(const struct Model0_page *Model0_page)
{
 return ((void *)((unsigned long)(((Model0_phys_addr_t)((unsigned long)((Model0_page) - ((struct Model0_page *)(0xffffea0000000000UL)))) << 12))+((unsigned long)(0xffff880000000000UL))));
}
extern void *Model0_page_rmapping(struct Model0_page *Model0_page);
extern struct Model0_anon_vma *Model0_page_anon_vma(struct Model0_page *Model0_page);
extern struct Model0_address_space *Model0_page_mapping(struct Model0_page *Model0_page);

extern struct Model0_address_space *Model0___page_file_mapping(struct Model0_page *);

static inline __attribute__((no_instrument_function))
struct Model0_address_space *Model0_page_file_mapping(struct Model0_page *Model0_page)
{
 if (__builtin_expect(!!(Model0_PageSwapCache(Model0_page)), 0))
  return Model0___page_file_mapping(Model0_page);

 return Model0_page->Model0_mapping;
}

/*
 * Return the pagecache index of the passed page.  Regular pagecache pages
 * use ->index whereas swapcache pages use ->private
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_page_index(struct Model0_page *Model0_page)
{
 if (__builtin_expect(!!(Model0_PageSwapCache(Model0_page)), 0))
  return ((Model0_page)->Model0_private);
 return Model0_page->Model0_index;
}

extern unsigned long Model0___page_file_index(struct Model0_page *Model0_page);

/*
 * Return the file index of the page. Regular pagecache pages use ->index
 * whereas swapcache pages use swp_offset(->private)
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_page_file_index(struct Model0_page *Model0_page)
{
 if (__builtin_expect(!!(Model0_PageSwapCache(Model0_page)), 0))
  return Model0___page_file_index(Model0_page);

 return Model0_page->Model0_index;
}

bool Model0_page_mapped(struct Model0_page *Model0_page);
struct Model0_address_space *Model0_page_mapping(struct Model0_page *Model0_page);

/*
 * Return true only if the page has been allocated with
 * ALLOC_NO_WATERMARKS and the low watermark was not
 * met implying that the system is under some pressure.
 */
static inline __attribute__((no_instrument_function)) bool Model0_page_is_pfmemalloc(struct Model0_page *Model0_page)
{
 /*
	 * Page index cannot be this large so this must be
	 * a pfmemalloc page.
	 */
 return Model0_page->Model0_index == -1UL;
}

/*
 * Only to be called by the page allocator on a freshly allocated
 * page.
 */
static inline __attribute__((no_instrument_function)) void Model0_set_page_pfmemalloc(struct Model0_page *Model0_page)
{
 Model0_page->Model0_index = -1UL;
}

static inline __attribute__((no_instrument_function)) void Model0_clear_page_pfmemalloc(struct Model0_page *Model0_page)
{
 Model0_page->Model0_index = 0;
}

/*
 * Different kinds of faults, as returned by handle_mm_fault().
 * Used to decide whether a process gets delivered SIGBUS or
 * just gets major/minor fault counters bumped up.
 */
/* Encode hstate index for a hwpoisoned large page */



/*
 * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.
 */
extern void Model0_pagefault_out_of_memory(void);



/*
 * Flags passed to show_mem() and show_free_areas() to suppress output in
 * various contexts.
 */


extern void Model0_show_free_areas(unsigned int Model0_flags);
extern bool Model0_skip_free_areas_node(unsigned int Model0_flags, int Model0_nid);

int Model0_shmem_zero_setup(struct Model0_vm_area_struct *);

bool Model0_shmem_mapping(struct Model0_address_space *Model0_mapping);







extern bool Model0_can_do_mlock(void);
extern int Model0_user_shm_lock(Model0_size_t, struct Model0_user_struct *);
extern void Model0_user_shm_unlock(Model0_size_t, struct Model0_user_struct *);

/*
 * Parameter block passed down to zap_pte_range in exceptional cases.
 */
struct Model0_zap_details {
 struct Model0_address_space *Model0_check_mapping; /* Check page->mapping if set */
 unsigned long Model0_first_index; /* Lowest page->index to unmap */
 unsigned long Model0_last_index; /* Highest page->index to unmap */
 bool Model0_ignore_dirty; /* Ignore dirty pages */
 bool Model0_check_swap_entries; /* Check also swap entries */
};

struct Model0_page *Model0_vm_normal_page(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr,
  Model0_pte_t Model0_pte);
struct Model0_page *Model0_vm_normal_page_pmd(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr,
    Model0_pmd_t Model0_pmd);

int Model0_zap_vma_ptes(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_address,
  unsigned long Model0_size);
void Model0_zap_page_range(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_address,
  unsigned long Model0_size, struct Model0_zap_details *);
void Model0_unmap_vmas(struct Model0_mmu_gather *Model0_tlb, struct Model0_vm_area_struct *Model0_start_vma,
  unsigned long Model0_start, unsigned long Model0_end);

/**
 * mm_walk - callbacks for walk_page_range
 * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry
 *	       this handler is required to be able to handle
 *	       pmd_trans_huge() pmds.  They may simply choose to
 *	       split_huge_page() instead of handling it explicitly.
 * @pte_entry: if set, called for each non-empty PTE (4th-level) entry
 * @pte_hole: if set, called for each hole at all levels
 * @hugetlb_entry: if set, called for each hugetlb entry
 * @test_walk: caller specific callback function to determine whether
 *             we walk over the current vma or not. A positive returned
 *             value means "do page table walk over the current vma,"
 *             and a negative one means "abort current page table walk
 *             right now." 0 means "skip the current vma."
 * @mm:        mm_struct representing the target process of page table walk
 * @vma:       vma currently walked (NULL if walking outside vmas)
 * @private:   private data for callbacks' usage
 *
 * (see the comment on walk_page_range() for more details)
 */
struct Model0_mm_walk {
 int (*Model0_pmd_entry)(Model0_pmd_t *Model0_pmd, unsigned long Model0_addr,
    unsigned long Model0_next, struct Model0_mm_walk *Model0_walk);
 int (*Model0_pte_entry)(Model0_pte_t *Model0_pte, unsigned long Model0_addr,
    unsigned long Model0_next, struct Model0_mm_walk *Model0_walk);
 int (*Model0_pte_hole)(unsigned long Model0_addr, unsigned long Model0_next,
   struct Model0_mm_walk *Model0_walk);
 int (*Model0_hugetlb_entry)(Model0_pte_t *Model0_pte, unsigned long Model0_hmask,
        unsigned long Model0_addr, unsigned long Model0_next,
        struct Model0_mm_walk *Model0_walk);
 int (*Model0_test_walk)(unsigned long Model0_addr, unsigned long Model0_next,
   struct Model0_mm_walk *Model0_walk);
 struct Model0_mm_struct *Model0_mm;
 struct Model0_vm_area_struct *Model0_vma;
 void *Model0_private;
};

int Model0_walk_page_range(unsigned long Model0_addr, unsigned long Model0_end,
  struct Model0_mm_walk *Model0_walk);
int Model0_walk_page_vma(struct Model0_vm_area_struct *Model0_vma, struct Model0_mm_walk *Model0_walk);
void Model0_free_pgd_range(struct Model0_mmu_gather *Model0_tlb, unsigned long Model0_addr,
  unsigned long Model0_end, unsigned long Model0_floor, unsigned long Model0_ceiling);
int Model0_copy_page_range(struct Model0_mm_struct *Model0_dst, struct Model0_mm_struct *Model0_src,
   struct Model0_vm_area_struct *Model0_vma);
void Model0_unmap_mapping_range(struct Model0_address_space *Model0_mapping,
  Model0_loff_t const Model0_holebegin, Model0_loff_t const Model0_holelen, int Model0_even_cows);
int Model0_follow_pfn(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_address,
 unsigned long *Model0_pfn);
int Model0_follow_phys(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_address,
  unsigned int Model0_flags, unsigned long *Model0_prot, Model0_resource_size_t *Model0_phys);
int Model0_generic_access_phys(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr,
   void *Model0_buf, int Model0_len, int Model0_write);

static inline __attribute__((no_instrument_function)) void Model0_unmap_shared_mapping_range(struct Model0_address_space *Model0_mapping,
  Model0_loff_t const Model0_holebegin, Model0_loff_t const Model0_holelen)
{
 Model0_unmap_mapping_range(Model0_mapping, Model0_holebegin, Model0_holelen, 0);
}

extern void Model0_truncate_pagecache(struct Model0_inode *Model0_inode, Model0_loff_t Model0_new);
extern void Model0_truncate_setsize(struct Model0_inode *Model0_inode, Model0_loff_t Model0_newsize);
void Model0_pagecache_isize_extended(struct Model0_inode *Model0_inode, Model0_loff_t Model0_from, Model0_loff_t Model0_to);
void Model0_truncate_pagecache_range(struct Model0_inode *Model0_inode, Model0_loff_t Model0_offset, Model0_loff_t Model0_end);
int Model0_truncate_inode_page(struct Model0_address_space *Model0_mapping, struct Model0_page *Model0_page);
int Model0_generic_error_remove_page(struct Model0_address_space *Model0_mapping, struct Model0_page *Model0_page);
int Model0_invalidate_inode_page(struct Model0_page *Model0_page);


extern int Model0_handle_mm_fault(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_address,
  unsigned int Model0_flags);
extern int Model0_fixup_user_fault(struct Model0_task_struct *Model0_tsk, struct Model0_mm_struct *Model0_mm,
       unsigned long Model0_address, unsigned int Model0_fault_flags,
       bool *Model0_unlocked);
extern int Model0_access_process_vm(struct Model0_task_struct *Model0_tsk, unsigned long Model0_addr, void *Model0_buf, int Model0_len, int Model0_write);
extern int Model0_access_remote_vm(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr,
  void *Model0_buf, int Model0_len, int Model0_write);

long Model0___get_user_pages(struct Model0_task_struct *Model0_tsk, struct Model0_mm_struct *Model0_mm,
        unsigned long Model0_start, unsigned long Model0_nr_pages,
        unsigned int Model0_foll_flags, struct Model0_page **Model0_pages,
        struct Model0_vm_area_struct **Model0_vmas, int *Model0_nonblocking);
long Model0_get_user_pages_remote(struct Model0_task_struct *Model0_tsk, struct Model0_mm_struct *Model0_mm,
       unsigned long Model0_start, unsigned long Model0_nr_pages,
       int Model0_write, int Model0_force, struct Model0_page **Model0_pages,
       struct Model0_vm_area_struct **Model0_vmas);
long Model0_get_user_pages(unsigned long Model0_start, unsigned long Model0_nr_pages,
       int Model0_write, int Model0_force, struct Model0_page **Model0_pages,
       struct Model0_vm_area_struct **Model0_vmas);
long Model0_get_user_pages_locked(unsigned long Model0_start, unsigned long Model0_nr_pages,
      int Model0_write, int Model0_force, struct Model0_page **Model0_pages, int *Model0_locked);
long Model0___get_user_pages_unlocked(struct Model0_task_struct *Model0_tsk, struct Model0_mm_struct *Model0_mm,
          unsigned long Model0_start, unsigned long Model0_nr_pages,
          int Model0_write, int Model0_force, struct Model0_page **Model0_pages,
          unsigned int Model0_gup_flags);
long Model0_get_user_pages_unlocked(unsigned long Model0_start, unsigned long Model0_nr_pages,
      int Model0_write, int Model0_force, struct Model0_page **Model0_pages);
int Model0_get_user_pages_fast(unsigned long Model0_start, int Model0_nr_pages, int Model0_write,
   struct Model0_page **Model0_pages);

/* Container for pinned pfns / pages */
struct Model0_frame_vector {
 unsigned int Model0_nr_allocated; /* Number of frames we have space for */
 unsigned int Model0_nr_frames; /* Number of frames stored in ptrs array */
 bool Model0_got_ref; /* Did we pin pages by getting page ref? */
 bool Model0_is_pfns; /* Does array contain pages or pfns? */
 void *Model0_ptrs[0]; /* Array of pinned pfns / pages. Use
				 * pfns_vector_pages() or pfns_vector_pfns()
				 * for access */
};

struct Model0_frame_vector *Model0_frame_vector_create(unsigned int Model0_nr_frames);
void Model0_frame_vector_destroy(struct Model0_frame_vector *Model0_vec);
int Model0_get_vaddr_frames(unsigned long Model0_start, unsigned int Model0_nr_pfns,
       bool Model0_write, bool Model0_force, struct Model0_frame_vector *Model0_vec);
void Model0_put_vaddr_frames(struct Model0_frame_vector *Model0_vec);
int Model0_frame_vector_to_pages(struct Model0_frame_vector *Model0_vec);
void Model0_frame_vector_to_pfns(struct Model0_frame_vector *Model0_vec);

static inline __attribute__((no_instrument_function)) unsigned int Model0_frame_vector_count(struct Model0_frame_vector *Model0_vec)
{
 return Model0_vec->Model0_nr_frames;
}

static inline __attribute__((no_instrument_function)) struct Model0_page **Model0_frame_vector_pages(struct Model0_frame_vector *Model0_vec)
{
 if (Model0_vec->Model0_is_pfns) {
  int err = Model0_frame_vector_to_pages(Model0_vec);

  if (err)
   return Model0_ERR_PTR(err);
 }
 return (struct Model0_page **)(Model0_vec->Model0_ptrs);
}

static inline __attribute__((no_instrument_function)) unsigned long *Model0_frame_vector_pfns(struct Model0_frame_vector *Model0_vec)
{
 if (!Model0_vec->Model0_is_pfns)
  Model0_frame_vector_to_pfns(Model0_vec);
 return (unsigned long *)(Model0_vec->Model0_ptrs);
}

struct Model0_kvec;
int Model0_get_kernel_pages(const struct Model0_kvec *Model0_iov, int Model0_nr_pages, int Model0_write,
   struct Model0_page **Model0_pages);
int Model0_get_kernel_page(unsigned long Model0_start, int Model0_write, struct Model0_page **Model0_pages);
struct Model0_page *Model0_get_dump_page(unsigned long Model0_addr);

extern int Model0_try_to_release_page(struct Model0_page * Model0_page, Model0_gfp_t Model0_gfp_mask);
extern void Model0_do_invalidatepage(struct Model0_page *Model0_page, unsigned int Model0_offset,
         unsigned int Model0_length);

int Model0___set_page_dirty_nobuffers(struct Model0_page *Model0_page);
int Model0___set_page_dirty_no_writeback(struct Model0_page *Model0_page);
int Model0_redirty_page_for_writepage(struct Model0_writeback_control *Model0_wbc,
    struct Model0_page *Model0_page);
void Model0_account_page_dirtied(struct Model0_page *Model0_page, struct Model0_address_space *Model0_mapping);
void Model0_account_page_cleaned(struct Model0_page *Model0_page, struct Model0_address_space *Model0_mapping,
     struct Model0_bdi_writeback *Model0_wb);
int Model0_set_page_dirty(struct Model0_page *Model0_page);
int Model0_set_page_dirty_lock(struct Model0_page *Model0_page);
void Model0_cancel_dirty_page(struct Model0_page *Model0_page);
int Model0_clear_page_dirty_for_io(struct Model0_page *Model0_page);

int Model0_get_cmdline(struct Model0_task_struct *Model0_task, char *Model0_buffer, int Model0_buflen);

/* Is the vma a continuation of the stack vma above it? */
static inline __attribute__((no_instrument_function)) int Model0_vma_growsdown(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr)
{
 return Model0_vma && (Model0_vma->Model0_vm_end == Model0_addr) && (Model0_vma->Model0_vm_flags & 0x00000100);
}

static inline __attribute__((no_instrument_function)) bool Model0_vma_is_anonymous(struct Model0_vm_area_struct *Model0_vma)
{
 return !Model0_vma->Model0_vm_ops;
}

static inline __attribute__((no_instrument_function)) int Model0_stack_guard_page_start(struct Model0_vm_area_struct *Model0_vma,
          unsigned long Model0_addr)
{
 return (Model0_vma->Model0_vm_flags & 0x00000100) &&
  (Model0_vma->Model0_vm_start == Model0_addr) &&
  !Model0_vma_growsdown(Model0_vma->Model0_vm_prev, Model0_addr);
}

/* Is the vma a continuation of the stack vma below it? */
static inline __attribute__((no_instrument_function)) int Model0_vma_growsup(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr)
{
 return Model0_vma && (Model0_vma->Model0_vm_start == Model0_addr) && (Model0_vma->Model0_vm_flags & 0x00000000);
}

static inline __attribute__((no_instrument_function)) int Model0_stack_guard_page_end(struct Model0_vm_area_struct *Model0_vma,
        unsigned long Model0_addr)
{
 return (Model0_vma->Model0_vm_flags & 0x00000000) &&
  (Model0_vma->Model0_vm_end == Model0_addr) &&
  !Model0_vma_growsup(Model0_vma->Model0_vm_next, Model0_addr);
}

int Model0_vma_is_stack_for_task(struct Model0_vm_area_struct *Model0_vma, struct Model0_task_struct *Model0_t);

extern unsigned long Model0_move_page_tables(struct Model0_vm_area_struct *Model0_vma,
  unsigned long Model0_old_addr, struct Model0_vm_area_struct *Model0_new_vma,
  unsigned long Model0_new_addr, unsigned long Model0_len,
  bool Model0_need_rmap_locks);
extern unsigned long Model0_change_protection(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_start,
         unsigned long Model0_end, Model0_pgprot_t Model0_newprot,
         int Model0_dirty_accountable, int Model0_prot_numa);
extern int Model0_mprotect_fixup(struct Model0_vm_area_struct *Model0_vma,
     struct Model0_vm_area_struct **Model0_pprev, unsigned long Model0_start,
     unsigned long Model0_end, unsigned long Model0_newflags);

/*
 * doesn't attempt to fault and will return short.
 */
int Model0___get_user_pages_fast(unsigned long Model0_start, int Model0_nr_pages, int Model0_write,
     struct Model0_page **Model0_pages);
/*
 * per-process(per-mm_struct) statistics.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_get_mm_counter(struct Model0_mm_struct *Model0_mm, int Model0_member)
{
 long Model0_val = Model0_atomic_long_read(&Model0_mm->Model0_rss_stat.Model0_count[Model0_member]);


 /*
	 * counter is updated in asynchronous manner and may go to minus.
	 * But it's never be expected number for users.
	 */
 if (Model0_val < 0)
  Model0_val = 0;

 return (unsigned long)Model0_val;
}

static inline __attribute__((no_instrument_function)) void Model0_add_mm_counter(struct Model0_mm_struct *Model0_mm, int Model0_member, long Model0_value)
{
 Model0_atomic_long_add(Model0_value, &Model0_mm->Model0_rss_stat.Model0_count[Model0_member]);
}

static inline __attribute__((no_instrument_function)) void Model0_inc_mm_counter(struct Model0_mm_struct *Model0_mm, int Model0_member)
{
 Model0_atomic_long_inc(&Model0_mm->Model0_rss_stat.Model0_count[Model0_member]);
}

static inline __attribute__((no_instrument_function)) void Model0_dec_mm_counter(struct Model0_mm_struct *Model0_mm, int Model0_member)
{
 Model0_atomic_long_dec(&Model0_mm->Model0_rss_stat.Model0_count[Model0_member]);
}

/* Optimized variant when page is already known not to be PageAnon */
static inline __attribute__((no_instrument_function)) int Model0_mm_counter_file(struct Model0_page *Model0_page)
{
 if (Model0_PageSwapBacked(Model0_page))
  return Model0_MM_SHMEMPAGES;
 return Model0_MM_FILEPAGES;
}

static inline __attribute__((no_instrument_function)) int Model0_mm_counter(struct Model0_page *Model0_page)
{
 if (Model0_PageAnon(Model0_page))
  return Model0_MM_ANONPAGES;
 return Model0_mm_counter_file(Model0_page);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_get_mm_rss(struct Model0_mm_struct *Model0_mm)
{
 return Model0_get_mm_counter(Model0_mm, Model0_MM_FILEPAGES) +
  Model0_get_mm_counter(Model0_mm, Model0_MM_ANONPAGES) +
  Model0_get_mm_counter(Model0_mm, Model0_MM_SHMEMPAGES);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_get_mm_hiwater_rss(struct Model0_mm_struct *Model0_mm)
{
 return ({ typeof(Model0_mm->Model0_hiwater_rss) Model0__max1 = (Model0_mm->Model0_hiwater_rss); typeof(Model0_get_mm_rss(Model0_mm)) Model0__max2 = (Model0_get_mm_rss(Model0_mm)); (void) (&Model0__max1 == &Model0__max2); Model0__max1 > Model0__max2 ? Model0__max1 : Model0__max2; });
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_get_mm_hiwater_vm(struct Model0_mm_struct *Model0_mm)
{
 return ({ typeof(Model0_mm->Model0_hiwater_vm) Model0__max1 = (Model0_mm->Model0_hiwater_vm); typeof(Model0_mm->Model0_total_vm) Model0__max2 = (Model0_mm->Model0_total_vm); (void) (&Model0__max1 == &Model0__max2); Model0__max1 > Model0__max2 ? Model0__max1 : Model0__max2; });
}

static inline __attribute__((no_instrument_function)) void Model0_update_hiwater_rss(struct Model0_mm_struct *Model0_mm)
{
 unsigned long Model0__rss = Model0_get_mm_rss(Model0_mm);

 if ((Model0_mm)->Model0_hiwater_rss < Model0__rss)
  (Model0_mm)->Model0_hiwater_rss = Model0__rss;
}

static inline __attribute__((no_instrument_function)) void Model0_update_hiwater_vm(struct Model0_mm_struct *Model0_mm)
{
 if (Model0_mm->Model0_hiwater_vm < Model0_mm->Model0_total_vm)
  Model0_mm->Model0_hiwater_vm = Model0_mm->Model0_total_vm;
}

static inline __attribute__((no_instrument_function)) void Model0_reset_mm_hiwater_rss(struct Model0_mm_struct *Model0_mm)
{
 Model0_mm->Model0_hiwater_rss = Model0_get_mm_rss(Model0_mm);
}

static inline __attribute__((no_instrument_function)) void Model0_setmax_mm_hiwater_rss(unsigned long *Model0_maxrss,
      struct Model0_mm_struct *Model0_mm)
{
 unsigned long Model0_hiwater_rss = Model0_get_mm_hiwater_rss(Model0_mm);

 if (*Model0_maxrss < Model0_hiwater_rss)
  *Model0_maxrss = Model0_hiwater_rss;
}


void Model0_sync_mm_rss(struct Model0_mm_struct *Model0_mm);
int Model0_vma_wants_writenotify(struct Model0_vm_area_struct *Model0_vma);

extern Model0_pte_t *Model0___get_locked_pte(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr,
          Model0_spinlock_t **Model0_ptl);
static inline __attribute__((no_instrument_function)) Model0_pte_t *Model0_get_locked_pte(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr,
        Model0_spinlock_t **Model0_ptl)
{
 Model0_pte_t *Model0_ptep;
 (Model0_ptep = Model0___get_locked_pte(Model0_mm, Model0_addr, Model0_ptl));
 return Model0_ptep;
}
int Model0___pud_alloc(struct Model0_mm_struct *Model0_mm, Model0_pgd_t *Model0_pgd, unsigned long Model0_address);
int Model0___pmd_alloc(struct Model0_mm_struct *Model0_mm, Model0_pud_t *Model0_pud, unsigned long Model0_address);

static inline __attribute__((no_instrument_function)) void Model0_mm_nr_pmds_init(struct Model0_mm_struct *Model0_mm)
{
 Model0_atomic_long_set(&Model0_mm->Model0_nr_pmds, 0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_mm_nr_pmds(struct Model0_mm_struct *Model0_mm)
{
 return Model0_atomic_long_read(&Model0_mm->Model0_nr_pmds);
}

static inline __attribute__((no_instrument_function)) void Model0_mm_inc_nr_pmds(struct Model0_mm_struct *Model0_mm)
{
 Model0_atomic_long_inc(&Model0_mm->Model0_nr_pmds);
}

static inline __attribute__((no_instrument_function)) void Model0_mm_dec_nr_pmds(struct Model0_mm_struct *Model0_mm)
{
 Model0_atomic_long_dec(&Model0_mm->Model0_nr_pmds);
}


int Model0___pte_alloc(struct Model0_mm_struct *Model0_mm, Model0_pmd_t *Model0_pmd, unsigned long Model0_address);
int Model0___pte_alloc_kernel(Model0_pmd_t *Model0_pmd, unsigned long Model0_address);

/*
 * The following ifdef needed to get the 4level-fixup.h header to work.
 * Remove it when 4level-fixup.h has been removed.
 */

static inline __attribute__((no_instrument_function)) Model0_pud_t *Model0_pud_alloc(struct Model0_mm_struct *Model0_mm, Model0_pgd_t *Model0_pgd, unsigned long Model0_address)
{
 return (__builtin_expect(!!(Model0_pgd_none(*Model0_pgd)), 0) && Model0___pud_alloc(Model0_mm, Model0_pgd, Model0_address))?
  ((void *)0): Model0_pud_offset(Model0_pgd, Model0_address);
}

static inline __attribute__((no_instrument_function)) Model0_pmd_t *Model0_pmd_alloc(struct Model0_mm_struct *Model0_mm, Model0_pud_t *Model0_pud, unsigned long Model0_address)
{
 return (__builtin_expect(!!(Model0_pud_none(*Model0_pud)), 0) && Model0___pmd_alloc(Model0_mm, Model0_pud, Model0_address))?
  ((void *)0): Model0_pmd_offset(Model0_pud, Model0_address);
}
static inline __attribute__((no_instrument_function)) void Model0_ptlock_cache_init(void)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_ptlock_alloc(struct Model0_page *Model0_page)
{
 return true;
}

static inline __attribute__((no_instrument_function)) void Model0_ptlock_free(struct Model0_page *Model0_page)
{
}

static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_ptlock_ptr(struct Model0_page *Model0_page)
{
 return &Model0_page->Model0_ptl;
}


static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_pte_lockptr(struct Model0_mm_struct *Model0_mm, Model0_pmd_t *Model0_pmd)
{
 return Model0_ptlock_ptr((((struct Model0_page *)(0xffffea0000000000UL)) + ((Model0_native_pmd_val(*Model0_pmd) & Model0_pmd_pfn_mask(*Model0_pmd)) >> 12)));
}

static inline __attribute__((no_instrument_function)) bool Model0_ptlock_init(struct Model0_page *Model0_page)
{
 /*
	 * prep_new_page() initialize page->private (and therefore page->ptl)
	 * with 0. Make sure nobody took it in use in between.
	 *
	 * It can happen if arch try to use slab for page table allocation:
	 * slab code uses page->slab_cache, which share storage with page->ptl.
	 */
 ((void)(sizeof(( long)(*(unsigned long *)&Model0_page->Model0_ptl))));
 if (!Model0_ptlock_alloc(Model0_page))
  return false;
 do { Model0_spinlock_check(Model0_ptlock_ptr(Model0_page)); do { *(&(Model0_ptlock_ptr(Model0_page))->Model0_rlock) = (Model0_raw_spinlock_t) { .Model0_raw_lock = { { (0) } }, }; } while (0); } while (0);
 return true;
}

/* Reset page->mapping so free_pages_check won't complain. */
static inline __attribute__((no_instrument_function)) void Model0_pte_lock_deinit(struct Model0_page *Model0_page)
{
 Model0_page->Model0_mapping = ((void *)0);
 Model0_ptlock_free(Model0_page);
}
static inline __attribute__((no_instrument_function)) void Model0_pgtable_init(void)
{
 Model0_ptlock_cache_init();
 do { } while (0);
}

static inline __attribute__((no_instrument_function)) bool Model0_pgtable_page_ctor(struct Model0_page *Model0_page)
{
 if (!Model0_ptlock_init(Model0_page))
  return false;
 Model0_inc_zone_page_state(Model0_page, Model0_NR_PAGETABLE);
 return true;
}

static inline __attribute__((no_instrument_function)) void Model0_pgtable_page_dtor(struct Model0_page *Model0_page)
{
 Model0_pte_lock_deinit(Model0_page);
 Model0_dec_zone_page_state(Model0_page, Model0_NR_PAGETABLE);
}
static struct Model0_page *Model0_pmd_to_page(Model0_pmd_t *Model0_pmd)
{
 unsigned long Model0_mask = ~(512 * sizeof(Model0_pmd_t) - 1);
 return (((struct Model0_page *)(0xffffea0000000000UL)) + (Model0___phys_addr_nodebug((unsigned long)((void *)((unsigned long) Model0_pmd & Model0_mask))) >> 12));
}

static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_pmd_lockptr(struct Model0_mm_struct *Model0_mm, Model0_pmd_t *Model0_pmd)
{
 return Model0_ptlock_ptr(Model0_pmd_to_page(Model0_pmd));
}

static inline __attribute__((no_instrument_function)) bool Model0_pgtable_pmd_page_ctor(struct Model0_page *Model0_page)
{



 return Model0_ptlock_init(Model0_page);
}

static inline __attribute__((no_instrument_function)) void Model0_pgtable_pmd_page_dtor(struct Model0_page *Model0_page)
{



 Model0_ptlock_free(Model0_page);
}
static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_pmd_lock(struct Model0_mm_struct *Model0_mm, Model0_pmd_t *Model0_pmd)
{
 Model0_spinlock_t *Model0_ptl = Model0_pmd_lockptr(Model0_mm, Model0_pmd);
 Model0_spin_lock(Model0_ptl);
 return Model0_ptl;
}

extern void Model0_free_area_init(unsigned long * Model0_zones_size);
extern void Model0_free_area_init_node(int Model0_nid, unsigned long * Model0_zones_size,
  unsigned long Model0_zone_start_pfn, unsigned long *Model0_zholes_size);
extern void Model0_free_initmem(void);

/*
 * Free reserved pages within range [PAGE_ALIGN(start), end & PAGE_MASK)
 * into the buddy system. The freed pages will be poisoned with pattern
 * "poison" if it's within range [0, UCHAR_MAX].
 * Return pages freed into the buddy system.
 */
extern unsigned long Model0_free_reserved_area(void *Model0_start, void *Model0_end,
     int Model0_poison, char *Model0_s);
extern void Model0_adjust_managed_page_count(struct Model0_page *Model0_page, long Model0_count);
extern void Model0_mem_init_print_info(const char *Model0_str);

extern void Model0_reserve_bootmem_region(Model0_phys_addr_t Model0_start, Model0_phys_addr_t Model0_end);

/* Free the reserved page into the buddy system, so it gets managed. */
static inline __attribute__((no_instrument_function)) void Model0___free_reserved_page(struct Model0_page *Model0_page)
{
 Model0_ClearPageReserved(Model0_page);
 Model0_init_page_count(Model0_page);
 Model0___free_pages((Model0_page), 0);
}

static inline __attribute__((no_instrument_function)) void Model0_free_reserved_page(struct Model0_page *Model0_page)
{
 Model0___free_reserved_page(Model0_page);
 Model0_adjust_managed_page_count(Model0_page, 1);
}

static inline __attribute__((no_instrument_function)) void Model0_mark_page_reserved(struct Model0_page *Model0_page)
{
 Model0_SetPageReserved(Model0_page);
 Model0_adjust_managed_page_count(Model0_page, -1);
}

/*
 * Default method to free all the __init memory into the buddy system.
 * The freed pages will be poisoned with pattern "poison" if it's within
 * range [0, UCHAR_MAX].
 * Return pages freed into the buddy system.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_free_initmem_default(int Model0_poison)
{
 extern char Model0___init_begin[], Model0___init_end[];

 return Model0_free_reserved_area(&Model0___init_begin, &Model0___init_end,
      Model0_poison, "unused kernel");
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_get_num_physpages(void)
{
 int Model0_nid;
 unsigned long Model0_phys_pages = 0;

 for (((Model0_nid)) = Model0___first_node(&(Model0_node_states[Model0_N_ONLINE])); ((Model0_nid)) < (1 << 6); ((Model0_nid)) = Model0___next_node((((Model0_nid))), &((Model0_node_states[Model0_N_ONLINE]))))
  Model0_phys_pages += ((Model0_node_data[Model0_nid])->Model0_node_present_pages);

 return Model0_phys_pages;
}


/*
 * With CONFIG_HAVE_MEMBLOCK_NODE_MAP set, an architecture may initialise its
 * zones, allocate the backing mem_map and account for memory holes in a more
 * architecture independent manner. This is a substitute for creating the
 * zone_sizes[] and zholes_size[] arrays and passing them to
 * free_area_init_node()
 *
 * An architecture is expected to register range of page frames backed by
 * physical memory with memblock_add[_node]() before calling
 * free_area_init_nodes() passing in the PFN each zone ends at. At a basic
 * usage, an architecture is expected to do something like
 *
 * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,
 * 							 max_highmem_pfn};
 * for_each_valid_physical_page_range()
 * 	memblock_add_node(base, size, nid)
 * free_area_init_nodes(max_zone_pfns);
 *
 * free_bootmem_with_active_regions() calls free_bootmem_node() for each
 * registered physical page range.  Similarly
 * sparse_memory_present_with_active_regions() calls memory_present() for
 * each range when SPARSEMEM is enabled.
 *
 * See mm/page_alloc.c for more information on each function exposed by
 * CONFIG_HAVE_MEMBLOCK_NODE_MAP.
 */
extern void Model0_free_area_init_nodes(unsigned long *Model0_max_zone_pfn);
unsigned long Model0_node_map_pfn_alignment(void);
unsigned long Model0___absent_pages_in_range(int Model0_nid, unsigned long Model0_start_pfn,
      unsigned long Model0_end_pfn);
extern unsigned long Model0_absent_pages_in_range(unsigned long Model0_start_pfn,
      unsigned long Model0_end_pfn);
extern void Model0_get_pfn_range_for_nid(unsigned int Model0_nid,
   unsigned long *Model0_start_pfn, unsigned long *Model0_end_pfn);
extern unsigned long Model0_find_min_pfn_with_active_regions(void);
extern void Model0_free_bootmem_with_active_regions(int Model0_nid,
      unsigned long Model0_max_low_pfn);
extern void Model0_sparse_memory_present_with_active_regions(int Model0_nid);
/* please see mm/page_alloc.c */
extern int __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model0_early_pfn_to_nid(unsigned long Model0_pfn);
/* there is a per-arch backend function. */
extern int __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model0___early_pfn_to_nid(unsigned long Model0_pfn,
     struct Model0_mminit_pfnnid_cache *Model0_state);


extern void Model0_set_dma_reserve(unsigned long Model0_new_dma_reserve);
extern void Model0_memmap_init_zone(unsigned long, int, unsigned long,
    unsigned long, enum Model0_memmap_context);
extern void Model0_setup_per_zone_wmarks(void);
extern int __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model0_init_per_zone_wmark_min(void);
extern void Model0_mem_init(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_mmap_init(void);
extern void Model0_show_mem(unsigned int Model0_flags);
extern long Model0_si_mem_available(void);
extern void Model0_si_meminfo(struct Model0_sysinfo * Model0_val);
extern void Model0_si_meminfo_node(struct Model0_sysinfo *Model0_val, int Model0_nid);

extern __attribute__((format(printf, 3, 4)))
void Model0_warn_alloc_failed(Model0_gfp_t Model0_gfp_mask, unsigned int Model0_order,
  const char *Model0_fmt, ...);

extern void Model0_setup_per_cpu_pageset(void);

extern void Model0_zone_pcp_update(struct Model0_zone *Model0_zone);
extern void Model0_zone_pcp_reset(struct Model0_zone *Model0_zone);

/* page_alloc.c */
extern int Model0_min_free_kbytes;
extern int Model0_watermark_scale_factor;

/* nommu.c */
extern Model0_atomic_long_t Model0_mmap_pages_allocated;
extern int Model0_nommu_shrink_inode_mappings(struct Model0_inode *, Model0_size_t, Model0_size_t);

/* interval_tree.c */
void Model0_vma_interval_tree_insert(struct Model0_vm_area_struct *Model0_node,
         struct Model0_rb_root *Model0_root);
void Model0_vma_interval_tree_insert_after(struct Model0_vm_area_struct *Model0_node,
        struct Model0_vm_area_struct *Model0_prev,
        struct Model0_rb_root *Model0_root);
void Model0_vma_interval_tree_remove(struct Model0_vm_area_struct *Model0_node,
         struct Model0_rb_root *Model0_root);
struct Model0_vm_area_struct *Model0_vma_interval_tree_iter_first(struct Model0_rb_root *Model0_root,
    unsigned long Model0_start, unsigned long Model0_last);
struct Model0_vm_area_struct *Model0_vma_interval_tree_iter_next(struct Model0_vm_area_struct *Model0_node,
    unsigned long Model0_start, unsigned long Model0_last);





void Model0_anon_vma_interval_tree_insert(struct Model0_anon_vma_chain *Model0_node,
       struct Model0_rb_root *Model0_root);
void Model0_anon_vma_interval_tree_remove(struct Model0_anon_vma_chain *Model0_node,
       struct Model0_rb_root *Model0_root);
struct Model0_anon_vma_chain *Model0_anon_vma_interval_tree_iter_first(
 struct Model0_rb_root *Model0_root, unsigned long Model0_start, unsigned long Model0_last);
struct Model0_anon_vma_chain *Model0_anon_vma_interval_tree_iter_next(
 struct Model0_anon_vma_chain *Model0_node, unsigned long Model0_start, unsigned long Model0_last);
/* mmap.c */
extern int Model0___vm_enough_memory(struct Model0_mm_struct *Model0_mm, long Model0_pages, int Model0_cap_sys_admin);
extern int Model0_vma_adjust(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_start,
 unsigned long Model0_end, unsigned long Model0_pgoff, struct Model0_vm_area_struct *Model0_insert);
extern struct Model0_vm_area_struct *Model0_vma_merge(struct Model0_mm_struct *,
 struct Model0_vm_area_struct *Model0_prev, unsigned long Model0_addr, unsigned long Model0_end,
 unsigned long Model0_vm_flags, struct Model0_anon_vma *, struct Model0_file *, unsigned long,
 struct Model0_mempolicy *, struct Model0_vm_userfaultfd_ctx);
extern struct Model0_anon_vma *Model0_find_mergeable_anon_vma(struct Model0_vm_area_struct *);
extern int Model0_split_vma(struct Model0_mm_struct *,
 struct Model0_vm_area_struct *, unsigned long Model0_addr, int Model0_new_below);
extern int Model0_insert_vm_struct(struct Model0_mm_struct *, struct Model0_vm_area_struct *);
extern void Model0___vma_link_rb(struct Model0_mm_struct *, struct Model0_vm_area_struct *,
 struct Model0_rb_node **, struct Model0_rb_node *);
extern void Model0_unlink_file_vma(struct Model0_vm_area_struct *);
extern struct Model0_vm_area_struct *Model0_copy_vma(struct Model0_vm_area_struct **,
 unsigned long Model0_addr, unsigned long Model0_len, unsigned long Model0_pgoff,
 bool *Model0_need_rmap_locks);
extern void Model0_exit_mmap(struct Model0_mm_struct *);

static inline __attribute__((no_instrument_function)) int Model0_check_data_rlimit(unsigned long Model0_rlim,
        unsigned long Model0_new,
        unsigned long Model0_start,
        unsigned long Model0_end_data,
        unsigned long Model0_start_data)
{
 if (Model0_rlim < (~0UL)) {
  if (((Model0_new - Model0_start) + (Model0_end_data - Model0_start_data)) > Model0_rlim)
   return -28;
 }

 return 0;
}

extern int Model0_mm_take_all_locks(struct Model0_mm_struct *Model0_mm);
extern void Model0_mm_drop_all_locks(struct Model0_mm_struct *Model0_mm);

extern void Model0_set_mm_exe_file(struct Model0_mm_struct *Model0_mm, struct Model0_file *Model0_new_exe_file);
extern struct Model0_file *Model0_get_mm_exe_file(struct Model0_mm_struct *Model0_mm);
extern struct Model0_file *Model0_get_task_exe_file(struct Model0_task_struct *Model0_task);

extern bool Model0_may_expand_vm(struct Model0_mm_struct *, Model0_vm_flags_t, unsigned long Model0_npages);
extern void Model0_vm_stat_account(struct Model0_mm_struct *, Model0_vm_flags_t, long Model0_npages);

extern struct Model0_vm_area_struct *Model0__install_special_mapping(struct Model0_mm_struct *Model0_mm,
       unsigned long Model0_addr, unsigned long Model0_len,
       unsigned long Model0_flags,
       const struct Model0_vm_special_mapping *Model0_spec);
/* This is an obsolete alternative to _install_special_mapping. */
extern int Model0_install_special_mapping(struct Model0_mm_struct *Model0_mm,
       unsigned long Model0_addr, unsigned long Model0_len,
       unsigned long Model0_flags, struct Model0_page **Model0_pages);

extern unsigned long Model0_get_unmapped_area(struct Model0_file *, unsigned long, unsigned long, unsigned long, unsigned long);

extern unsigned long Model0_mmap_region(struct Model0_file *Model0_file, unsigned long Model0_addr,
 unsigned long Model0_len, Model0_vm_flags_t Model0_vm_flags, unsigned long Model0_pgoff);
extern unsigned long Model0_do_mmap(struct Model0_file *Model0_file, unsigned long Model0_addr,
 unsigned long Model0_len, unsigned long Model0_prot, unsigned long Model0_flags,
 Model0_vm_flags_t Model0_vm_flags, unsigned long Model0_pgoff, unsigned long *Model0_populate);
extern int Model0_do_munmap(struct Model0_mm_struct *, unsigned long, Model0_size_t);

static inline __attribute__((no_instrument_function)) unsigned long
Model0_do_mmap_pgoff(struct Model0_file *Model0_file, unsigned long Model0_addr,
 unsigned long Model0_len, unsigned long Model0_prot, unsigned long Model0_flags,
 unsigned long Model0_pgoff, unsigned long *Model0_populate)
{
 return Model0_do_mmap(Model0_file, Model0_addr, Model0_len, Model0_prot, Model0_flags, 0, Model0_pgoff, Model0_populate);
}


extern int Model0___mm_populate(unsigned long Model0_addr, unsigned long Model0_len,
    int Model0_ignore_errors);
static inline __attribute__((no_instrument_function)) void Model0_mm_populate(unsigned long Model0_addr, unsigned long Model0_len)
{
 /* Ignore errors */
 (void) Model0___mm_populate(Model0_addr, Model0_len, 1);
}




/* These take the mm semaphore themselves */
extern int __attribute__((warn_unused_result)) Model0_vm_brk(unsigned long, unsigned long);
extern int Model0_vm_munmap(unsigned long, Model0_size_t);
extern unsigned long __attribute__((warn_unused_result)) Model0_vm_mmap(struct Model0_file *, unsigned long,
        unsigned long, unsigned long,
        unsigned long, unsigned long);

struct Model0_vm_unmapped_area_info {

 unsigned long Model0_flags;
 unsigned long Model0_length;
 unsigned long Model0_low_limit;
 unsigned long Model0_high_limit;
 unsigned long Model0_align_mask;
 unsigned long Model0_align_offset;
};

extern unsigned long Model0_unmapped_area(struct Model0_vm_unmapped_area_info *Model0_info);
extern unsigned long Model0_unmapped_area_topdown(struct Model0_vm_unmapped_area_info *Model0_info);

/*
 * Search for an unmapped address range.
 *
 * We are looking for a range that:
 * - does not intersect with any VMA;
 * - is contained within the [low_limit, high_limit) interval;
 * - is at least the desired size.
 * - satisfies (begin_addr & align_mask) == (align_offset & align_mask)
 */
static inline __attribute__((no_instrument_function)) unsigned long
Model0_vm_unmapped_area(struct Model0_vm_unmapped_area_info *Model0_info)
{
 if (Model0_info->Model0_flags & 1)
  return Model0_unmapped_area_topdown(Model0_info);
 else
  return Model0_unmapped_area(Model0_info);
}

/* truncate.c */
extern void Model0_truncate_inode_pages(struct Model0_address_space *, Model0_loff_t);
extern void Model0_truncate_inode_pages_range(struct Model0_address_space *,
           Model0_loff_t Model0_lstart, Model0_loff_t Model0_lend);
extern void Model0_truncate_inode_pages_final(struct Model0_address_space *);

/* generic vm_area_ops exported for stackable file systems */
extern int Model0_filemap_fault(struct Model0_vm_area_struct *, struct Model0_vm_fault *);
extern void Model0_filemap_map_pages(struct Model0_fault_env *Model0_fe,
  unsigned long Model0_start_pgoff, unsigned long Model0_end_pgoff);
extern int Model0_filemap_page_mkwrite(struct Model0_vm_area_struct *Model0_vma, struct Model0_vm_fault *Model0_vmf);

/* mm/page-writeback.c */
int Model0_write_one_page(struct Model0_page *Model0_page, int Model0_wait);
void Model0_task_dirty_inc(struct Model0_task_struct *Model0_tsk);

/* readahead.c */



int Model0_force_page_cache_readahead(struct Model0_address_space *Model0_mapping, struct Model0_file *Model0_filp,
   unsigned long Model0_offset, unsigned long Model0_nr_to_read);

void Model0_page_cache_sync_readahead(struct Model0_address_space *Model0_mapping,
          struct Model0_file_ra_state *Model0_ra,
          struct Model0_file *Model0_filp,
          unsigned long Model0_offset,
          unsigned long Model0_size);

void Model0_page_cache_async_readahead(struct Model0_address_space *Model0_mapping,
    struct Model0_file_ra_state *Model0_ra,
    struct Model0_file *Model0_filp,
    struct Model0_page *Model0_pg,
    unsigned long Model0_offset,
    unsigned long Model0_size);

/* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
extern int Model0_expand_stack(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_address);

/* CONFIG_STACK_GROWSUP still needs to to grow downwards at some places */
extern int Model0_expand_downwards(struct Model0_vm_area_struct *Model0_vma,
  unsigned long Model0_address);






/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */
extern struct Model0_vm_area_struct * Model0_find_vma(struct Model0_mm_struct * Model0_mm, unsigned long Model0_addr);
extern struct Model0_vm_area_struct * Model0_find_vma_prev(struct Model0_mm_struct * Model0_mm, unsigned long Model0_addr,
          struct Model0_vm_area_struct **Model0_pprev);

/* Look up the first VMA which intersects the interval start_addr..end_addr-1,
   NULL if none.  Assume start_addr < end_addr. */
static inline __attribute__((no_instrument_function)) struct Model0_vm_area_struct * Model0_find_vma_intersection(struct Model0_mm_struct * Model0_mm, unsigned long Model0_start_addr, unsigned long Model0_end_addr)
{
 struct Model0_vm_area_struct * Model0_vma = Model0_find_vma(Model0_mm,Model0_start_addr);

 if (Model0_vma && Model0_end_addr <= Model0_vma->Model0_vm_start)
  Model0_vma = ((void *)0);
 return Model0_vma;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_vma_pages(struct Model0_vm_area_struct *Model0_vma)
{
 return (Model0_vma->Model0_vm_end - Model0_vma->Model0_vm_start) >> 12;
}

/* Look up the first VMA which exactly match the interval vm_start ... vm_end */
static inline __attribute__((no_instrument_function)) struct Model0_vm_area_struct *Model0_find_exact_vma(struct Model0_mm_struct *Model0_mm,
    unsigned long Model0_vm_start, unsigned long Model0_vm_end)
{
 struct Model0_vm_area_struct *Model0_vma = Model0_find_vma(Model0_mm, Model0_vm_start);

 if (Model0_vma && (Model0_vma->Model0_vm_start != Model0_vm_start || Model0_vma->Model0_vm_end != Model0_vm_end))
  Model0_vma = ((void *)0);

 return Model0_vma;
}


Model0_pgprot_t Model0_vm_get_page_prot(unsigned long Model0_vm_flags);
void Model0_vma_set_page_prot(struct Model0_vm_area_struct *Model0_vma);
struct Model0_vm_area_struct *Model0_find_extend_vma(struct Model0_mm_struct *, unsigned long Model0_addr);
int Model0_remap_pfn_range(struct Model0_vm_area_struct *, unsigned long Model0_addr,
   unsigned long Model0_pfn, unsigned long Model0_size, Model0_pgprot_t);
int Model0_vm_insert_page(struct Model0_vm_area_struct *, unsigned long Model0_addr, struct Model0_page *);
int Model0_vm_insert_pfn(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr,
   unsigned long Model0_pfn);
int Model0_vm_insert_pfn_prot(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr,
   unsigned long Model0_pfn, Model0_pgprot_t Model0_pgprot);
int Model0_vm_insert_mixed(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_addr,
   Model0_pfn_t Model0_pfn);
int Model0_vm_iomap_memory(struct Model0_vm_area_struct *Model0_vma, Model0_phys_addr_t Model0_start, unsigned long Model0_len);


struct Model0_page *Model0_follow_page_mask(struct Model0_vm_area_struct *Model0_vma,
         unsigned long Model0_address, unsigned int Model0_foll_flags,
         unsigned int *Model0_page_mask);

static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_follow_page(struct Model0_vm_area_struct *Model0_vma,
  unsigned long Model0_address, unsigned int Model0_foll_flags)
{
 unsigned int Model0_unused_page_mask;
 return Model0_follow_page_mask(Model0_vma, Model0_address, Model0_foll_flags, &Model0_unused_page_mask);
}
typedef int (*Model0_pte_fn_t)(Model0_pte_t *Model0_pte, Model0_pgtable_t Model0_token, unsigned long Model0_addr,
   void *Model0_data);
extern int Model0_apply_to_page_range(struct Model0_mm_struct *Model0_mm, unsigned long Model0_address,
          unsigned long Model0_size, Model0_pte_fn_t Model0_fn, void *Model0_data);







static inline __attribute__((no_instrument_function)) bool Model0_page_poisoning_enabled(void) { return false; }
static inline __attribute__((no_instrument_function)) void Model0_kernel_poison_pages(struct Model0_page *Model0_page, int Model0_numpages,
     int Model0_enable) { }
static inline __attribute__((no_instrument_function)) bool Model0_page_is_poisoned(struct Model0_page *Model0_page) { return false; }
static inline __attribute__((no_instrument_function)) void
Model0_kernel_map_pages(struct Model0_page *Model0_page, int Model0_numpages, int Model0_enable) {}

static inline __attribute__((no_instrument_function)) bool Model0_kernel_page_present(struct Model0_page *Model0_page) { return true; }

static inline __attribute__((no_instrument_function)) bool Model0_debug_pagealloc_enabled(void)
{
 return false;
}



extern struct Model0_vm_area_struct *Model0_get_gate_vma(struct Model0_mm_struct *Model0_mm);
extern int Model0_in_gate_area_no_mm(unsigned long Model0_addr);
extern int Model0_in_gate_area(struct Model0_mm_struct *Model0_mm, unsigned long Model0_addr);
extern bool Model0_process_shares_mm(struct Model0_task_struct *Model0_p, struct Model0_mm_struct *Model0_mm);


extern int Model0_sysctl_drop_caches;
int Model0_drop_caches_sysctl_handler(struct Model0_ctl_table *, int,
     void *, Model0_size_t *, Model0_loff_t *);


void Model0_drop_slab(void);
void Model0_drop_slab_node(int Model0_nid);




extern int Model0_randomize_va_space;


const char * Model0_arch_vma_name(struct Model0_vm_area_struct *Model0_vma);
void Model0_print_vma_addr(char *Model0_prefix, unsigned long Model0_rip);

void Model0_sparse_mem_maps_populate_node(struct Model0_page **Model0_map_map,
       unsigned long Model0_pnum_begin,
       unsigned long Model0_pnum_end,
       unsigned long Model0_map_count,
       int Model0_nodeid);

struct Model0_page *Model0_sparse_mem_map_populate(unsigned long Model0_pnum, int Model0_nid);
Model0_pgd_t *Model0_vmemmap_pgd_populate(unsigned long Model0_addr, int Model0_node);
Model0_pud_t *Model0_vmemmap_pud_populate(Model0_pgd_t *Model0_pgd, unsigned long Model0_addr, int Model0_node);
Model0_pmd_t *Model0_vmemmap_pmd_populate(Model0_pud_t *Model0_pud, unsigned long Model0_addr, int Model0_node);
Model0_pte_t *Model0_vmemmap_pte_populate(Model0_pmd_t *Model0_pmd, unsigned long Model0_addr, int Model0_node);
void *Model0_vmemmap_alloc_block(unsigned long Model0_size, int Model0_node);
struct Model0_vmem_altmap;
void *Model0___vmemmap_alloc_block_buf(unsigned long Model0_size, int Model0_node,
  struct Model0_vmem_altmap *Model0_altmap);
static inline __attribute__((no_instrument_function)) void *Model0_vmemmap_alloc_block_buf(unsigned long Model0_size, int Model0_node)
{
 return Model0___vmemmap_alloc_block_buf(Model0_size, Model0_node, ((void *)0));
}

void Model0_vmemmap_verify(Model0_pte_t *, int, unsigned long, unsigned long);
int Model0_vmemmap_populate_basepages(unsigned long Model0_start, unsigned long Model0_end,
          int Model0_node);
int Model0_vmemmap_populate(unsigned long Model0_start, unsigned long Model0_end, int Model0_node);
void Model0_vmemmap_populate_print_last(void);



void Model0_register_page_bootmem_memmap(unsigned long Model0_section_nr, struct Model0_page *Model0_map,
      unsigned long Model0_size);

enum Model0_mf_flags {
 Model0_MF_COUNT_INCREASED = 1 << 0,
 Model0_MF_ACTION_REQUIRED = 1 << 1,
 Model0_MF_MUST_KILL = 1 << 2,
 Model0_MF_SOFT_OFFLINE = 1 << 3,
};
extern int Model0_memory_failure(unsigned long Model0_pfn, int Model0_trapno, int Model0_flags);
extern void Model0_memory_failure_queue(unsigned long Model0_pfn, int Model0_trapno, int Model0_flags);
extern int Model0_unpoison_memory(unsigned long Model0_pfn);
extern int Model0_get_hwpoison_page(struct Model0_page *Model0_page);

extern int Model0_sysctl_memory_failure_early_kill;
extern int Model0_sysctl_memory_failure_recovery;
extern void Model0_shake_page(struct Model0_page *Model0_p, int Model0_access);
extern Model0_atomic_long_t Model0_num_poisoned_pages;
extern int Model0_soft_offline_page(struct Model0_page *Model0_page, int Model0_flags);


/*
 * Error handlers for various types of pages.
 */
enum Model0_mf_result {
 Model0_MF_IGNORED, /* Error: cannot be handled */
 Model0_MF_FAILED, /* Error: handling failed */
 Model0_MF_DELAYED, /* Will be handled later */
 Model0_MF_RECOVERED, /* Successfully recovered */
};

enum Model0_mf_action_page_type {
 Model0_MF_MSG_KERNEL,
 Model0_MF_MSG_KERNEL_HIGH_ORDER,
 Model0_MF_MSG_SLAB,
 Model0_MF_MSG_DIFFERENT_COMPOUND,
 Model0_MF_MSG_POISONED_HUGE,
 Model0_MF_MSG_HUGE,
 Model0_MF_MSG_FREE_HUGE,
 Model0_MF_MSG_UNMAP_FAILED,
 Model0_MF_MSG_DIRTY_SWAPCACHE,
 Model0_MF_MSG_CLEAN_SWAPCACHE,
 Model0_MF_MSG_DIRTY_MLOCKED_LRU,
 Model0_MF_MSG_CLEAN_MLOCKED_LRU,
 Model0_MF_MSG_DIRTY_UNEVICTABLE_LRU,
 Model0_MF_MSG_CLEAN_UNEVICTABLE_LRU,
 Model0_MF_MSG_DIRTY_LRU,
 Model0_MF_MSG_CLEAN_LRU,
 Model0_MF_MSG_TRUNCATED_LRU,
 Model0_MF_MSG_BUDDY,
 Model0_MF_MSG_BUDDY_2ND,
 Model0_MF_MSG_UNKNOWN,
};


extern void Model0_clear_huge_page(struct Model0_page *Model0_page,
       unsigned long Model0_addr,
       unsigned int Model0_pages_per_huge_page);
extern void Model0_copy_user_huge_page(struct Model0_page *Model0_dst, struct Model0_page *Model0_src,
    unsigned long Model0_addr, struct Model0_vm_area_struct *Model0_vma,
    unsigned int Model0_pages_per_huge_page);


extern struct Model0_page_ext_operations Model0_debug_guardpage_ops;
extern struct Model0_page_ext_operations Model0_page_poisoning_ops;
static inline __attribute__((no_instrument_function)) unsigned int Model0_debug_guardpage_minorder(void) { return 0; }
static inline __attribute__((no_instrument_function)) bool Model0_debug_guardpage_enabled(void) { return false; }
static inline __attribute__((no_instrument_function)) bool Model0_page_is_guard(struct Model0_page *Model0_page) { return false; }



void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_setup_nr_node_ids(void);
/*
 * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
 *
 * (C) SGI 2006, Christoph Lameter
 * 	Cleaned up and restructured to ease the addition of alternative
 * 	implementations of SLAB allocators.
 * (C) Linux Foundation 2008-2013
 *      Unified interface for all slab allocators
 */
/*
 * Flags to pass to kmem_cache_create().
 * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.
 */







/*
 * SLAB_DESTROY_BY_RCU - **WARNING** READ THIS!
 *
 * This delays freeing the SLAB page by a grace period, it does _NOT_
 * delay object freeing. This means that if you do kmem_cache_free()
 * that memory location is free to be reused at any time. Thus it may
 * be possible to see another object there in the same RCU grace period.
 *
 * This feature only ensures the memory location backing the object
 * stays valid, the trick to using this is relying on an independent
 * object validation pass. Something like:
 *
 *  rcu_read_lock()
 * again:
 *  obj = lockless_lookup(key);
 *  if (obj) {
 *    if (!try_get_ref(obj)) // might fail for free objects
 *      goto again;
 *
 *    if (obj->key != key) { // not the object we expected
 *      put_ref(obj);
 *      goto again;
 *    }
 *  }
 *  rcu_read_unlock();
 *
 * This is useful if we need to approach a kernel structure obliquely,
 * from its address obtained without the usual locking. We can lock
 * the structure to stabilize it and check it's still at the given address,
 * only if we can be sure that the memory has not been meanwhile reused
 * for some other kind of object (which our subsystem's lock might corrupt).
 *
 * rcu_read_lock before reading the address, then rcu_read_unlock after
 * taking the spinlock within the structure expected at that address.
 */




/* Flag to prevent checks on free */
/* Don't track use of uninitialized memory */
/* The following flags affect the page allocator grouping pages by mobility */


/*
 * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
 *
 * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.
 *
 * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.
 * Both make kfree a no-op.
 */






/*
 * include/linux/kmemleak.h
 *
 * Copyright (C) 2008 ARM Limited
 * Written by Catalin Marinas <catalin.marinas@arm.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
 */





/*
 * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
 *
 * (C) SGI 2006, Christoph Lameter
 * 	Cleaned up and restructured to ease the addition of alternative
 * 	implementations of SLAB allocators.
 * (C) Linux Foundation 2008-2013
 *      Unified interface for all slab allocators
 */
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_init(void)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_alloc(const void *Model0_ptr, Model0_size_t Model0_size, int Model0_min_count,
      Model0_gfp_t Model0_gfp)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_alloc_recursive(const void *Model0_ptr, Model0_size_t Model0_size,
         int Model0_min_count, unsigned long Model0_flags,
         Model0_gfp_t Model0_gfp)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_alloc_percpu(const void *Model0_ptr, Model0_size_t Model0_size,
      Model0_gfp_t Model0_gfp)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_free(const void *Model0_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_free_part(const void *Model0_ptr, Model0_size_t Model0_size)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_free_recursive(const void *Model0_ptr, unsigned long Model0_flags)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_free_percpu(const void *Model0_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_update_trace(const void *Model0_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_not_leak(const void *Model0_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_ignore(const void *Model0_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_scan_area(const void *Model0_ptr, Model0_size_t Model0_size, Model0_gfp_t Model0_gfp)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_erase(void **Model0_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model0_kmemleak_no_scan(const void *Model0_ptr)
{
}









/*
 * cloning flags:
 */
/*
 * Scheduling policies
 */




/* SCHED_ISO: reserved but not implemented yet */



/* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */


/*
 * For the sched_{set,get}attr() calls
 */








/*
 * Priority of a process goes from 0..MAX_PRIO-1, valid RT
 * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
 * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
 * values are inverted: lower p->prio value means higher priority.
 *
 * The MAX_USER_RT_PRIO value allows the actual maximum
 * RT priority to be separate from the value exported to
 * user-space.  This allows kernel threads to set their
 * priority to a value higher than any user task. Note:
 * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
 */







/*
 * Convert user-nice values [ -20 ... 0 ... 19 ]
 * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
 * and back.
 */



/*
 * 'User priority' is the nice value converted to something we
 * can work with better when scaling various scheduler parameters,
 * it's a [ 0 ... 39 ] range.
 */




/*
 * Convert nice value [19,-20] to rlimit style value [1,40].
 */
static inline __attribute__((no_instrument_function)) long Model0_nice_to_rlimit(long Model0_nice)
{
 return (19 - Model0_nice + 1);
}

/*
 * Convert rlimit style value [1,40] to nice value [-20, 19].
 */
static inline __attribute__((no_instrument_function)) long Model0_rlimit_to_nice(long Model0_prio)
{
 return (19 - Model0_prio + 1);
}


struct Model0_sched_param {
 int Model0_sched_priority;
};



/*
 * This is <linux/capability.h>
 *
 * Andrew G. Morgan <morgan@kernel.org>
 * Alexander Kjeldaas <astor@guardian.no>
 * with help from Aleph1, Roland Buresund and Andrew Main.
 *
 * See here for the libcap library ("POSIX draft" compliance):
 *
 * ftp://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.6/
 */




/*
 * This is <linux/capability.h>
 *
 * Andrew G. Morgan <morgan@kernel.org>
 * Alexander Kjeldaas <astor@guardian.no>
 * with help from Aleph1, Roland Buresund and Andrew Main.
 *
 * See here for the libcap library ("POSIX draft" compliance):
 *
 * ftp://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.6/
 */






/* User-level do most of the mapping between kernel and user
   capabilities based on the version tag given by the kernel. The
   kernel might be somewhat backwards compatible, but don't bet on
   it. */

/* Note, cap_t, is defined by POSIX (draft) to be an "opaque" pointer to
   a set of three capability sets.  The transposition of 3*the
   following structure to such a composite is better handled in a user
   library since the draft standard requires the use of malloc/free
   etc.. */
typedef struct Model0___user_cap_header_struct {
 __u32 Model0_version;
 int Model0_pid;
} *Model0_cap_user_header_t;

typedef struct Model0___user_cap_data_struct {
        __u32 Model0_effective;
        __u32 Model0_permitted;
        __u32 Model0_inheritable;
} *Model0_cap_user_data_t;
struct Model0_vfs_cap_data {
 Model0___le32 Model0_magic_etc; /* Little endian */
 struct {
  Model0___le32 Model0_permitted; /* Little endian */
  Model0___le32 Model0_inheritable; /* Little endian */
 } Model0_data[2];
};
/**
 ** POSIX-draft defined capabilities.
 **/

/* In a system with the [_POSIX_CHOWN_RESTRICTED] option defined, this
   overrides the restriction of changing file ownership and group
   ownership. */



/* Override all DAC access, including ACL execute access if
   [_POSIX_ACL] is defined. Excluding DAC access covered by
   CAP_LINUX_IMMUTABLE. */



/* Overrides all DAC restrictions regarding read and search on files
   and directories, including ACL restrictions if [_POSIX_ACL] is
   defined. Excluding DAC access covered by CAP_LINUX_IMMUTABLE. */



/* Overrides all restrictions about allowed operations on files, where
   file owner ID must be equal to the user ID, except where CAP_FSETID
   is applicable. It doesn't override MAC and DAC restrictions. */



/* Overrides the following restrictions that the effective user ID
   shall match the file owner ID when setting the S_ISUID and S_ISGID
   bits on that file; that the effective group ID (or one of the
   supplementary group IDs) shall match the file owner ID when setting
   the S_ISGID bit on that file; that the S_ISUID and S_ISGID bits are
   cleared on successful return from chown(2) (not implemented). */



/* Overrides the restriction that the real or effective user ID of a
   process sending a signal must match the real or effective user ID
   of the process receiving the signal. */



/* Allows setgid(2) manipulation */
/* Allows setgroups(2) */
/* Allows forged gids on socket credentials passing. */



/* Allows set*uid(2) manipulation (including fsuid). */
/* Allows forged pids on socket credentials passing. */




/**
 ** Linux-specific capabilities
 **/

/* Without VFS support for capabilities:
 *   Transfer any capability in your permitted set to any pid,
 *   remove any capability in your permitted set from any pid
 * With VFS support for capabilities (neither of above, but)
 *   Add any capability from current's capability bounding set
 *       to the current process' inheritable set
 *   Allow taking bits out of capability bounding set
 *   Allow modification of the securebits for a process
 */



/* Allow modification of S_IMMUTABLE and S_APPEND file attributes */



/* Allows binding to TCP/UDP sockets below 1024 */
/* Allows binding to ATM VCIs below 32 */



/* Allow broadcasting, listen to multicast */



/* Allow interface configuration */
/* Allow administration of IP firewall, masquerading and accounting */
/* Allow setting debug option on sockets */
/* Allow modification of routing tables */
/* Allow setting arbitrary process / process group ownership on
   sockets */
/* Allow binding to any address for transparent proxying (also via NET_RAW) */
/* Allow setting TOS (type of service) */
/* Allow setting promiscuous mode */
/* Allow clearing driver statistics */
/* Allow multicasting */
/* Allow read/write of device-specific registers */
/* Allow activation of ATM control sockets */



/* Allow use of RAW sockets */
/* Allow use of PACKET sockets */
/* Allow binding to any address for transparent proxying (also via NET_ADMIN) */



/* Allow locking of shared memory segments */
/* Allow mlock and mlockall (which doesn't really have anything to do
   with IPC) */



/* Override IPC ownership checks */



/* Insert and remove kernel modules - modify kernel without limit */


/* Allow ioperm/iopl access */
/* Allow sending USB messages to any device via /proc/bus/usb */



/* Allow use of chroot() */



/* Allow ptrace() of any process */



/* Allow configuration of process accounting */



/* Allow configuration of the secure attention key */
/* Allow administration of the random device */
/* Allow examination and configuration of disk quotas */
/* Allow setting the domainname */
/* Allow setting the hostname */
/* Allow calling bdflush() */
/* Allow mount() and umount(), setting up new smb connection */
/* Allow some autofs root ioctls */
/* Allow nfsservctl */
/* Allow VM86_REQUEST_IRQ */
/* Allow to read/write pci config on alpha */
/* Allow irix_prctl on mips (setstacksize) */
/* Allow flushing all cache on m68k (sys_cacheflush) */
/* Allow removing semaphores */
/* Used instead of CAP_CHOWN to "chown" IPC message queues, semaphores
   and shared memory */
/* Allow locking/unlocking of shared memory segment */
/* Allow turning swap on/off */
/* Allow forged pids on socket credentials passing */
/* Allow setting readahead and flushing buffers on block devices */
/* Allow setting geometry in floppy driver */
/* Allow turning DMA on/off in xd driver */
/* Allow administration of md devices (mostly the above, but some
   extra ioctls) */
/* Allow tuning the ide driver */
/* Allow access to the nvram device */
/* Allow administration of apm_bios, serial and bttv (TV) device */
/* Allow manufacturer commands in isdn CAPI support driver */
/* Allow reading non-standardized portions of pci configuration space */
/* Allow DDI debug ioctl on sbpcd driver */
/* Allow setting up serial ports */
/* Allow sending raw qic-117 commands */
/* Allow enabling/disabling tagged queuing on SCSI controllers and sending
   arbitrary SCSI commands */
/* Allow setting encryption key on loopback filesystem */
/* Allow setting zone reclaim policy */



/* Allow use of reboot() */



/* Allow raising priority and setting priority on other (different
   UID) processes */
/* Allow use of FIFO and round-robin (realtime) scheduling on own
   processes and setting the scheduling algorithm used by another
   process. */
/* Allow setting cpu affinity on other processes */



/* Override resource limits. Set resource limits. */
/* Override quota limits. */
/* Override reserved space on ext2 filesystem */
/* Modify data journaling mode on ext3 filesystem (uses journaling
   resources) */
/* NOTE: ext2 honors fsuid when checking for resource overrides, so
   you can override using fsuid too */
/* Override size restrictions on IPC message queues */
/* Allow more than 64hz interrupts from the real-time clock */
/* Override max number of consoles on console allocation */
/* Override max number of keymaps */



/* Allow manipulation of system clock */
/* Allow irix_stime on mips */
/* Allow setting the real-time clock */



/* Allow configuration of tty devices */
/* Allow vhangup() of tty */



/* Allow the privileged aspects of mknod() */



/* Allow taking of leases on files */



/* Allow writing the audit log via unicast netlink socket */



/* Allow configuration of audit via unicast netlink socket */





/* Override MAC access.
   The base kernel enforces no MAC policy.
   An LSM may enforce a MAC policy, and if it does and it chooses
   to implement capability based overrides of that policy, this is
   the capability it should use to do so. */



/* Allow MAC configuration or state changes.
   The base kernel requires no MAC configuration.
   An LSM may enforce a MAC policy, and if it does and it chooses
   to implement capability based checks on modifications to that
   policy or the data required to maintain it, this is the
   capability it should use to do so. */



/* Allow configuring the kernel's syslog (printk behaviour) */



/* Allow triggering something that will wake the system */



/* Allow preventing system suspends */



/* Allow reading the audit log via multicast netlink socket */
/*
 * Bit location of each capability (used by user-space library and kernel)
 */





extern int Model0_file_caps_enabled;

typedef struct Model0_kernel_cap_struct {
 __u32 Model0_cap[2];
} Model0_kernel_cap_t;

/* exact same as vfs_cap_data but in cpu endian and always filled completely */
struct Model0_cpu_vfs_cap_data {
 __u32 Model0_magic_etc;
 Model0_kernel_cap_t Model0_permitted;
 Model0_kernel_cap_t Model0_inheritable;
};





struct Model0_file;
struct Model0_inode;
struct Model0_dentry;
struct Model0_task_struct;
struct Model0_user_namespace;

extern const Model0_kernel_cap_t Model0___cap_empty_set;
extern const Model0_kernel_cap_t Model0___cap_init_eff_set;

/*
 * Internal kernel functions only
 */




/*
 * CAP_FS_MASK and CAP_NFSD_MASKS:
 *
 * The fs mask is all the privileges that fsuid==0 historically meant.
 * At one time in the past, that included CAP_MKNOD and CAP_LINUX_IMMUTABLE.
 *
 * It has never meant setting security.* and trusted.* xattrs.
 *
 * We could also define fsmask as follows:
 *   1. CAP_FS_MASK is the privilege to bypass all fs-related DAC permissions
 *   2. The security.* and trusted.* xattrs are fs-related MAC permissions
 */
static inline __attribute__((no_instrument_function)) Model0_kernel_cap_t Model0_cap_combine(const Model0_kernel_cap_t Model0_a,
           const Model0_kernel_cap_t Model0_b)
{
 Model0_kernel_cap_t Model0_dest;
 do { unsigned Model0___capi; for (Model0___capi = 0; Model0___capi < 2; ++Model0___capi) { Model0_dest.Model0_cap[Model0___capi] = Model0_a.Model0_cap[Model0___capi] | Model0_b.Model0_cap[Model0___capi]; } } while (0);
 return Model0_dest;
}

static inline __attribute__((no_instrument_function)) Model0_kernel_cap_t Model0_cap_intersect(const Model0_kernel_cap_t Model0_a,
      const Model0_kernel_cap_t Model0_b)
{
 Model0_kernel_cap_t Model0_dest;
 do { unsigned Model0___capi; for (Model0___capi = 0; Model0___capi < 2; ++Model0___capi) { Model0_dest.Model0_cap[Model0___capi] = Model0_a.Model0_cap[Model0___capi] & Model0_b.Model0_cap[Model0___capi]; } } while (0);
 return Model0_dest;
}

static inline __attribute__((no_instrument_function)) Model0_kernel_cap_t Model0_cap_drop(const Model0_kernel_cap_t Model0_a,
        const Model0_kernel_cap_t Model0_drop)
{
 Model0_kernel_cap_t Model0_dest;
 do { unsigned Model0___capi; for (Model0___capi = 0; Model0___capi < 2; ++Model0___capi) { Model0_dest.Model0_cap[Model0___capi] = Model0_a.Model0_cap[Model0___capi] &~ Model0_drop.Model0_cap[Model0___capi]; } } while (0);
 return Model0_dest;
}

static inline __attribute__((no_instrument_function)) Model0_kernel_cap_t Model0_cap_invert(const Model0_kernel_cap_t Model0_c)
{
 Model0_kernel_cap_t Model0_dest;
 do { unsigned Model0___capi; for (Model0___capi = 0; Model0___capi < 2; ++Model0___capi) { Model0_dest.Model0_cap[Model0___capi] = ~ Model0_c.Model0_cap[Model0___capi]; } } while (0);
 return Model0_dest;
}

static inline __attribute__((no_instrument_function)) bool Model0_cap_isclear(const Model0_kernel_cap_t Model0_a)
{
 unsigned Model0___capi;
 for (Model0___capi = 0; Model0___capi < 2; ++Model0___capi) {
  if (Model0_a.Model0_cap[Model0___capi] != 0)
   return false;
 }
 return true;
}

/*
 * Check if "a" is a subset of "set".
 * return true if ALL of the capabilities in "a" are also in "set"
 *	cap_issubset(0101, 1111) will return true
 * return false if ANY of the capabilities in "a" are not in "set"
 *	cap_issubset(1111, 0101) will return false
 */
static inline __attribute__((no_instrument_function)) bool Model0_cap_issubset(const Model0_kernel_cap_t Model0_a, const Model0_kernel_cap_t Model0_set)
{
 Model0_kernel_cap_t Model0_dest;
 Model0_dest = Model0_cap_drop(Model0_a, Model0_set);
 return Model0_cap_isclear(Model0_dest);
}

/* Used to decide between falling back on the old suser() or fsuser(). */

static inline __attribute__((no_instrument_function)) Model0_kernel_cap_t Model0_cap_drop_fs_set(const Model0_kernel_cap_t Model0_a)
{
 const Model0_kernel_cap_t Model0___cap_fs_set = ((Model0_kernel_cap_t){{ ((1 << ((0) & 31)) | (1 << ((27) & 31)) | (1 << ((1) & 31)) | (1 << ((2) & 31)) | (1 << ((3) & 31)) | (1 << ((4) & 31))) | (1 << ((9) & 31)), ((1 << ((32) & 31))) } });
 return Model0_cap_drop(Model0_a, Model0___cap_fs_set);
}

static inline __attribute__((no_instrument_function)) Model0_kernel_cap_t Model0_cap_raise_fs_set(const Model0_kernel_cap_t Model0_a,
         const Model0_kernel_cap_t Model0_permitted)
{
 const Model0_kernel_cap_t Model0___cap_fs_set = ((Model0_kernel_cap_t){{ ((1 << ((0) & 31)) | (1 << ((27) & 31)) | (1 << ((1) & 31)) | (1 << ((2) & 31)) | (1 << ((3) & 31)) | (1 << ((4) & 31))) | (1 << ((9) & 31)), ((1 << ((32) & 31))) } });
 return Model0_cap_combine(Model0_a,
      Model0_cap_intersect(Model0_permitted, Model0___cap_fs_set));
}

static inline __attribute__((no_instrument_function)) Model0_kernel_cap_t Model0_cap_drop_nfsd_set(const Model0_kernel_cap_t Model0_a)
{
 const Model0_kernel_cap_t Model0___cap_fs_set = ((Model0_kernel_cap_t){{ ((1 << ((0) & 31)) | (1 << ((27) & 31)) | (1 << ((1) & 31)) | (1 << ((2) & 31)) | (1 << ((3) & 31)) | (1 << ((4) & 31))) | (1 << ((24) & 31)), ((1 << ((32) & 31))) } });
 return Model0_cap_drop(Model0_a, Model0___cap_fs_set);
}

static inline __attribute__((no_instrument_function)) Model0_kernel_cap_t Model0_cap_raise_nfsd_set(const Model0_kernel_cap_t Model0_a,
           const Model0_kernel_cap_t Model0_permitted)
{
 const Model0_kernel_cap_t Model0___cap_nfsd_set = ((Model0_kernel_cap_t){{ ((1 << ((0) & 31)) | (1 << ((27) & 31)) | (1 << ((1) & 31)) | (1 << ((2) & 31)) | (1 << ((3) & 31)) | (1 << ((4) & 31))) | (1 << ((24) & 31)), ((1 << ((32) & 31))) } });
 return Model0_cap_combine(Model0_a,
      Model0_cap_intersect(Model0_permitted, Model0___cap_nfsd_set));
}


extern bool Model0_has_capability(struct Model0_task_struct *Model0_t, int Model0_cap);
extern bool Model0_has_ns_capability(struct Model0_task_struct *Model0_t,
         struct Model0_user_namespace *Model0_ns, int Model0_cap);
extern bool Model0_has_capability_noaudit(struct Model0_task_struct *Model0_t, int Model0_cap);
extern bool Model0_has_ns_capability_noaudit(struct Model0_task_struct *Model0_t,
          struct Model0_user_namespace *Model0_ns, int Model0_cap);
extern bool Model0_capable(int Model0_cap);
extern bool Model0_ns_capable(struct Model0_user_namespace *Model0_ns, int Model0_cap);
extern bool Model0_ns_capable_noaudit(struct Model0_user_namespace *Model0_ns, int Model0_cap);
extern bool Model0_capable_wrt_inode_uidgid(const struct Model0_inode *Model0_inode, int Model0_cap);
extern bool Model0_file_ns_capable(const struct Model0_file *Model0_file, struct Model0_user_namespace *Model0_ns, int Model0_cap);

/* audit system wants to get cap info from files as well */
extern int Model0_get_vfs_caps_from_disk(const struct Model0_dentry *Model0_dentry, struct Model0_cpu_vfs_cap_data *Model0_cpu_caps);





/*
 * Descending-priority-sorted double-linked list
 *
 * (C) 2002-2003 Intel Corp
 * Inaky Perez-Gonzalez <inaky.perez-gonzalez@intel.com>.
 *
 * 2001-2005 (c) MontaVista Software, Inc.
 * Daniel Walker <dwalker@mvista.com>
 *
 * (C) 2005 Thomas Gleixner <tglx@linutronix.de>
 *
 * Simplifications of the original code by
 * Oleg Nesterov <oleg@tv-sign.ru>
 *
 * Licensed under the FSF's GNU Public License v2 or later.
 *
 * Based on simple lists (include/linux/list.h).
 *
 * This is a priority-sorted list of nodes; each node has a
 * priority from INT_MIN (highest) to INT_MAX (lowest).
 *
 * Addition is O(K), removal is O(1), change of priority of a node is
 * O(K) and K is the number of RT priority levels used in the system.
 * (1 <= K <= 99)
 *
 * This list is really a list of lists:
 *
 *  - The tier 1 list is the prio_list, different priority nodes.
 *
 *  - The tier 2 list is the node_list, serialized nodes.
 *
 * Simple ASCII art explanation:
 *
 * pl:prio_list (only for plist_node)
 * nl:node_list
 *   HEAD|             NODE(S)
 *       |
 *       ||------------------------------------|
 *       ||->|pl|<->|pl|<--------------->|pl|<-|
 *       |   |10|   |21|   |21|   |21|   |40|   (prio)
 *       |   |  |   |  |   |  |   |  |   |  |
 *       |   |  |   |  |   |  |   |  |   |  |
 * |->|nl|<->|nl|<->|nl|<->|nl|<->|nl|<->|nl|<-|
 * |-------------------------------------------|
 *
 * The nodes on the prio_list list are sorted by priority to simplify
 * the insertion of new nodes. There are no nodes with duplicate
 * priorites on the list.
 *
 * The nodes on the node_list are ordered by priority and can contain
 * entries which have the same priority. Those entries are ordered
 * FIFO
 *
 * Addition means: look for the prio_list node in the prio_list
 * for the priority of the node and insert it before the node_list
 * entry of the next prio_list node. If it is the first node of
 * that priority, add it to the prio_list in the right position and
 * insert it into the serialized node_list list
 *
 * Removal means remove it from the node_list and remove it from
 * the prio_list if the node_list list_head is non empty. In case
 * of removal from the prio_list it must be checked whether other
 * entries of the same priority are on the list or not. If there
 * is another entry of the same priority then this entry has to
 * replace the removed entry on the prio_list. If the entry which
 * is removed is the only entry of this priority then a simple
 * remove from both list is sufficient.
 *
 * INT_MIN is the highest priority, 0 is the medium highest, INT_MAX
 * is lowest priority.
 *
 * No locking is done, up to the caller.
 *
 */






struct Model0_plist_head {
 struct Model0_list_head Model0_node_list;
};

struct Model0_plist_node {
 int Model0_prio;
 struct Model0_list_head Model0_prio_list;
 struct Model0_list_head Model0_node_list;
};

/**
 * PLIST_HEAD_INIT - static struct plist_head initializer
 * @head:	struct plist_head variable name
 */





/**
 * PLIST_HEAD - declare and init plist_head
 * @head:	name for struct plist_head variable
 */



/**
 * PLIST_NODE_INIT - static struct plist_node initializer
 * @node:	struct plist_node variable name
 * @__prio:	initial node priority
 */







/**
 * plist_head_init - dynamic struct plist_head initializer
 * @head:	&struct plist_head pointer
 */
static inline __attribute__((no_instrument_function)) void
Model0_plist_head_init(struct Model0_plist_head *Model0_head)
{
 Model0_INIT_LIST_HEAD(&Model0_head->Model0_node_list);
}

/**
 * plist_node_init - Dynamic struct plist_node initializer
 * @node:	&struct plist_node pointer
 * @prio:	initial node priority
 */
static inline __attribute__((no_instrument_function)) void Model0_plist_node_init(struct Model0_plist_node *Model0_node, int Model0_prio)
{
 Model0_node->Model0_prio = Model0_prio;
 Model0_INIT_LIST_HEAD(&Model0_node->Model0_prio_list);
 Model0_INIT_LIST_HEAD(&Model0_node->Model0_node_list);
}

extern void Model0_plist_add(struct Model0_plist_node *Model0_node, struct Model0_plist_head *Model0_head);
extern void Model0_plist_del(struct Model0_plist_node *Model0_node, struct Model0_plist_head *Model0_head);

extern void Model0_plist_requeue(struct Model0_plist_node *Model0_node, struct Model0_plist_head *Model0_head);

/**
 * plist_for_each - iterate over the plist
 * @pos:	the type * to use as a loop counter
 * @head:	the head for your list
 */



/**
 * plist_for_each_continue - continue iteration over the plist
 * @pos:	the type * to use as a loop cursor
 * @head:	the head for your list
 *
 * Continue to iterate over plist, continuing after the current position.
 */



/**
 * plist_for_each_safe - iterate safely over a plist of given type
 * @pos:	the type * to use as a loop counter
 * @n:	another type * to use as temporary storage
 * @head:	the head for your list
 *
 * Iterate over a plist of given type, safe against removal of list entry.
 */



/**
 * plist_for_each_entry	- iterate over list of given type
 * @pos:	the type * to use as a loop counter
 * @head:	the head for your list
 * @mem:	the name of the list_head within the struct
 */



/**
 * plist_for_each_entry_continue - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor
 * @head:	the head for your list
 * @m:		the name of the list_head within the struct
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */



/**
 * plist_for_each_entry_safe - iterate safely over list of given type
 * @pos:	the type * to use as a loop counter
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list
 * @m:		the name of the list_head within the struct
 *
 * Iterate over list of given type, safe against removal of list entry.
 */



/**
 * plist_head_empty - return !0 if a plist_head is empty
 * @head:	&struct plist_head pointer
 */
static inline __attribute__((no_instrument_function)) int Model0_plist_head_empty(const struct Model0_plist_head *Model0_head)
{
 return Model0_list_empty(&Model0_head->Model0_node_list);
}

/**
 * plist_node_empty - return !0 if plist_node is not on a list
 * @node:	&struct plist_node pointer
 */
static inline __attribute__((no_instrument_function)) int Model0_plist_node_empty(const struct Model0_plist_node *Model0_node)
{
 return Model0_list_empty(&Model0_node->Model0_node_list);
}

/* All functions below assume the plist_head is not empty. */

/**
 * plist_first_entry - get the struct for the first entry
 * @head:	the &struct plist_head pointer
 * @type:	the type of the struct this is embedded in
 * @member:	the name of the list_head within the struct
 */
/**
 * plist_last_entry - get the struct for the last entry
 * @head:	the &struct plist_head pointer
 * @type:	the type of the struct this is embedded in
 * @member:	the name of the list_head within the struct
 */
/**
 * plist_next - get the next entry in list
 * @pos:	the type * to cursor
 */



/**
 * plist_prev - get the prev entry in list
 * @pos:	the type * to cursor
 */



/**
 * plist_first - return the first node (and thus, highest priority)
 * @head:	the &struct plist_head pointer
 *
 * Assumes the plist is _not_ empty.
 */
static inline __attribute__((no_instrument_function)) struct Model0_plist_node *Model0_plist_first(const struct Model0_plist_head *Model0_head)
{
 return ({ const typeof( ((struct Model0_plist_node *)0)->Model0_node_list ) *Model0___mptr = (Model0_head->Model0_node_list.Model0_next); (struct Model0_plist_node *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_plist_node, Model0_node_list) );});

}

/**
 * plist_last - return the last node (and thus, lowest priority)
 * @head:	the &struct plist_head pointer
 *
 * Assumes the plist is _not_ empty.
 */
static inline __attribute__((no_instrument_function)) struct Model0_plist_node *Model0_plist_last(const struct Model0_plist_head *Model0_head)
{
 return ({ const typeof( ((struct Model0_plist_node *)0)->Model0_node_list ) *Model0___mptr = (Model0_head->Model0_node_list.Model0_prev); (struct Model0_plist_node *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_plist_node, Model0_node_list) );});

}













typedef unsigned long Model0_cputime_t;
typedef Model0_u64 Model0_cputime64_t;





/*
 * Convert nanoseconds <-> cputime
 */
/*
 * Convert cputime to microseconds and back.
 */







/*
 * Convert cputime to seconds and back.
 */



/*
 * Convert cputime to timespec and back.
 */





/*
 * Convert cputime to timeval and back.
 */





/*
 * Convert cputime to clock and back.
 */





/*
 * Convert cputime64 to clock.
 */


















/*
 * A set of types for the internal kernel types representing uids and gids.
 *
 * The types defined in this header allow distinguishing which uids and gids in
 * the kernel are values used by userspace and which uid and gid values are
 * the internal kernel values.  With the addition of user namespaces the values
 * can be different.  Using the type system makes it possible for the compiler
 * to detect when we overlook these differences.
 *
 */







/*
 * general notes:
 *
 * CONFIG_UID16 is defined if the given architecture needs to
 * support backwards compatibility for old system calls.
 *
 * kernel code should use uid_t and gid_t at all times when dealing with
 * kernel-private data.
 *
 * old_uid_t and old_gid_t should only be different if CONFIG_UID16 is
 * defined, else the platform should provide dummy typedefs for them
 * such that they are equivalent to __kernel_{u,g}id_t.
 *
 * uid16_t and gid16_t are used on all architectures. (when dealing
 * with structures hard coded to 16 bits, such as in filesystems)
 */


/*
 * This is the "overflow" UID and GID. They are used to signify uid/gid
 * overflow to old programs when they request uid/gid information but are
 * using the old 16 bit interfaces.
 * When you run a libc5 program, it will think that all highuid files or
 * processes are owned by this uid/gid.
 * The idea is that it's better to do so than possibly return 0 in lieu of
 * 65536, etc.
 */

extern int Model0_overflowuid;
extern int Model0_overflowgid;

extern void Model0___bad_uid(void);
extern void Model0___bad_gid(void);






/* prevent uid mod 65536 effect by returning a default value for high UIDs */


/*
 * -1 is different in 16 bits than it is in 32 bits
 * these macros are used by chown(), setreuid(), ...,
 */
/* uid/gid input should be always 32bit uid_t */



/*
 * Everything below this line is needed on all architectures, to deal with
 * filesystems that only store 16 bits of the UID/GID, etc.
 */

/*
 * This is the UID and GID that will get written to disk if a filesystem
 * only supports 16-bit UIDs and the kernel has a high UID/GID to write
 */
extern int Model0_fs_overflowuid;
extern int Model0_fs_overflowgid;




/*
 * Since these macros are used in architectures that only need limited
 * 16-bit UID back compatibility, we won't use old_uid_t and old_gid_t
 */

struct Model0_user_namespace;
extern struct Model0_user_namespace Model0_init_user_ns;

typedef struct {
 Model0_uid_t Model0_val;
} Model0_kuid_t;


typedef struct {
 Model0_gid_t Model0_val;
} Model0_kgid_t;





static inline __attribute__((no_instrument_function)) Model0_uid_t Model0___kuid_val(Model0_kuid_t Model0_uid)
{
 return Model0_uid.Model0_val;
}

static inline __attribute__((no_instrument_function)) Model0_gid_t Model0___kgid_val(Model0_kgid_t Model0_gid)
{
 return Model0_gid.Model0_val;
}
static inline __attribute__((no_instrument_function)) bool Model0_uid_eq(Model0_kuid_t Model0_left, Model0_kuid_t Model0_right)
{
 return Model0___kuid_val(Model0_left) == Model0___kuid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_gid_eq(Model0_kgid_t Model0_left, Model0_kgid_t Model0_right)
{
 return Model0___kgid_val(Model0_left) == Model0___kgid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_uid_gt(Model0_kuid_t Model0_left, Model0_kuid_t Model0_right)
{
 return Model0___kuid_val(Model0_left) > Model0___kuid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_gid_gt(Model0_kgid_t Model0_left, Model0_kgid_t Model0_right)
{
 return Model0___kgid_val(Model0_left) > Model0___kgid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_uid_gte(Model0_kuid_t Model0_left, Model0_kuid_t Model0_right)
{
 return Model0___kuid_val(Model0_left) >= Model0___kuid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_gid_gte(Model0_kgid_t Model0_left, Model0_kgid_t Model0_right)
{
 return Model0___kgid_val(Model0_left) >= Model0___kgid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_uid_lt(Model0_kuid_t Model0_left, Model0_kuid_t Model0_right)
{
 return Model0___kuid_val(Model0_left) < Model0___kuid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_gid_lt(Model0_kgid_t Model0_left, Model0_kgid_t Model0_right)
{
 return Model0___kgid_val(Model0_left) < Model0___kgid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_uid_lte(Model0_kuid_t Model0_left, Model0_kuid_t Model0_right)
{
 return Model0___kuid_val(Model0_left) <= Model0___kuid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_gid_lte(Model0_kgid_t Model0_left, Model0_kgid_t Model0_right)
{
 return Model0___kgid_val(Model0_left) <= Model0___kgid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_uid_valid(Model0_kuid_t Model0_uid)
{
 return Model0___kuid_val(Model0_uid) != (Model0_uid_t) -1;
}

static inline __attribute__((no_instrument_function)) bool Model0_gid_valid(Model0_kgid_t Model0_gid)
{
 return Model0___kgid_val(Model0_gid) != (Model0_gid_t) -1;
}
static inline __attribute__((no_instrument_function)) Model0_kuid_t Model0_make_kuid(struct Model0_user_namespace *Model0_from, Model0_uid_t Model0_uid)
{
 return (Model0_kuid_t){ Model0_uid };
}

static inline __attribute__((no_instrument_function)) Model0_kgid_t Model0_make_kgid(struct Model0_user_namespace *Model0_from, Model0_gid_t Model0_gid)
{
 return (Model0_kgid_t){ Model0_gid };
}

static inline __attribute__((no_instrument_function)) Model0_uid_t Model0_from_kuid(struct Model0_user_namespace *Model0_to, Model0_kuid_t Model0_kuid)
{
 return Model0___kuid_val(Model0_kuid);
}

static inline __attribute__((no_instrument_function)) Model0_gid_t Model0_from_kgid(struct Model0_user_namespace *Model0_to, Model0_kgid_t Model0_kgid)
{
 return Model0___kgid_val(Model0_kgid);
}

static inline __attribute__((no_instrument_function)) Model0_uid_t Model0_from_kuid_munged(struct Model0_user_namespace *Model0_to, Model0_kuid_t Model0_kuid)
{
 Model0_uid_t Model0_uid = Model0_from_kuid(Model0_to, Model0_kuid);
 if (Model0_uid == (Model0_uid_t)-1)
  Model0_uid = Model0_overflowuid;
 return Model0_uid;
}

static inline __attribute__((no_instrument_function)) Model0_gid_t Model0_from_kgid_munged(struct Model0_user_namespace *Model0_to, Model0_kgid_t Model0_kgid)
{
 Model0_gid_t Model0_gid = Model0_from_kgid(Model0_to, Model0_kgid);
 if (Model0_gid == (Model0_gid_t)-1)
  Model0_gid = Model0_overflowgid;
 return Model0_gid;
}

static inline __attribute__((no_instrument_function)) bool Model0_kuid_has_mapping(struct Model0_user_namespace *Model0_ns, Model0_kuid_t Model0_uid)
{
 return Model0_uid_valid(Model0_uid);
}

static inline __attribute__((no_instrument_function)) bool Model0_kgid_has_mapping(struct Model0_user_namespace *Model0_ns, Model0_kgid_t Model0_gid)
{
 return Model0_gid_valid(Model0_gid);
}







/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct Model0_ipc_perm
{
 Model0___kernel_key_t Model0_key;
 Model0___kernel_uid_t Model0_uid;
 Model0___kernel_gid_t Model0_gid;
 Model0___kernel_uid_t Model0_cuid;
 Model0___kernel_gid_t Model0_cgid;
 Model0___kernel_mode_t Model0_mode;
 unsigned short Model0_seq;
};

/* Include the definition of ipc64_perm */




/*
 * The generic ipc64_perm structure:
 * Note extra padding because this structure is passed back and forth
 * between kernel and user space.
 *
 * ipc64_perm was originally meant to be architecture specific, but
 * everyone just ended up making identical copies without specific
 * optimizations, so we may just as well all use the same one.
 *
 * Pad space is left for:
 * - 32-bit mode_t on architectures that only had 16 bit
 * - 32-bit seq
 * - 2 miscellaneous 32-bit values
 */

struct Model0_ipc64_perm {
 Model0___kernel_key_t Model0_key;
 Model0___kernel_uid32_t Model0_uid;
 Model0___kernel_gid32_t Model0_gid;
 Model0___kernel_uid32_t Model0_cuid;
 Model0___kernel_gid32_t Model0_cgid;
 Model0___kernel_mode_t Model0_mode;
    /* pad if mode_t is u16: */
 unsigned char Model0___pad1[4 - sizeof(Model0___kernel_mode_t)];
 unsigned short Model0_seq;
 unsigned short Model0___pad2;
 Model0___kernel_ulong_t Model0___unused1;
 Model0___kernel_ulong_t Model0___unused2;
};

/* resource get request flags */




/* these fields are used by the DIPC package so the kernel as standard
   should avoid using them if possible */




/* 
 * Control commands used with semctl, msgctl and shmctl 
 * see also specific commands in sem.h, msg.h and shm.h
 */





/*
 * Version flags for semctl, msgctl, and shmctl commands
 * These are passed as bitflags or-ed with the actual command
 */





/*
 * These are used to wrap system calls.
 *
 * See architecture code for ugly details..
 */
struct Model0_ipc_kludge {
 struct Model0_msgbuf *Model0_msgp;
 long Model0_msgtyp;
};
/* Used by the DIPC package, try and avoid reusing it */



/* used by in-kernel data structures */
struct Model0_kern_ipc_perm
{
 Model0_spinlock_t Model0_lock;
 bool Model0_deleted;
 int Model0_id;
 Model0_key_t Model0_key;
 Model0_kuid_t Model0_uid;
 Model0_kgid_t Model0_gid;
 Model0_kuid_t Model0_cuid;
 Model0_kgid_t Model0_cgid;
 Model0_umode_t Model0_mode;
 unsigned long Model0_seq;
 void *Model0_security;
};

/* semop flags */


/* semctl Command Definitions. */
/* ipcs ctl cmds */



/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct Model0_semid_ds {
 struct Model0_ipc_perm Model0_sem_perm; /* permissions .. see ipc.h */
 Model0___kernel_time_t Model0_sem_otime; /* last semop time */
 Model0___kernel_time_t Model0_sem_ctime; /* last change time */
 struct Model0_sem *Model0_sem_base; /* ptr to first semaphore in array */
 struct Model0_sem_queue *Model0_sem_pending; /* pending operations to be processed */
 struct Model0_sem_queue **Model0_sem_pending_last; /* last pending operation */
 struct Model0_sem_undo *Model0_undo; /* undo requests on this array */
 unsigned short Model0_sem_nsems; /* no. of semaphores in array */
};

/* Include the definition of semid64_ds */




/*
 * The semid64_ds structure for x86 architecture.
 * Note extra padding because this structure is passed back and forth
 * between kernel and user space.
 *
 * Pad space is left for:
 * - 64-bit time_t to solve y2038 problem
 * - 2 miscellaneous 32-bit values
 */
struct Model0_semid64_ds {
 struct Model0_ipc64_perm Model0_sem_perm; /* permissions .. see ipc.h */
 Model0___kernel_time_t Model0_sem_otime; /* last semop time */
 Model0___kernel_ulong_t Model0___unused1;
 Model0___kernel_time_t Model0_sem_ctime; /* last change time */
 Model0___kernel_ulong_t Model0___unused2;
 Model0___kernel_ulong_t Model0_sem_nsems; /* no. of semaphores in array */
 Model0___kernel_ulong_t Model0___unused3;
 Model0___kernel_ulong_t Model0___unused4;
};

/* semop system calls takes an array of these. */
struct Model0_sembuf {
 unsigned short Model0_sem_num; /* semaphore index in array */
 short Model0_sem_op; /* semaphore operation */
 short Model0_sem_flg; /* operation flags */
};

/* arg for semctl system calls. */
union Model0_semun {
 int Model0_val; /* value for SETVAL */
 struct Model0_semid_ds *Model0_buf; /* buffer for IPC_STAT & IPC_SET */
 unsigned short *Model0_array; /* array for GETALL & SETALL */
 struct Model0_seminfo *Model0___buf; /* buffer for IPC_INFO */
 void *Model0___pad;
};

struct Model0_seminfo {
 int Model0_semmap;
 int Model0_semmni;
 int Model0_semmns;
 int Model0_semmnu;
 int Model0_semmsl;
 int Model0_semopm;
 int Model0_semume;
 int Model0_semusz;
 int Model0_semvmx;
 int Model0_semaem;
};

/*
 * SEMMNI, SEMMSL and SEMMNS are default values which can be
 * modified by sysctl.
 * The values has been chosen to be larger than necessary for any
 * known configuration.
 *
 * SEMOPM should not be increased beyond 1000, otherwise there is the
 * risk that semop()/semtimedop() fails due to kernel memory fragmentation when
 * allocating the sop array.
 */
/* unused */

struct Model0_task_struct;

/* One sem_array data structure for each set of semaphores in the system. */
struct Model0_sem_array {
 struct Model0_kern_ipc_perm __attribute__((__aligned__((1 << (6)))))
    Model0_sem_perm; /* permissions .. see ipc.h */
 Model0_time_t Model0_sem_ctime; /* last change time */
 struct Model0_sem *Model0_sem_base; /* ptr to first semaphore in array */
 struct Model0_list_head Model0_pending_alter; /* pending operations */
      /* that alter the array */
 struct Model0_list_head Model0_pending_const; /* pending complex operations */
      /* that do not alter semvals */
 struct Model0_list_head Model0_list_id; /* undo requests on this array */
 int Model0_sem_nsems; /* no. of semaphores in array */
 int Model0_complex_count; /* pending complex operations */
};



struct Model0_sysv_sem {
 struct Model0_sem_undo_list *Model0_undo_list;
};

extern int Model0_copy_semundo(unsigned long Model0_clone_flags, struct Model0_task_struct *Model0_tsk);
extern void Model0_exit_sem(struct Model0_task_struct *Model0_tsk);





/*
 * SHMMNI, SHMMAX and SHMALL are default upper limits which can be
 * modified by sysctl. The SHMMAX and SHMALL values have been chosen to
 * be as large possible without facilitating scenarios where userspace
 * causes overflows when adjusting the limits via operations of the form
 * "retrieve current limit; add X; update limit". It is therefore not
 * advised to make SHMMAX and SHMALL any larger. These limits are
 * suitable for both 32 and 64-bit systems.
 */






/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct Model0_shmid_ds {
 struct Model0_ipc_perm Model0_shm_perm; /* operation perms */
 int Model0_shm_segsz; /* size of segment (bytes) */
 Model0___kernel_time_t Model0_shm_atime; /* last attach time */
 Model0___kernel_time_t Model0_shm_dtime; /* last detach time */
 Model0___kernel_time_t Model0_shm_ctime; /* last change time */
 Model0___kernel_ipc_pid_t Model0_shm_cpid; /* pid of creator */
 Model0___kernel_ipc_pid_t Model0_shm_lpid; /* pid of last operator */
 unsigned short Model0_shm_nattch; /* no. of current attaches */
 unsigned short Model0_shm_unused; /* compatibility */
 void *Model0_shm_unused2; /* ditto - used by DIPC */
 void *Model0_shm_unused3; /* unused */
};

/* Include the definition of shmid64_ds and shminfo64 */






/*
 * The shmid64_ds structure for x86 architecture.
 * Note extra padding because this structure is passed back and forth
 * between kernel and user space.
 *
 * shmid64_ds was originally meant to be architecture specific, but
 * everyone just ended up making identical copies without specific
 * optimizations, so we may just as well all use the same one.
 *
 * 64 bit architectures typically define a 64 bit __kernel_time_t,
 * so they do not need the first two padding words.
 * On big-endian systems, the padding is in the wrong place.
 *
 *
 * Pad space is left for:
 * - 64-bit time_t to solve y2038 problem
 * - 2 miscellaneous 32-bit values
 */

struct Model0_shmid64_ds {
 struct Model0_ipc64_perm Model0_shm_perm; /* operation perms */
 Model0_size_t Model0_shm_segsz; /* size of segment (bytes) */
 Model0___kernel_time_t Model0_shm_atime; /* last attach time */



 Model0___kernel_time_t Model0_shm_dtime; /* last detach time */



 Model0___kernel_time_t Model0_shm_ctime; /* last change time */



 Model0___kernel_pid_t Model0_shm_cpid; /* pid of creator */
 Model0___kernel_pid_t Model0_shm_lpid; /* pid of last operator */
 Model0___kernel_ulong_t Model0_shm_nattch; /* no. of current attaches */
 Model0___kernel_ulong_t Model0___unused4;
 Model0___kernel_ulong_t Model0___unused5;
};

struct Model0_shminfo64 {
 Model0___kernel_ulong_t Model0_shmmax;
 Model0___kernel_ulong_t Model0_shmmin;
 Model0___kernel_ulong_t Model0_shmmni;
 Model0___kernel_ulong_t Model0_shmseg;
 Model0___kernel_ulong_t Model0_shmall;
 Model0___kernel_ulong_t Model0___unused1;
 Model0___kernel_ulong_t Model0___unused2;
 Model0___kernel_ulong_t Model0___unused3;
 Model0___kernel_ulong_t Model0___unused4;
};

/* permission flag for shmget */



/* mode for attach */





/* super user shmctl commands */



/* ipcs ctl commands */



/* Obsolete, used only for backwards compatibility */
struct Model0_shminfo {
 int Model0_shmmax;
 int Model0_shmmin;
 int Model0_shmmni;
 int Model0_shmseg;
 int Model0_shmall;
};

struct Model0_shm_info {
 int Model0_used_ids;
 Model0___kernel_ulong_t Model0_shm_tot; /* total allocated shm */
 Model0___kernel_ulong_t Model0_shm_rss; /* total resident shm */
 Model0___kernel_ulong_t Model0_shm_swp; /* total swapped shm */
 Model0___kernel_ulong_t Model0_swap_attempts;
 Model0___kernel_ulong_t Model0_swap_successes;
};

struct Model0_shmid_kernel /* private to the kernel */
{
 struct Model0_kern_ipc_perm Model0_shm_perm;
 struct Model0_file *Model0_shm_file;
 unsigned long Model0_shm_nattch;
 unsigned long Model0_shm_segsz;
 Model0_time_t Model0_shm_atim;
 Model0_time_t Model0_shm_dtim;
 Model0_time_t Model0_shm_ctim;
 Model0_pid_t Model0_shm_cprid;
 Model0_pid_t Model0_shm_lprid;
 struct Model0_user_struct *Model0_mlock_user;

 /* The task created the shm object.  NULL if the task is dead. */
 struct Model0_task_struct *Model0_shm_creator;
 struct Model0_list_head Model0_shm_clist; /* list by creator */
};

/* shm_mode upper byte flags */





/* Bits [26:31] are reserved */

/*
 * When SHM_HUGETLB is set bits [26:31] encode the log2 of the huge page size.
 * This gives us 6 bits, which is enough until someone invents 128 bit address
 * spaces.
 *
 * Assume these are all power of twos.
 * When 0 use the default page size.
 */






struct Model0_sysv_shm {
 struct Model0_list_head Model0_shm_clist;
};

long Model0_do_shmat(int Model0_shmid, char *Model0_shmaddr, int Model0_shmflg, unsigned long *Model0_addr,
       unsigned long Model0_shmlba);
bool Model0_is_file_shm_hugepages(struct Model0_file *Model0_file);
void Model0_exit_shm(struct Model0_task_struct *Model0_task);














/* Most things should be clean enough to redefine this at will, if care
   is taken to make libc match.  */
typedef unsigned long Model0_old_sigset_t; /* at least 32 bits */

typedef struct {
 unsigned long Model0_sig[(64 / 64)];
} Model0_sigset_t;















/* Avoid too many header ordering problems.  */
struct Model0_siginfo;
/*
#define SIGLOST		29
*/




/* These should not be considered constants from userland.  */



/*
 * SA_FLAGS values:
 *
 * SA_ONSTACK indicates that a registered stack_t will be used.
 * SA_RESTART flag to get restarting signals (which were the default long ago)
 * SA_NOCLDSTOP flag to turn off SIGCHLD when children stop.
 * SA_RESETHAND clears the handler when the signal is delivered.
 * SA_NOCLDWAIT flag on SIGCHLD to inhibit zombies.
 * SA_NODEFER prevents the current signal from being masked in the handler.
 *
 * SA_ONESHOT and SA_NOMASK are the historical Linux names for the Single
 * Unix names RESETHAND and NODEFER respectively.
 */
typedef void Model0___signalfn_t(int);
typedef Model0___signalfn_t *Model0___sighandler_t;

typedef void Model0___restorefn_t(void);
typedef Model0___restorefn_t *Model0___sigrestore_t;
typedef struct Model0_sigaltstack {
 void *Model0_ss_sp;
 int Model0_ss_flags;
 Model0_size_t Model0_ss_size;
} Model0_stack_t;

extern void Model0_do_signal(struct Model0_pt_regs *Model0_regs);









typedef union Model0_sigval {
 int Model0_sival_int;
 void *Model0_sival_ptr;
} Model0_sigval_t;

/*
 * This is the size (including padding) of the part of the
 * struct siginfo that is before the union.
 */
/*
 * The default "si_band" type is "long", as specified by POSIX.
 * However, some architectures want to override this to "int"
 * for historical compatibility reasons, so we allow that.
 */
typedef struct Model0_siginfo {
 int Model0_si_signo;
 int Model0_si_errno;
 int Model0_si_code;

 union {
  int Model0__pad[((128 - (4 * sizeof(int))) / sizeof(int))];

  /* kill() */
  struct {
   Model0___kernel_pid_t Model0__pid; /* sender's pid */
   Model0___kernel_uid32_t Model0__uid; /* sender's uid */
  } Model0__kill;

  /* POSIX.1b timers */
  struct {
   Model0___kernel_timer_t Model0__tid; /* timer id */
   int Model0__overrun; /* overrun count */
   char Model0__pad[sizeof( Model0___kernel_uid32_t) - sizeof(int)];
   Model0_sigval_t Model0__sigval; /* same as below */
   int Model0__sys_private; /* not to be passed to user */
  } Model0__timer;

  /* POSIX.1b signals */
  struct {
   Model0___kernel_pid_t Model0__pid; /* sender's pid */
   Model0___kernel_uid32_t Model0__uid; /* sender's uid */
   Model0_sigval_t Model0__sigval;
  } Model0__rt;

  /* SIGCHLD */
  struct {
   Model0___kernel_pid_t Model0__pid; /* which child */
   Model0___kernel_uid32_t Model0__uid; /* sender's uid */
   int Model0__status; /* exit code */
   Model0___kernel_clock_t Model0__utime;
   Model0___kernel_clock_t Model0__stime;
  } Model0__sigchld;

  /* SIGILL, SIGFPE, SIGSEGV, SIGBUS */
  struct {
   void *Model0__addr; /* faulting insn/memory ref. */



   short Model0__addr_lsb; /* LSB of the reported address */
   union {
    /* used when si_code=SEGV_BNDERR */
    struct {
     void *Model0__lower;
     void *Model0__upper;
    } Model0__addr_bnd;
    /* used when si_code=SEGV_PKUERR */
    __u32 Model0__pkey;
   };
  } Model0__sigfault;

  /* SIGPOLL */
  struct {
   long Model0__band; /* POLL_IN, POLL_OUT, POLL_MSG */
   int Model0__fd;
  } Model0__sigpoll;

  /* SIGSYS */
  struct {
   void *Model0__call_addr; /* calling user insn */
   int Model0__syscall; /* triggering system call number */
   unsigned int Model0__arch; /* AUDIT_ARCH_* of syscall */
  } Model0__sigsys;
 } Model0__sifields;
} Model0_siginfo_t;

/* If the arch shares siginfo, then it has SIGSYS. */



/*
 * How these fields are to be accessed.
 */
/*
 * si_code values
 * Digital reserves positive values for kernel-generated signals.
 */
/*
 * SIGILL si_codes
 */
/*
 * SIGFPE si_codes
 */
/*
 * SIGSEGV si_codes
 */






/*
 * SIGBUS si_codes
 */



/* hardware memory error consumed on a machine check: action required */

/* hardware memory error detected in process but not consumed: action optional*/



/*
 * SIGTRAP si_codes
 */






/*
 * SIGCHLD si_codes
 */
/*
 * SIGPOLL si_codes
 */
/*
 * SIGSYS si_codes
 */



/*
 * sigevent definitions
 * 
 * It seems likely that SIGEV_THREAD will have to be handled from 
 * userspace, libpthread transmuting it to SIGEV_SIGNAL, which the
 * thread manager then catches and does the appropriate nonsense.
 * However, everything is written out here so as to not get lost.
 */





/*
 * This works because the alignment is ok on all current architectures
 * but we leave open this being overridden in the future
 */
typedef struct Model0_sigevent {
 Model0_sigval_t Model0_sigev_value;
 int Model0_sigev_signo;
 int Model0_sigev_notify;
 union {
  int Model0__pad[((64 - (sizeof(int) * 2 + sizeof(Model0_sigval_t))) / sizeof(int))];
   int Model0__tid;

  struct {
   void (*Model0__function)(Model0_sigval_t);
   void *Model0__attribute; /* really pthread_attr_t */
  } Model0__sigev_thread;
 } Model0__sigev_un;
} Model0_sigevent_t;
struct Model0_siginfo;
void Model0_do_schedule_next_timer(struct Model0_siginfo *Model0_info);

extern int Model0_copy_siginfo_to_user(struct Model0_siginfo *Model0_to, const struct Model0_siginfo *Model0_from);




/* bit-flags */

/* mask for all SS_xxx flags */

struct Model0_task_struct;

/* for sysctl */
extern int Model0_print_fatal_signals;
/*
 * Real Time signals may be queued.
 */

struct Model0_sigqueue {
 struct Model0_list_head Model0_list;
 int Model0_flags;
 Model0_siginfo_t Model0_info;
 struct Model0_user_struct *Model0_user;
};

/* flags values. */


struct Model0_sigpending {
 struct Model0_list_head Model0_list;
 Model0_sigset_t Model0_signal;
};





static inline __attribute__((no_instrument_function)) void Model0_copy_siginfo(struct Model0_siginfo *Model0_to, struct Model0_siginfo *Model0_from)
{
 if (Model0_from->Model0_si_code < 0)
  ({ Model0_size_t Model0___len = (sizeof(*Model0_to)); void *Model0___ret; if (__builtin_constant_p(sizeof(*Model0_to)) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_to), (Model0_from), Model0___len); else Model0___ret = __builtin_memcpy((Model0_to), (Model0_from), Model0___len); Model0___ret; });
 else
  /* _sigchld is currently the largest know union member */
  ({ Model0_size_t Model0___len = ((4 * sizeof(int)) + sizeof(Model0_from->Model0__sifields.Model0__sigchld)); void *Model0___ret; if (__builtin_constant_p((4 * sizeof(int)) + sizeof(Model0_from->Model0__sifields.Model0__sigchld)) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_to), (Model0_from), Model0___len); else Model0___ret = __builtin_memcpy((Model0_to), (Model0_from), Model0___len); Model0___ret; });
}



/*
 * Define some primitives to manipulate sigset_t.
 */




/* We don't use <linux/bitops.h> for these because there is no need to
   be atomic.  */
static inline __attribute__((no_instrument_function)) void Model0_sigaddset(Model0_sigset_t *Model0_set, int Model0__sig)
{
 unsigned long Model0_sig = Model0__sig - 1;
 if ((64 / 64) == 1)
  Model0_set->Model0_sig[0] |= 1UL << Model0_sig;
 else
  Model0_set->Model0_sig[Model0_sig / 64] |= 1UL << (Model0_sig % 64);
}

static inline __attribute__((no_instrument_function)) void Model0_sigdelset(Model0_sigset_t *Model0_set, int Model0__sig)
{
 unsigned long Model0_sig = Model0__sig - 1;
 if ((64 / 64) == 1)
  Model0_set->Model0_sig[0] &= ~(1UL << Model0_sig);
 else
  Model0_set->Model0_sig[Model0_sig / 64] &= ~(1UL << (Model0_sig % 64));
}

static inline __attribute__((no_instrument_function)) int Model0_sigismember(Model0_sigset_t *Model0_set, int Model0__sig)
{
 unsigned long Model0_sig = Model0__sig - 1;
 if ((64 / 64) == 1)
  return 1 & (Model0_set->Model0_sig[0] >> Model0_sig);
 else
  return 1 & (Model0_set->Model0_sig[Model0_sig / 64] >> (Model0_sig % 64));
}



static inline __attribute__((no_instrument_function)) int Model0_sigisemptyset(Model0_sigset_t *Model0_set)
{
 switch ((64 / 64)) {
 case 4:
  return (Model0_set->Model0_sig[3] | Model0_set->Model0_sig[2] |
   Model0_set->Model0_sig[1] | Model0_set->Model0_sig[0]) == 0;
 case 2:
  return (Model0_set->Model0_sig[1] | Model0_set->Model0_sig[0]) == 0;
 case 1:
  return Model0_set->Model0_sig[0] == 0;
 default:
  do { bool Model0___cond = !(!(1)); extern void Model0___compiletime_assert_95(void) ; if (Model0___cond) Model0___compiletime_assert_95(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
  return 0;
 }
}
static inline __attribute__((no_instrument_function)) void Model0_sigorsets(Model0_sigset_t *Model0_r, const Model0_sigset_t *Model0_a, const Model0_sigset_t *Model0_b) { unsigned long Model0_a0, Model0_a1, Model0_a2, Model0_a3, Model0_b0, Model0_b1, Model0_b2, Model0_b3; switch ((64 / 64)) { case 4: Model0_a3 = Model0_a->Model0_sig[3]; Model0_a2 = Model0_a->Model0_sig[2]; Model0_b3 = Model0_b->Model0_sig[3]; Model0_b2 = Model0_b->Model0_sig[2]; Model0_r->Model0_sig[3] = ((Model0_a3) | (Model0_b3)); Model0_r->Model0_sig[2] = ((Model0_a2) | (Model0_b2)); case 2: Model0_a1 = Model0_a->Model0_sig[1]; Model0_b1 = Model0_b->Model0_sig[1]; Model0_r->Model0_sig[1] = ((Model0_a1) | (Model0_b1)); case 1: Model0_a0 = Model0_a->Model0_sig[0]; Model0_b0 = Model0_b->Model0_sig[0]; Model0_r->Model0_sig[0] = ((Model0_a0) | (Model0_b0)); break; default: do { bool Model0___cond = !(!(1)); extern void Model0___compiletime_assert_129(void) ; if (Model0___cond) Model0___compiletime_assert_129(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); } }


static inline __attribute__((no_instrument_function)) void Model0_sigandsets(Model0_sigset_t *Model0_r, const Model0_sigset_t *Model0_a, const Model0_sigset_t *Model0_b) { unsigned long Model0_a0, Model0_a1, Model0_a2, Model0_a3, Model0_b0, Model0_b1, Model0_b2, Model0_b3; switch ((64 / 64)) { case 4: Model0_a3 = Model0_a->Model0_sig[3]; Model0_a2 = Model0_a->Model0_sig[2]; Model0_b3 = Model0_b->Model0_sig[3]; Model0_b2 = Model0_b->Model0_sig[2]; Model0_r->Model0_sig[3] = ((Model0_a3) & (Model0_b3)); Model0_r->Model0_sig[2] = ((Model0_a2) & (Model0_b2)); case 2: Model0_a1 = Model0_a->Model0_sig[1]; Model0_b1 = Model0_b->Model0_sig[1]; Model0_r->Model0_sig[1] = ((Model0_a1) & (Model0_b1)); case 1: Model0_a0 = Model0_a->Model0_sig[0]; Model0_b0 = Model0_b->Model0_sig[0]; Model0_r->Model0_sig[0] = ((Model0_a0) & (Model0_b0)); break; default: do { bool Model0___cond = !(!(1)); extern void Model0___compiletime_assert_132(void) ; if (Model0___cond) Model0___compiletime_assert_132(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); } }


static inline __attribute__((no_instrument_function)) void Model0_sigandnsets(Model0_sigset_t *Model0_r, const Model0_sigset_t *Model0_a, const Model0_sigset_t *Model0_b) { unsigned long Model0_a0, Model0_a1, Model0_a2, Model0_a3, Model0_b0, Model0_b1, Model0_b2, Model0_b3; switch ((64 / 64)) { case 4: Model0_a3 = Model0_a->Model0_sig[3]; Model0_a2 = Model0_a->Model0_sig[2]; Model0_b3 = Model0_b->Model0_sig[3]; Model0_b2 = Model0_b->Model0_sig[2]; Model0_r->Model0_sig[3] = ((Model0_a3) & ~(Model0_b3)); Model0_r->Model0_sig[2] = ((Model0_a2) & ~(Model0_b2)); case 2: Model0_a1 = Model0_a->Model0_sig[1]; Model0_b1 = Model0_b->Model0_sig[1]; Model0_r->Model0_sig[1] = ((Model0_a1) & ~(Model0_b1)); case 1: Model0_a0 = Model0_a->Model0_sig[0]; Model0_b0 = Model0_b->Model0_sig[0]; Model0_r->Model0_sig[0] = ((Model0_a0) & ~(Model0_b0)); break; default: do { bool Model0___cond = !(!(1)); extern void Model0___compiletime_assert_135(void) ; if (Model0___cond) Model0___compiletime_assert_135(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); } }
static inline __attribute__((no_instrument_function)) void Model0_signotset(Model0_sigset_t *Model0_set) { switch ((64 / 64)) { case 4: Model0_set->Model0_sig[3] = (~(Model0_set->Model0_sig[3])); Model0_set->Model0_sig[2] = (~(Model0_set->Model0_sig[2])); case 2: Model0_set->Model0_sig[1] = (~(Model0_set->Model0_sig[1])); case 1: Model0_set->Model0_sig[0] = (~(Model0_set->Model0_sig[0])); break; default: do { bool Model0___cond = !(!(1)); extern void Model0___compiletime_assert_157(void) ; if (Model0___cond) Model0___compiletime_assert_157(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); } }




static inline __attribute__((no_instrument_function)) void Model0_sigemptyset(Model0_sigset_t *Model0_set)
{
 switch ((64 / 64)) {
 default:
  memset(Model0_set, 0, sizeof(Model0_sigset_t));
  break;
 case 2: Model0_set->Model0_sig[1] = 0;
 case 1: Model0_set->Model0_sig[0] = 0;
  break;
 }
}

static inline __attribute__((no_instrument_function)) void Model0_sigfillset(Model0_sigset_t *Model0_set)
{
 switch ((64 / 64)) {
 default:
  memset(Model0_set, -1, sizeof(Model0_sigset_t));
  break;
 case 2: Model0_set->Model0_sig[1] = -1;
 case 1: Model0_set->Model0_sig[0] = -1;
  break;
 }
}

/* Some extensions for manipulating the low 32 signals in particular.  */

static inline __attribute__((no_instrument_function)) void Model0_sigaddsetmask(Model0_sigset_t *Model0_set, unsigned long Model0_mask)
{
 Model0_set->Model0_sig[0] |= Model0_mask;
}

static inline __attribute__((no_instrument_function)) void Model0_sigdelsetmask(Model0_sigset_t *Model0_set, unsigned long Model0_mask)
{
 Model0_set->Model0_sig[0] &= ~Model0_mask;
}

static inline __attribute__((no_instrument_function)) int Model0_sigtestsetmask(Model0_sigset_t *Model0_set, unsigned long Model0_mask)
{
 return (Model0_set->Model0_sig[0] & Model0_mask) != 0;
}

static inline __attribute__((no_instrument_function)) void Model0_siginitset(Model0_sigset_t *Model0_set, unsigned long Model0_mask)
{
 Model0_set->Model0_sig[0] = Model0_mask;
 switch ((64 / 64)) {
 default:
  memset(&Model0_set->Model0_sig[1], 0, sizeof(long)*((64 / 64)-1));
  break;
 case 2: Model0_set->Model0_sig[1] = 0;
 case 1: ;
 }
}

static inline __attribute__((no_instrument_function)) void Model0_siginitsetinv(Model0_sigset_t *Model0_set, unsigned long Model0_mask)
{
 Model0_set->Model0_sig[0] = ~Model0_mask;
 switch ((64 / 64)) {
 default:
  memset(&Model0_set->Model0_sig[1], -1, sizeof(long)*((64 / 64)-1));
  break;
 case 2: Model0_set->Model0_sig[1] = -1;
 case 1: ;
 }
}



static inline __attribute__((no_instrument_function)) void Model0_init_sigpending(struct Model0_sigpending *Model0_sig)
{
 Model0_sigemptyset(&Model0_sig->Model0_signal);
 Model0_INIT_LIST_HEAD(&Model0_sig->Model0_list);
}

extern void Model0_flush_sigqueue(struct Model0_sigpending *Model0_queue);

/* Test if 'sig' is valid signal. Use this instead of testing _NSIG directly */
static inline __attribute__((no_instrument_function)) int Model0_valid_signal(unsigned long Model0_sig)
{
 return Model0_sig <= 64 ? 1 : 0;
}

struct Model0_timespec;
struct Model0_pt_regs;

extern int Model0_next_signal(struct Model0_sigpending *Model0_pending, Model0_sigset_t *Model0_mask);
extern int Model0_do_send_sig_info(int Model0_sig, struct Model0_siginfo *Model0_info,
    struct Model0_task_struct *Model0_p, bool Model0_group);
extern int Model0_group_send_sig_info(int Model0_sig, struct Model0_siginfo *Model0_info, struct Model0_task_struct *Model0_p);
extern int Model0___group_send_sig_info(int, struct Model0_siginfo *, struct Model0_task_struct *);
extern int Model0_do_sigtimedwait(const Model0_sigset_t *, Model0_siginfo_t *,
    const struct Model0_timespec *);
extern int Model0_sigprocmask(int, Model0_sigset_t *, Model0_sigset_t *);
extern void Model0_set_current_blocked(Model0_sigset_t *);
extern void Model0___set_current_blocked(const Model0_sigset_t *);
extern int Model0_show_unhandled_signals;

struct Model0_sigaction {

 Model0___sighandler_t Model0_sa_handler;
 unsigned long Model0_sa_flags;





 Model0___sigrestore_t Model0_sa_restorer;

 Model0_sigset_t Model0_sa_mask; /* mask last for extensibility */
};

struct Model0_k_sigaction {
 struct Model0_sigaction Model0_sa;



};
struct Model0_ksignal {
 struct Model0_k_sigaction Model0_ka;
 Model0_siginfo_t Model0_info;
 int Model0_sig;
};

extern int Model0_get_signal(struct Model0_ksignal *Model0_ksig);
extern void Model0_signal_setup_done(int Model0_failed, struct Model0_ksignal *Model0_ksig, int Model0_stepping);
extern void Model0_exit_signals(struct Model0_task_struct *Model0_tsk);
extern void Model0_kernel_sigaction(int, Model0___sighandler_t);

static inline __attribute__((no_instrument_function)) void Model0_allow_signal(int Model0_sig)
{
 /*
	 * Kernel threads handle their own signals. Let the signal code
	 * know it'll be handled, so that they don't get converted to
	 * SIGKILL or just silently dropped.
	 */
 Model0_kernel_sigaction(Model0_sig, ( Model0___sighandler_t)2);
}

static inline __attribute__((no_instrument_function)) void Model0_disallow_signal(int Model0_sig)
{
 Model0_kernel_sigaction(Model0_sig, (( Model0___sighandler_t)1));
}

extern struct Model0_kmem_cache *Model0_sighand_cachep;

int Model0_unhandled_signal(struct Model0_task_struct *Model0_tsk, int Model0_sig);

/*
 * In POSIX a signal is sent either to a specific thread (Linux task)
 * or to the process as a whole (Linux thread group).  How the signal
 * is sent determines whether it's to one thread or the whole group,
 * which determines which signal mask(s) are involved in blocking it
 * from being delivered until later.  When the signal is delivered,
 * either it's caught or ignored by a user handler or it has a default
 * effect that applies to the whole thread group (POSIX process).
 *
 * The possible effects an unblocked signal set to SIG_DFL can have are:
 *   ignore	- Nothing Happens
 *   terminate	- kill the process, i.e. all threads in the group,
 * 		  similar to exit_group.  The group leader (only) reports
 *		  WIFSIGNALED status to its parent.
 *   coredump	- write a core dump file describing all threads using
 *		  the same mm and then kill all those threads
 *   stop 	- stop all the threads in the group, i.e. TASK_STOPPED state
 *
 * SIGKILL and SIGSTOP cannot be caught, blocked, or ignored.
 * Other signals when not blocked and set to SIG_DFL behaves as follows.
 * The job control signals also have other special effects.
 *
 *	+--------------------+------------------+
 *	|  POSIX signal      |  default action  |
 *	+--------------------+------------------+
 *	|  SIGHUP            |  terminate	|
 *	|  SIGINT            |	terminate	|
 *	|  SIGQUIT           |	coredump 	|
 *	|  SIGILL            |	coredump 	|
 *	|  SIGTRAP           |	coredump 	|
 *	|  SIGABRT/SIGIOT    |	coredump 	|
 *	|  SIGBUS            |	coredump 	|
 *	|  SIGFPE            |	coredump 	|
 *	|  SIGKILL           |	terminate(+)	|
 *	|  SIGUSR1           |	terminate	|
 *	|  SIGSEGV           |	coredump 	|
 *	|  SIGUSR2           |	terminate	|
 *	|  SIGPIPE           |	terminate	|
 *	|  SIGALRM           |	terminate	|
 *	|  SIGTERM           |	terminate	|
 *	|  SIGCHLD           |	ignore   	|
 *	|  SIGCONT           |	ignore(*)	|
 *	|  SIGSTOP           |	stop(*)(+)  	|
 *	|  SIGTSTP           |	stop(*)  	|
 *	|  SIGTTIN           |	stop(*)  	|
 *	|  SIGTTOU           |	stop(*)  	|
 *	|  SIGURG            |	ignore   	|
 *	|  SIGXCPU           |	coredump 	|
 *	|  SIGXFSZ           |	coredump 	|
 *	|  SIGVTALRM         |	terminate	|
 *	|  SIGPROF           |	terminate	|
 *	|  SIGPOLL/SIGIO     |	terminate	|
 *	|  SIGSYS/SIGUNUSED  |	coredump 	|
 *	|  SIGSTKFLT         |	terminate	|
 *	|  SIGWINCH          |	ignore   	|
 *	|  SIGPWR            |	terminate	|
 *	|  SIGRTMIN-SIGRTMAX |	terminate       |
 *	+--------------------+------------------+
 *	|  non-POSIX signal  |  default action  |
 *	+--------------------+------------------+
 *	|  SIGEMT            |  coredump	|
 *	+--------------------+------------------+
 *
 * (+) For SIGKILL and SIGSTOP the action is "always", not just "default".
 * (*) Special job control effects:
 * When SIGCONT is sent, it resumes the process (all threads in the group)
 * from TASK_STOPPED state and also clears any pending/queued stop signals
 * (any of those marked with "stop(*)").  This happens regardless of blocking,
 * catching, or ignoring SIGCONT.  When any stop signal is sent, it clears
 * any pending/queued SIGCONT signals; this happens regardless of blocking,
 * catching, or ignored the stop signal, though (except for SIGSTOP) the
 * default action of stopping the process may happen later or never.
 */
void Model0_signals_init(void);

int Model0_restore_altstack(const Model0_stack_t *);
int Model0___save_altstack(Model0_stack_t *, unsigned long);
struct Model0_seq_file;
extern void Model0_render_sigset_t(struct Model0_seq_file *, const char *, Model0_sigset_t *);







enum Model0_pid_type
{
 Model0_PIDTYPE_PID,
 Model0_PIDTYPE_PGID,
 Model0_PIDTYPE_SID,
 Model0_PIDTYPE_MAX
};

/*
 * What is struct pid?
 *
 * A struct pid is the kernel's internal notion of a process identifier.
 * It refers to individual tasks, process groups, and sessions.  While
 * there are processes attached to it the struct pid lives in a hash
 * table, so it and then the processes that it refers to can be found
 * quickly from the numeric pid value.  The attached processes may be
 * quickly accessed by following pointers from struct pid.
 *
 * Storing pid_t values in the kernel and referring to them later has a
 * problem.  The process originally with that pid may have exited and the
 * pid allocator wrapped, and another process could have come along
 * and been assigned that pid.
 *
 * Referring to user space processes by holding a reference to struct
 * task_struct has a problem.  When the user space process exits
 * the now useless task_struct is still kept.  A task_struct plus a
 * stack consumes around 10K of low kernel memory.  More precisely
 * this is THREAD_SIZE + sizeof(struct task_struct).  By comparison
 * a struct pid is about 64 bytes.
 *
 * Holding a reference to struct pid solves both of these problems.
 * It is small so holding a reference does not consume a lot of
 * resources, and since a new struct pid is allocated when the numeric pid
 * value is reused (when pids wrap around) we don't mistakenly refer to new
 * processes.
 */


/*
 * struct upid is used to get the id of the struct pid, as it is
 * seen in particular namespace. Later the struct pid is found with
 * find_pid_ns() using the int nr and struct pid_namespace *ns.
 */

struct Model0_upid {
 /* Try to keep pid_chain in the same cacheline as nr for find_vpid */
 int Model0_nr;
 struct Model0_pid_namespace *Model0_ns;
 struct Model0_hlist_node Model0_pid_chain;
};

struct Model0_pid
{
 Model0_atomic_t Model0_count;
 unsigned int Model0_level;
 /* lists of tasks that use this pid */
 struct Model0_hlist_head Model0_tasks[Model0_PIDTYPE_MAX];
 struct Model0_callback_head Model0_rcu;
 struct Model0_upid Model0_numbers[1];
};

extern struct Model0_pid Model0_init_struct_pid;

struct Model0_pid_link
{
 struct Model0_hlist_node Model0_node;
 struct Model0_pid *Model0_pid;
};

static inline __attribute__((no_instrument_function)) struct Model0_pid *Model0_get_pid(struct Model0_pid *Model0_pid)
{
 if (Model0_pid)
  Model0_atomic_inc(&Model0_pid->Model0_count);
 return Model0_pid;
}

extern void Model0_put_pid(struct Model0_pid *Model0_pid);
extern struct Model0_task_struct *Model0_pid_task(struct Model0_pid *Model0_pid, enum Model0_pid_type);
extern struct Model0_task_struct *Model0_get_pid_task(struct Model0_pid *Model0_pid, enum Model0_pid_type);

extern struct Model0_pid *Model0_get_task_pid(struct Model0_task_struct *Model0_task, enum Model0_pid_type Model0_type);

/*
 * these helpers must be called with the tasklist_lock write-held.
 */
extern void Model0_attach_pid(struct Model0_task_struct *Model0_task, enum Model0_pid_type);
extern void Model0_detach_pid(struct Model0_task_struct *Model0_task, enum Model0_pid_type);
extern void Model0_change_pid(struct Model0_task_struct *Model0_task, enum Model0_pid_type,
   struct Model0_pid *Model0_pid);
extern void Model0_transfer_pid(struct Model0_task_struct *old, struct Model0_task_struct *Model0_new,
    enum Model0_pid_type);

struct Model0_pid_namespace;
extern struct Model0_pid_namespace Model0_init_pid_ns;

/*
 * look up a PID in the hash table. Must be called with the tasklist_lock
 * or rcu_read_lock() held.
 *
 * find_pid_ns() finds the pid in the namespace specified
 * find_vpid() finds the pid by its virtual id, i.e. in the current namespace
 *
 * see also find_task_by_vpid() set in include/linux/sched.h
 */
extern struct Model0_pid *Model0_find_pid_ns(int Model0_nr, struct Model0_pid_namespace *Model0_ns);
extern struct Model0_pid *Model0_find_vpid(int Model0_nr);

/*
 * Lookup a PID in the hash table, and return with it's count elevated.
 */
extern struct Model0_pid *Model0_find_get_pid(int Model0_nr);
extern struct Model0_pid *Model0_find_ge_pid(int Model0_nr, struct Model0_pid_namespace *);
int Model0_next_pidmap(struct Model0_pid_namespace *Model0_pid_ns, unsigned int Model0_last);

extern struct Model0_pid *Model0_alloc_pid(struct Model0_pid_namespace *Model0_ns);
extern void Model0_free_pid(struct Model0_pid *Model0_pid);
extern void Model0_disable_pid_allocation(struct Model0_pid_namespace *Model0_ns);

/*
 * ns_of_pid() returns the pid namespace in which the specified pid was
 * allocated.
 *
 * NOTE:
 * 	ns_of_pid() is expected to be called for a process (task) that has
 * 	an attached 'struct pid' (see attach_pid(), detach_pid()) i.e @pid
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline __attribute__((no_instrument_function)) struct Model0_pid_namespace *Model0_ns_of_pid(struct Model0_pid *Model0_pid)
{
 struct Model0_pid_namespace *Model0_ns = ((void *)0);
 if (Model0_pid)
  Model0_ns = Model0_pid->Model0_numbers[Model0_pid->Model0_level].Model0_ns;
 return Model0_ns;
}

/*
 * is_child_reaper returns true if the pid is the init process
 * of the current namespace. As this one could be checked before
 * pid_ns->child_reaper is assigned in copy_process, we check
 * with the pid number.
 */
static inline __attribute__((no_instrument_function)) bool Model0_is_child_reaper(struct Model0_pid *Model0_pid)
{
 return Model0_pid->Model0_numbers[Model0_pid->Model0_level].Model0_nr == 1;
}

/*
 * the helpers to get the pid's id seen from different namespaces
 *
 * pid_nr()    : global id, i.e. the id seen from the init namespace;
 * pid_vnr()   : virtual id, i.e. the id seen from the pid namespace of
 *               current.
 * pid_nr_ns() : id seen from the ns specified.
 *
 * see also task_xid_nr() etc in include/linux/sched.h
 */

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_pid_nr(struct Model0_pid *Model0_pid)
{
 Model0_pid_t Model0_nr = 0;
 if (Model0_pid)
  Model0_nr = Model0_pid->Model0_numbers[0].Model0_nr;
 return Model0_nr;
}

Model0_pid_t Model0_pid_nr_ns(struct Model0_pid *Model0_pid, struct Model0_pid_namespace *Model0_ns);
Model0_pid_t Model0_pid_vnr(struct Model0_pid *Model0_pid);







   /*
			 * Both old and new leaders may be attached to
			 * the same pid in the middle of de_thread().
			 */












/* Valid values for seccomp.mode and prctl(PR_SET_SECCOMP, <mode>) */




/* Valid operations for seccomp syscall. */



/* Valid flags for SECCOMP_SET_MODE_FILTER */


/*
 * All BPF programs must return a 32-bit value.
 * The bottom 16-bits are for optional return data.
 * The upper 16-bits are ordered from least permissive values to most.
 *
 * The ordering ensures that a min_t() over composed return values always
 * selects the least permissive choice.
 */






/* Masks for the return value sections. */



/**
 * struct seccomp_data - the format the BPF program executes over.
 * @nr: the system call number
 * @arch: indicates system call convention as an AUDIT_ARCH_* value
 *        as defined in <linux/audit.h>.
 * @instruction_pointer: at the time of the system call.
 * @args: up to 6 system call arguments always stored as 64-bit values
 *        regardless of the architecture.
 */
struct Model0_seccomp_data {
 int Model0_nr;
 __u32 Model0_arch;
 __u64 Model0_instruction_pointer;
 __u64 Model0_args[6];
};















/* x32 syscall flag bit */









/*
 * This file contains the system call numbers of the ia32 compat ABI,
 * this is for the kernel only.
 */








/*
 * include/asm-generic/seccomp.h
 *
 * Copyright (C) 2014 Linaro Limited
 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */







/*
 * Include machine specific syscall numbers
 */
static inline __attribute__((no_instrument_function)) const int *Model0_get_compat_mode1_syscalls(void)
{
 static const int Model0_mode1_syscalls_32[] = {
  3, 4,
  1, 119,
  0, /* null terminated */
 };
 return Model0_mode1_syscalls_32;
}

struct Model0_seccomp_filter;
/**
 * struct seccomp - the state of a seccomp'ed process
 *
 * @mode:  indicates one of the valid values above for controlled
 *         system calls available to a process.
 * @filter: must always point to a valid seccomp-filter or NULL as it is
 *          accessed without locking during system call entry.
 *
 *          @filter must only be accessed from the context of current as there
 *          is no read locking.
 */
struct Model0_seccomp {
 int Model0_mode;
 struct Model0_seccomp_filter *Model0_filter;
};


extern int Model0___secure_computing(const struct Model0_seccomp_data *Model0_sd);
static inline __attribute__((no_instrument_function)) int Model0_secure_computing(const struct Model0_seccomp_data *Model0_sd)
{
 if (__builtin_expect(!!(Model0_test_ti_thread_flag(Model0_current_thread_info(), 8)), 0))
  return Model0___secure_computing(Model0_sd);
 return 0;
}




extern long Model0_prctl_get_seccomp(void);
extern long Model0_prctl_set_seccomp(unsigned long, char *);

static inline __attribute__((no_instrument_function)) int Model0_seccomp_mode(struct Model0_seccomp *Model0_s)
{
 return Model0_s->Model0_mode;
}
extern void Model0_put_seccomp_filter(struct Model0_task_struct *Model0_tsk);
extern void Model0_get_seccomp_filter(struct Model0_task_struct *Model0_tsk);
static inline __attribute__((no_instrument_function)) long Model0_seccomp_get_filter(struct Model0_task_struct *Model0_task,
          unsigned long Model0_n, void *Model0_data)
{
 return -22;
}






/*
 * RCU-protected list version
 */



/*
 * Why is there no list_empty_rcu()?  Because list_empty() serves this
 * purpose.  The list_empty() function fetches the RCU-protected pointer
 * and compares it to the address of the list head, but neither dereferences
 * this pointer itself nor provides this pointer to the caller.  Therefore,
 * it is not necessary to use rcu_dereference(), so that list_empty() can
 * be used anywhere you would want to use a list_empty_rcu().
 */

/*
 * INIT_LIST_HEAD_RCU - Initialize a list_head visible to RCU readers
 * @list: list to be initialized
 *
 * You should instead use INIT_LIST_HEAD() for normal initialization and
 * cleanup tasks, when readers have no access to the list being initialized.
 * However, if the list being initialized is visible to readers, you
 * need to keep the compiler from being too mischievous.
 */
static inline __attribute__((no_instrument_function)) void Model0_INIT_LIST_HEAD_RCU(struct Model0_list_head *Model0_list)
{
 ({ union { typeof(Model0_list->Model0_next) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_list->Model0_next)) (Model0_list) }; Model0___write_once_size(&(Model0_list->Model0_next), Model0___u.Model0___c, sizeof(Model0_list->Model0_next)); Model0___u.Model0___val; });
 ({ union { typeof(Model0_list->Model0_prev) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_list->Model0_prev)) (Model0_list) }; Model0___write_once_size(&(Model0_list->Model0_prev), Model0___u.Model0___c, sizeof(Model0_list->Model0_prev)); Model0___u.Model0___val; });
}

/*
 * return the ->next pointer of a list_head in an rcu safe
 * way, we must not access it directly
 */


/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */

static inline __attribute__((no_instrument_function)) void Model0___list_add_rcu(struct Model0_list_head *Model0_new,
  struct Model0_list_head *Model0_prev, struct Model0_list_head *Model0_next)
{
 Model0_new->Model0_next = Model0_next;
 Model0_new->Model0_prev = Model0_prev;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_new); if (__builtin_constant_p(Model0_new) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next))))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))) ((typeof((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) == sizeof(char) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) == sizeof(short) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) == sizeof(int) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) == sizeof(long))); extern void Model0___compiletime_assert_54(void) ; if (Model0___cond) Model0___compiletime_assert_54(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next))))) ((typeof(*((typeof((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next))))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 Model0_next->Model0_prev = Model0_new;
}





/**
 * list_add_rcu - add a new entry to rcu-protected list
 * @new: new entry to be added
 * @head: list head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_add_rcu()
 * or list_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model0_list_add_rcu(struct Model0_list_head *Model0_new, struct Model0_list_head *Model0_head)
{
 Model0___list_add_rcu(Model0_new, Model0_head, Model0_head->Model0_next);
}

/**
 * list_add_tail_rcu - add a new entry to rcu-protected list
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_add_tail_rcu()
 * or list_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model0_list_add_tail_rcu(struct Model0_list_head *Model0_new,
     struct Model0_list_head *Model0_head)
{
 Model0___list_add_rcu(Model0_new, Model0_head->Model0_prev, Model0_head);
}

/**
 * list_del_rcu - deletes entry from list without re-initialization
 * @entry: the element to delete from the list.
 *
 * Note: list_empty() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_del_rcu()
 * or list_add_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 *
 * Note that the caller is not permitted to immediately free
 * the newly deleted entry.  Instead, either synchronize_rcu()
 * or call_rcu() must be used to defer freeing until an RCU
 * grace period has elapsed.
 */
static inline __attribute__((no_instrument_function)) void Model0_list_del_rcu(struct Model0_list_head *Model0_entry)
{
 Model0___list_del_entry(Model0_entry);
 Model0_entry->Model0_prev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_del_init_rcu - deletes entry from hash list with re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: list_unhashed() on the node return true after this. It is
 * useful for RCU based read lockfree traversal if the writer side
 * must know if the list entry is still hashed or already unhashed.
 *
 * In particular, it means that we can not poison the forward pointers
 * that may still be used for walking the hash list and we can only
 * zero the pprev pointer so list_unhashed() will return true after
 * this.
 *
 * The caller must take whatever precautions are necessary (such as
 * holding appropriate locks) to avoid racing with another
 * list-mutation primitive, such as hlist_add_head_rcu() or
 * hlist_del_rcu(), running on this same list.  However, it is
 * perfectly legal to run concurrently with the _rcu list-traversal
 * primitives, such as hlist_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_del_init_rcu(struct Model0_hlist_node *Model0_n)
{
 if (!Model0_hlist_unhashed(Model0_n)) {
  Model0___hlist_del(Model0_n);
  Model0_n->Model0_pprev = ((void *)0);
 }
}

/**
 * list_replace_rcu - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * The @old entry will be replaced with the @new entry atomically.
 * Note: @old should not be empty.
 */
static inline __attribute__((no_instrument_function)) void Model0_list_replace_rcu(struct Model0_list_head *old,
    struct Model0_list_head *Model0_new)
{
 Model0_new->Model0_next = old->Model0_next;
 Model0_new->Model0_prev = old->Model0_prev;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_new); if (__builtin_constant_p(Model0_new) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next))))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))))) ((typeof((*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))) == sizeof(char) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))) == sizeof(short) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))) == sizeof(int) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))) == sizeof(long))); extern void Model0___compiletime_assert_176(void) ; if (Model0___cond) Model0___compiletime_assert_176(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next))))) ((typeof(*((typeof((*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next)))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_list_head **)(&(Model0_new->Model0_prev)->Model0_next))))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 Model0_new->Model0_next->Model0_prev = Model0_new;
 old->Model0_prev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * __list_splice_init_rcu - join an RCU-protected list into an existing list.
 * @list:	the RCU-protected list to splice
 * @prev:	points to the last element of the existing list
 * @next:	points to the first element of the existing list
 * @sync:	function to sync: synchronize_rcu(), synchronize_sched(), ...
 *
 * The list pointed to by @prev and @next can be RCU-read traversed
 * concurrently with this function.
 *
 * Note that this function blocks.
 *
 * Important note: the caller must take whatever action is necessary to prevent
 * any other updates to the existing list.  In principle, it is possible to
 * modify the list as soon as sync() begins execution. If this sort of thing
 * becomes necessary, an alternative version based on call_rcu() could be
 * created.  But only if -really- needed -- there is no shortage of RCU API
 * members.
 */
static inline __attribute__((no_instrument_function)) void Model0___list_splice_init_rcu(struct Model0_list_head *Model0_list,
       struct Model0_list_head *Model0_prev,
       struct Model0_list_head *Model0_next,
       void (*Model0_sync)(void))
{
 struct Model0_list_head *Model0_first = Model0_list->Model0_next;
 struct Model0_list_head *Model0_last = Model0_list->Model0_prev;

 /*
	 * "first" and "last" tracking list, so initialize it.  RCU readers
	 * have access to this list, so we must use INIT_LIST_HEAD_RCU()
	 * instead of INIT_LIST_HEAD().
	 */

 Model0_INIT_LIST_HEAD_RCU(Model0_list);

 /*
	 * At this point, the list body still points to the source list.
	 * Wait for any readers to finish using the list before splicing
	 * the list body into the new list.  Any new readers will see
	 * an empty list.
	 */

 Model0_sync();

 /*
	 * Readers are finished with the source list, so perform splice.
	 * The order is important if the new list is global and accessible
	 * to concurrent RCU readers.  Note that RCU readers are not
	 * permitted to traverse the prev pointers without excluding
	 * this function.
	 */

 Model0_last->Model0_next = Model0_next;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_first); if (__builtin_constant_p(Model0_first) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next))))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))) ((typeof((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) == sizeof(char) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) == sizeof(short) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) == sizeof(int) || sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) == sizeof(long))); extern void Model0___compiletime_assert_234(void) ; if (Model0___cond) Model0___compiletime_assert_234(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next))))) ((typeof(*((typeof((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next)))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_list_head **)(&(Model0_prev)->Model0_next))))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 Model0_first->Model0_prev = Model0_prev;
 Model0_next->Model0_prev = Model0_last;
}

/**
 * list_splice_init_rcu - splice an RCU-protected list into an existing list,
 *                        designed for stacks.
 * @list:	the RCU-protected list to splice
 * @head:	the place in the existing list to splice the first list into
 * @sync:	function to sync: synchronize_rcu(), synchronize_sched(), ...
 */
static inline __attribute__((no_instrument_function)) void Model0_list_splice_init_rcu(struct Model0_list_head *Model0_list,
     struct Model0_list_head *Model0_head,
     void (*Model0_sync)(void))
{
 if (!Model0_list_empty(Model0_list))
  Model0___list_splice_init_rcu(Model0_list, Model0_head, Model0_head->Model0_next, Model0_sync);
}

/**
 * list_splice_tail_init_rcu - splice an RCU-protected list into an existing
 *                             list, designed for queues.
 * @list:	the RCU-protected list to splice
 * @head:	the place in the existing list to splice the first list into
 * @sync:	function to sync: synchronize_rcu(), synchronize_sched(), ...
 */
static inline __attribute__((no_instrument_function)) void Model0_list_splice_tail_init_rcu(struct Model0_list_head *Model0_list,
          struct Model0_list_head *Model0_head,
          void (*Model0_sync)(void))
{
 if (!Model0_list_empty(Model0_list))
  Model0___list_splice_init_rcu(Model0_list, Model0_head->Model0_prev, Model0_head, Model0_sync);
}

/**
 * list_entry_rcu - get the struct for this entry
 * @ptr:        the &struct list_head pointer.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */



/**
 * Where are list_empty_rcu() and list_first_entry_rcu()?
 *
 * Implementing those functions following their counterparts list_empty() and
 * list_first_entry() is not advisable because they lead to subtle race
 * conditions as the following snippet shows:
 *
 * if (!list_empty_rcu(mylist)) {
 *	struct foo *bar = list_first_entry_rcu(mylist, struct foo, list_member);
 *	do_something(bar);
 * }
 *
 * The list may not be empty when list_empty_rcu checks it, but it may be when
 * list_first_entry_rcu rereads the ->next pointer.
 *
 * Rereading the ->next pointer is not a problem for list_empty() and
 * list_first_entry() because they would be protected by a lock that blocks
 * writers.
 *
 * See list_first_or_null_rcu for an alternative.
 */

/**
 * list_first_or_null_rcu - get the first element from a list
 * @ptr:        the list head to take the element from.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * Note that if the list is empty, it returns NULL.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */







/**
 * list_next_or_null_rcu - get the first element from a list
 * @head:	the head for the list.
 * @ptr:        the list head to take the next element from.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * Note that if the ptr is at the end of the list, NULL is returned.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */
/**
 * list_for_each_entry_rcu	-	iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as list_add_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */





/**
 * list_entry_lockless - get the struct for this entry
 * @ptr:        the &struct list_head pointer.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu(), but requires some implicit RCU
 * read-side guarding.  One example is running within a special
 * exception-time environment where preemption is disabled and where
 * lockdep cannot be invoked (in which case updaters must use RCU-sched,
 * as in synchronize_sched(), call_rcu_sched(), and friends).  Another
 * example is when items are added to the list, but never deleted.
 */



/**
 * list_for_each_entry_lockless - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu(), but requires some implicit RCU
 * read-side guarding.  One example is running within a special
 * exception-time environment where preemption is disabled and where
 * lockdep cannot be invoked (in which case updaters must use RCU-sched,
 * as in synchronize_sched(), call_rcu_sched(), and friends).  Another
 * example is when items are added to the list, but never deleted.
 */





/**
 * list_for_each_entry_continue_rcu - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */





/**
 * hlist_del_rcu - deletes entry from hash list without re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: list_unhashed() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the hash list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_del_rcu(struct Model0_hlist_node *Model0_n)
{
 Model0___hlist_del(Model0_n);
 Model0_n->Model0_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_replace_rcu - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * The @old entry will be replaced with the @new entry atomically.
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_replace_rcu(struct Model0_hlist_node *old,
     struct Model0_hlist_node *Model0_new)
{
 struct Model0_hlist_node *Model0_next = old->Model0_next;

 Model0_new->Model0_next = Model0_next;
 Model0_new->Model0_pprev = old->Model0_pprev;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_new); if (__builtin_constant_p(Model0_new) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof((*(struct Model0_hlist_node **)Model0_new->Model0_pprev)) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof((*(struct Model0_hlist_node **)Model0_new->Model0_pprev))) ((typeof(*(struct Model0_hlist_node **)Model0_new->Model0_pprev))(Model0__r_a_p__v)) }; Model0___write_once_size(&((*(struct Model0_hlist_node **)Model0_new->Model0_pprev)), Model0___u.Model0___c, sizeof((*(struct Model0_hlist_node **)Model0_new->Model0_pprev))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&*(struct Model0_hlist_node **)Model0_new->Model0_pprev) == sizeof(char) || sizeof(*&*(struct Model0_hlist_node **)Model0_new->Model0_pprev) == sizeof(short) || sizeof(*&*(struct Model0_hlist_node **)Model0_new->Model0_pprev) == sizeof(int) || sizeof(*&*(struct Model0_hlist_node **)Model0_new->Model0_pprev) == sizeof(long))); extern void Model0___compiletime_assert_446(void) ; if (Model0___cond) Model0___compiletime_assert_446(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&*(struct Model0_hlist_node **)Model0_new->Model0_pprev) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&*(struct Model0_hlist_node **)Model0_new->Model0_pprev)) ((typeof(*((typeof(*(struct Model0_hlist_node **)Model0_new->Model0_pprev))Model0__r_a_p__v)) *)((typeof(*(struct Model0_hlist_node **)Model0_new->Model0_pprev))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&*(struct Model0_hlist_node **)Model0_new->Model0_pprev), Model0___u.Model0___c, sizeof(*&*(struct Model0_hlist_node **)Model0_new->Model0_pprev)); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 if (Model0_next)
  Model0_new->Model0_next->Model0_pprev = &Model0_new->Model0_next;
 old->Model0_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/*
 * return the first or the next element in an RCU protected hlist
 */




/**
 * hlist_add_head_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_add_head_rcu(struct Model0_hlist_node *Model0_n,
     struct Model0_hlist_head *Model0_h)
{
 struct Model0_hlist_node *Model0_first = Model0_h->Model0_first;

 Model0_n->Model0_next = Model0_first;
 Model0_n->Model0_pprev = &Model0_h->Model0_first;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_n); if (__builtin_constant_p(Model0_n) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first))))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))))) ((typeof((*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first))))), Model0___u.Model0___c, sizeof(((*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))) == sizeof(char) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))) == sizeof(short) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))) == sizeof(int) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))) == sizeof(long))); extern void Model0___compiletime_assert_485(void) ; if (Model0___cond) Model0___compiletime_assert_485(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first))))) ((typeof(*((typeof((*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first)))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first))))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 if (Model0_first)
  Model0_first->Model0_pprev = &Model0_n->Model0_next;
}

/**
 * hlist_add_tail_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_add_tail_rcu(struct Model0_hlist_node *Model0_n,
          struct Model0_hlist_head *Model0_h)
{
 struct Model0_hlist_node *Model0_i, *Model0_last = ((void *)0);

 for (Model0_i = (*((struct Model0_hlist_node **)(&(Model0_h)->Model0_first))); Model0_i; Model0_i = (*((struct Model0_hlist_node **)(&(Model0_i)->Model0_next))))
  Model0_last = Model0_i;

 if (Model0_last) {
  Model0_n->Model0_next = Model0_last->Model0_next;
  Model0_n->Model0_pprev = &Model0_last->Model0_next;
  ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_n); if (__builtin_constant_p(Model0_n) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next))))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))))) ((typeof((*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))) == sizeof(char) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))) == sizeof(short) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))) == sizeof(int) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))) == sizeof(long))); extern void Model0___compiletime_assert_520(void) ; if (Model0___cond) Model0___compiletime_assert_520(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next))))) ((typeof(*((typeof((*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next)))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_last)->Model0_next))))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 } else {
  Model0_hlist_add_head_rcu(Model0_n, Model0_h);
 }
}

/**
 * hlist_add_before_rcu
 * @n: the new element to add to the hash list.
 * @next: the existing element to add the new element before.
 *
 * Description:
 * Adds the specified element to the specified hlist
 * before the specified node while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_add_before_rcu(struct Model0_hlist_node *Model0_n,
     struct Model0_hlist_node *Model0_next)
{
 Model0_n->Model0_pprev = Model0_next->Model0_pprev;
 Model0_n->Model0_next = Model0_next;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_n); if (__builtin_constant_p(Model0_n) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev))))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))))) ((typeof((*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev))))), Model0___u.Model0___c, sizeof(((*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))) == sizeof(char) || sizeof(*&(*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))) == sizeof(short) || sizeof(*&(*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))) == sizeof(int) || sizeof(*&(*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))) == sizeof(long))); extern void Model0___compiletime_assert_549(void) ; if (Model0___cond) Model0___compiletime_assert_549(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev))))) ((typeof(*((typeof((*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev)))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_hlist_node **)((Model0_n)->Model0_pprev))))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 Model0_next->Model0_pprev = &Model0_n->Model0_next;
}

/**
 * hlist_add_behind_rcu
 * @n: the new element to add to the hash list.
 * @prev: the existing element to add the new element after.
 *
 * Description:
 * Adds the specified element to the specified hlist
 * after the specified node while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_add_behind_rcu(struct Model0_hlist_node *Model0_n,
     struct Model0_hlist_node *Model0_prev)
{
 Model0_n->Model0_next = Model0_prev->Model0_next;
 Model0_n->Model0_pprev = &Model0_prev->Model0_next;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_n); if (__builtin_constant_p(Model0_n) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next))))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))))) ((typeof((*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))) == sizeof(char) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))) == sizeof(short) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))) == sizeof(int) || sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))) == sizeof(long))); extern void Model0___compiletime_assert_576(void) ; if (Model0___cond) Model0___compiletime_assert_576(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next))))) ((typeof(*((typeof((*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next)))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_hlist_node **)(&(Model0_prev)->Model0_next))))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 if (Model0_n->Model0_next)
  Model0_n->Model0_next->Model0_pprev = &Model0_n->Model0_next;
}






/**
 * hlist_for_each_entry_rcu - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */







/**
 * hlist_for_each_entry_rcu_notrace - iterate over rcu list of given type (for tracing)
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 *
 * This is the same as hlist_for_each_entry_rcu() except that it does
 * not do any RCU debugging or tracing.
 */







/**
 * hlist_for_each_entry_rcu_bh - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */







/**
 * hlist_for_each_entry_continue_rcu - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */







/**
 * hlist_for_each_entry_continue_rcu_bh - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */







/**
 * hlist_for_each_entry_from_rcu - iterate over a hlist continuing from current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */
/*
 * RT Mutexes: blocking mutual exclusion locks with PI support
 *
 * started by Ingo Molnar and Thomas Gleixner:
 *
 *  Copyright (C) 2004-2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
 *
 * This file contains the public data structure and API definitions.
 */
extern int Model0_max_lock_depth; /* for sysctl */

/**
 * The rt_mutex structure
 *
 * @wait_lock:	spinlock to protect the structure
 * @waiters:	rbtree root to enqueue waiters in priority order
 * @waiters_leftmost: top waiter
 * @owner:	the mutex owner
 */
struct Model0_rt_mutex {
 Model0_raw_spinlock_t Model0_wait_lock;
 struct Model0_rb_root Model0_waiters;
 struct Model0_rb_node *Model0_waiters_leftmost;
 struct Model0_task_struct *Model0_owner;






};

struct Model0_rt_mutex_waiter;
struct Model0_hrtimer_sleeper;






 static inline __attribute__((no_instrument_function)) int Model0_rt_mutex_debug_check_no_locks_freed(const void *Model0_from,
             unsigned long Model0_len)
 {
 return 0;
 }
/**
 * rt_mutex_is_locked - is the mutex locked
 * @lock: the mutex to be queried
 *
 * Returns 1 if the mutex is locked, 0 if unlocked.
 */
static inline __attribute__((no_instrument_function)) int Model0_rt_mutex_is_locked(struct Model0_rt_mutex *Model0_lock)
{
 return Model0_lock->Model0_owner != ((void *)0);
}

extern void Model0___rt_mutex_init(struct Model0_rt_mutex *Model0_lock, const char *Model0_name);
extern void Model0_rt_mutex_destroy(struct Model0_rt_mutex *Model0_lock);

extern void Model0_rt_mutex_lock(struct Model0_rt_mutex *Model0_lock);
extern int Model0_rt_mutex_lock_interruptible(struct Model0_rt_mutex *Model0_lock);
extern int Model0_rt_mutex_timed_lock(struct Model0_rt_mutex *Model0_lock,
          struct Model0_hrtimer_sleeper *Model0_timeout);

extern int Model0_rt_mutex_trylock(struct Model0_rt_mutex *Model0_lock);

extern void Model0_rt_mutex_unlock(struct Model0_rt_mutex *Model0_lock);





/*
 *  include/linux/hrtimer.h
 *
 *  hrtimers - High-resolution kernel timers
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 *
 *  For licencing details see kernel-base/COPYING
 */







struct Model0_timerqueue_node {
 struct Model0_rb_node Model0_node;
 Model0_ktime_t Model0_expires;
};

struct Model0_timerqueue_head {
 struct Model0_rb_root Model0_head;
 struct Model0_timerqueue_node *Model0_next;
};


extern bool Model0_timerqueue_add(struct Model0_timerqueue_head *Model0_head,
      struct Model0_timerqueue_node *Model0_node);
extern bool Model0_timerqueue_del(struct Model0_timerqueue_head *Model0_head,
      struct Model0_timerqueue_node *Model0_node);
extern struct Model0_timerqueue_node *Model0_timerqueue_iterate_next(
      struct Model0_timerqueue_node *Model0_node);

/**
 * timerqueue_getnext - Returns the timer with the earliest expiration time
 *
 * @head: head of timerqueue
 *
 * Returns a pointer to the timer node that has the
 * earliest expiration time.
 */
static inline __attribute__((no_instrument_function))
struct Model0_timerqueue_node *Model0_timerqueue_getnext(struct Model0_timerqueue_head *Model0_head)
{
 return Model0_head->Model0_next;
}

static inline __attribute__((no_instrument_function)) void Model0_timerqueue_init(struct Model0_timerqueue_node *Model0_node)
{
 ((&Model0_node->Model0_node)->Model0___rb_parent_color = (unsigned long)(&Model0_node->Model0_node));
}

static inline __attribute__((no_instrument_function)) void Model0_timerqueue_init_head(struct Model0_timerqueue_head *Model0_head)
{
 Model0_head->Model0_head = (struct Model0_rb_root) { ((void *)0), };
 Model0_head->Model0_next = ((void *)0);
}

struct Model0_hrtimer_clock_base;
struct Model0_hrtimer_cpu_base;

/*
 * Mode arguments of xxx_hrtimer functions:
 */
enum Model0_hrtimer_mode {
 Model0_HRTIMER_MODE_ABS = 0x0, /* Time value is absolute */
 Model0_HRTIMER_MODE_REL = 0x1, /* Time value is relative to now */
 Model0_HRTIMER_MODE_PINNED = 0x02, /* Timer is bound to CPU */
 Model0_HRTIMER_MODE_ABS_PINNED = 0x02,
 Model0_HRTIMER_MODE_REL_PINNED = 0x03,
};

/*
 * Return values for the callback function
 */
enum Model0_hrtimer_restart {
 Model0_HRTIMER_NORESTART, /* Timer is not restarted */
 Model0_HRTIMER_RESTART, /* Timer must be restarted */
};

/*
 * Values to track state of the timer
 *
 * Possible states:
 *
 * 0x00		inactive
 * 0x01		enqueued into rbtree
 *
 * The callback state is not part of the timer->state because clearing it would
 * mean touching the timer after the callback, this makes it impossible to free
 * the timer from the callback function.
 *
 * Therefore we track the callback state in:
 *
 *	timer->base->cpu_base->running == timer
 *
 * On SMP it is possible to have a "callback function running and enqueued"
 * status. It happens for example when a posix timer expired and the callback
 * queued a signal. Between dropping the lock which protects the posix timer
 * and reacquiring the base lock of the hrtimer, another CPU can deliver the
 * signal and rearm the timer.
 *
 * All state transitions are protected by cpu_base->lock.
 */



/**
 * struct hrtimer - the basic hrtimer structure
 * @node:	timerqueue node, which also manages node.expires,
 *		the absolute expiry time in the hrtimers internal
 *		representation. The time is related to the clock on
 *		which the timer is based. Is setup by adding
 *		slack to the _softexpires value. For non range timers
 *		identical to _softexpires.
 * @_softexpires: the absolute earliest expiry time of the hrtimer.
 *		The time which was given as expiry time when the timer
 *		was armed.
 * @function:	timer expiry callback function
 * @base:	pointer to the timer base (per cpu and per clock)
 * @state:	state information (See bit values above)
 * @is_rel:	Set if the timer was armed relative
 * @start_pid:  timer statistics field to store the pid of the task which
 *		started the timer
 * @start_site:	timer statistics field to store the site where the timer
 *		was started
 * @start_comm: timer statistics field to store the name of the process which
 *		started the timer
 *
 * The hrtimer structure must be initialized by hrtimer_init()
 */
struct Model0_hrtimer {
 struct Model0_timerqueue_node Model0_node;
 Model0_ktime_t Model0__softexpires;
 enum Model0_hrtimer_restart (*Model0_function)(struct Model0_hrtimer *);
 struct Model0_hrtimer_clock_base *Model0_base;
 Model0_u8 Model0_state;
 Model0_u8 Model0_is_rel;

 int Model0_start_pid;
 void *Model0_start_site;
 char Model0_start_comm[16];

};

/**
 * struct hrtimer_sleeper - simple sleeper structure
 * @timer:	embedded timer structure
 * @task:	task to wake up
 *
 * task is set to NULL, when the timer expires.
 */
struct Model0_hrtimer_sleeper {
 struct Model0_hrtimer Model0_timer;
 struct Model0_task_struct *Model0_task;
};







/**
 * struct hrtimer_clock_base - the timer base for a specific clock
 * @cpu_base:		per cpu clock base
 * @index:		clock type index for per_cpu support when moving a
 *			timer to a base on another cpu.
 * @clockid:		clock id for per_cpu support
 * @active:		red black tree root node for the active timers
 * @get_time:		function to retrieve the current time of the clock
 * @offset:		offset of this clock to the monotonic base
 */
struct Model0_hrtimer_clock_base {
 struct Model0_hrtimer_cpu_base *Model0_cpu_base;
 int Model0_index;
 Model0_clockid_t Model0_clockid;
 struct Model0_timerqueue_head Model0_active;
 Model0_ktime_t (*Model0_get_time)(void);
 Model0_ktime_t Model0_offset;
} __attribute__((__aligned__(64)));

enum Model0_hrtimer_base_type {
 Model0_HRTIMER_BASE_MONOTONIC,
 Model0_HRTIMER_BASE_REALTIME,
 Model0_HRTIMER_BASE_BOOTTIME,
 Model0_HRTIMER_BASE_TAI,
 Model0_HRTIMER_MAX_CLOCK_BASES,
};

/*
 * struct hrtimer_cpu_base - the per cpu clock bases
 * @lock:		lock protecting the base and associated clock bases
 *			and timers
 * @seq:		seqcount around __run_hrtimer
 * @running:		pointer to the currently running hrtimer
 * @cpu:		cpu number
 * @active_bases:	Bitfield to mark bases with active timers
 * @clock_was_set_seq:	Sequence counter of clock was set events
 * @migration_enabled:	The migration of hrtimers to other cpus is enabled
 * @nohz_active:	The nohz functionality is enabled
 * @expires_next:	absolute time of the next event which was scheduled
 *			via clock_set_next_event()
 * @next_timer:		Pointer to the first expiring timer
 * @in_hrtirq:		hrtimer_interrupt() is currently executing
 * @hres_active:	State of high resolution mode
 * @hang_detected:	The last hrtimer interrupt detected a hang
 * @nr_events:		Total number of hrtimer interrupt events
 * @nr_retries:		Total number of hrtimer interrupt retries
 * @nr_hangs:		Total number of hrtimer interrupt hangs
 * @max_hang_time:	Maximum time spent in hrtimer_interrupt
 * @clock_base:		array of clock bases for this cpu
 *
 * Note: next_timer is just an optimization for __remove_hrtimer().
 *	 Do not dereference the pointer because it is not reliable on
 *	 cross cpu removals.
 */
struct Model0_hrtimer_cpu_base {
 Model0_raw_spinlock_t Model0_lock;
 Model0_seqcount_t Model0_seq;
 struct Model0_hrtimer *Model0_running;
 unsigned int Model0_cpu;
 unsigned int Model0_active_bases;
 unsigned int Model0_clock_was_set_seq;
 bool Model0_migration_enabled;
 bool Model0_nohz_active;

 unsigned int Model0_in_hrtirq : 1,
     Model0_hres_active : 1,
     Model0_hang_detected : 1;
 Model0_ktime_t Model0_expires_next;
 struct Model0_hrtimer *Model0_next_timer;
 unsigned int Model0_nr_events;
 unsigned int Model0_nr_retries;
 unsigned int Model0_nr_hangs;
 unsigned int Model0_max_hang_time;

 struct Model0_hrtimer_clock_base Model0_clock_base[Model0_HRTIMER_MAX_CLOCK_BASES];
} __attribute__((__aligned__((1 << (6)))));

static inline __attribute__((no_instrument_function)) void Model0_hrtimer_set_expires(struct Model0_hrtimer *Model0_timer, Model0_ktime_t Model0_time)
{
 do { bool Model0___cond = !(!(sizeof(struct Model0_hrtimer_clock_base) > 64)); extern void Model0___compiletime_assert_211(void) ; if (Model0___cond) Model0___compiletime_assert_211(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);

 Model0_timer->Model0_node.Model0_expires = Model0_time;
 Model0_timer->Model0__softexpires = Model0_time;
}

static inline __attribute__((no_instrument_function)) void Model0_hrtimer_set_expires_range(struct Model0_hrtimer *Model0_timer, Model0_ktime_t Model0_time, Model0_ktime_t Model0_delta)
{
 Model0_timer->Model0__softexpires = Model0_time;
 Model0_timer->Model0_node.Model0_expires = Model0_ktime_add_safe(Model0_time, Model0_delta);
}

static inline __attribute__((no_instrument_function)) void Model0_hrtimer_set_expires_range_ns(struct Model0_hrtimer *Model0_timer, Model0_ktime_t Model0_time, Model0_u64 Model0_delta)
{
 Model0_timer->Model0__softexpires = Model0_time;
 Model0_timer->Model0_node.Model0_expires = Model0_ktime_add_safe(Model0_time, Model0_ns_to_ktime(Model0_delta));
}

static inline __attribute__((no_instrument_function)) void Model0_hrtimer_set_expires_tv64(struct Model0_hrtimer *Model0_timer, Model0_s64 Model0_tv64)
{
 Model0_timer->Model0_node.Model0_expires.Model0_tv64 = Model0_tv64;
 Model0_timer->Model0__softexpires.Model0_tv64 = Model0_tv64;
}

static inline __attribute__((no_instrument_function)) void Model0_hrtimer_add_expires(struct Model0_hrtimer *Model0_timer, Model0_ktime_t Model0_time)
{
 Model0_timer->Model0_node.Model0_expires = Model0_ktime_add_safe(Model0_timer->Model0_node.Model0_expires, Model0_time);
 Model0_timer->Model0__softexpires = Model0_ktime_add_safe(Model0_timer->Model0__softexpires, Model0_time);
}

static inline __attribute__((no_instrument_function)) void Model0_hrtimer_add_expires_ns(struct Model0_hrtimer *Model0_timer, Model0_u64 Model0_ns)
{
 Model0_timer->Model0_node.Model0_expires = ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_timer->Model0_node.Model0_expires).Model0_tv64 + (Model0_ns) }; });
 Model0_timer->Model0__softexpires = ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_timer->Model0__softexpires).Model0_tv64 + (Model0_ns) }; });
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_hrtimer_get_expires(const struct Model0_hrtimer *Model0_timer)
{
 return Model0_timer->Model0_node.Model0_expires;
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_hrtimer_get_softexpires(const struct Model0_hrtimer *Model0_timer)
{
 return Model0_timer->Model0__softexpires;
}

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_hrtimer_get_expires_tv64(const struct Model0_hrtimer *Model0_timer)
{
 return Model0_timer->Model0_node.Model0_expires.Model0_tv64;
}
static inline __attribute__((no_instrument_function)) Model0_s64 Model0_hrtimer_get_softexpires_tv64(const struct Model0_hrtimer *Model0_timer)
{
 return Model0_timer->Model0__softexpires.Model0_tv64;
}

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_hrtimer_get_expires_ns(const struct Model0_hrtimer *Model0_timer)
{
 return ((Model0_timer->Model0_node.Model0_expires).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_hrtimer_expires_remaining(const struct Model0_hrtimer *Model0_timer)
{
 return ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_timer->Model0_node.Model0_expires).Model0_tv64 - (Model0_timer->Model0_base->Model0_get_time()).Model0_tv64 }; });
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_hrtimer_cb_get_time(struct Model0_hrtimer *Model0_timer)
{
 return Model0_timer->Model0_base->Model0_get_time();
}


struct Model0_clock_event_device;

extern void Model0_hrtimer_interrupt(struct Model0_clock_event_device *Model0_dev);

static inline __attribute__((no_instrument_function)) int Model0_hrtimer_is_hres_active(struct Model0_hrtimer *Model0_timer)
{
 return Model0_timer->Model0_base->Model0_cpu_base->Model0_hres_active;
}

extern void Model0_hrtimer_peek_ahead_timers(void);

/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */





extern void Model0_clock_was_set_delayed(void);

extern unsigned int Model0_hrtimer_resolution;
static inline __attribute__((no_instrument_function)) Model0_ktime_t
Model0___hrtimer_expires_remaining_adjusted(const struct Model0_hrtimer *Model0_timer, Model0_ktime_t Model0_now)
{
 Model0_ktime_t Model0_rem = ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_timer->Model0_node.Model0_expires).Model0_tv64 - (Model0_now).Model0_tv64 }; });

 /*
	 * Adjust relative timers for the extra we added in
	 * hrtimer_start_range_ns() to prevent short timeouts.
	 */
 if (0 && Model0_timer->Model0_is_rel)
  Model0_rem.Model0_tv64 -= Model0_hrtimer_resolution;
 return Model0_rem;
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t
Model0_hrtimer_expires_remaining_adjusted(const struct Model0_hrtimer *Model0_timer)
{
 return Model0___hrtimer_expires_remaining_adjusted(Model0_timer,
          Model0_timer->Model0_base->Model0_get_time());
}

extern void Model0_clock_was_set(void);

extern void Model0_timerfd_clock_was_set(void);



extern void Model0_hrtimers_resume(void);

extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model0_tick_device) Model0_tick_cpu_device;


/* Exported timer functions: */

/* Initialize timers: */
extern void Model0_hrtimer_init(struct Model0_hrtimer *Model0_timer, Model0_clockid_t Model0_which_clock,
    enum Model0_hrtimer_mode Model0_mode);







static inline __attribute__((no_instrument_function)) void Model0_hrtimer_init_on_stack(struct Model0_hrtimer *Model0_timer,
      Model0_clockid_t Model0_which_clock,
      enum Model0_hrtimer_mode Model0_mode)
{
 Model0_hrtimer_init(Model0_timer, Model0_which_clock, Model0_mode);
}
static inline __attribute__((no_instrument_function)) void Model0_destroy_hrtimer_on_stack(struct Model0_hrtimer *Model0_timer) { }


/* Basic timer operations: */
extern void Model0_hrtimer_start_range_ns(struct Model0_hrtimer *Model0_timer, Model0_ktime_t Model0_tim,
       Model0_u64 Model0_range_ns, const enum Model0_hrtimer_mode Model0_mode);

/**
 * hrtimer_start - (re)start an hrtimer on the current CPU
 * @timer:	the timer to be added
 * @tim:	expiry time
 * @mode:	expiry mode: absolute (HRTIMER_MODE_ABS) or
 *		relative (HRTIMER_MODE_REL)
 */
static inline __attribute__((no_instrument_function)) void Model0_hrtimer_start(struct Model0_hrtimer *Model0_timer, Model0_ktime_t Model0_tim,
     const enum Model0_hrtimer_mode Model0_mode)
{
 Model0_hrtimer_start_range_ns(Model0_timer, Model0_tim, 0, Model0_mode);
}

extern int Model0_hrtimer_cancel(struct Model0_hrtimer *Model0_timer);
extern int Model0_hrtimer_try_to_cancel(struct Model0_hrtimer *Model0_timer);

static inline __attribute__((no_instrument_function)) void Model0_hrtimer_start_expires(struct Model0_hrtimer *Model0_timer,
      enum Model0_hrtimer_mode Model0_mode)
{
 Model0_u64 Model0_delta;
 Model0_ktime_t Model0_soft, Model0_hard;
 Model0_soft = Model0_hrtimer_get_softexpires(Model0_timer);
 Model0_hard = Model0_hrtimer_get_expires(Model0_timer);
 Model0_delta = ((({ (Model0_ktime_t){ .Model0_tv64 = (Model0_hard).Model0_tv64 - (Model0_soft).Model0_tv64 }; })).Model0_tv64);
 Model0_hrtimer_start_range_ns(Model0_timer, Model0_soft, Model0_delta, Model0_mode);
}

static inline __attribute__((no_instrument_function)) void Model0_hrtimer_restart(struct Model0_hrtimer *Model0_timer)
{
 Model0_hrtimer_start_expires(Model0_timer, Model0_HRTIMER_MODE_ABS);
}

/* Query timers: */
extern Model0_ktime_t Model0___hrtimer_get_remaining(const struct Model0_hrtimer *Model0_timer, bool Model0_adjust);

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_hrtimer_get_remaining(const struct Model0_hrtimer *Model0_timer)
{
 return Model0___hrtimer_get_remaining(Model0_timer, false);
}

extern Model0_u64 Model0_hrtimer_get_next_event(void);

extern bool Model0_hrtimer_active(const struct Model0_hrtimer *Model0_timer);

/*
 * Helper function to check, whether the timer is on one of the queues
 */
static inline __attribute__((no_instrument_function)) int Model0_hrtimer_is_queued(struct Model0_hrtimer *Model0_timer)
{
 return Model0_timer->Model0_state & 0x01;
}

/*
 * Helper function to check, whether the timer is running the callback
 * function
 */
static inline __attribute__((no_instrument_function)) int Model0_hrtimer_callback_running(struct Model0_hrtimer *Model0_timer)
{
 return Model0_timer->Model0_base->Model0_cpu_base->Model0_running == Model0_timer;
}

/* Forward a hrtimer so it expires after now: */
extern Model0_u64
Model0_hrtimer_forward(struct Model0_hrtimer *Model0_timer, Model0_ktime_t Model0_now, Model0_ktime_t Model0_interval);

/**
 * hrtimer_forward_now - forward the timer expiry so it expires after now
 * @timer:	hrtimer to forward
 * @interval:	the interval to forward
 *
 * Forward the timer expiry so it will expire after the current time
 * of the hrtimer clock base. Returns the number of overruns.
 *
 * Can be safely called from the callback function of @timer. If
 * called from other contexts @timer must neither be enqueued nor
 * running the callback and the caller needs to take care of
 * serialization.
 *
 * Note: This only updates the timer expiry value and does not requeue
 * the timer.
 */
static inline __attribute__((no_instrument_function)) Model0_u64 Model0_hrtimer_forward_now(struct Model0_hrtimer *Model0_timer,
          Model0_ktime_t Model0_interval)
{
 return Model0_hrtimer_forward(Model0_timer, Model0_timer->Model0_base->Model0_get_time(), Model0_interval);
}

/* Precise sleep: */
extern long Model0_hrtimer_nanosleep(struct Model0_timespec *Model0_rqtp,
         struct Model0_timespec *Model0_rmtp,
         const enum Model0_hrtimer_mode Model0_mode,
         const Model0_clockid_t Model0_clockid);
extern long Model0_hrtimer_nanosleep_restart(struct Model0_restart_block *Model0_restart_block);

extern void Model0_hrtimer_init_sleeper(struct Model0_hrtimer_sleeper *Model0_sl,
     struct Model0_task_struct *Model0_tsk);

extern int Model0_schedule_hrtimeout_range(Model0_ktime_t *Model0_expires, Model0_u64 Model0_delta,
      const enum Model0_hrtimer_mode Model0_mode);
extern int Model0_schedule_hrtimeout_range_clock(Model0_ktime_t *Model0_expires,
       Model0_u64 Model0_delta,
       const enum Model0_hrtimer_mode Model0_mode,
       int Model0_clock);
extern int Model0_schedule_hrtimeout(Model0_ktime_t *Model0_expires, const enum Model0_hrtimer_mode Model0_mode);

/* Soft interrupt function to run the hrtimer queues: */
extern void Model0_hrtimer_run_queues(void);

/* Bootup initialization: */
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_hrtimers_init(void);

/* Show pending timers: */
extern void Model0_sysrq_timer_list_show(void);

int Model0_hrtimers_prepare_cpu(unsigned int Model0_cpu);

int Model0_hrtimers_dead_cpu(unsigned int Model0_cpu);




struct Model0_task_struct;
static inline __attribute__((no_instrument_function)) void Model0_kcov_task_init(struct Model0_task_struct *Model0_t) {}
static inline __attribute__((no_instrument_function)) void Model0_kcov_task_exit(struct Model0_task_struct *Model0_t) {}
/*
 * task_io_accounting: a structure which is used for recording a single task's
 * IO statistics.
 *
 * Don't include this header file directly - it is designed to be dragged in via
 * sched.h.
 *
 * Blame Andrew Morton for all this.
 */

struct Model0_task_io_accounting {

 /* bytes read */
 Model0_u64 Model0_rchar;
 /*  bytes written */
 Model0_u64 Model0_wchar;
 /* # of read syscalls */
 Model0_u64 Model0_syscr;
 /* # of write syscalls */
 Model0_u64 Model0_syscw;



 /*
	 * The number of bytes which this task has caused to be read from
	 * storage.
	 */
 Model0_u64 Model0_read_bytes;

 /*
	 * The number of bytes which this task has caused, or shall cause to be
	 * written to disk.
	 */
 Model0_u64 Model0_write_bytes;

 /*
	 * A task can cause "negative" IO too.  If this task truncates some
	 * dirty pagecache, some IO which another task has been accounted for
	 * (in its write_bytes) will not be happening.  We _could_ just
	 * subtract that from the truncating task's write_bytes, but there is
	 * information loss in doing that.
	 */
 Model0_u64 Model0_cancelled_write_bytes;

};
/*
 * latencytop.h: Infrastructure for displaying latency
 *
 * (C) Copyright 2008 Intel Corporation
 * Author: Arjan van de Ven <arjan@linux.intel.com>
 *
 */





struct Model0_task_struct;
static inline __attribute__((no_instrument_function)) void
Model0_account_scheduler_latency(struct Model0_task_struct *Model0_task, int Model0_usecs, int Model0_inter)
{
}

static inline __attribute__((no_instrument_function)) void Model0_clear_all_latency_tracing(struct Model0_task_struct *Model0_p)
{
}
/* Credentials management - see Documentation/security/credentials.txt
 *
 * Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public Licence
 * as published by the Free Software Foundation; either version
 * 2 of the Licence, or (at your option) any later version.
 */







/* Authentication token and access key management
 *
 * Copyright (C) 2004, 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 *
 *
 * See Documentation/security/keys.txt for information on keys/keyrings.
 */
/* Generic associative array implementation.
 *
 * See Documentation/assoc_array.txt for information.
 *
 * Copyright (C) 2013 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public Licence
 * as published by the Free Software Foundation; either version
 * 2 of the Licence, or (at your option) any later version.
 */
/*
 * Generic associative array.
 */
struct Model0_assoc_array {
 struct Model0_assoc_array_ptr *Model0_root; /* The node at the root of the tree */
 unsigned long Model0_nr_leaves_on_tree;
};

/*
 * Operations on objects and index keys for use by array manipulation routines.
 */
struct Model0_assoc_array_ops {
 /* Method to get a chunk of an index key from caller-supplied data */
 unsigned long (*Model0_get_key_chunk)(const void *Model0_index_key, int Model0_level);

 /* Method to get a piece of an object's index key */
 unsigned long (*Model0_get_object_key_chunk)(const void *Model0_object, int Model0_level);

 /* Is this the object we're looking for? */
 bool (*Model0_compare_object)(const void *Model0_object, const void *Model0_index_key);

 /* How different is an object from an index key, to a bit position in
	 * their keys? (or -1 if they're the same)
	 */
 int (*Model0_diff_objects)(const void *Model0_object, const void *Model0_index_key);

 /* Method to free an object. */
 void (*Model0_free_object)(void *Model0_object);
};

/*
 * Access and manipulation functions.
 */
struct Model0_assoc_array_edit;

static inline __attribute__((no_instrument_function)) void Model0_assoc_array_init(struct Model0_assoc_array *Model0_array)
{
 Model0_array->Model0_root = ((void *)0);
 Model0_array->Model0_nr_leaves_on_tree = 0;
}

extern int Model0_assoc_array_iterate(const struct Model0_assoc_array *Model0_array,
          int (*Model0_iterator)(const void *Model0_object,
            void *Model0_iterator_data),
          void *Model0_iterator_data);
extern void *Model0_assoc_array_find(const struct Model0_assoc_array *Model0_array,
         const struct Model0_assoc_array_ops *Model0_ops,
         const void *Model0_index_key);
extern void Model0_assoc_array_destroy(struct Model0_assoc_array *Model0_array,
    const struct Model0_assoc_array_ops *Model0_ops);
extern struct Model0_assoc_array_edit *Model0_assoc_array_insert(struct Model0_assoc_array *Model0_array,
         const struct Model0_assoc_array_ops *Model0_ops,
         const void *Model0_index_key,
         void *Model0_object);
extern void Model0_assoc_array_insert_set_object(struct Model0_assoc_array_edit *Model0_edit,
       void *Model0_object);
extern struct Model0_assoc_array_edit *Model0_assoc_array_delete(struct Model0_assoc_array *Model0_array,
         const struct Model0_assoc_array_ops *Model0_ops,
         const void *Model0_index_key);
extern struct Model0_assoc_array_edit *Model0_assoc_array_clear(struct Model0_assoc_array *Model0_array,
        const struct Model0_assoc_array_ops *Model0_ops);
extern void Model0_assoc_array_apply_edit(struct Model0_assoc_array_edit *Model0_edit);
extern void Model0_assoc_array_cancel_edit(struct Model0_assoc_array_edit *Model0_edit);
extern int Model0_assoc_array_gc(struct Model0_assoc_array *Model0_array,
     const struct Model0_assoc_array_ops *Model0_ops,
     bool (*Model0_iterator)(void *Model0_object, void *Model0_iterator_data),
     void *Model0_iterator_data);




/* key handle serial number */
typedef Model0_int32_t Model0_key_serial_t;

/* key handle permissions mask */
typedef Model0_uint32_t Model0_key_perm_t;

struct Model0_key;
struct Model0_seq_file;
struct Model0_user_struct;
struct Model0_signal_struct;
struct Model0_cred;

struct Model0_key_type;
struct Model0_key_owner;
struct Model0_keyring_list;
struct Model0_keyring_name;

struct Model0_keyring_index_key {
 struct Model0_key_type *Model0_type;
 const char *Model0_description;
 Model0_size_t Model0_desc_len;
};

union Model0_key_payload {
 void *Model0_rcu_data0;
 void *Model0_data[4];
};

/*****************************************************************************/
/*
 * key reference with possession attribute handling
 *
 * NOTE! key_ref_t is a typedef'd pointer to a type that is not actually
 * defined. This is because we abuse the bottom bit of the reference to carry a
 * flag to indicate whether the calling process possesses that key in one of
 * its keyrings.
 *
 * the key_ref_t has been made a separate type so that the compiler can reject
 * attempts to dereference it without proper conversion.
 *
 * the three functions are used to assemble and disassemble references
 */
typedef struct Model0___key_reference_with_attributes *Model0_key_ref_t;

static inline __attribute__((no_instrument_function)) Model0_key_ref_t Model0_make_key_ref(const struct Model0_key *Model0_key,
         bool Model0_possession)
{
 return (Model0_key_ref_t) ((unsigned long) Model0_key | Model0_possession);
}

static inline __attribute__((no_instrument_function)) struct Model0_key *Model0_key_ref_to_ptr(const Model0_key_ref_t Model0_key_ref)
{
 return (struct Model0_key *) ((unsigned long) Model0_key_ref & ~1UL);
}

static inline __attribute__((no_instrument_function)) bool Model0_is_key_possessed(const Model0_key_ref_t Model0_key_ref)
{
 return (unsigned long) Model0_key_ref & 1UL;
}

/*****************************************************************************/
/*
 * authentication token / access credential / keyring
 * - types of key include:
 *   - keyrings
 *   - disk encryption IDs
 *   - Kerberos TGTs and tickets
 */
struct Model0_key {
 Model0_atomic_t Model0_usage; /* number of references */
 Model0_key_serial_t Model0_serial; /* key serial number */
 union {
  struct Model0_list_head Model0_graveyard_link;
  struct Model0_rb_node Model0_serial_node;
 };
 struct Model0_rw_semaphore Model0_sem; /* change vs change sem */
 struct Model0_key_user *Model0_user; /* owner of this key */
 void *Model0_security; /* security data for this key */
 union {
  Model0_time_t Model0_expiry; /* time at which key expires (or 0) */
  Model0_time_t Model0_revoked_at; /* time at which key was revoked */
 };
 Model0_time_t Model0_last_used_at; /* last time used for LRU keyring discard */
 Model0_kuid_t Model0_uid;
 Model0_kgid_t Model0_gid;
 Model0_key_perm_t Model0_perm; /* access permissions */
 unsigned short Model0_quotalen; /* length added to quota */
 unsigned short Model0_datalen; /* payload data length
						 * - may not match RCU dereferenced payload
						 * - payload should contain own length
						 */







 unsigned long Model0_flags; /* status flags (change with bitops) */
 /* the key type and key description string
	 * - the desc is used to match a key against search criteria
	 * - it should be a printable string
	 * - eg: for krb5 AFS, this might be "afs@REDHAT.COM"
	 */
 union {
  struct Model0_keyring_index_key Model0_index_key;
  struct {
   struct Model0_key_type *Model0_type; /* type of key */
   char *Model0_description;
  };
 };

 /* key data
	 * - this is used to hold the data actually used in cryptography or
	 *   whatever
	 */
 union {
  union Model0_key_payload Model0_payload;
  struct {
   /* Keyring bits */
   struct Model0_list_head Model0_name_link;
   struct Model0_assoc_array Model0_keys;
  };
  int Model0_reject_error;
 };

 /* This is set on a keyring to restrict the addition of a link to a key
	 * to it.  If this method isn't provided then it is assumed that the
	 * keyring is open to any addition.  It is ignored for non-keyring
	 * keys.
	 *
	 * This is intended for use with rings of trusted keys whereby addition
	 * to the keyring needs to be controlled.  KEY_ALLOC_BYPASS_RESTRICTION
	 * overrides this, allowing the kernel to add extra keys without
	 * restriction.
	 */
 int (*Model0_restrict_link)(struct Model0_key *Model0_keyring,
        const struct Model0_key_type *Model0_type,
        const union Model0_key_payload *Model0_payload);
};

extern struct Model0_key *Model0_key_alloc(struct Model0_key_type *Model0_type,
        const char *Model0_desc,
        Model0_kuid_t Model0_uid, Model0_kgid_t Model0_gid,
        const struct Model0_cred *Model0_cred,
        Model0_key_perm_t Model0_perm,
        unsigned long Model0_flags,
        int (*Model0_restrict_link)(struct Model0_key *,
        const struct Model0_key_type *,
        const union Model0_key_payload *));
extern void Model0_key_revoke(struct Model0_key *Model0_key);
extern void Model0_key_invalidate(struct Model0_key *Model0_key);
extern void Model0_key_put(struct Model0_key *Model0_key);

static inline __attribute__((no_instrument_function)) struct Model0_key *Model0___key_get(struct Model0_key *Model0_key)
{
 Model0_atomic_inc(&Model0_key->Model0_usage);
 return Model0_key;
}

static inline __attribute__((no_instrument_function)) struct Model0_key *Model0_key_get(struct Model0_key *Model0_key)
{
 return Model0_key ? Model0___key_get(Model0_key) : Model0_key;
}

static inline __attribute__((no_instrument_function)) void Model0_key_ref_put(Model0_key_ref_t Model0_key_ref)
{
 Model0_key_put(Model0_key_ref_to_ptr(Model0_key_ref));
}

extern struct Model0_key *Model0_request_key(struct Model0_key_type *Model0_type,
          const char *Model0_description,
          const char *Model0_callout_info);

extern struct Model0_key *Model0_request_key_with_auxdata(struct Model0_key_type *Model0_type,
         const char *Model0_description,
         const void *Model0_callout_info,
         Model0_size_t Model0_callout_len,
         void *Model0_aux);

extern struct Model0_key *Model0_request_key_async(struct Model0_key_type *Model0_type,
         const char *Model0_description,
         const void *Model0_callout_info,
         Model0_size_t Model0_callout_len);

extern struct Model0_key *Model0_request_key_async_with_auxdata(struct Model0_key_type *Model0_type,
        const char *Model0_description,
        const void *Model0_callout_info,
        Model0_size_t Model0_callout_len,
        void *Model0_aux);

extern int Model0_wait_for_key_construction(struct Model0_key *Model0_key, bool Model0_intr);

extern int Model0_key_validate(const struct Model0_key *Model0_key);

extern Model0_key_ref_t Model0_key_create_or_update(Model0_key_ref_t Model0_keyring,
          const char *Model0_type,
          const char *Model0_description,
          const void *Model0_payload,
          Model0_size_t Model0_plen,
          Model0_key_perm_t Model0_perm,
          unsigned long Model0_flags);

extern int Model0_key_update(Model0_key_ref_t Model0_key,
        const void *Model0_payload,
        Model0_size_t Model0_plen);

extern int Model0_key_link(struct Model0_key *Model0_keyring,
      struct Model0_key *Model0_key);

extern int Model0_key_unlink(struct Model0_key *Model0_keyring,
        struct Model0_key *Model0_key);

extern struct Model0_key *Model0_keyring_alloc(const char *Model0_description, Model0_kuid_t Model0_uid, Model0_kgid_t Model0_gid,
     const struct Model0_cred *Model0_cred,
     Model0_key_perm_t Model0_perm,
     unsigned long Model0_flags,
     int (*Model0_restrict_link)(struct Model0_key *,
            const struct Model0_key_type *,
            const union Model0_key_payload *),
     struct Model0_key *Model0_dest);

extern int Model0_restrict_link_reject(struct Model0_key *Model0_keyring,
    const struct Model0_key_type *Model0_type,
    const union Model0_key_payload *Model0_payload);

extern int Model0_keyring_clear(struct Model0_key *Model0_keyring);

extern Model0_key_ref_t Model0_keyring_search(Model0_key_ref_t Model0_keyring,
    struct Model0_key_type *Model0_type,
    const char *Model0_description);

extern int Model0_keyring_add_key(struct Model0_key *Model0_keyring,
      struct Model0_key *Model0_key);

extern struct Model0_key *Model0_key_lookup(Model0_key_serial_t Model0_id);

static inline __attribute__((no_instrument_function)) Model0_key_serial_t Model0_key_serial(const struct Model0_key *Model0_key)
{
 return Model0_key ? Model0_key->Model0_serial : 0;
}

extern void Model0_key_set_timeout(struct Model0_key *, unsigned);

/*
 * The permissions required on a key that we're looking up.
 */
/**
 * key_is_instantiated - Determine if a key has been positively instantiated
 * @key: The key to check.
 *
 * Return true if the specified key has been positively instantiated, false
 * otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_key_is_instantiated(const struct Model0_key *Model0_key)
{
 return (__builtin_constant_p((0)) ? Model0_constant_test_bit((0), (&Model0_key->Model0_flags)) : Model0_variable_test_bit((0), (&Model0_key->Model0_flags))) &&
  !(__builtin_constant_p((5)) ? Model0_constant_test_bit((5), (&Model0_key->Model0_flags)) : Model0_variable_test_bit((5), (&Model0_key->Model0_flags)));
}
extern struct Model0_ctl_table Model0_key_sysctls[];

/*
 * the userspace interface
 */
extern int Model0_install_thread_keyring_to_cred(struct Model0_cred *Model0_cred);
extern void Model0_key_fsuid_changed(struct Model0_task_struct *Model0_tsk);
extern void Model0_key_fsgid_changed(struct Model0_task_struct *Model0_tsk);
extern void Model0_key_init(void);
/*
 * SELinux services exported to the rest of the kernel.
 *
 * Author: James Morris <jmorris@redhat.com>
 *
 * Copyright (C) 2005 Red Hat, Inc., James Morris <jmorris@redhat.com>
 * Copyright (C) 2006 Trusted Computer Solutions, Inc. <dgoeddel@trustedcs.com>
 * Copyright (C) 2006 IBM Corporation, Timothy R. Chavez <tinytim@us.ibm.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2,
 * as published by the Free Software Foundation.
 */



struct Model0_selinux_audit_rule;
struct Model0_audit_context;
struct Model0_kern_ipc_perm;



/**
 * selinux_is_enabled - is SELinux enabled?
 */
bool Model0_selinux_is_enabled(void);



struct Model0_user_struct;
struct Model0_cred;
struct Model0_inode;

/*
 * COW Supplementary groups list
 */



struct Model0_group_info {
 Model0_atomic_t Model0_usage;
 int Model0_ngroups;
 int Model0_nblocks;
 Model0_kgid_t Model0_small_block[32];
 Model0_kgid_t *Model0_blocks[0];
};

/**
 * get_group_info - Get a reference to a group info structure
 * @group_info: The group info to reference
 *
 * This gets a reference to a set of supplementary groups.
 *
 * If the caller is accessing a task's credentials, they must hold the RCU read
 * lock when reading.
 */
static inline __attribute__((no_instrument_function)) struct Model0_group_info *Model0_get_group_info(struct Model0_group_info *Model0_gi)
{
 Model0_atomic_inc(&Model0_gi->Model0_usage);
 return Model0_gi;
}

/**
 * put_group_info - Release a reference to a group info structure
 * @group_info: The group info to release
 */






extern struct Model0_group_info Model0_init_groups;

extern struct Model0_group_info *Model0_groups_alloc(int);
extern void Model0_groups_free(struct Model0_group_info *);

extern int Model0_in_group_p(Model0_kgid_t);
extern int Model0_in_egroup_p(Model0_kgid_t);
extern int Model0_set_current_groups(struct Model0_group_info *);
extern void Model0_set_groups(struct Model0_cred *, struct Model0_group_info *);
extern int Model0_groups_search(const struct Model0_group_info *, Model0_kgid_t);
extern bool Model0_may_setgroups(void);

/* access the groups "array" with this macro */



/*
 * The security context of a task
 *
 * The parts of the context break down into two categories:
 *
 *  (1) The objective context of a task.  These parts are used when some other
 *	task is attempting to affect this one.
 *
 *  (2) The subjective context.  These details are used when the task is acting
 *	upon another object, be that a file, a task, a key or whatever.
 *
 * Note that some members of this structure belong to both categories - the
 * LSM security pointer for instance.
 *
 * A task has two security pointers.  task->real_cred points to the objective
 * context that defines that task's actual details.  The objective part of this
 * context is used whenever that task is acted upon.
 *
 * task->cred points to the subjective context that defines the details of how
 * that task is going to act upon another object.  This may be overridden
 * temporarily to point to another security context, but normally points to the
 * same context as task->real_cred.
 */
struct Model0_cred {
 Model0_atomic_t Model0_usage;







 Model0_kuid_t Model0_uid; /* real UID of the task */
 Model0_kgid_t Model0_gid; /* real GID of the task */
 Model0_kuid_t Model0_suid; /* saved UID of the task */
 Model0_kgid_t Model0_sgid; /* saved GID of the task */
 Model0_kuid_t Model0_euid; /* effective UID of the task */
 Model0_kgid_t Model0_egid; /* effective GID of the task */
 Model0_kuid_t Model0_fsuid; /* UID for VFS ops */
 Model0_kgid_t Model0_fsgid; /* GID for VFS ops */
 unsigned Model0_securebits; /* SUID-less security management */
 Model0_kernel_cap_t Model0_cap_inheritable; /* caps our children can inherit */
 Model0_kernel_cap_t Model0_cap_permitted; /* caps we're permitted */
 Model0_kernel_cap_t Model0_cap_effective; /* caps we can actually use */
 Model0_kernel_cap_t Model0_cap_bset; /* capability bounding set */
 Model0_kernel_cap_t Model0_cap_ambient; /* Ambient capability set */

 unsigned char Model0_jit_keyring; /* default keyring to attach requested
					 * keys to */
 struct Model0_key *Model0_session_keyring; /* keyring inherited over fork */
 struct Model0_key *Model0_process_keyring; /* keyring private to this process */
 struct Model0_key *Model0_thread_keyring; /* keyring private to this thread */
 struct Model0_key *Model0_request_key_auth; /* assumed request_key authority */


 void *Model0_security; /* subjective LSM security */

 struct Model0_user_struct *Model0_user; /* real user ID subscription */
 struct Model0_user_namespace *Model0_user_ns; /* user_ns the caps and keyrings are relative to. */
 struct Model0_group_info *Model0_group_info; /* supplementary groups for euid/fsgid */
 struct Model0_callback_head Model0_rcu; /* RCU deletion hook */
};

extern void Model0___put_cred(struct Model0_cred *);
extern void Model0_exit_creds(struct Model0_task_struct *);
extern int Model0_copy_creds(struct Model0_task_struct *, unsigned long);
extern const struct Model0_cred *Model0_get_task_cred(struct Model0_task_struct *);
extern struct Model0_cred *Model0_cred_alloc_blank(void);
extern struct Model0_cred *Model0_prepare_creds(void);
extern struct Model0_cred *Model0_prepare_exec_creds(void);
extern int Model0_commit_creds(struct Model0_cred *);
extern void Model0_abort_creds(struct Model0_cred *);
extern const struct Model0_cred *Model0_override_creds(const struct Model0_cred *);
extern void Model0_revert_creds(const struct Model0_cred *);
extern struct Model0_cred *Model0_prepare_kernel_cred(struct Model0_task_struct *);
extern int Model0_change_create_files_as(struct Model0_cred *, struct Model0_inode *);
extern int Model0_set_security_override(struct Model0_cred *, Model0_u32);
extern int Model0_set_security_override_from_ctx(struct Model0_cred *, const char *);
extern int Model0_set_create_files_as(struct Model0_cred *, struct Model0_inode *);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_cred_init(void);

/*
 * check for validity of credentials
 */
static inline __attribute__((no_instrument_function)) void Model0_validate_creds(const struct Model0_cred *Model0_cred)
{
}
static inline __attribute__((no_instrument_function)) void Model0_validate_creds_for_do_exit(struct Model0_task_struct *Model0_tsk)
{
}
static inline __attribute__((no_instrument_function)) void Model0_validate_process_creds(void)
{
}


static inline __attribute__((no_instrument_function)) bool Model0_cap_ambient_invariant_ok(const struct Model0_cred *Model0_cred)
{
 return Model0_cap_issubset(Model0_cred->Model0_cap_ambient,
       Model0_cap_intersect(Model0_cred->Model0_cap_permitted,
       Model0_cred->Model0_cap_inheritable));
}

/**
 * get_new_cred - Get a reference on a new set of credentials
 * @cred: The new credentials to reference
 *
 * Get a reference on the specified set of new credentials.  The caller must
 * release the reference.
 */
static inline __attribute__((no_instrument_function)) struct Model0_cred *Model0_get_new_cred(struct Model0_cred *Model0_cred)
{
 Model0_atomic_inc(&Model0_cred->Model0_usage);
 return Model0_cred;
}

/**
 * get_cred - Get a reference on a set of credentials
 * @cred: The credentials to reference
 *
 * Get a reference on the specified set of credentials.  The caller must
 * release the reference.
 *
 * This is used to deal with a committed set of credentials.  Although the
 * pointer is const, this will temporarily discard the const and increment the
 * usage count.  The purpose of this is to attempt to catch at compile time the
 * accidental alteration of a set of credentials that should be considered
 * immutable.
 */
static inline __attribute__((no_instrument_function)) const struct Model0_cred *Model0_get_cred(const struct Model0_cred *Model0_cred)
{
 struct Model0_cred *Model0_nonconst_cred = (struct Model0_cred *) Model0_cred;
 Model0_validate_creds(Model0_cred);
 return Model0_get_new_cred(Model0_nonconst_cred);
}

/**
 * put_cred - Release a reference to a set of credentials
 * @cred: The credentials to release
 *
 * Release a reference to a set of credentials, deleting them when the last ref
 * is released.
 *
 * This takes a const pointer to a set of credentials because the credentials
 * on task_struct are attached by const pointers to prevent accidental
 * alteration of otherwise immutable credential sets.
 */
static inline __attribute__((no_instrument_function)) void Model0_put_cred(const struct Model0_cred *Model0__cred)
{
 struct Model0_cred *Model0_cred = (struct Model0_cred *) Model0__cred;

 Model0_validate_creds(Model0_cred);
 if (Model0_atomic_dec_and_test(&(Model0_cred)->Model0_usage))
  Model0___put_cred(Model0_cred);
}

/**
 * current_cred - Access the current task's subjective credentials
 *
 * Access the subjective credentials of the current task.  RCU-safe,
 * since nobody else can modify it.
 */



/**
 * current_real_cred - Access the current task's objective credentials
 *
 * Access the objective credentials of the current task.  RCU-safe,
 * since nobody else can modify it.
 */



/**
 * __task_cred - Access a task's objective credentials
 * @task: The task to query
 *
 * Access the objective credentials of a task.  The caller must hold the RCU
 * readlock.
 *
 * The result of this function should not be passed directly to get_cred();
 * rather get_task_cred() should be used instead.
 */



/**
 * get_current_cred - Get the current task's subjective credentials
 *
 * Get the subjective credentials of the current task, pinning them so that
 * they can't go away.  Accessing the current task's credentials directly is
 * not permitted.
 */



/**
 * get_current_user - Get the current task's user_struct
 *
 * Get the user record of the current task, pinning it so that it can't go
 * away.
 */
/**
 * get_current_groups - Get the current task's supplementary group list
 *
 * Get the supplementary group list of the current task, pinning it so that it
 * can't go away.
 */
extern struct Model0_user_namespace Model0_init_user_ns;



static inline __attribute__((no_instrument_function)) struct Model0_user_namespace *Model0_current_user_ns(void)
{
 return &Model0_init_user_ns;
}



     /* used by file system utilities that
	                                   look at the superblock, etc.  */
/* Since UDF 2.01 is ISO 13346 based... */
/*
 * linux/cgroup-defs.h - basic definitions for cgroup
 *
 * This file provides basic type and interface.  Include this file directly
 * only if necessary to avoid cyclic dependencies.
 */





/*
 * include/linux/idr.h
 * 
 * 2002-10-18  written by Jim Houston jim.houston@ccur.com
 *	Copyright (C) 2002 by Concurrent Computer Corporation
 *	Distributed under the GNU GPL license version 2.
 *
 * Small id to pointer translation service avoiding fixed sized
 * tables.
 */
/*
 * We want shallower trees and thus more bits covered at each layer.  8
 * bits gives us large enough first layer for most use cases and maximum
 * tree depth of 4.  Each idr_layer is slightly larger than 2k on 64bit and
 * 1k on 32bit.
 */




struct Model0_idr_layer {
 int Model0_prefix; /* the ID prefix of this idr_layer */
 int Model0_layer; /* distance from leaf */
 struct Model0_idr_layer *Model0_ary[1<<8];
 int Model0_count; /* When zero, we can release it */
 union {
  /* A zero bit means "space here" */
  unsigned long Model0_bitmap[((((1 << 8)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
  struct Model0_callback_head Model0_callback_head;
 };
};

struct Model0_idr {
 struct Model0_idr_layer *Model0_hint; /* the last layer allocated from */
 struct Model0_idr_layer *Model0_top;
 int Model0_layers; /* only valid w/o concurrent changes */
 int Model0_cur; /* current pos for cyclic allocation */
 Model0_spinlock_t Model0_lock;
 int Model0_id_free_cnt;
 struct Model0_idr_layer *Model0_id_free;
};







/**
 * DOC: idr sync
 * idr synchronization (stolen from radix-tree.h)
 *
 * idr_find() is able to be called locklessly, using RCU. The caller must
 * ensure calls to this function are made within rcu_read_lock() regions.
 * Other readers (lock-free or otherwise) and modifications may be running
 * concurrently.
 *
 * It is still required that the caller manage the synchronization and
 * lifetimes of the items. So if RCU lock-free lookups are used, typically
 * this would mean that the items have their own locks, or are amenable to
 * lock-free access; and that the items are freed by RCU (or only freed after
 * having been deleted from the idr tree *and* a synchronize_rcu() grace
 * period).
 */

/*
 * This is what we export.
 */

void *Model0_idr_find_slowpath(struct Model0_idr *Model0_idp, int Model0_id);
void Model0_idr_preload(Model0_gfp_t Model0_gfp_mask);
int Model0_idr_alloc(struct Model0_idr *Model0_idp, void *Model0_ptr, int Model0_start, int Model0_end, Model0_gfp_t Model0_gfp_mask);
int Model0_idr_alloc_cyclic(struct Model0_idr *Model0_idr, void *Model0_ptr, int Model0_start, int Model0_end, Model0_gfp_t Model0_gfp_mask);
int Model0_idr_for_each(struct Model0_idr *Model0_idp,
   int (*Model0_fn)(int Model0_id, void *Model0_p, void *Model0_data), void *Model0_data);
void *Model0_idr_get_next(struct Model0_idr *Model0_idp, int *Model0_nextid);
void *Model0_idr_replace(struct Model0_idr *Model0_idp, void *Model0_ptr, int Model0_id);
void Model0_idr_remove(struct Model0_idr *Model0_idp, int Model0_id);
void Model0_idr_destroy(struct Model0_idr *Model0_idp);
void Model0_idr_init(struct Model0_idr *Model0_idp);
bool Model0_idr_is_empty(struct Model0_idr *Model0_idp);

/**
 * idr_preload_end - end preload section started with idr_preload()
 *
 * Each idr_preload() should be matched with an invocation of this
 * function.  See idr_preload() for details.
 */
static inline __attribute__((no_instrument_function)) void Model0_idr_preload_end(void)
{
 __asm__ __volatile__("": : :"memory");
}

/**
 * idr_find - return pointer for given id
 * @idr: idr handle
 * @id: lookup key
 *
 * Return the pointer given the id it has been registered with.  A %NULL
 * return indicates that @id is not valid or you passed %NULL in
 * idr_get_new().
 *
 * This function can be called under rcu_read_lock(), given that the leaf
 * pointers lifetimes are correctly managed.
 */
static inline __attribute__((no_instrument_function)) void *Model0_idr_find(struct Model0_idr *Model0_idr, int Model0_id)
{
 struct Model0_idr_layer *Model0_hint = ({ typeof(Model0_idr->Model0_hint) Model0_________p1 = ({ typeof(Model0_idr->Model0_hint) Model0__________p1 = ({ union { typeof(Model0_idr->Model0_hint) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_idr->Model0_hint), Model0___u.Model0___c, sizeof(Model0_idr->Model0_hint)); else Model0___read_once_size_nocheck(&(Model0_idr->Model0_hint), Model0___u.Model0___c, sizeof(Model0_idr->Model0_hint)); Model0___u.Model0___val; }); typeof(*(Model0_idr->Model0_hint)) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); ((typeof(*Model0_idr->Model0_hint) *)(Model0_________p1)); });

 if (Model0_hint && (Model0_id & ~((1 << 8)-1)) == Model0_hint->Model0_prefix)
  return ({ typeof(Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)]) Model0_________p1 = ({ typeof(Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)]) Model0__________p1 = ({ union { typeof(Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)]) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)]), Model0___u.Model0___c, sizeof(Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)])); else Model0___read_once_size_nocheck(&(Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)]), Model0___u.Model0___c, sizeof(Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)])); Model0___u.Model0___val; }); typeof(*(Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)])) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); ((typeof(*Model0_hint->Model0_ary[Model0_id & ((1 << 8)-1)]) *)(Model0_________p1)); });

 return Model0_idr_find_slowpath(Model0_idr, Model0_id);
}

/**
 * idr_for_each_entry - iterate over an idr's elements of a given type
 * @idp:     idr handle
 * @entry:   the type * to use as cursor
 * @id:      id entry's key
 *
 * @entry and @id do not need to be initialized before the loop, and
 * after normal terminatinon @entry is left with the value NULL.  This
 * is convenient for a "not found" value.
 */



/**
 * idr_for_each_entry - continue iteration over an idr's elements of a given type
 * @idp:     idr handle
 * @entry:   the type * to use as cursor
 * @id:      id entry's key
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */





/*
 * IDA - IDR based id allocator, use when translation from id to
 * pointer isn't necessary.
 *
 * IDA_BITMAP_LONGS is calculated to be one less to accommodate
 * ida_bitmap->nr_busy so that the whole struct fits in 128 bytes.
 */




struct Model0_ida_bitmap {
 long Model0_nr_busy;
 unsigned long Model0_bitmap[(128 / sizeof(long) - 1)];
};

struct Model0_ida {
 struct Model0_idr Model0_idr;
 struct Model0_ida_bitmap *Model0_free_bitmap;
};




int Model0_ida_pre_get(struct Model0_ida *Model0_ida, Model0_gfp_t Model0_gfp_mask);
int Model0_ida_get_new_above(struct Model0_ida *Model0_ida, int Model0_starting_id, int *Model0_p_id);
void Model0_ida_remove(struct Model0_ida *Model0_ida, int Model0_id);
void Model0_ida_destroy(struct Model0_ida *Model0_ida);
void Model0_ida_init(struct Model0_ida *Model0_ida);

int Model0_ida_simple_get(struct Model0_ida *Model0_ida, unsigned int Model0_start, unsigned int Model0_end,
     Model0_gfp_t Model0_gfp_mask);
void Model0_ida_simple_remove(struct Model0_ida *Model0_ida, unsigned int Model0_id);

/**
 * ida_get_new - allocate new ID
 * @ida:	idr handle
 * @p_id:	pointer to the allocated handle
 *
 * Simple wrapper around ida_get_new_above() w/ @starting_id of zero.
 */
static inline __attribute__((no_instrument_function)) int Model0_ida_get_new(struct Model0_ida *Model0_ida, int *Model0_p_id)
{
 return Model0_ida_get_new_above(Model0_ida, 0, Model0_p_id);
}

void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_idr_init_cache(void);











/*
 * RCU-based infrastructure for lightweight reader-writer locking
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 * Copyright (c) 2015, Red Hat, Inc.
 *
 * Author: Oleg Nesterov <oleg@redhat.com>
 */







enum Model0_rcu_sync_type { Model0_RCU_SYNC, Model0_RCU_SCHED_SYNC, Model0_RCU_BH_SYNC };

/* Structure to mediate between updaters and fastpath-using readers.  */
struct Model0_rcu_sync {
 int Model0_gp_state;
 int Model0_gp_count;
 Model0_wait_queue_head_t Model0_gp_wait;

 int Model0_cb_state;
 struct Model0_callback_head Model0_cb_head;

 enum Model0_rcu_sync_type Model0_gp_type;
};

extern void Model0_rcu_sync_lockdep_assert(struct Model0_rcu_sync *);

/**
 * rcu_sync_is_idle() - Are readers permitted to use their fastpaths?
 * @rsp: Pointer to rcu_sync structure to use for synchronization
 *
 * Returns true if readers are permitted to use their fastpaths.
 * Must be invoked within an RCU read-side critical section whose
 * flavor matches that of the rcu_sync struture.
 */
static inline __attribute__((no_instrument_function)) bool Model0_rcu_sync_is_idle(struct Model0_rcu_sync *Model0_rsp)
{



 return !Model0_rsp->Model0_gp_state; /* GP_IDLE */
}

extern void Model0_rcu_sync_init(struct Model0_rcu_sync *, enum Model0_rcu_sync_type);
extern void Model0_rcu_sync_enter(struct Model0_rcu_sync *);
extern void Model0_rcu_sync_exit(struct Model0_rcu_sync *);
extern void Model0_rcu_sync_dtor(struct Model0_rcu_sync *);


struct Model0_percpu_rw_semaphore {
 struct Model0_rcu_sync Model0_rss;
 unsigned int *Model0_fast_read_ctr;
 struct Model0_rw_semaphore Model0_rw_sem;
 Model0_atomic_t Model0_slow_read_ctr;
 Model0_wait_queue_head_t Model0_write_waitq;
};

extern void Model0_percpu_down_read(struct Model0_percpu_rw_semaphore *);
extern int Model0_percpu_down_read_trylock(struct Model0_percpu_rw_semaphore *);
extern void Model0_percpu_up_read(struct Model0_percpu_rw_semaphore *);

extern void Model0_percpu_down_write(struct Model0_percpu_rw_semaphore *);
extern void Model0_percpu_up_write(struct Model0_percpu_rw_semaphore *);

extern int Model0___percpu_init_rwsem(struct Model0_percpu_rw_semaphore *,
    const char *, struct Model0_lock_class_key *);
extern void Model0_percpu_free_rwsem(struct Model0_percpu_rw_semaphore *);
static inline __attribute__((no_instrument_function)) void Model0_percpu_rwsem_release(struct Model0_percpu_rw_semaphore *Model0_sem,
     bool Model0_read, unsigned long Model0_ip)
{
 do { } while (0);

 if (!Model0_read)
  Model0_sem->Model0_rw_sem.Model0_owner = ((void *)0);

}

static inline __attribute__((no_instrument_function)) void Model0_percpu_rwsem_acquire(struct Model0_percpu_rw_semaphore *Model0_sem,
     bool Model0_read, unsigned long Model0_ip)
{
 do { } while (0);
}




struct Model0_cgroup;
struct Model0_cgroup_root;
struct Model0_cgroup_subsys;
struct Model0_cgroup_taskset;
struct Model0_kernfs_node;
struct Model0_kernfs_ops;
struct Model0_kernfs_open_file;
struct Model0_seq_file;





/* define the enumeration of all cgroup subsystems */

enum Model0_cgroup_subsys_id {

/*
 * List of cgroup subsystems.
 *
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */

/*
 * This file *must* be included with SUBSYS() defined.
 */


Model0_cpuset_cgrp_id,



Model0_cpu_cgrp_id,



Model0_cpuacct_cgrp_id,
Model0_freezer_cgrp_id,
/*
 * The following subsystems are not supported on the default hierarchy.
 */




/*
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */
 Model0_CGROUP_SUBSYS_COUNT,
};


/* bits in struct cgroup_subsys_state flags field */
enum {
 Model0_CSS_NO_REF = (1 << 0), /* no reference counting for this css */
 Model0_CSS_ONLINE = (1 << 1), /* between ->css_online() and ->css_offline() */
 Model0_CSS_RELEASED = (1 << 2), /* refcnt reached zero, released */
 Model0_CSS_VISIBLE = (1 << 3), /* css is visible to userland */
};

/* bits in struct cgroup flags field */
enum {
 /* Control Group requires release notifications to userspace */
 Model0_CGRP_NOTIFY_ON_RELEASE,
 /*
	 * Clone the parent's configuration when creating a new child
	 * cpuset cgroup.  For historical reasons, this option can be
	 * specified at mount time and thus is implemented here.
	 */
 Model0_CGRP_CPUSET_CLONE_CHILDREN,
};

/* cgroup_root->flags */
enum {
 Model0_CGRP_ROOT_NOPREFIX = (1 << 1), /* mounted subsystems have no named prefix */
 Model0_CGRP_ROOT_XATTR = (1 << 2), /* supports extended attributes */
};

/* cftype->flags */
enum {
 Model0_CFTYPE_ONLY_ON_ROOT = (1 << 0), /* only create on root cgrp */
 Model0_CFTYPE_NOT_ON_ROOT = (1 << 1), /* don't create on root cgrp */
 Model0_CFTYPE_NO_PREFIX = (1 << 3), /* (DON'T USE FOR NEW FILES) no subsys prefix */
 Model0_CFTYPE_WORLD_WRITABLE = (1 << 4), /* (DON'T USE FOR NEW FILES) S_IWUGO */

 /* internal flags, do not use outside cgroup core proper */
 Model0___CFTYPE_ONLY_ON_DFL = (1 << 16), /* only on default hierarchy */
 Model0___CFTYPE_NOT_ON_DFL = (1 << 17), /* not on default hierarchy */
};

/*
 * cgroup_file is the handle for a file instance created in a cgroup which
 * is used, for example, to generate file changed notifications.  This can
 * be obtained by setting cftype->file_offset.
 */
struct Model0_cgroup_file {
 /* do not access any fields from outside cgroup core */
 struct Model0_kernfs_node *Model0_kn;
};

/*
 * Per-subsystem/per-cgroup state maintained by the system.  This is the
 * fundamental structural building block that controllers deal with.
 *
 * Fields marked with "PI:" are public and immutable and may be accessed
 * directly without synchronization.
 */
struct Model0_cgroup_subsys_state {
 /* PI: the cgroup that this css is attached to */
 struct Model0_cgroup *Model0_cgroup;

 /* PI: the cgroup subsystem that this css is attached to */
 struct Model0_cgroup_subsys *Model0_ss;

 /* reference count - access via css_[try]get() and css_put() */
 struct Model0_percpu_ref Model0_refcnt;

 /* PI: the parent css */
 struct Model0_cgroup_subsys_state *Model0_parent;

 /* siblings list anchored at the parent's ->children */
 struct Model0_list_head Model0_sibling;
 struct Model0_list_head Model0_children;

 /*
	 * PI: Subsys-unique ID.  0 is unused and root is always 1.  The
	 * matching css can be looked up using css_from_id().
	 */
 int Model0_id;

 unsigned int Model0_flags;

 /*
	 * Monotonically increasing unique serial number which defines a
	 * uniform order among all csses.  It's guaranteed that all
	 * ->children lists are in the ascending order of ->serial_nr and
	 * used to allow interrupting and resuming iterations.
	 */
 Model0_u64 Model0_serial_nr;

 /*
	 * Incremented by online self and children.  Used to guarantee that
	 * parents are not offlined before their children.
	 */
 Model0_atomic_t Model0_online_cnt;

 /* percpu_ref killing and RCU release */
 struct Model0_callback_head Model0_callback_head;
 struct Model0_work_struct Model0_destroy_work;
};

/*
 * A css_set is a structure holding pointers to a set of
 * cgroup_subsys_state objects. This saves space in the task struct
 * object and speeds up fork()/exit(), since a single inc/dec and a
 * list_add()/del() can bump the reference count on the entire cgroup
 * set for a task.
 */
struct Model0_css_set {
 /* Reference count */
 Model0_atomic_t Model0_refcount;

 /*
	 * List running through all cgroup groups in the same hash
	 * slot. Protected by css_set_lock
	 */
 struct Model0_hlist_node Model0_hlist;

 /*
	 * Lists running through all tasks using this cgroup group.
	 * mg_tasks lists tasks which belong to this cset but are in the
	 * process of being migrated out or in.  Protected by
	 * css_set_rwsem, but, during migration, once tasks are moved to
	 * mg_tasks, it can be read safely while holding cgroup_mutex.
	 */
 struct Model0_list_head Model0_tasks;
 struct Model0_list_head Model0_mg_tasks;

 /*
	 * List of cgrp_cset_links pointing at cgroups referenced from this
	 * css_set.  Protected by css_set_lock.
	 */
 struct Model0_list_head Model0_cgrp_links;

 /* the default cgroup associated with this css_set */
 struct Model0_cgroup *Model0_dfl_cgrp;

 /*
	 * Set of subsystem states, one for each subsystem. This array is
	 * immutable after creation apart from the init_css_set during
	 * subsystem registration (at boot time).
	 */
 struct Model0_cgroup_subsys_state *Model0_subsys[Model0_CGROUP_SUBSYS_COUNT];

 /*
	 * List of csets participating in the on-going migration either as
	 * source or destination.  Protected by cgroup_mutex.
	 */
 struct Model0_list_head Model0_mg_preload_node;
 struct Model0_list_head Model0_mg_node;

 /*
	 * If this cset is acting as the source of migration the following
	 * two fields are set.  mg_src_cgrp and mg_dst_cgrp are
	 * respectively the source and destination cgroups of the on-going
	 * migration.  mg_dst_cset is the destination cset the target tasks
	 * on this cset should be migrated to.  Protected by cgroup_mutex.
	 */
 struct Model0_cgroup *Model0_mg_src_cgrp;
 struct Model0_cgroup *Model0_mg_dst_cgrp;
 struct Model0_css_set *Model0_mg_dst_cset;

 /*
	 * On the default hierarhcy, ->subsys[ssid] may point to a css
	 * attached to an ancestor instead of the cgroup this css_set is
	 * associated with.  The following node is anchored at
	 * ->subsys[ssid]->cgroup->e_csets[ssid] and provides a way to
	 * iterate through all css's attached to a given cgroup.
	 */
 struct Model0_list_head Model0_e_cset_node[Model0_CGROUP_SUBSYS_COUNT];

 /* all css_task_iters currently walking this cset */
 struct Model0_list_head Model0_task_iters;

 /* dead and being drained, ignore for migration */
 bool Model0_dead;

 /* For RCU-protected deletion */
 struct Model0_callback_head Model0_callback_head;
};

struct Model0_cgroup {
 /* self css with NULL ->ss, points back to this cgroup */
 struct Model0_cgroup_subsys_state Model0_self;

 unsigned long Model0_flags; /* "unsigned long" so bitops work */

 /*
	 * idr allocated in-hierarchy ID.
	 *
	 * ID 0 is not used, the ID of the root cgroup is always 1, and a
	 * new cgroup will be assigned with a smallest available ID.
	 *
	 * Allocating/Removing ID must be protected by cgroup_mutex.
	 */
 int Model0_id;

 /*
	 * The depth this cgroup is at.  The root is at depth zero and each
	 * step down the hierarchy increments the level.  This along with
	 * ancestor_ids[] can determine whether a given cgroup is a
	 * descendant of another without traversing the hierarchy.
	 */
 int Model0_level;

 /*
	 * Each non-empty css_set associated with this cgroup contributes
	 * one to populated_cnt.  All children with non-zero popuplated_cnt
	 * of their own contribute one.  The count is zero iff there's no
	 * task in this cgroup or its subtree.
	 */
 int Model0_populated_cnt;

 struct Model0_kernfs_node *Model0_kn; /* cgroup kernfs entry */
 struct Model0_cgroup_file Model0_procs_file; /* handle for "cgroup.procs" */
 struct Model0_cgroup_file Model0_events_file; /* handle for "cgroup.events" */

 /*
	 * The bitmask of subsystems enabled on the child cgroups.
	 * ->subtree_control is the one configured through
	 * "cgroup.subtree_control" while ->child_ss_mask is the effective
	 * one which may have more subsystems enabled.  Controller knobs
	 * are made available iff it's enabled in ->subtree_control.
	 */
 Model0_u16 Model0_subtree_control;
 Model0_u16 Model0_subtree_ss_mask;
 Model0_u16 Model0_old_subtree_control;
 Model0_u16 Model0_old_subtree_ss_mask;

 /* Private pointers for each registered subsystem */
 struct Model0_cgroup_subsys_state *Model0_subsys[Model0_CGROUP_SUBSYS_COUNT];

 struct Model0_cgroup_root *Model0_root;

 /*
	 * List of cgrp_cset_links pointing at css_sets with tasks in this
	 * cgroup.  Protected by css_set_lock.
	 */
 struct Model0_list_head Model0_cset_links;

 /*
	 * On the default hierarchy, a css_set for a cgroup with some
	 * susbsys disabled will point to css's which are associated with
	 * the closest ancestor which has the subsys enabled.  The
	 * following lists all css_sets which point to this cgroup's css
	 * for the given subsystem.
	 */
 struct Model0_list_head Model0_e_csets[Model0_CGROUP_SUBSYS_COUNT];

 /*
	 * list of pidlists, up to two for each namespace (one for procs, one
	 * for tasks); created on demand.
	 */
 struct Model0_list_head Model0_pidlists;
 struct Model0_mutex Model0_pidlist_mutex;

 /* used to wait for offlining of csses */
 Model0_wait_queue_head_t Model0_offline_waitq;

 /* used to schedule release agent */
 struct Model0_work_struct Model0_release_agent_work;

 /* ids of the ancestors at each level including self */
 int Model0_ancestor_ids[];
};

/*
 * A cgroup_root represents the root of a cgroup hierarchy, and may be
 * associated with a kernfs_root to form an active hierarchy.  This is
 * internal to cgroup core.  Don't access directly from controllers.
 */
struct Model0_cgroup_root {
 struct Model0_kernfs_root *Model0_kf_root;

 /* The bitmask of subsystems attached to this hierarchy */
 unsigned int Model0_subsys_mask;

 /* Unique id for this hierarchy. */
 int Model0_hierarchy_id;

 /* The root cgroup.  Root is destroyed on its release. */
 struct Model0_cgroup Model0_cgrp;

 /* for cgrp->ancestor_ids[0] */
 int Model0_cgrp_ancestor_id_storage;

 /* Number of cgroups in the hierarchy, used only for /proc/cgroups */
 Model0_atomic_t Model0_nr_cgrps;

 /* A list running through the active hierarchies */
 struct Model0_list_head Model0_root_list;

 /* Hierarchy-specific flags */
 unsigned int Model0_flags;

 /* IDs for cgroups in this hierarchy */
 struct Model0_idr Model0_cgroup_idr;

 /* The path to use for release notifications. */
 char Model0_release_agent_path[4096];

 /* The name for this hierarchy - may be empty */
 char Model0_name[64];
};

/*
 * struct cftype: handler definitions for cgroup control files
 *
 * When reading/writing to a file:
 *	- the cgroup to use is file->f_path.dentry->d_parent->d_fsdata
 *	- the 'cftype' of the file is file->f_path.dentry->d_fsdata
 */
struct Model0_cftype {
 /*
	 * By convention, the name should begin with the name of the
	 * subsystem, followed by a period.  Zero length string indicates
	 * end of cftype array.
	 */
 char Model0_name[64];
 unsigned long Model0_private;

 /*
	 * The maximum length of string, excluding trailing nul, that can
	 * be passed to write.  If < PAGE_SIZE-1, PAGE_SIZE-1 is assumed.
	 */
 Model0_size_t Model0_max_write_len;

 /* CFTYPE_* flags */
 unsigned int Model0_flags;

 /*
	 * If non-zero, should contain the offset from the start of css to
	 * a struct cgroup_file field.  cgroup will record the handle of
	 * the created file into it.  The recorded handle can be used as
	 * long as the containing css remains accessible.
	 */
 unsigned int Model0_file_offset;

 /*
	 * Fields used for internal bookkeeping.  Initialized automatically
	 * during registration.
	 */
 struct Model0_cgroup_subsys *Model0_ss; /* NULL for cgroup core files */
 struct Model0_list_head Model0_node; /* anchored at ss->cfts */
 struct Model0_kernfs_ops *Model0_kf_ops;

 /*
	 * read_u64() is a shortcut for the common case of returning a
	 * single integer. Use it in place of read()
	 */
 Model0_u64 (*Model0_read_u64)(struct Model0_cgroup_subsys_state *Model0_css, struct Model0_cftype *Model0_cft);
 /*
	 * read_s64() is a signed version of read_u64()
	 */
 Model0_s64 (*Model0_read_s64)(struct Model0_cgroup_subsys_state *Model0_css, struct Model0_cftype *Model0_cft);

 /* generic seq_file read interface */
 int (*Model0_seq_show)(struct Model0_seq_file *Model0_sf, void *Model0_v);

 /* optional ops, implement all or none */
 void *(*Model0_seq_start)(struct Model0_seq_file *Model0_sf, Model0_loff_t *Model0_ppos);
 void *(*Model0_seq_next)(struct Model0_seq_file *Model0_sf, void *Model0_v, Model0_loff_t *Model0_ppos);
 void (*Model0_seq_stop)(struct Model0_seq_file *Model0_sf, void *Model0_v);

 /*
	 * write_u64() is a shortcut for the common case of accepting
	 * a single integer (as parsed by simple_strtoull) from
	 * userspace. Use in place of write(); return 0 or error.
	 */
 int (*Model0_write_u64)(struct Model0_cgroup_subsys_state *Model0_css, struct Model0_cftype *Model0_cft,
    Model0_u64 Model0_val);
 /*
	 * write_s64() is a signed version of write_u64()
	 */
 int (*Model0_write_s64)(struct Model0_cgroup_subsys_state *Model0_css, struct Model0_cftype *Model0_cft,
    Model0_s64 Model0_val);

 /*
	 * write() is the generic write callback which maps directly to
	 * kernfs write operation and overrides all other operations.
	 * Maximum write size is determined by ->max_write_len.  Use
	 * of_css/cft() to access the associated css and cft.
	 */
 Model0_ssize_t (*Model0_write)(struct Model0_kernfs_open_file *Model0_of,
    char *Model0_buf, Model0_size_t Model0_nbytes, Model0_loff_t Model0_off);




};

/*
 * Control Group subsystem type.
 * See Documentation/cgroups/cgroups.txt for details
 */
struct Model0_cgroup_subsys {
 struct Model0_cgroup_subsys_state *(*Model0_css_alloc)(struct Model0_cgroup_subsys_state *Model0_parent_css);
 int (*Model0_css_online)(struct Model0_cgroup_subsys_state *Model0_css);
 void (*Model0_css_offline)(struct Model0_cgroup_subsys_state *Model0_css);
 void (*Model0_css_released)(struct Model0_cgroup_subsys_state *Model0_css);
 void (*Model0_css_free)(struct Model0_cgroup_subsys_state *Model0_css);
 void (*Model0_css_reset)(struct Model0_cgroup_subsys_state *Model0_css);

 int (*Model0_can_attach)(struct Model0_cgroup_taskset *Model0_tset);
 void (*Model0_cancel_attach)(struct Model0_cgroup_taskset *Model0_tset);
 void (*Model0_attach)(struct Model0_cgroup_taskset *Model0_tset);
 void (*Model0_post_attach)(void);
 int (*Model0_can_fork)(struct Model0_task_struct *Model0_task);
 void (*Model0_cancel_fork)(struct Model0_task_struct *Model0_task);
 void (*Model0_fork)(struct Model0_task_struct *Model0_task);
 void (*Model0_exit)(struct Model0_task_struct *Model0_task);
 void (*Model0_free)(struct Model0_task_struct *Model0_task);
 void (*Model0_bind)(struct Model0_cgroup_subsys_state *Model0_root_css);

 bool Model0_early_init:1;

 /*
	 * If %true, the controller, on the default hierarchy, doesn't show
	 * up in "cgroup.controllers" or "cgroup.subtree_control", is
	 * implicitly enabled on all cgroups on the default hierarchy, and
	 * bypasses the "no internal process" constraint.  This is for
	 * utility type controllers which is transparent to userland.
	 *
	 * An implicit controller can be stolen from the default hierarchy
	 * anytime and thus must be okay with offline csses from previous
	 * hierarchies coexisting with csses for the current one.
	 */
 bool Model0_implicit_on_dfl:1;

 /*
	 * If %false, this subsystem is properly hierarchical -
	 * configuration, resource accounting and restriction on a parent
	 * cgroup cover those of its children.  If %true, hierarchy support
	 * is broken in some ways - some subsystems ignore hierarchy
	 * completely while others are only implemented half-way.
	 *
	 * It's now disallowed to create nested cgroups if the subsystem is
	 * broken and cgroup core will emit a warning message on such
	 * cases.  Eventually, all subsystems will be made properly
	 * hierarchical and this will go away.
	 */
 bool Model0_broken_hierarchy:1;
 bool Model0_warned_broken_hierarchy:1;

 /* the following two fields are initialized automtically during boot */
 int Model0_id;
 const char *Model0_name;

 /* optional, initialized automatically during boot if not set */
 const char *Model0_legacy_name;

 /* link to parent, protected by cgroup_lock() */
 struct Model0_cgroup_root *Model0_root;

 /* idr for css->id */
 struct Model0_idr Model0_css_idr;

 /*
	 * List of cftypes.  Each entry is the first entry of an array
	 * terminated by zero length name.
	 */
 struct Model0_list_head Model0_cfts;

 /*
	 * Base cftypes which are automatically registered.  The two can
	 * point to the same array.
	 */
 struct Model0_cftype *Model0_dfl_cftypes; /* for the default hierarchy */
 struct Model0_cftype *Model0_legacy_cftypes; /* for the legacy hierarchies */

 /*
	 * A subsystem may depend on other subsystems.  When such subsystem
	 * is enabled on a cgroup, the depended-upon subsystems are enabled
	 * together if available.  Subsystems enabled due to dependency are
	 * not visible to userland until explicitly enabled.  The following
	 * specifies the mask of subsystems that this one depends on.
	 */
 unsigned int Model0_depends_on;
};

extern struct Model0_percpu_rw_semaphore Model0_cgroup_threadgroup_rwsem;

/**
 * cgroup_threadgroup_change_begin - threadgroup exclusion for cgroups
 * @tsk: target task
 *
 * Called from threadgroup_change_begin() and allows cgroup operations to
 * synchronize against threadgroup changes using a percpu_rw_semaphore.
 */
static inline __attribute__((no_instrument_function)) void Model0_cgroup_threadgroup_change_begin(struct Model0_task_struct *Model0_tsk)
{
 Model0_percpu_down_read(&Model0_cgroup_threadgroup_rwsem);
}

/**
 * cgroup_threadgroup_change_end - threadgroup exclusion for cgroups
 * @tsk: target task
 *
 * Called from threadgroup_change_end().  Counterpart of
 * cgroup_threadcgroup_change_begin().
 */
static inline __attribute__((no_instrument_function)) void Model0_cgroup_threadgroup_change_end(struct Model0_task_struct *Model0_tsk)
{
 Model0_percpu_up_read(&Model0_cgroup_threadgroup_rwsem);
}
struct Model0_sock_cgroup_data {
};





/*
 * Extended scheduling parameters data structure.
 *
 * This is needed because the original struct sched_param can not be
 * altered without introducing ABI issues with legacy applications
 * (e.g., in sched_getparam()).
 *
 * However, the possibility of specifying more than just a priority for
 * the tasks may be useful for a wide variety of application fields, e.g.,
 * multimedia, streaming, automation and control, and many others.
 *
 * This variant (sched_attr) is meant at describing a so-called
 * sporadic time-constrained task. In such model a task is specified by:
 *  - the activation period or minimum instance inter-arrival time;
 *  - the maximum (or average, depending on the actual scheduling
 *    discipline) computation time of all instances, a.k.a. runtime;
 *  - the deadline (relative to the actual activation time) of each
 *    instance.
 * Very briefly, a periodic (sporadic) task asks for the execution of
 * some specific computation --which is typically called an instance--
 * (at most) every period. Moreover, each instance typically lasts no more
 * than the runtime and must be completed by time instant t equal to
 * the instance activation time + the deadline.
 *
 * This is reflected by the actual fields of the sched_attr structure:
 *
 *  @size		size of the structure, for fwd/bwd compat.
 *
 *  @sched_policy	task's scheduling policy
 *  @sched_flags	for customizing the scheduler behaviour
 *  @sched_nice		task's nice value      (SCHED_NORMAL/BATCH)
 *  @sched_priority	task's static priority (SCHED_FIFO/RR)
 *  @sched_deadline	representative of the task's deadline
 *  @sched_runtime	representative of the task's runtime
 *  @sched_period	representative of the task's period
 *
 * Given this task model, there are a multiplicity of scheduling algorithms
 * and policies, that can be used to ensure all the tasks will make their
 * timing constraints.
 *
 * As of now, the SCHED_DEADLINE policy (sched_dl scheduling class) is the
 * only user of this new interface. More information about the algorithm
 * available in the scheduling class file or in Documentation/.
 */
struct Model0_sched_attr {
 Model0_u32 Model0_size;

 Model0_u32 Model0_sched_policy;
 Model0_u64 Model0_sched_flags;

 /* SCHED_NORMAL, SCHED_BATCH */
 Model0_s32 Model0_sched_nice;

 /* SCHED_FIFO, SCHED_RR */
 Model0_u32 Model0_sched_priority;

 /* SCHED_DEADLINE */
 Model0_u64 Model0_sched_runtime;
 Model0_u64 Model0_sched_deadline;
 Model0_u64 Model0_sched_period;
};

struct Model0_futex_pi_state;
struct Model0_robust_list_head;
struct Model0_bio_list;
struct Model0_fs_struct;
struct Model0_perf_event_context;
struct Model0_blk_plug;
struct Model0_filename;
struct Model0_nameidata;





/*
 * These are the constant used to fake the fixed-point load-average
 * counting. Some notes:
 *  - 11 bit fractions expand to 22 bits by the multiplies: this gives
 *    a load-average precision of 10 bits integer + 11 bits fractional
 *  - if you want to count load-averages more often, you need more
 *    precision, or rounding will get you. With 2-second counting freq,
 *    the EXP_n values would be 1981, 2034 and 2043 if still using only
 *    11 bit fractions.
 */
extern unsigned long Model0_avenrun[]; /* Load averages */
extern void Model0_get_avenrun(unsigned long *Model0_loads, unsigned long Model0_offset, int Model0_shift);
extern unsigned long Model0_total_forks;
extern int Model0_nr_threads;
extern __attribute__((section(".data..percpu" ""))) __typeof__(unsigned long) Model0_process_counts;
extern int Model0_nr_processes(void);
extern unsigned long Model0_nr_running(void);
extern bool Model0_single_task_running(void);
extern unsigned long Model0_nr_iowait(void);
extern unsigned long Model0_nr_iowait_cpu(int Model0_cpu);
extern void Model0_get_iowait_load(unsigned long *Model0_nr_waiters, unsigned long *Model0_load);

extern void Model0_calc_global_load(unsigned long Model0_ticks);


extern void Model0_cpu_load_update_nohz_start(void);
extern void Model0_cpu_load_update_nohz_stop(void);





extern void Model0_dump_cpu_task(int Model0_cpu);

struct Model0_seq_file;
struct Model0_cfs_rq;
struct Model0_task_group;





/*
 * Task state bitmask. NOTE! These bits are also
 * encoded in fs/proc/array.c: get_task_state().
 *
 * We have two separate sets of flags: task->state
 * is about runnability, while task->exit_state are
 * about the task exiting. Confusing, but this way
 * modifying one set can't modify the other one by
 * mistake.
 */





/* in tsk->exit_state */



/* in tsk->state again */
extern char Model0____assert_task_state[1 - 2*!!(
  sizeof("RSDTtXZxKWPNn")-1 != ( __builtin_constant_p(4096) ? ( (4096) < 1 ? Model0_____ilog2_NaN() : (4096) & (1ULL << 63) ? 63 : (4096) & (1ULL << 62) ? 62 : (4096) & (1ULL << 61) ? 61 : (4096) & (1ULL << 60) ? 60 : (4096) & (1ULL << 59) ? 59 : (4096) & (1ULL << 58) ? 58 : (4096) & (1ULL << 57) ? 57 : (4096) & (1ULL << 56) ? 56 : (4096) & (1ULL << 55) ? 55 : (4096) & (1ULL << 54) ? 54 : (4096) & (1ULL << 53) ? 53 : (4096) & (1ULL << 52) ? 52 : (4096) & (1ULL << 51) ? 51 : (4096) & (1ULL << 50) ? 50 : (4096) & (1ULL << 49) ? 49 : (4096) & (1ULL << 48) ? 48 : (4096) & (1ULL << 47) ? 47 : (4096) & (1ULL << 46) ? 46 : (4096) & (1ULL << 45) ? 45 : (4096) & (1ULL << 44) ? 44 : (4096) & (1ULL << 43) ? 43 : (4096) & (1ULL << 42) ? 42 : (4096) & (1ULL << 41) ? 41 : (4096) & (1ULL << 40) ? 40 : (4096) & (1ULL << 39) ? 39 : (4096) & (1ULL << 38) ? 38 : (4096) & (1ULL << 37) ? 37 : (4096) & (1ULL << 36) ? 36 : (4096) & (1ULL << 35) ? 35 : (4096) & (1ULL << 34) ? 34 : (4096) & (1ULL << 33) ? 33 : (4096) & (1ULL << 32) ? 32 : (4096) & (1ULL << 31) ? 31 : (4096) & (1ULL << 30) ? 30 : (4096) & (1ULL << 29) ? 29 : (4096) & (1ULL << 28) ? 28 : (4096) & (1ULL << 27) ? 27 : (4096) & (1ULL << 26) ? 26 : (4096) & (1ULL << 25) ? 25 : (4096) & (1ULL << 24) ? 24 : (4096) & (1ULL << 23) ? 23 : (4096) & (1ULL << 22) ? 22 : (4096) & (1ULL << 21) ? 21 : (4096) & (1ULL << 20) ? 20 : (4096) & (1ULL << 19) ? 19 : (4096) & (1ULL << 18) ? 18 : (4096) & (1ULL << 17) ? 17 : (4096) & (1ULL << 16) ? 16 : (4096) & (1ULL << 15) ? 15 : (4096) & (1ULL << 14) ? 14 : (4096) & (1ULL << 13) ? 13 : (4096) & (1ULL << 12) ? 12 : (4096) & (1ULL << 11) ? 11 : (4096) & (1ULL << 10) ? 10 : (4096) & (1ULL << 9) ? 9 : (4096) & (1ULL << 8) ? 8 : (4096) & (1ULL << 7) ? 7 : (4096) & (1ULL << 6) ? 6 : (4096) & (1ULL << 5) ? 5 : (4096) & (1ULL << 4) ? 4 : (4096) & (1ULL << 3) ? 3 : (4096) & (1ULL << 2) ? 2 : (4096) & (1ULL << 1) ? 1 : (4096) & (1ULL << 0) ? 0 : Model0_____ilog2_NaN() ) : (sizeof(4096) <= 4) ? Model0___ilog2_u32(4096) : Model0___ilog2_u64(4096) )+1)];

/* Convenience macros for the sake of set_task_state */






/* Convenience macros for the sake of wake_up */



/* get_task_state() */
/*
 * set_current_state() includes a barrier so that the write of current->state
 * is correctly serialised wrt the caller's subsequent test of whether to
 * actually sleep:
 *
 *	set_current_state(TASK_UNINTERRUPTIBLE);
 *	if (do_i_need_to_sleep())
 *		schedule();
 *
 * If the caller does not need such serialisation then use __set_current_state()
 */







/* Task command name length */




/*
 * This serializes "schedule()" and also protects
 * the run-queue from deletions/modifications (but
 * _adding_ to the beginning of the run-queue has
 * a separate lock).
 */
extern Model0_rwlock_t Model0_tasklist_lock;
extern Model0_spinlock_t Model0_mmlist_lock;

struct Model0_task_struct;





extern void Model0_sched_init(void);
extern void Model0_sched_init_smp(void);
extern void Model0_schedule_tail(struct Model0_task_struct *Model0_prev);
extern void Model0_init_idle(struct Model0_task_struct *Model0_idle, int Model0_cpu);
extern void Model0_init_idle_bootup_task(struct Model0_task_struct *Model0_idle);

extern Model0_cpumask_var_t Model0_cpu_isolated_map;

extern int Model0_runqueue_is_locked(int Model0_cpu);


extern void Model0_nohz_balance_enter_idle(int Model0_cpu);
extern void Model0_set_cpu_sd_state_idle(void);
extern int Model0_get_nohz_timer_target(void);





/*
 * Only dump TASK_* tasks. (0 for all tasks)
 */
extern void Model0_show_state_filter(unsigned long Model0_state_filter);

static inline __attribute__((no_instrument_function)) void Model0_show_state(void)
{
 Model0_show_state_filter(0);
}

extern void Model0_show_regs(struct Model0_pt_regs *);

/*
 * TASK is a pointer to the task whose backtrace we want to see (or NULL for current
 * task), SP is the stack pointer of the first frame that should be shown in the back
 * trace (or NULL if the entire call-chain of the task should be shown).
 */
extern void Model0_show_stack(struct Model0_task_struct *Model0_task, unsigned long *Model0_sp);

extern void Model0_cpu_init (void);
extern void Model0_trap_init(void);
extern void Model0_update_process_times(int Model0_user);
extern void Model0_scheduler_tick(void);
extern int Model0_sched_cpu_starting(unsigned int Model0_cpu);
extern int Model0_sched_cpu_activate(unsigned int Model0_cpu);
extern int Model0_sched_cpu_deactivate(unsigned int Model0_cpu);


extern int Model0_sched_cpu_dying(unsigned int Model0_cpu);




extern void Model0_sched_show_task(struct Model0_task_struct *Model0_p);
static inline __attribute__((no_instrument_function)) void Model0_touch_softlockup_watchdog_sched(void)
{
}
static inline __attribute__((no_instrument_function)) void Model0_touch_softlockup_watchdog(void)
{
}
static inline __attribute__((no_instrument_function)) void Model0_touch_softlockup_watchdog_sync(void)
{
}
static inline __attribute__((no_instrument_function)) void Model0_touch_all_softlockup_watchdogs(void)
{
}
static inline __attribute__((no_instrument_function)) void Model0_lockup_detector_init(void)
{
}





static inline __attribute__((no_instrument_function)) void Model0_reset_hung_task_detector(void)
{
}


/* Attach to any functions which should be ignored in wchan output. */


/* Linker adds these: start and end of __sched functions */
extern char Model0___sched_text_start[], Model0___sched_text_end[];

/* Is this address in the __sched functions? */
extern int Model0_in_sched_functions(unsigned long Model0_addr);


extern signed long Model0_schedule_timeout(signed long Model0_timeout);
extern signed long Model0_schedule_timeout_interruptible(signed long Model0_timeout);
extern signed long Model0_schedule_timeout_killable(signed long Model0_timeout);
extern signed long Model0_schedule_timeout_uninterruptible(signed long Model0_timeout);
extern signed long Model0_schedule_timeout_idle(signed long Model0_timeout);
           void Model0_schedule(void);
extern void Model0_schedule_preempt_disabled(void);

extern long Model0_io_schedule_timeout(long Model0_timeout);

static inline __attribute__((no_instrument_function)) void Model0_io_schedule(void)
{
 Model0_io_schedule_timeout(((long)(~0UL>>1)));
}

struct Model0_nsproxy;
struct Model0_user_namespace;


extern void Model0_arch_pick_mmap_layout(struct Model0_mm_struct *Model0_mm);
extern unsigned long
Model0_arch_get_unmapped_area(struct Model0_file *, unsigned long, unsigned long,
         unsigned long, unsigned long);
extern unsigned long
Model0_arch_get_unmapped_area_topdown(struct Model0_file *Model0_filp, unsigned long Model0_addr,
     unsigned long Model0_len, unsigned long Model0_pgoff,
     unsigned long Model0_flags);
/* mm flags */

/* for SUID_DUMP_* above */



extern void Model0_set_dumpable(struct Model0_mm_struct *Model0_mm, int Model0_value);
/*
 * This returns the actual value of the suid_dumpable flag. For things
 * that are using this for checking for privilege transitions, it must
 * test against SUID_DUMP_USER rather than treating it as a boolean
 * value.
 */
static inline __attribute__((no_instrument_function)) int Model0___get_dumpable(unsigned long Model0_mm_flags)
{
 return Model0_mm_flags & ((1 << 2) - 1);
}

static inline __attribute__((no_instrument_function)) int Model0_get_dumpable(struct Model0_mm_struct *Model0_mm)
{
 return Model0___get_dumpable(Model0_mm->Model0_flags);
}

/* coredump filter bits */
     /* leave room for more dump flags */
struct Model0_sighand_struct {
 Model0_atomic_t Model0_count;
 struct Model0_k_sigaction Model0_action[64];
 Model0_spinlock_t Model0_siglock;
 Model0_wait_queue_head_t Model0_signalfd_wqh;
};

struct Model0_pacct_struct {
 int Model0_ac_flag;
 long Model0_ac_exitcode;
 unsigned long Model0_ac_mem;
 Model0_cputime_t Model0_ac_utime, Model0_ac_stime;
 unsigned long Model0_ac_minflt, Model0_ac_majflt;
};

struct Model0_cpu_itimer {
 Model0_cputime_t Model0_expires;
 Model0_cputime_t Model0_incr;
 Model0_u32 error;
 Model0_u32 Model0_incr_error;
};

/**
 * struct prev_cputime - snaphsot of system and user cputime
 * @utime: time spent in user mode
 * @stime: time spent in system mode
 * @lock: protects the above two fields
 *
 * Stores previous user/system time values such that we can guarantee
 * monotonicity.
 */
struct Model0_prev_cputime {

 Model0_cputime_t Model0_utime;
 Model0_cputime_t Model0_stime;
 Model0_raw_spinlock_t Model0_lock;

};

static inline __attribute__((no_instrument_function)) void Model0_prev_cputime_init(struct Model0_prev_cputime *Model0_prev)
{

 Model0_prev->Model0_utime = Model0_prev->Model0_stime = 0;
 do { *(&Model0_prev->Model0_lock) = (Model0_raw_spinlock_t) { .Model0_raw_lock = { { (0) } }, }; } while (0);

}

/**
 * struct task_cputime - collected CPU time counts
 * @utime:		time spent in user mode, in &cputime_t units
 * @stime:		time spent in kernel mode, in &cputime_t units
 * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
 *
 * This structure groups together three kinds of CPU time that are tracked for
 * threads and thread groups.  Most things considering CPU time want to group
 * these counts together and treat all three of them in parallel.
 */
struct Model0_task_cputime {
 Model0_cputime_t Model0_utime;
 Model0_cputime_t Model0_stime;
 unsigned long long Model0_sum_exec_runtime;
};

/* Alternate field names when used to cache expirations. */
/*
 * This is the atomic variant of task_cputime, which can be used for
 * storing and updating task_cputime statistics without locking.
 */
struct Model0_task_cputime_atomic {
 Model0_atomic64_t Model0_utime;
 Model0_atomic64_t Model0_stime;
 Model0_atomic64_t Model0_sum_exec_runtime;
};
/*
 * Disable preemption until the scheduler is running -- use an unconditional
 * value so that it also works on !PREEMPT_COUNT kernels.
 *
 * Reset by start_kernel()->sched_init()->init_idle()->init_idle_preempt_count().
 */


/*
 * Initial preempt_count value; reflects the preempt_count schedule invariant
 * which states that during context switches:
 *
 *    preempt_count() == 2*PREEMPT_DISABLE_OFFSET
 *
 * Note: PREEMPT_DISABLE_OFFSET is 0 for !PREEMPT_COUNT kernels.
 * Note: See finish_task_switch().
 */


/**
 * struct thread_group_cputimer - thread group interval timer counts
 * @cputime_atomic:	atomic thread group interval timers.
 * @running:		true when there are timers running and
 *			@cputime_atomic receives updates.
 * @checking_timer:	true when a thread in the group is in the
 *			process of checking for thread group timers.
 *
 * This structure contains the version of task_cputime, above, that is
 * used for thread group CPU timer calculations.
 */
struct Model0_thread_group_cputimer {
 struct Model0_task_cputime_atomic Model0_cputime_atomic;
 bool Model0_running;
 bool Model0_checking_timer;
};


struct Model0_autogroup;

/*
 * NOTE! "signal_struct" does not have its own
 * locking, because a shared signal_struct always
 * implies a shared sighand_struct, so locking
 * sighand_struct is always a proper superset of
 * the locking of signal_struct.
 */
struct Model0_signal_struct {
 Model0_atomic_t Model0_sigcnt;
 Model0_atomic_t Model0_live;
 int Model0_nr_threads;
 Model0_atomic_t Model0_oom_victims; /* # of TIF_MEDIE threads in this thread group */
 struct Model0_list_head Model0_thread_head;

 Model0_wait_queue_head_t Model0_wait_chldexit; /* for wait4() */

 /* current thread group signal load-balancing target: */
 struct Model0_task_struct *Model0_curr_target;

 /* shared signal handling: */
 struct Model0_sigpending Model0_shared_pending;

 /* thread group exit support */
 int Model0_group_exit_code;
 /* overloaded:
	 * - notify group_exit_task when ->count is equal to notify_count
	 * - everyone except group_exit_task is stopped during signal delivery
	 *   of fatal signals, group_exit_task processes the signal.
	 */
 int Model0_notify_count;
 struct Model0_task_struct *Model0_group_exit_task;

 /* thread group stop support, overloads group_exit_code too */
 int Model0_group_stop_count;
 unsigned int Model0_flags; /* see SIGNAL_* flags below */

 /*
	 * PR_SET_CHILD_SUBREAPER marks a process, like a service
	 * manager, to re-parent orphan (double-forking) child processes
	 * to this process instead of 'init'. The service manager is
	 * able to receive SIGCHLD signals and is able to investigate
	 * the process until it calls wait(). All children of this
	 * process will inherit a flag if they should look for a
	 * child_subreaper process at exit.
	 */
 unsigned int Model0_is_child_subreaper:1;
 unsigned int Model0_has_child_subreaper:1;

 /* POSIX.1b Interval Timers */
 int Model0_posix_timer_id;
 struct Model0_list_head Model0_posix_timers;

 /* ITIMER_REAL timer for the process */
 struct Model0_hrtimer Model0_real_timer;
 struct Model0_pid *Model0_leader_pid;
 Model0_ktime_t Model0_it_real_incr;

 /*
	 * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use
	 * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these
	 * values are defined to 0 and 1 respectively
	 */
 struct Model0_cpu_itimer Model0_it[2];

 /*
	 * Thread group totals for process CPU timers.
	 * See thread_group_cputimer(), et al, for details.
	 */
 struct Model0_thread_group_cputimer Model0_cputimer;

 /* Earliest-expiration cache. */
 struct Model0_task_cputime Model0_cputime_expires;





 struct Model0_list_head Model0_cpu_timers[3];

 struct Model0_pid *Model0_tty_old_pgrp;

 /* boolean value for session group leader */
 int Model0_leader;

 struct Model0_tty_struct *Model0_tty; /* NULL if no tty */




 /*
	 * Cumulative resource counters for dead threads in the group,
	 * and for reaped dead child processes forked by this group.
	 * Live threads maintain their own counters and add to these
	 * in __exit_signal, except for the group leader.
	 */
 Model0_seqlock_t Model0_stats_lock;
 Model0_cputime_t Model0_utime, Model0_stime, Model0_cutime, Model0_cstime;
 Model0_cputime_t Model0_gtime;
 Model0_cputime_t Model0_cgtime;
 struct Model0_prev_cputime Model0_prev_cputime;
 unsigned long Model0_nvcsw, Model0_nivcsw, Model0_cnvcsw, Model0_cnivcsw;
 unsigned long Model0_min_flt, Model0_maj_flt, Model0_cmin_flt, Model0_cmaj_flt;
 unsigned long Model0_inblock, Model0_oublock, Model0_cinblock, Model0_coublock;
 unsigned long Model0_maxrss, Model0_cmaxrss;
 struct Model0_task_io_accounting Model0_ioac;

 /*
	 * Cumulative ns of schedule CPU time fo dead threads in the
	 * group, not including a zombie group leader, (This only differs
	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
	 * other than jiffies.)
	 */
 unsigned long long Model0_sum_sched_runtime;

 /*
	 * We don't bother to synchronize most readers of this at all,
	 * because there is no reader checking a limit that actually needs
	 * to get both rlim_cur and rlim_max atomically, and either one
	 * alone is a single word that can safely be read normally.
	 * getrlimit/setrlimit use task_lock(current->group_leader) to
	 * protect this instead of the siglock, because they really
	 * have no need to disable irqs.
	 */
 struct Model0_rlimit Model0_rlim[16];


 struct Model0_pacct_struct Model0_pacct; /* per-process accounting information */


 struct Model0_taskstats *Model0_stats;


 unsigned Model0_audit_tty;
 struct Model0_tty_audit_buf *Model0_tty_audit_buf;


 /*
	 * Thread is the potential origin of an oom condition; kill first on
	 * oom
	 */
 bool Model0_oom_flag_origin;
 short Model0_oom_score_adj; /* OOM kill score adjustment */
 short Model0_oom_score_adj_min; /* OOM kill score adjustment min value.
					 * Only settable by CAP_SYS_RESOURCE. */

 struct Model0_mutex Model0_cred_guard_mutex; /* guard against foreign influences on
					 * credential calculations
					 * (notably. ptrace) */
};

/*
 * Bits in flags field of signal_struct.
 */




/*
 * Pending notifications to parent.
 */






/* If true, all threads except ->group_exit_task have pending SIGKILL */
static inline __attribute__((no_instrument_function)) int Model0_signal_group_exit(const struct Model0_signal_struct *Model0_sig)
{
 return (Model0_sig->Model0_flags & 0x00000004) ||
  (Model0_sig->Model0_group_exit_task != ((void *)0));
}

/*
 * Some day this will be a full-fledged user tracking system..
 */
struct Model0_user_struct {
 Model0_atomic_t Model0___count; /* reference count */
 Model0_atomic_t Model0_processes; /* How many processes does this user have? */
 Model0_atomic_t Model0_sigpending; /* How many pending signals does this user have? */

 Model0_atomic_t Model0_inotify_watches; /* How many inotify watches does this user have? */
 Model0_atomic_t Model0_inotify_devs; /* How many inotify devs does this user have opened? */





 Model0_atomic_long_t Model0_epoll_watches; /* The number of file descriptors currently watched */


 /* protected by mq_lock	*/
 unsigned long Model0_mq_bytes; /* How many bytes can be allocated to mqueue? */

 unsigned long Model0_locked_shm; /* How many pages of mlocked shm ? */
 unsigned long Model0_unix_inflight; /* How many files in flight in unix sockets */
 Model0_atomic_long_t Model0_pipe_bufs; /* how many pages are allocated in pipe buffers */


 struct Model0_key *Model0_uid_keyring; /* UID specific keyring */
 struct Model0_key *Model0_session_keyring; /* UID's default session keyring */


 /* Hash table maintenance information */
 struct Model0_hlist_node Model0_uidhash_node;
 Model0_kuid_t Model0_uid;


 Model0_atomic_long_t Model0_locked_vm;

};

extern int Model0_uids_sysfs_init(void);

extern struct Model0_user_struct *Model0_find_user(Model0_kuid_t);

extern struct Model0_user_struct Model0_root_user;



struct Model0_backing_dev_info;
struct Model0_reclaim_state;


struct Model0_sched_info {
 /* cumulative counters */
 unsigned long Model0_pcount; /* # of times run on this cpu */
 unsigned long long Model0_run_delay; /* time spent waiting on a runqueue */

 /* timestamps */
 unsigned long long Model0_last_arrival,/* when we last ran on a cpu */
      Model0_last_queued; /* when we were last queued to run */
};



struct Model0_task_delay_info {
 Model0_spinlock_t Model0_lock;
 unsigned int Model0_flags; /* Private per-task flags */

 /* For each stat XXX, add following, aligned appropriately
	 *
	 * struct timespec XXX_start, XXX_end;
	 * u64 XXX_delay;
	 * u32 XXX_count;
	 *
	 * Atomicity of updates to XXX_delay, XXX_count protected by
	 * single lock above (split into XXX_lock if contention is an issue).
	 */

 /*
	 * XXX_count is incremented on every XXX operation, the delay
	 * associated with the operation is added to XXX_delay.
	 * XXX_delay contains the accumulated delay time in nanoseconds.
	 */
 Model0_u64 Model0_blkio_start; /* Shared by blkio, swapin */
 Model0_u64 Model0_blkio_delay; /* wait for sync block io completion */
 Model0_u64 Model0_swapin_delay; /* wait for swapin block io completion */
 Model0_u32 Model0_blkio_count; /* total count of the number of sync block */
    /* io operations performed */
 Model0_u32 Model0_swapin_count; /* total count of the number of swapin block */
    /* io operations performed */

 Model0_u64 Model0_freepages_start;
 Model0_u64 Model0_freepages_delay; /* wait for memory reclaim */
 Model0_u32 Model0_freepages_count; /* total count of memory reclaim */
};


static inline __attribute__((no_instrument_function)) int Model0_sched_info_on(void)
{

 return 1;






}


void Model0_force_schedstat_enabled(void);


enum Model0_cpu_idle_type {
 Model0_CPU_IDLE,
 Model0_CPU_NOT_IDLE,
 Model0_CPU_NEWLY_IDLE,
 Model0_CPU_MAX_IDLE_TYPES
};

/*
 * Integer metrics need fixed point arithmetic, e.g., sched/fair
 * has a few: load, load_avg, util_avg, freq, and capacity.
 *
 * We define a basic fixed point arithmetic range, and then formalize
 * all these metrics based on that basic range.
 */



/*
 * Increase resolution of cpu_capacity calculations
 */



/*
 * Wake-queues are lists of tasks with a pending wakeup, whose
 * callers have already marked the task as woken internally,
 * and can thus carry on. A common use case is being able to
 * do the wakeups once the corresponding user lock as been
 * released.
 *
 * We hold reference to each task in the list across the wakeup,
 * thus guaranteeing that the memory is still valid by the time
 * the actual wakeups are performed in wake_up_q().
 *
 * One per task suffices, because there's never a need for a task to be
 * in two wake queues simultaneously; it is forbidden to abandon a task
 * in a wake queue (a call to wake_up_q() _must_ follow), so if a task is
 * already in a wake queue, the wakeup will happen soon and the second
 * waker can just skip it.
 *
 * The WAKE_Q macro declares and initializes the list head.
 * wake_up_q() does NOT reinitialize the list; it's expected to be
 * called near the end of a function, where the fact that the queue is
 * not used again will be easy to see by inspection.
 *
 * Note that this can cause spurious wakeups. schedule() callers
 * must ensure the call is done inside a loop, confirming that the
 * wakeup condition has in fact occurred.
 */
struct Model0_wake_q_node {
 struct Model0_wake_q_node *Model0_next;
};

struct Model0_wake_q_head {
 struct Model0_wake_q_node *Model0_first;
 struct Model0_wake_q_node **Model0_lastp;
};






extern void Model0_wake_q_add(struct Model0_wake_q_head *Model0_head,
         struct Model0_task_struct *Model0_task);
extern void Model0_wake_up_q(struct Model0_wake_q_head *Model0_head);

/*
 * sched-domains (multiprocessor balancing) declarations:
 */
static inline __attribute__((no_instrument_function)) int Model0_cpu_smt_flags(void)
{
 return 0x0080 | 0x0200;
}



static inline __attribute__((no_instrument_function)) int Model0_cpu_core_flags(void)
{
 return 0x0200;
}



static inline __attribute__((no_instrument_function)) int Model0_cpu_numa_flags(void)
{
 return 0x4000;
}


struct Model0_sched_domain_attr {
 int Model0_relax_domain_level;
};





extern int Model0_sched_domain_level_max;

struct Model0_sched_group;

struct Model0_sched_domain {
 /* These fields must be setup */
 struct Model0_sched_domain *Model0_parent; /* top domain must be null terminated */
 struct Model0_sched_domain *Model0_child; /* bottom domain must be null terminated */
 struct Model0_sched_group *Model0_groups; /* the balancing groups of the domain */
 unsigned long Model0_min_interval; /* Minimum balance interval ms */
 unsigned long Model0_max_interval; /* Maximum balance interval ms */
 unsigned int Model0_busy_factor; /* less balancing by factor if busy */
 unsigned int Model0_imbalance_pct; /* No balance until over watermark */
 unsigned int Model0_cache_nice_tries; /* Leave cache hot tasks for # tries */
 unsigned int Model0_busy_idx;
 unsigned int Model0_idle_idx;
 unsigned int Model0_newidle_idx;
 unsigned int Model0_wake_idx;
 unsigned int Model0_forkexec_idx;
 unsigned int Model0_smt_gain;

 int Model0_nohz_idle; /* NOHZ IDLE status */
 int Model0_flags; /* See SD_* */
 int Model0_level;

 /* Runtime fields. */
 unsigned long Model0_last_balance; /* init to jiffies. units in jiffies */
 unsigned int Model0_balance_interval; /* initialise to 1. units in ms. */
 unsigned int Model0_nr_balance_failed; /* initialise to 0 */

 /* idle_balance() stats */
 Model0_u64 Model0_max_newidle_lb_cost;
 unsigned long Model0_next_decay_max_lb_cost;


 /* load_balance() stats */
 unsigned int Model0_lb_count[Model0_CPU_MAX_IDLE_TYPES];
 unsigned int Model0_lb_failed[Model0_CPU_MAX_IDLE_TYPES];
 unsigned int Model0_lb_balanced[Model0_CPU_MAX_IDLE_TYPES];
 unsigned int Model0_lb_imbalance[Model0_CPU_MAX_IDLE_TYPES];
 unsigned int Model0_lb_gained[Model0_CPU_MAX_IDLE_TYPES];
 unsigned int Model0_lb_hot_gained[Model0_CPU_MAX_IDLE_TYPES];
 unsigned int Model0_lb_nobusyg[Model0_CPU_MAX_IDLE_TYPES];
 unsigned int Model0_lb_nobusyq[Model0_CPU_MAX_IDLE_TYPES];

 /* Active load balancing */
 unsigned int Model0_alb_count;
 unsigned int Model0_alb_failed;
 unsigned int Model0_alb_pushed;

 /* SD_BALANCE_EXEC stats */
 unsigned int Model0_sbe_count;
 unsigned int Model0_sbe_balanced;
 unsigned int Model0_sbe_pushed;

 /* SD_BALANCE_FORK stats */
 unsigned int Model0_sbf_count;
 unsigned int Model0_sbf_balanced;
 unsigned int Model0_sbf_pushed;

 /* try_to_wake_up() stats */
 unsigned int Model0_ttwu_wake_remote;
 unsigned int Model0_ttwu_move_affine;
 unsigned int Model0_ttwu_move_balance;




 union {
  void *Model0_private; /* used during construction */
  struct Model0_callback_head Model0_rcu; /* used during destruction */
 };

 unsigned int Model0_span_weight;
 /*
	 * Span of all CPUs in this domain.
	 *
	 * NOTE: this field is variable length. (Allocated dynamically
	 * by attaching extra space to the end of the structure,
	 * depending on how many CPUs the kernel has booted up with)
	 */
 unsigned long Model0_span[0];
};

static inline __attribute__((no_instrument_function)) struct Model0_cpumask *Model0_sched_domain_span(struct Model0_sched_domain *Model0_sd)
{
 return ((struct Model0_cpumask *)(1 ? (Model0_sd->Model0_span) : (void *)sizeof(Model0___check_is_bitmap(Model0_sd->Model0_span))));
}

extern void Model0_partition_sched_domains(int Model0_ndoms_new, Model0_cpumask_var_t Model0_doms_new[],
        struct Model0_sched_domain_attr *Model0_dattr_new);

/* Allocate an array of sched domains, for partition_sched_domains(). */
Model0_cpumask_var_t *Model0_alloc_sched_domains(unsigned int Model0_ndoms);
void Model0_free_sched_domains(Model0_cpumask_var_t Model0_doms[], unsigned int Model0_ndoms);

bool Model0_cpus_share_cache(int Model0_this_cpu, int Model0_that_cpu);

typedef const struct Model0_cpumask *(*Model0_sched_domain_mask_f)(int Model0_cpu);
typedef int (*Model0_sched_domain_flags_f)(void);



struct Model0_sd_data {
 struct Model0_sched_domain ** Model0_sd;
 struct Model0_sched_group ** Model0_sg;
 struct Model0_sched_group_capacity ** Model0_sgc;
};

struct Model0_sched_domain_topology_level {
 Model0_sched_domain_mask_f Model0_mask;
 Model0_sched_domain_flags_f Model0_sd_flags;
 int Model0_flags;
 int Model0_numa_level;
 struct Model0_sd_data Model0_data;



};

extern void Model0_set_sched_topology(struct Model0_sched_domain_topology_level *Model0_tl);
extern void Model0_wake_up_if_idle(int Model0_cpu);
struct Model0_io_context; /* See blkdev.h */





static inline __attribute__((no_instrument_function)) void Model0_prefetch_stack(struct Model0_task_struct *Model0_t) { }


struct Model0_audit_context; /* See audit.c */
struct Model0_mempolicy;
struct Model0_pipe_inode_info;
struct Model0_uts_namespace;

struct Model0_load_weight {
 unsigned long Model0_weight;
 Model0_u32 Model0_inv_weight;
};

/*
 * The load_avg/util_avg accumulates an infinite geometric series
 * (see __update_load_avg() in kernel/sched/fair.c).
 *
 * [load_avg definition]
 *
 *   load_avg = runnable% * scale_load_down(load)
 *
 * where runnable% is the time ratio that a sched_entity is runnable.
 * For cfs_rq, it is the aggregated load_avg of all runnable and
 * blocked sched_entities.
 *
 * load_avg may also take frequency scaling into account:
 *
 *   load_avg = runnable% * scale_load_down(load) * freq%
 *
 * where freq% is the CPU frequency normalized to the highest frequency.
 *
 * [util_avg definition]
 *
 *   util_avg = running% * SCHED_CAPACITY_SCALE
 *
 * where running% is the time ratio that a sched_entity is running on
 * a CPU. For cfs_rq, it is the aggregated util_avg of all runnable
 * and blocked sched_entities.
 *
 * util_avg may also factor frequency scaling and CPU capacity scaling:
 *
 *   util_avg = running% * SCHED_CAPACITY_SCALE * freq% * capacity%
 *
 * where freq% is the same as above, and capacity% is the CPU capacity
 * normalized to the greatest capacity (due to uarch differences, etc).
 *
 * N.B., the above ratios (runnable%, running%, freq%, and capacity%)
 * themselves are in the range of [0, 1]. To do fixed point arithmetics,
 * we therefore scale them to as large a range as necessary. This is for
 * example reflected by util_avg's SCHED_CAPACITY_SCALE.
 *
 * [Overflow issue]
 *
 * The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities
 * with the highest load (=88761), always runnable on a single cfs_rq,
 * and should not overflow as the number already hits PID_MAX_LIMIT.
 *
 * For all other cases (including 32-bit kernels), struct load_weight's
 * weight will overflow first before we do, because:
 *
 *    Max(load_avg) <= Max(load.weight)
 *
 * Then it is the load_weight's responsibility to consider overflow
 * issues.
 */
struct Model0_sched_avg {
 Model0_u64 Model0_last_update_time, Model0_load_sum;
 Model0_u32 Model0_util_sum, Model0_period_contrib;
 unsigned long Model0_load_avg, Model0_util_avg;
};


struct Model0_sched_statistics {
 Model0_u64 Model0_wait_start;
 Model0_u64 Model0_wait_max;
 Model0_u64 Model0_wait_count;
 Model0_u64 Model0_wait_sum;
 Model0_u64 Model0_iowait_count;
 Model0_u64 Model0_iowait_sum;

 Model0_u64 Model0_sleep_start;
 Model0_u64 Model0_sleep_max;
 Model0_s64 Model0_sum_sleep_runtime;

 Model0_u64 Model0_block_start;
 Model0_u64 Model0_block_max;
 Model0_u64 Model0_exec_max;
 Model0_u64 Model0_slice_max;

 Model0_u64 Model0_nr_migrations_cold;
 Model0_u64 Model0_nr_failed_migrations_affine;
 Model0_u64 Model0_nr_failed_migrations_running;
 Model0_u64 Model0_nr_failed_migrations_hot;
 Model0_u64 Model0_nr_forced_migrations;

 Model0_u64 Model0_nr_wakeups;
 Model0_u64 Model0_nr_wakeups_sync;
 Model0_u64 Model0_nr_wakeups_migrate;
 Model0_u64 Model0_nr_wakeups_local;
 Model0_u64 Model0_nr_wakeups_remote;
 Model0_u64 Model0_nr_wakeups_affine;
 Model0_u64 Model0_nr_wakeups_affine_attempts;
 Model0_u64 Model0_nr_wakeups_passive;
 Model0_u64 Model0_nr_wakeups_idle;
};


struct Model0_sched_entity {
 struct Model0_load_weight Model0_load; /* for load-balancing */
 struct Model0_rb_node Model0_run_node;
 struct Model0_list_head Model0_group_node;
 unsigned int Model0_on_rq;

 Model0_u64 Model0_exec_start;
 Model0_u64 Model0_sum_exec_runtime;
 Model0_u64 Model0_vruntime;
 Model0_u64 Model0_prev_sum_exec_runtime;

 Model0_u64 Model0_nr_migrations;


 struct Model0_sched_statistics Model0_statistics;



 int Model0_depth;
 struct Model0_sched_entity *Model0_parent;
 /* rq on which this entity is (to be) queued: */
 struct Model0_cfs_rq *Model0_cfs_rq;
 /* rq "owned" by this entity/group: */
 struct Model0_cfs_rq *Model0_my_q;



 /*
	 * Per entity load average tracking.
	 *
	 * Put into separate cache line so it does not
	 * collide with read-mostly values above.
	 */
 struct Model0_sched_avg Model0_avg __attribute__((__aligned__((1 << (6)))));

};

struct Model0_sched_rt_entity {
 struct Model0_list_head Model0_run_list;
 unsigned long Model0_timeout;
 unsigned long Model0_watchdog_stamp;
 unsigned int Model0_time_slice;
 unsigned short Model0_on_rq;
 unsigned short Model0_on_list;

 struct Model0_sched_rt_entity *Model0_back;







};

struct Model0_sched_dl_entity {
 struct Model0_rb_node Model0_rb_node;

 /*
	 * Original scheduling parameters. Copied here from sched_attr
	 * during sched_setattr(), they will remain the same until
	 * the next sched_setattr().
	 */
 Model0_u64 Model0_dl_runtime; /* maximum runtime for each instance	*/
 Model0_u64 Model0_dl_deadline; /* relative deadline of each instance	*/
 Model0_u64 Model0_dl_period; /* separation of two instances (period) */
 Model0_u64 Model0_dl_bw; /* dl_runtime / dl_deadline		*/

 /*
	 * Actual scheduling parameters. Initialized with the values above,
	 * they are continously updated during task execution. Note that
	 * the remaining runtime could be < 0 in case we are in overrun.
	 */
 Model0_s64 Model0_runtime; /* remaining runtime for this instance	*/
 Model0_u64 Model0_deadline; /* absolute deadline for this instance	*/
 unsigned int Model0_flags; /* specifying the scheduler behaviour	*/

 /*
	 * Some bool flags:
	 *
	 * @dl_throttled tells if we exhausted the runtime. If so, the
	 * task has to wait for a replenishment to be performed at the
	 * next firing of dl_timer.
	 *
	 * @dl_boosted tells if we are boosted due to DI. If so we are
	 * outside bandwidth enforcement mechanism (but only until we
	 * exit the critical section);
	 *
	 * @dl_yielded tells if task gave up the cpu before consuming
	 * all its available runtime during the last job.
	 */
 int Model0_dl_throttled, Model0_dl_boosted, Model0_dl_yielded;

 /*
	 * Bandwidth enforcement timer. Each -deadline task has its
	 * own bandwidth to be enforced, thus we need one timer per task.
	 */
 struct Model0_hrtimer Model0_dl_timer;
};

union Model0_rcu_special {
 struct {
  Model0_u8 Model0_blocked;
  Model0_u8 Model0_need_qs;
  Model0_u8 Model0_exp_need_qs;
  Model0_u8 Model0_pad; /* Otherwise the compiler can store garbage here. */
 } Model0_b; /* Bits. */
 Model0_u32 Model0_s; /* Set of bits. */
};
struct Model0_rcu_node;

enum Model0_perf_event_task_context {
 Model0_perf_invalid_context = -1,
 Model0_perf_hw_context = 0,
 Model0_perf_sw_context,
 Model0_perf_nr_task_contexts,
};

/* Track pages that require TLB flushes */
struct Model0_tlbflush_unmap_batch {
 /*
	 * Each bit set is a CPU that potentially has a TLB entry for one of
	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().
	 */
 struct Model0_cpumask Model0_cpumask;

 /* True if any bit in cpumask is set */
 bool Model0_flush_required;

 /*
	 * If true then the PTE was dirty when unmapped. The entry must be
	 * flushed before IO is initiated or a stale TLB entry potentially
	 * allows an update without redirtying the page.
	 */
 bool Model0_writable;
};

struct Model0_task_struct {
 volatile long Model0_state; /* -1 unrunnable, 0 runnable, >0 stopped */
 void *Model0_stack;
 Model0_atomic_t Model0_usage;
 unsigned int Model0_flags; /* per process flags, defined below */
 unsigned int Model0_ptrace;


 struct Model0_llist_node Model0_wake_entry;
 int Model0_on_cpu;
 unsigned int Model0_wakee_flips;
 unsigned long Model0_wakee_flip_decay_ts;
 struct Model0_task_struct *Model0_last_wakee;

 int Model0_wake_cpu;

 int Model0_on_rq;

 int Model0_prio, Model0_static_prio, Model0_normal_prio;
 unsigned int Model0_rt_priority;
 const struct Model0_sched_class *Model0_sched_class;
 struct Model0_sched_entity Model0_se;
 struct Model0_sched_rt_entity Model0_rt;

 struct Model0_task_group *Model0_sched_task_group;

 struct Model0_sched_dl_entity Model0_dl;







 unsigned int Model0_btrace_seq;


 unsigned int Model0_policy;
 int Model0_nr_cpus_allowed;
 Model0_cpumask_t Model0_cpus_allowed;
 struct Model0_sched_info Model0_sched_info;


 struct Model0_list_head Model0_tasks;

 struct Model0_plist_node Model0_pushable_tasks;
 struct Model0_rb_node Model0_pushable_dl_tasks;


 struct Model0_mm_struct *Model0_mm, *Model0_active_mm;
 /* per-thread vma caching */
 Model0_u32 Model0_vmacache_seqnum;
 struct Model0_vm_area_struct *Model0_vmacache[(1U << 2)];

 struct Model0_task_rss_stat Model0_rss_stat;

/* task state */
 int Model0_exit_state;
 int Model0_exit_code, Model0_exit_signal;
 int Model0_pdeath_signal; /*  The signal sent when the parent dies  */
 unsigned long Model0_jobctl; /* JOBCTL_*, siglock protected */

 /* Used for emulating ABI behavior of previous Linux versions */
 unsigned int Model0_personality;

 /* scheduler bits, serialized by scheduler locks */
 unsigned Model0_sched_reset_on_fork:1;
 unsigned Model0_sched_contributes_to_load:1;
 unsigned Model0_sched_migrated:1;
 unsigned Model0_sched_remote_wakeup:1;
 unsigned :0; /* force alignment to the next boundary */

 /* unserialized, strictly 'current' */
 unsigned Model0_in_execve:1; /* bit to tell LSMs we're in execve */
 unsigned Model0_in_iowait:1;

 unsigned Model0_restore_sigmask:1;
 unsigned long Model0_atomic_flags; /* Flags needing atomic access. */

 struct Model0_restart_block Model0_restart_block;

 Model0_pid_t Model0_pid;
 Model0_pid_t Model0_tgid;





 /*
	 * pointers to (original) parent process, youngest child, younger sibling,
	 * older sibling, respectively.  (p->father can be replaced with
	 * p->real_parent->pid)
	 */
 struct Model0_task_struct *Model0_real_parent; /* real parent process */
 struct Model0_task_struct *Model0_parent; /* recipient of SIGCHLD, wait4() reports */
 /*
	 * children/sibling forms the list of my natural children
	 */
 struct Model0_list_head Model0_children; /* list of my children */
 struct Model0_list_head Model0_sibling; /* linkage in my parent's children list */
 struct Model0_task_struct *Model0_group_leader; /* threadgroup leader */

 /*
	 * ptraced is the list of tasks this task is using ptrace on.
	 * This includes both natural children and PTRACE_ATTACH targets.
	 * p->ptrace_entry is p's link on the p->parent->ptraced list.
	 */
 struct Model0_list_head Model0_ptraced;
 struct Model0_list_head Model0_ptrace_entry;

 /* PID/PID hash table linkage. */
 struct Model0_pid_link Model0_pids[Model0_PIDTYPE_MAX];
 struct Model0_list_head Model0_thread_group;
 struct Model0_list_head Model0_thread_node;

 struct Model0_completion *Model0_vfork_done; /* for vfork() */
 int *Model0_set_child_tid; /* CLONE_CHILD_SETTID */
 int *Model0_clear_child_tid; /* CLONE_CHILD_CLEARTID */

 Model0_cputime_t Model0_utime, Model0_stime, Model0_utimescaled, Model0_stimescaled;
 Model0_cputime_t Model0_gtime;
 struct Model0_prev_cputime Model0_prev_cputime;
 unsigned long Model0_nvcsw, Model0_nivcsw; /* context switch counts */
 Model0_u64 Model0_start_time; /* monotonic time in nsec */
 Model0_u64 Model0_real_start_time; /* boot based time in nsec */
/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 unsigned long Model0_min_flt, Model0_maj_flt;

 struct Model0_task_cputime Model0_cputime_expires;
 struct Model0_list_head Model0_cpu_timers[3];

/* process credentials */
 const struct Model0_cred *Model0_real_cred; /* objective and real subjective task
					 * credentials (COW) */
 const struct Model0_cred *Model0_cred; /* effective (overridable) subjective task
					 * credentials (COW) */
 char Model0_comm[16]; /* executable name excluding path
				     - access with [gs]et_task_comm (which lock
				       it with task_lock())
				     - initialized normally by setup_new_exec */
/* file system info */
 struct Model0_nameidata *Model0_nameidata;

/* ipc stuff */
 struct Model0_sysv_sem Model0_sysvsem;
 struct Model0_sysv_shm Model0_sysvshm;





/* filesystem information */
 struct Model0_fs_struct *Model0_fs;
/* open file information */
 struct Model0_files_struct *Model0_files;
/* namespaces */
 struct Model0_nsproxy *Model0_nsproxy;
/* signal handlers */
 struct Model0_signal_struct *Model0_signal;
 struct Model0_sighand_struct *Model0_sighand;

 Model0_sigset_t Model0_blocked, Model0_real_blocked;
 Model0_sigset_t Model0_saved_sigmask; /* restored if set_restore_sigmask() was used */
 struct Model0_sigpending Model0_pending;

 unsigned long Model0_sas_ss_sp;
 Model0_size_t Model0_sas_ss_size;
 unsigned Model0_sas_ss_flags;

 struct Model0_callback_head *Model0_task_works;

 struct Model0_audit_context *Model0_audit_context;

 Model0_kuid_t Model0_loginuid;
 unsigned int Model0_sessionid;

 struct Model0_seccomp Model0_seccomp;

/* Thread group tracking */
    Model0_u32 Model0_parent_exec_id;
    Model0_u32 Model0_self_exec_id;
/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,
 * mempolicy */
 Model0_spinlock_t Model0_alloc_lock;

 /* Protection of the PI data structures: */
 Model0_raw_spinlock_t Model0_pi_lock;

 struct Model0_wake_q_node Model0_wake_q;


 /* PI waiters blocked on a rt_mutex held by this task */
 struct Model0_rb_root Model0_pi_waiters;
 struct Model0_rb_node *Model0_pi_waiters_leftmost;
 /* Deadlock detection and priority inheritance handling */
 struct Model0_rt_mutex_waiter *Model0_pi_blocked_on;
/* journalling filesystem info */
 void *Model0_journal_info;

/* stacked block device info */
 struct Model0_bio_list *Model0_bio_list;


/* stack plugging */
 struct Model0_blk_plug *Model0_plug;


/* VM state */
 struct Model0_reclaim_state *Model0_reclaim_state;

 struct Model0_backing_dev_info *Model0_backing_dev_info;

 struct Model0_io_context *Model0_io_context;

 unsigned long Model0_ptrace_message;
 Model0_siginfo_t *Model0_last_siginfo; /* For ptrace use.  */
 struct Model0_task_io_accounting Model0_ioac;

 Model0_u64 Model0_acct_rss_mem1; /* accumulated rss usage */
 Model0_u64 Model0_acct_vm_mem1; /* accumulated virtual memory usage */
 Model0_cputime_t Model0_acct_timexpd; /* stime + utime since last update */


 Model0_nodemask_t Model0_mems_allowed; /* Protected by alloc_lock */
 Model0_seqcount_t Model0_mems_allowed_seq; /* Seqence no to catch updates */
 int Model0_cpuset_mem_spread_rotor;
 int Model0_cpuset_slab_spread_rotor;


 /* Control Group info protected by css_set_lock */
 struct Model0_css_set *Model0_cgroups;
 /* cg_list protected by css_set_lock and tsk->alloc_lock */
 struct Model0_list_head Model0_cg_list;


 struct Model0_robust_list_head *Model0_robust_list;

 struct Model0_compat_robust_list_head *Model0_compat_robust_list;

 struct Model0_list_head Model0_pi_state_list;
 struct Model0_futex_pi_state *Model0_pi_state_cache;


 struct Model0_perf_event_context *Model0_perf_event_ctxp[Model0_perf_nr_task_contexts];
 struct Model0_mutex Model0_perf_event_mutex;
 struct Model0_list_head Model0_perf_event_list;





 struct Model0_mempolicy *Model0_mempolicy; /* Protected by alloc_lock */
 short Model0_il_next;
 short Model0_pref_node_fork;
 struct Model0_tlbflush_unmap_batch Model0_tlb_ubc;


 struct Model0_callback_head Model0_rcu;

 /*
	 * cache last used pipe for splice
	 */
 struct Model0_pipe_inode_info *Model0_splice_pipe;

 struct Model0_page_frag Model0_task_frag;


 struct Model0_task_delay_info *Model0_delays;




 /*
	 * when (nr_dirtied >= nr_dirtied_pause), it's time to call
	 * balance_dirty_pages() for some dirty throttling pause
	 */
 int Model0_nr_dirtied;
 int Model0_nr_dirtied_pause;
 unsigned long Model0_dirty_paused_when; /* start of a write-and-pause period */





 /*
	 * time slack values; these are used to round up poll() and
	 * select() etc timeout values. These are in nanoseconds.
	 */
 Model0_u64 Model0_timer_slack_ns;
 Model0_u64 Model0_default_timer_slack_ns;
 /* state flags for use by tracers */
 unsigned long Model0_trace;
 /* bitmask and counter of trace recursion */
 unsigned long Model0_trace_recursion;
 int Model0_pagefault_disabled;

 struct Model0_task_struct *Model0_oom_reaper_list;

/* CPU-specific state of this task */
 struct Model0_thread_struct thread;
/*
 * WARNING: on x86, 'thread_struct' contains a variable-sized
 * structure.  It *MUST* be at the end of 'task_struct'.
 *
 * Do not put anything below here!
 */
};


extern int Model0_arch_task_struct_size __attribute__((__section__(".data..read_mostly")));




/* Future-safe accessor for struct task_struct's cpus_allowed. */


static inline __attribute__((no_instrument_function)) int Model0_tsk_nr_cpus_allowed(struct Model0_task_struct *Model0_p)
{
 return Model0_p->Model0_nr_cpus_allowed;
}







static inline __attribute__((no_instrument_function)) bool Model0_in_vfork(struct Model0_task_struct *Model0_tsk)
{
 bool Model0_ret;

 /*
	 * need RCU to access ->real_parent if CLONE_VM was used along with
	 * CLONE_PARENT.
	 *
	 * We check real_parent->mm == tsk->mm because CLONE_VFORK does not
	 * imply CLONE_VM
	 *
	 * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus
	 * ->real_parent is not necessarily the task doing vfork(), so in
	 * theory we can't rely on task_lock() if we want to dereference it.
	 *
	 * And in this case we can't trust the real_parent->mm == tsk->mm
	 * check, it can be false negative. But we do not care, if init or
	 * another oom-unkillable task does this it should blame itself.
	 */
 Model0_rcu_read_lock();
 Model0_ret = Model0_tsk->Model0_vfork_done && Model0_tsk->Model0_real_parent->Model0_mm == Model0_tsk->Model0_mm;
 Model0_rcu_read_unlock();

 return Model0_ret;
}
static inline __attribute__((no_instrument_function)) void Model0_task_numa_fault(int Model0_last_node, int Model0_node, int Model0_pages,
       int Model0_flags)
{
}
static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_numa_group_id(struct Model0_task_struct *Model0_p)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) void Model0_set_numabalancing_state(bool Model0_enabled)
{
}
static inline __attribute__((no_instrument_function)) void Model0_task_numa_free(struct Model0_task_struct *Model0_p)
{
}
static inline __attribute__((no_instrument_function)) bool Model0_should_numa_migrate_memory(struct Model0_task_struct *Model0_p,
    struct Model0_page *Model0_page, int Model0_src_nid, int Model0_dst_cpu)
{
 return true;
}


static inline __attribute__((no_instrument_function)) struct Model0_pid *Model0_task_pid(struct Model0_task_struct *Model0_task)
{
 return Model0_task->Model0_pids[Model0_PIDTYPE_PID].Model0_pid;
}

static inline __attribute__((no_instrument_function)) struct Model0_pid *Model0_task_tgid(struct Model0_task_struct *Model0_task)
{
 return Model0_task->Model0_group_leader->Model0_pids[Model0_PIDTYPE_PID].Model0_pid;
}

/*
 * Without tasklist or rcu lock it is not safe to dereference
 * the result of task_pgrp/task_session even if task == current,
 * we can race with another thread doing sys_setsid/sys_setpgid.
 */
static inline __attribute__((no_instrument_function)) struct Model0_pid *Model0_task_pgrp(struct Model0_task_struct *Model0_task)
{
 return Model0_task->Model0_group_leader->Model0_pids[Model0_PIDTYPE_PGID].Model0_pid;
}

static inline __attribute__((no_instrument_function)) struct Model0_pid *Model0_task_session(struct Model0_task_struct *Model0_task)
{
 return Model0_task->Model0_group_leader->Model0_pids[Model0_PIDTYPE_SID].Model0_pid;
}

struct Model0_pid_namespace;

/*
 * the helpers to get the task's different pids as they are seen
 * from various namespaces
 *
 * task_xid_nr()     : global id, i.e. the id seen from the init namespace;
 * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of
 *                     current.
 * task_xid_nr_ns()  : id seen from the ns specified;
 *
 * set_task_vxid()   : assigns a virtual id to a task;
 *
 * see also pid_nr() etc in include/linux/pid.h
 */
Model0_pid_t Model0___task_pid_nr_ns(struct Model0_task_struct *Model0_task, enum Model0_pid_type Model0_type,
   struct Model0_pid_namespace *Model0_ns);

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_pid_nr(struct Model0_task_struct *Model0_tsk)
{
 return Model0_tsk->Model0_pid;
}

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_pid_nr_ns(struct Model0_task_struct *Model0_tsk,
     struct Model0_pid_namespace *Model0_ns)
{
 return Model0___task_pid_nr_ns(Model0_tsk, Model0_PIDTYPE_PID, Model0_ns);
}

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_pid_vnr(struct Model0_task_struct *Model0_tsk)
{
 return Model0___task_pid_nr_ns(Model0_tsk, Model0_PIDTYPE_PID, ((void *)0));
}


static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_tgid_nr(struct Model0_task_struct *Model0_tsk)
{
 return Model0_tsk->Model0_tgid;
}

Model0_pid_t Model0_task_tgid_nr_ns(struct Model0_task_struct *Model0_tsk, struct Model0_pid_namespace *Model0_ns);

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_tgid_vnr(struct Model0_task_struct *Model0_tsk)
{
 return Model0_pid_vnr(Model0_task_tgid(Model0_tsk));
}


static inline __attribute__((no_instrument_function)) int Model0_pid_alive(const struct Model0_task_struct *Model0_p);
static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_ppid_nr_ns(const struct Model0_task_struct *Model0_tsk, struct Model0_pid_namespace *Model0_ns)
{
 Model0_pid_t Model0_pid = 0;

 Model0_rcu_read_lock();
 if (Model0_pid_alive(Model0_tsk))
  Model0_pid = Model0_task_tgid_nr_ns(({ typeof(*(Model0_tsk->Model0_real_parent)) *Model0_________p1 = (typeof(*(Model0_tsk->Model0_real_parent)) *)({ typeof((Model0_tsk->Model0_real_parent)) Model0__________p1 = ({ union { typeof((Model0_tsk->Model0_real_parent)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_tsk->Model0_real_parent)), Model0___u.Model0___c, sizeof((Model0_tsk->Model0_real_parent))); else Model0___read_once_size_nocheck(&((Model0_tsk->Model0_real_parent)), Model0___u.Model0___c, sizeof((Model0_tsk->Model0_real_parent))); Model0___u.Model0___val; }); typeof(*((Model0_tsk->Model0_real_parent))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_tsk->Model0_real_parent)) *)(Model0_________p1)); }), Model0_ns);
 Model0_rcu_read_unlock();

 return Model0_pid;
}

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_ppid_nr(const struct Model0_task_struct *Model0_tsk)
{
 return Model0_task_ppid_nr_ns(Model0_tsk, &Model0_init_pid_ns);
}

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_pgrp_nr_ns(struct Model0_task_struct *Model0_tsk,
     struct Model0_pid_namespace *Model0_ns)
{
 return Model0___task_pid_nr_ns(Model0_tsk, Model0_PIDTYPE_PGID, Model0_ns);
}

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_pgrp_vnr(struct Model0_task_struct *Model0_tsk)
{
 return Model0___task_pid_nr_ns(Model0_tsk, Model0_PIDTYPE_PGID, ((void *)0));
}


static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_session_nr_ns(struct Model0_task_struct *Model0_tsk,
     struct Model0_pid_namespace *Model0_ns)
{
 return Model0___task_pid_nr_ns(Model0_tsk, Model0_PIDTYPE_SID, Model0_ns);
}

static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_session_vnr(struct Model0_task_struct *Model0_tsk)
{
 return Model0___task_pid_nr_ns(Model0_tsk, Model0_PIDTYPE_SID, ((void *)0));
}

/* obsolete, do not use */
static inline __attribute__((no_instrument_function)) Model0_pid_t Model0_task_pgrp_nr(struct Model0_task_struct *Model0_tsk)
{
 return Model0_task_pgrp_nr_ns(Model0_tsk, &Model0_init_pid_ns);
}

/**
 * pid_alive - check that a task structure is not stale
 * @p: Task structure to be checked.
 *
 * Test if a process is not yet dead (at most zombie state)
 * If pid_alive fails, then pointers within the task structure
 * can be stale and must not be dereferenced.
 *
 * Return: 1 if the process is alive. 0 otherwise.
 */
static inline __attribute__((no_instrument_function)) int Model0_pid_alive(const struct Model0_task_struct *Model0_p)
{
 return Model0_p->Model0_pids[Model0_PIDTYPE_PID].Model0_pid != ((void *)0);
}

/**
 * is_global_init - check if a task structure is init. Since init
 * is free to have sub-threads we need to check tgid.
 * @tsk: Task structure to be checked.
 *
 * Check if a task structure is the first user space task the kernel created.
 *
 * Return: 1 if the task structure is init. 0 otherwise.
 */
static inline __attribute__((no_instrument_function)) int Model0_is_global_init(struct Model0_task_struct *Model0_tsk)
{
 return Model0_task_tgid_nr(Model0_tsk) == 1;
}

extern struct Model0_pid *Model0_cad_pid;

extern void Model0_free_task(struct Model0_task_struct *Model0_tsk);


extern void Model0___put_task_struct(struct Model0_task_struct *Model0_t);

static inline __attribute__((no_instrument_function)) void Model0_put_task_struct(struct Model0_task_struct *Model0_t)
{
 if (Model0_atomic_dec_and_test(&Model0_t->Model0_usage))
  Model0___put_task_struct(Model0_t);
}

struct Model0_task_struct *Model0_task_rcu_dereference(struct Model0_task_struct **Model0_ptask);
struct Model0_task_struct *Model0_try_get_task_struct(struct Model0_task_struct **Model0_ptask);
static inline __attribute__((no_instrument_function)) void Model0_task_cputime(struct Model0_task_struct *Model0_t,
    Model0_cputime_t *Model0_utime, Model0_cputime_t *Model0_stime)
{
 if (Model0_utime)
  *Model0_utime = Model0_t->Model0_utime;
 if (Model0_stime)
  *Model0_stime = Model0_t->Model0_stime;
}

static inline __attribute__((no_instrument_function)) void Model0_task_cputime_scaled(struct Model0_task_struct *Model0_t,
           Model0_cputime_t *Model0_utimescaled,
           Model0_cputime_t *Model0_stimescaled)
{
 if (Model0_utimescaled)
  *Model0_utimescaled = Model0_t->Model0_utimescaled;
 if (Model0_stimescaled)
  *Model0_stimescaled = Model0_t->Model0_stimescaled;
}

static inline __attribute__((no_instrument_function)) Model0_cputime_t Model0_task_gtime(struct Model0_task_struct *Model0_t)
{
 return Model0_t->Model0_gtime;
}

extern void Model0_task_cputime_adjusted(struct Model0_task_struct *Model0_p, Model0_cputime_t *Model0_ut, Model0_cputime_t *Model0_st);
extern void Model0_thread_group_cputime_adjusted(struct Model0_task_struct *Model0_p, Model0_cputime_t *Model0_ut, Model0_cputime_t *Model0_st);

/*
 * Per process flags
 */
/*
 * Only the _current_ task can read/write to tsk->flags, but other
 * tasks can access tsk->flags in readonly mode for example
 * with tsk_used_math (like during threaded core dumping).
 * There is however an exception to this rule during ptrace
 * or during fork: the ptracer task is allowed to write to the
 * child->flags of its traced child (same goes for fork, the parent
 * can write to the child->flags), because we're guaranteed the
 * child is not running and in turn not changing child->flags
 * at the same time the parent does it.
 */
/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */



/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags
 * __GFP_FS is also cleared as it implies __GFP_IO.
 */
static inline __attribute__((no_instrument_function)) Model0_gfp_t Model0_memalloc_noio_flags(Model0_gfp_t Model0_flags)
{
 if (__builtin_expect(!!(Model0_get_current()->Model0_flags & 0x00080000), 0))
  Model0_flags &= ~((( Model0_gfp_t)0x40u) | (( Model0_gfp_t)0x80u));
 return Model0_flags;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_memalloc_noio_save(void)
{
 unsigned int Model0_flags = Model0_get_current()->Model0_flags & 0x00080000;
 Model0_get_current()->Model0_flags |= 0x00080000;
 return Model0_flags;
}

static inline __attribute__((no_instrument_function)) void Model0_memalloc_noio_restore(unsigned int Model0_flags)
{
 Model0_get_current()->Model0_flags = (Model0_get_current()->Model0_flags & ~0x00080000) | Model0_flags;
}

/* Per-process atomic flags. */
static inline __attribute__((no_instrument_function)) bool Model0_task_no_new_privs(struct Model0_task_struct *Model0_p) { return (__builtin_constant_p((0)) ? Model0_constant_test_bit((0), (&Model0_p->Model0_atomic_flags)) : Model0_variable_test_bit((0), (&Model0_p->Model0_atomic_flags))); }
static inline __attribute__((no_instrument_function)) void Model0_task_set_no_new_privs(struct Model0_task_struct *Model0_p) { Model0_set_bit(0, &Model0_p->Model0_atomic_flags); }

static inline __attribute__((no_instrument_function)) bool Model0_task_spread_page(struct Model0_task_struct *Model0_p) { return (__builtin_constant_p((1)) ? Model0_constant_test_bit((1), (&Model0_p->Model0_atomic_flags)) : Model0_variable_test_bit((1), (&Model0_p->Model0_atomic_flags))); }
static inline __attribute__((no_instrument_function)) void Model0_task_set_spread_page(struct Model0_task_struct *Model0_p) { Model0_set_bit(1, &Model0_p->Model0_atomic_flags); }
static inline __attribute__((no_instrument_function)) void Model0_task_clear_spread_page(struct Model0_task_struct *Model0_p) { Model0_clear_bit(1, &Model0_p->Model0_atomic_flags); }

static inline __attribute__((no_instrument_function)) bool Model0_task_spread_slab(struct Model0_task_struct *Model0_p) { return (__builtin_constant_p((2)) ? Model0_constant_test_bit((2), (&Model0_p->Model0_atomic_flags)) : Model0_variable_test_bit((2), (&Model0_p->Model0_atomic_flags))); }
static inline __attribute__((no_instrument_function)) void Model0_task_set_spread_slab(struct Model0_task_struct *Model0_p) { Model0_set_bit(2, &Model0_p->Model0_atomic_flags); }
static inline __attribute__((no_instrument_function)) void Model0_task_clear_spread_slab(struct Model0_task_struct *Model0_p) { Model0_clear_bit(2, &Model0_p->Model0_atomic_flags); }

static inline __attribute__((no_instrument_function)) bool Model0_task_lmk_waiting(struct Model0_task_struct *Model0_p) { return (__builtin_constant_p((3)) ? Model0_constant_test_bit((3), (&Model0_p->Model0_atomic_flags)) : Model0_variable_test_bit((3), (&Model0_p->Model0_atomic_flags))); }
static inline __attribute__((no_instrument_function)) void Model0_task_set_lmk_waiting(struct Model0_task_struct *Model0_p) { Model0_set_bit(3, &Model0_p->Model0_atomic_flags); }

/*
 * task->jobctl flags
 */
extern bool Model0_task_set_jobctl_pending(struct Model0_task_struct *Model0_task,
        unsigned long Model0_mask);
extern void Model0_task_clear_jobctl_trapping(struct Model0_task_struct *Model0_task);
extern void Model0_task_clear_jobctl_pending(struct Model0_task_struct *Model0_task,
          unsigned long Model0_mask);

static inline __attribute__((no_instrument_function)) void Model0_rcu_copy_process(struct Model0_task_struct *Model0_p)
{
}

static inline __attribute__((no_instrument_function)) void Model0_tsk_restore_flags(struct Model0_task_struct *Model0_task,
    unsigned long Model0_orig_flags, unsigned long Model0_flags)
{
 Model0_task->Model0_flags &= ~Model0_flags;
 Model0_task->Model0_flags |= Model0_orig_flags & Model0_flags;
}

extern int Model0_cpuset_cpumask_can_shrink(const struct Model0_cpumask *Model0_cur,
         const struct Model0_cpumask *Model0_trial);
extern int Model0_task_can_attach(struct Model0_task_struct *Model0_p,
      const struct Model0_cpumask *Model0_cs_cpus_allowed);

extern void Model0_do_set_cpus_allowed(struct Model0_task_struct *Model0_p,
          const struct Model0_cpumask *Model0_new_mask);

extern int Model0_set_cpus_allowed_ptr(struct Model0_task_struct *Model0_p,
    const struct Model0_cpumask *Model0_new_mask);
void Model0_calc_load_enter_idle(void);
void Model0_calc_load_exit_idle(void);





/*
 * Do not use outside of architecture code which knows its limitations.
 *
 * sched_clock() has no promise of monotonicity or bounded drift between
 * CPUs, use (which you should not) requires disabling IRQs.
 *
 * Please use one of the three interfaces below.
 */
extern unsigned long long __attribute__((no_instrument_function)) Model0_sched_clock(void);
/*
 * See the comment in kernel/sched/clock.c
 */
extern Model0_u64 Model0_running_clock(void);
extern Model0_u64 Model0_sched_clock_cpu(int Model0_cpu);


extern void Model0_sched_clock_init(void);
/*
 * Architectures can set this to 1 if they have specified
 * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,
 * but then during bootup it turns out that sched_clock()
 * is reliable after all:
 */
extern int Model0_sched_clock_stable(void);
extern void Model0_set_sched_clock_stable(void);
extern void Model0_clear_sched_clock_stable(void);

extern void Model0_sched_clock_tick(void);
extern void Model0_sched_clock_idle_sleep_event(void);
extern void Model0_sched_clock_idle_wakeup_event(Model0_u64 Model0_delta_ns);

/*
 * As outlined in clock.c, provides a fast, high resolution, nanosecond
 * time source that is monotonic per cpu argument and has bounded drift
 * between cpus.
 *
 * ######################### BIG FAT WARNING ##########################
 * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #
 * # go backwards !!                                                  #
 * ####################################################################
 */
static inline __attribute__((no_instrument_function)) Model0_u64 Model0_cpu_clock(int Model0_cpu)
{
 return Model0_sched_clock_cpu(Model0_cpu);
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_local_clock(void)
{
 return Model0_sched_clock_cpu((({ typeof(Model0_cpu_number) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_number)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_cpu_number)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; })));
}
static inline __attribute__((no_instrument_function)) void Model0_enable_sched_clock_irqtime(void) {}
static inline __attribute__((no_instrument_function)) void Model0_disable_sched_clock_irqtime(void) {}


extern unsigned long long
Model0_task_sched_runtime(struct Model0_task_struct *Model0_task);

/* sched_exec is called by processes performing an exec */

extern void Model0_sched_exec(void);




extern void Model0_sched_clock_idle_sleep_event(void);
extern void Model0_sched_clock_idle_wakeup_event(Model0_u64 Model0_delta_ns);


extern void Model0_idle_task_exit(void);





extern void Model0_wake_up_nohz_cpu(int Model0_cpu);
static inline __attribute__((no_instrument_function)) void Model0_sched_autogroup_create_attach(struct Model0_task_struct *Model0_p) { }
static inline __attribute__((no_instrument_function)) void Model0_sched_autogroup_detach(struct Model0_task_struct *Model0_p) { }
static inline __attribute__((no_instrument_function)) void Model0_sched_autogroup_fork(struct Model0_signal_struct *Model0_sig) { }
static inline __attribute__((no_instrument_function)) void Model0_sched_autogroup_exit(struct Model0_signal_struct *Model0_sig) { }


extern int Model0_yield_to(struct Model0_task_struct *Model0_p, bool Model0_preempt);
extern void Model0_set_user_nice(struct Model0_task_struct *Model0_p, long Model0_nice);
extern int Model0_task_prio(const struct Model0_task_struct *Model0_p);
/**
 * task_nice - return the nice value of a given task.
 * @p: the task in question.
 *
 * Return: The nice value [ -20 ... 0 ... 19 ].
 */
static inline __attribute__((no_instrument_function)) int Model0_task_nice(const struct Model0_task_struct *Model0_p)
{
 return (((Model0_p)->Model0_static_prio) - (100 + (19 - -20 + 1) / 2));
}
extern int Model0_can_nice(const struct Model0_task_struct *Model0_p, const int Model0_nice);
extern int Model0_task_curr(const struct Model0_task_struct *Model0_p);
extern int Model0_idle_cpu(int Model0_cpu);
extern int Model0_sched_setscheduler(struct Model0_task_struct *, int,
         const struct Model0_sched_param *);
extern int Model0_sched_setscheduler_nocheck(struct Model0_task_struct *, int,
          const struct Model0_sched_param *);
extern int Model0_sched_setattr(struct Model0_task_struct *,
    const struct Model0_sched_attr *);
extern struct Model0_task_struct *Model0_idle_task(int Model0_cpu);
/**
 * is_idle_task - is the specified task an idle task?
 * @p: the task in question.
 *
 * Return: 1 if @p is an idle task. 0 otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_is_idle_task(const struct Model0_task_struct *Model0_p)
{
 return Model0_p->Model0_pid == 0;
}
extern struct Model0_task_struct *Model0_curr_task(int Model0_cpu);
extern void Model0_set_curr_task(int Model0_cpu, struct Model0_task_struct *Model0_p);

void Model0_yield(void);

union Model0_thread_union {
 struct Model0_thread_info Model0_thread_info;
 unsigned long Model0_stack[(((1UL) << 12) << (2 + 0))/sizeof(long)];
};


static inline __attribute__((no_instrument_function)) int Model0_kstack_end(void *Model0_addr)
{
 /* Reliable end of stack detection:
	 * Some APM bios versions misalign the stack
	 */
 return !(((unsigned long)Model0_addr+sizeof(void*)-1) & ((((1UL) << 12) << (2 + 0))-sizeof(void*)));
}


extern union Model0_thread_union Model0_init_thread_union;
extern struct Model0_task_struct Model0_init_task;

extern struct Model0_mm_struct Model0_init_mm;

extern struct Model0_pid_namespace Model0_init_pid_ns;

/*
 * find a task by one of its numerical ids
 *
 * find_task_by_pid_ns():
 *      finds a task by its pid in the specified namespace
 * find_task_by_vpid():
 *      finds a task by its virtual pid
 *
 * see also find_vpid() etc in include/linux/pid.h
 */

extern struct Model0_task_struct *Model0_find_task_by_vpid(Model0_pid_t Model0_nr);
extern struct Model0_task_struct *Model0_find_task_by_pid_ns(Model0_pid_t Model0_nr,
  struct Model0_pid_namespace *Model0_ns);

/* per-UID process charging. */
extern struct Model0_user_struct * Model0_alloc_uid(Model0_kuid_t);
static inline __attribute__((no_instrument_function)) struct Model0_user_struct *Model0_get_uid(struct Model0_user_struct *Model0_u)
{
 Model0_atomic_inc(&Model0_u->Model0___count);
 return Model0_u;
}
extern void Model0_free_uid(struct Model0_user_struct *);



extern void Model0_xtime_update(unsigned long Model0_ticks);

extern int Model0_wake_up_state(struct Model0_task_struct *Model0_tsk, unsigned int Model0_state);
extern int Model0_wake_up_process(struct Model0_task_struct *Model0_tsk);
extern void Model0_wake_up_new_task(struct Model0_task_struct *Model0_tsk);

 extern void Model0_kick_process(struct Model0_task_struct *Model0_tsk);



extern int Model0_sched_fork(unsigned long Model0_clone_flags, struct Model0_task_struct *Model0_p);
extern void Model0_sched_dead(struct Model0_task_struct *Model0_p);

extern void Model0_proc_caches_init(void);
extern void Model0_flush_signals(struct Model0_task_struct *);
extern void Model0_ignore_signals(struct Model0_task_struct *);
extern void Model0_flush_signal_handlers(struct Model0_task_struct *, int Model0_force_default);
extern int Model0_dequeue_signal(struct Model0_task_struct *Model0_tsk, Model0_sigset_t *Model0_mask, Model0_siginfo_t *Model0_info);

static inline __attribute__((no_instrument_function)) int Model0_kernel_dequeue_signal(Model0_siginfo_t *Model0_info)
{
 struct Model0_task_struct *Model0_tsk = Model0_get_current();
 Model0_siginfo_t Model0___info;
 int Model0_ret;

 Model0_spin_lock_irq(&Model0_tsk->Model0_sighand->Model0_siglock);
 Model0_ret = Model0_dequeue_signal(Model0_tsk, &Model0_tsk->Model0_blocked, Model0_info ?: &Model0___info);
 Model0_spin_unlock_irq(&Model0_tsk->Model0_sighand->Model0_siglock);

 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) void Model0_kernel_signal_stop(void)
{
 Model0_spin_lock_irq(&Model0_get_current()->Model0_sighand->Model0_siglock);
 if (Model0_get_current()->Model0_jobctl & (1UL << 16))
  do { Model0_get_current()->Model0_state = ((128 | 4)); } while (0);
 Model0_spin_unlock_irq(&Model0_get_current()->Model0_sighand->Model0_siglock);

 Model0_schedule();
}

extern void Model0_release_task(struct Model0_task_struct * Model0_p);
extern int Model0_send_sig_info(int, struct Model0_siginfo *, struct Model0_task_struct *);
extern int Model0_force_sigsegv(int, struct Model0_task_struct *);
extern int Model0_force_sig_info(int, struct Model0_siginfo *, struct Model0_task_struct *);
extern int Model0___kill_pgrp_info(int Model0_sig, struct Model0_siginfo *Model0_info, struct Model0_pid *Model0_pgrp);
extern int Model0_kill_pid_info(int Model0_sig, struct Model0_siginfo *Model0_info, struct Model0_pid *Model0_pid);
extern int Model0_kill_pid_info_as_cred(int, struct Model0_siginfo *, struct Model0_pid *,
    const struct Model0_cred *, Model0_u32);
extern int Model0_kill_pgrp(struct Model0_pid *Model0_pid, int Model0_sig, int Model0_priv);
extern int Model0_kill_pid(struct Model0_pid *Model0_pid, int Model0_sig, int Model0_priv);
extern int Model0_kill_proc_info(int, struct Model0_siginfo *, Model0_pid_t);
extern __attribute__((warn_unused_result)) bool Model0_do_notify_parent(struct Model0_task_struct *, int);
extern void Model0___wake_up_parent(struct Model0_task_struct *Model0_p, struct Model0_task_struct *Model0_parent);
extern void Model0_force_sig(int, struct Model0_task_struct *);
extern int Model0_send_sig(int, struct Model0_task_struct *, int);
extern int Model0_zap_other_threads(struct Model0_task_struct *Model0_p);
extern struct Model0_sigqueue *Model0_sigqueue_alloc(void);
extern void Model0_sigqueue_free(struct Model0_sigqueue *);
extern int Model0_send_sigqueue(struct Model0_sigqueue *, struct Model0_task_struct *, int Model0_group);
extern int Model0_do_sigaction(int, struct Model0_k_sigaction *, struct Model0_k_sigaction *);
/* Higher-quality implementation, used if TIF_RESTORE_SIGMASK doesn't exist. */
static inline __attribute__((no_instrument_function)) void Model0_set_restore_sigmask(void)
{
 Model0_get_current()->Model0_restore_sigmask = true;
 ({ int Model0___ret_warn_on = !!(!Model0_test_ti_thread_flag(Model0_current_thread_info(), 2)); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/sched.h", 2727); __builtin_expect(!!(Model0___ret_warn_on), 0); });
}
static inline __attribute__((no_instrument_function)) void Model0_clear_restore_sigmask(void)
{
 Model0_get_current()->Model0_restore_sigmask = false;
}
static inline __attribute__((no_instrument_function)) bool Model0_test_restore_sigmask(void)
{
 return Model0_get_current()->Model0_restore_sigmask;
}
static inline __attribute__((no_instrument_function)) bool Model0_test_and_clear_restore_sigmask(void)
{
 if (!Model0_get_current()->Model0_restore_sigmask)
  return false;
 Model0_get_current()->Model0_restore_sigmask = false;
 return true;
}


static inline __attribute__((no_instrument_function)) void Model0_restore_saved_sigmask(void)
{
 if (Model0_test_and_clear_restore_sigmask())
  Model0___set_current_blocked(&Model0_get_current()->Model0_saved_sigmask);
}

static inline __attribute__((no_instrument_function)) Model0_sigset_t *Model0_sigmask_to_save(void)
{
 Model0_sigset_t *Model0_res = &Model0_get_current()->Model0_blocked;
 if (__builtin_expect(!!(Model0_test_restore_sigmask()), 0))
  Model0_res = &Model0_get_current()->Model0_saved_sigmask;
 return Model0_res;
}

static inline __attribute__((no_instrument_function)) int Model0_kill_cad_pid(int Model0_sig, int Model0_priv)
{
 return Model0_kill_pid(Model0_cad_pid, Model0_sig, Model0_priv);
}

/* These can be the second arg to send_sig_info/send_group_sig_info.  */




/*
 * True if we are on the alternate signal stack.
 */
static inline __attribute__((no_instrument_function)) int Model0_on_sig_stack(unsigned long Model0_sp)
{
 /*
	 * If the signal stack is SS_AUTODISARM then, by construction, we
	 * can't be on the signal stack unless user code deliberately set
	 * SS_AUTODISARM when we were already on it.
	 *
	 * This improves reliability: if user state gets corrupted such that
	 * the stack pointer points very close to the end of the signal stack,
	 * then this check will enable the signal to be handled anyway.
	 */
 if (Model0_get_current()->Model0_sas_ss_flags & (1U << 31))
  return 0;





 return Model0_sp > Model0_get_current()->Model0_sas_ss_sp &&
  Model0_sp - Model0_get_current()->Model0_sas_ss_sp <= Model0_get_current()->Model0_sas_ss_size;

}

static inline __attribute__((no_instrument_function)) int Model0_sas_ss_flags(unsigned long Model0_sp)
{
 if (!Model0_get_current()->Model0_sas_ss_size)
  return 2;

 return Model0_on_sig_stack(Model0_sp) ? 1 : 0;
}

static inline __attribute__((no_instrument_function)) void Model0_sas_ss_reset(struct Model0_task_struct *Model0_p)
{
 Model0_p->Model0_sas_ss_sp = 0;
 Model0_p->Model0_sas_ss_size = 0;
 Model0_p->Model0_sas_ss_flags = 2;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_sigsp(unsigned long Model0_sp, struct Model0_ksignal *Model0_ksig)
{
 if (__builtin_expect(!!((Model0_ksig->Model0_ka.Model0_sa.Model0_sa_flags & 0x08000000u)), 0) && ! Model0_sas_ss_flags(Model0_sp))



  return Model0_get_current()->Model0_sas_ss_sp + Model0_get_current()->Model0_sas_ss_size;

 return Model0_sp;
}

/*
 * Routines for handling mm_structs
 */
extern struct Model0_mm_struct * Model0_mm_alloc(void);

/* mmdrop drops the mm and the page tables */
extern void Model0___mmdrop(struct Model0_mm_struct *);
static inline __attribute__((no_instrument_function)) void Model0_mmdrop(struct Model0_mm_struct *Model0_mm)
{
 if (__builtin_expect(!!(Model0_atomic_dec_and_test(&Model0_mm->Model0_mm_count)), 0))
  Model0___mmdrop(Model0_mm);
}

static inline __attribute__((no_instrument_function)) bool Model0_mmget_not_zero(struct Model0_mm_struct *Model0_mm)
{
 return Model0_atomic_add_unless((&Model0_mm->Model0_mm_users), 1, 0);
}

/* mmput gets rid of the mappings and all user-space */
extern void Model0_mmput(struct Model0_mm_struct *);

/* same as above but performs the slow path from the async context. Can
 * be called from the atomic context as well
 */
extern void Model0_mmput_async(struct Model0_mm_struct *);


/* Grab a reference to a task's mm, if it is not already going away */
extern struct Model0_mm_struct *Model0_get_task_mm(struct Model0_task_struct *Model0_task);
/*
 * Grab a reference to a task's mm, if it is not already going away
 * and ptrace_may_access with the mode parameter passed to it
 * succeeds.
 */
extern struct Model0_mm_struct *Model0_mm_access(struct Model0_task_struct *Model0_task, unsigned int Model0_mode);
/* Remove the current tasks stale references to the old mm_struct */
extern void Model0_mm_release(struct Model0_task_struct *, struct Model0_mm_struct *);


extern int Model0_copy_thread_tls(unsigned long, unsigned long, unsigned long,
   struct Model0_task_struct *, unsigned long);
extern void Model0_flush_thread(void);


extern void Model0_exit_thread(struct Model0_task_struct *Model0_tsk);






extern void Model0_exit_files(struct Model0_task_struct *);
extern void Model0___cleanup_sighand(struct Model0_sighand_struct *);

extern void Model0_exit_itimers(struct Model0_signal_struct *);
extern void Model0_flush_itimer_signals(void);

extern void Model0_do_group_exit(int);

extern int Model0_do_execve(struct Model0_filename *,
       const char * const *,
       const char * const *);
extern int Model0_do_execveat(int, struct Model0_filename *,
         const char * const *,
         const char * const *,
         int);
extern long Model0__do_fork(unsigned long, unsigned long, unsigned long, int *, int *, unsigned long);
extern long Model0_do_fork(unsigned long, unsigned long, unsigned long, int *, int *);
struct Model0_task_struct *Model0_fork_idle(int);
extern Model0_pid_t Model0_kernel_thread(int (*Model0_fn)(void *), void *Model0_arg, unsigned long Model0_flags);

extern void Model0___set_task_comm(struct Model0_task_struct *Model0_tsk, const char *Model0_from, bool Model0_exec);
static inline __attribute__((no_instrument_function)) void Model0_set_task_comm(struct Model0_task_struct *Model0_tsk, const char *Model0_from)
{
 Model0___set_task_comm(Model0_tsk, Model0_from, false);
}
extern char *Model0_get_task_comm(char *Model0_to, struct Model0_task_struct *Model0_tsk);


void Model0_scheduler_ipi(void);
extern unsigned long Model0_wait_task_inactive(struct Model0_task_struct *, long Model0_match_state);
extern bool Model0_current_is_single_threaded(void);

/*
 * Careful: do_each_thread/while_each_thread is a double loop so
 *          'break' will not work as expected - use goto instead.
 */
/* Careful: this is a double loop, 'break' won't work as expected. */



static inline __attribute__((no_instrument_function)) int Model0_get_nr_threads(struct Model0_task_struct *Model0_tsk)
{
 return Model0_tsk->Model0_signal->Model0_nr_threads;
}

static inline __attribute__((no_instrument_function)) bool Model0_thread_group_leader(struct Model0_task_struct *Model0_p)
{
 return Model0_p->Model0_exit_signal >= 0;
}

/* Do to the insanities of de_thread it is possible for a process
 * to have the pid of the thread group leader without actually being
 * the thread group leader.  For iteration through the pids in proc
 * all we care about is that we have a task with the appropriate
 * pid, we don't actually care if we have the right task.
 */
static inline __attribute__((no_instrument_function)) bool Model0_has_group_leader_pid(struct Model0_task_struct *Model0_p)
{
 return Model0_task_pid(Model0_p) == Model0_p->Model0_signal->Model0_leader_pid;
}

static inline __attribute__((no_instrument_function))
bool Model0_same_thread_group(struct Model0_task_struct *Model0_p1, struct Model0_task_struct *Model0_p2)
{
 return Model0_p1->Model0_signal == Model0_p2->Model0_signal;
}

static inline __attribute__((no_instrument_function)) struct Model0_task_struct *Model0_next_thread(const struct Model0_task_struct *Model0_p)
{
 return ({ const typeof( ((struct Model0_task_struct *)0)->Model0_thread_group ) *Model0___mptr = (({ typeof(Model0_p->Model0_thread_group.Model0_next) Model0__________p1 = ({ union { typeof(Model0_p->Model0_thread_group.Model0_next) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_p->Model0_thread_group.Model0_next), Model0___u.Model0___c, sizeof(Model0_p->Model0_thread_group.Model0_next)); else Model0___read_once_size_nocheck(&(Model0_p->Model0_thread_group.Model0_next), Model0___u.Model0___c, sizeof(Model0_p->Model0_thread_group.Model0_next)); Model0___u.Model0___val; }); typeof(*(Model0_p->Model0_thread_group.Model0_next)) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); })); (struct Model0_task_struct *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_task_struct, Model0_thread_group) );});

}

static inline __attribute__((no_instrument_function)) int Model0_thread_group_empty(struct Model0_task_struct *Model0_p)
{
 return Model0_list_empty(&Model0_p->Model0_thread_group);
}




/*
 * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
 * subscriptions and synchronises with wait4().  Also used in procfs.  Also
 * pins the final release of task.io_context.  Also protects ->cpuset and
 * ->cgroup.subsys[]. And ->vfork_done.
 *
 * Nests both inside and outside of read_lock(&tasklist_lock).
 * It must not be nested with write_lock_irq(&tasklist_lock),
 * neither inside nor outside.
 */
static inline __attribute__((no_instrument_function)) void Model0_task_lock(struct Model0_task_struct *Model0_p)
{
 Model0_spin_lock(&Model0_p->Model0_alloc_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_task_unlock(struct Model0_task_struct *Model0_p)
{
 Model0_spin_unlock(&Model0_p->Model0_alloc_lock);
}

extern struct Model0_sighand_struct *Model0___lock_task_sighand(struct Model0_task_struct *Model0_tsk,
       unsigned long *Model0_flags);

static inline __attribute__((no_instrument_function)) struct Model0_sighand_struct *Model0_lock_task_sighand(struct Model0_task_struct *Model0_tsk,
             unsigned long *Model0_flags)
{
 struct Model0_sighand_struct *Model0_ret;

 Model0_ret = Model0___lock_task_sighand(Model0_tsk, Model0_flags);
 (void)(Model0_ret);
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) void Model0_unlock_task_sighand(struct Model0_task_struct *Model0_tsk,
      unsigned long *Model0_flags)
{
 Model0_spin_unlock_irqrestore(&Model0_tsk->Model0_sighand->Model0_siglock, *Model0_flags);
}

/**
 * threadgroup_change_begin - mark the beginning of changes to a threadgroup
 * @tsk: task causing the changes
 *
 * All operations which modify a threadgroup - a new thread joining the
 * group, death of a member thread (the assertion of PF_EXITING) and
 * exec(2) dethreading the process and replacing the leader - are wrapped
 * by threadgroup_change_{begin|end}().  This is to provide a place which
 * subsystems needing threadgroup stability can hook into for
 * synchronization.
 */
static inline __attribute__((no_instrument_function)) void Model0_threadgroup_change_begin(struct Model0_task_struct *Model0_tsk)
{
 do { Model0__cond_resched(); } while (0);
 Model0_cgroup_threadgroup_change_begin(Model0_tsk);
}

/**
 * threadgroup_change_end - mark the end of changes to a threadgroup
 * @tsk: task causing the changes
 *
 * See threadgroup_change_begin().
 */
static inline __attribute__((no_instrument_function)) void Model0_threadgroup_change_end(struct Model0_task_struct *Model0_tsk)
{
 Model0_cgroup_threadgroup_change_end(Model0_tsk);
}






static inline __attribute__((no_instrument_function)) void Model0_setup_thread_stack(struct Model0_task_struct *Model0_p, struct Model0_task_struct *Model0_org)
{
 *((struct Model0_thread_info *)(Model0_p)->Model0_stack) = *((struct Model0_thread_info *)(Model0_org)->Model0_stack);
 ((struct Model0_thread_info *)(Model0_p)->Model0_stack)->Model0_task = Model0_p;
}

/*
 * Return the address of the last usable long on the stack.
 *
 * When the stack grows down, this is just above the thread
 * info struct. Going any lower will corrupt the threadinfo.
 *
 * When the stack grows up, this is the highest address.
 * Beyond that position, we corrupt data on the next page.
 */
static inline __attribute__((no_instrument_function)) unsigned long *Model0_end_of_stack(struct Model0_task_struct *Model0_p)
{



 return (unsigned long *)(((struct Model0_thread_info *)(Model0_p)->Model0_stack) + 1);

}





static inline __attribute__((no_instrument_function)) int Model0_object_is_on_stack(void *Model0_obj)
{
 void *Model0_stack = ((Model0_get_current())->Model0_stack);

 return (Model0_obj >= Model0_stack) && (Model0_obj < (Model0_stack + (((1UL) << 12) << (2 + 0))));
}

extern void Model0_thread_stack_cache_init(void);


static inline __attribute__((no_instrument_function)) unsigned long Model0_stack_not_used(struct Model0_task_struct *Model0_p)
{
 unsigned long *Model0_n = Model0_end_of_stack(Model0_p);

 do { /* Skip over canary */



  Model0_n++;

 } while (!*Model0_n);




 return (unsigned long)Model0_n - (unsigned long)Model0_end_of_stack(Model0_p);

}

extern void Model0_set_task_stack_end_magic(struct Model0_task_struct *Model0_tsk);

/* set thread flags in other task's structures
 * - see asm/thread_info.h for TIF_xxxx flags available
 */
static inline __attribute__((no_instrument_function)) void Model0_set_tsk_thread_flag(struct Model0_task_struct *Model0_tsk, int Model0_flag)
{
 Model0_set_ti_thread_flag(((struct Model0_thread_info *)(Model0_tsk)->Model0_stack), Model0_flag);
}

static inline __attribute__((no_instrument_function)) void Model0_clear_tsk_thread_flag(struct Model0_task_struct *Model0_tsk, int Model0_flag)
{
 Model0_clear_ti_thread_flag(((struct Model0_thread_info *)(Model0_tsk)->Model0_stack), Model0_flag);
}

static inline __attribute__((no_instrument_function)) int Model0_test_and_set_tsk_thread_flag(struct Model0_task_struct *Model0_tsk, int Model0_flag)
{
 return Model0_test_and_set_ti_thread_flag(((struct Model0_thread_info *)(Model0_tsk)->Model0_stack), Model0_flag);
}

static inline __attribute__((no_instrument_function)) int Model0_test_and_clear_tsk_thread_flag(struct Model0_task_struct *Model0_tsk, int Model0_flag)
{
 return Model0_test_and_clear_ti_thread_flag(((struct Model0_thread_info *)(Model0_tsk)->Model0_stack), Model0_flag);
}

static inline __attribute__((no_instrument_function)) int Model0_test_tsk_thread_flag(struct Model0_task_struct *Model0_tsk, int Model0_flag)
{
 return Model0_test_ti_thread_flag(((struct Model0_thread_info *)(Model0_tsk)->Model0_stack), Model0_flag);
}

static inline __attribute__((no_instrument_function)) void Model0_set_tsk_need_resched(struct Model0_task_struct *Model0_tsk)
{
 Model0_set_tsk_thread_flag(Model0_tsk,3);
}

static inline __attribute__((no_instrument_function)) void Model0_clear_tsk_need_resched(struct Model0_task_struct *Model0_tsk)
{
 Model0_clear_tsk_thread_flag(Model0_tsk,3);
}

static inline __attribute__((no_instrument_function)) int Model0_test_tsk_need_resched(struct Model0_task_struct *Model0_tsk)
{
 return __builtin_expect(!!(Model0_test_tsk_thread_flag(Model0_tsk,3)), 0);
}

static inline __attribute__((no_instrument_function)) int Model0_restart_syscall(void)
{
 Model0_set_tsk_thread_flag(Model0_get_current(), 2);
 return -513;
}

static inline __attribute__((no_instrument_function)) int Model0_signal_pending(struct Model0_task_struct *Model0_p)
{
 return __builtin_expect(!!(Model0_test_tsk_thread_flag(Model0_p,2)), 0);
}

static inline __attribute__((no_instrument_function)) int Model0___fatal_signal_pending(struct Model0_task_struct *Model0_p)
{
 return __builtin_expect(!!(Model0_sigismember(&Model0_p->Model0_pending.Model0_signal, 9)), 0);
}

static inline __attribute__((no_instrument_function)) int Model0_fatal_signal_pending(struct Model0_task_struct *Model0_p)
{
 return Model0_signal_pending(Model0_p) && Model0___fatal_signal_pending(Model0_p);
}

static inline __attribute__((no_instrument_function)) int Model0_signal_pending_state(long Model0_state, struct Model0_task_struct *Model0_p)
{
 if (!(Model0_state & (1 | 128)))
  return 0;
 if (!Model0_signal_pending(Model0_p))
  return 0;

 return (Model0_state & 1) || Model0___fatal_signal_pending(Model0_p);
}

/*
 * cond_resched() and cond_resched_lock(): latency reduction via
 * explicit rescheduling in places that are safe. The return
 * value indicates whether a reschedule was done in fact.
 * cond_resched_lock() will drop the spinlock before scheduling,
 * cond_resched_softirq() will enable bhs before scheduling.
 */
extern int Model0__cond_resched(void);






extern int Model0___cond_resched_lock(Model0_spinlock_t *Model0_lock);






extern int Model0___cond_resched_softirq(void);






static inline __attribute__((no_instrument_function)) void Model0_cond_resched_rcu(void)
{

 Model0_rcu_read_unlock();
 ({ Model0____might_sleep("./include/linux/sched.h", 3234, 0); Model0__cond_resched(); });
 Model0_rcu_read_lock();

}

/*
 * Does a critical section need to be broken due to another
 * task waiting?: (technically does not depend on CONFIG_PREEMPT,
 * but a general need for low latency)
 */
static inline __attribute__((no_instrument_function)) int Model0_spin_needbreak(Model0_spinlock_t *Model0_lock)
{



 return 0;

}

/*
 * Idle thread specific functions to determine the need_resched
 * polling state.
 */

static inline __attribute__((no_instrument_function)) int Model0_tsk_is_polling(struct Model0_task_struct *Model0_p)
{
 return Model0_test_tsk_thread_flag(Model0_p, 21);
}

static inline __attribute__((no_instrument_function)) void Model0___current_set_polling(void)
{
 Model0_set_ti_thread_flag(Model0_current_thread_info(), 21);
}

static inline __attribute__((no_instrument_function)) bool __attribute__((warn_unused_result)) Model0_current_set_polling_and_test(void)
{
 Model0___current_set_polling();

 /*
	 * Polling state must be visible before we test NEED_RESCHED,
	 * paired by resched_curr()
	 */
 __asm__ __volatile__("": : :"memory");

 return __builtin_expect(!!(Model0_test_ti_thread_flag(Model0_current_thread_info(), 3)), 0);
}

static inline __attribute__((no_instrument_function)) void Model0___current_clr_polling(void)
{
 Model0_clear_ti_thread_flag(Model0_current_thread_info(), 21);
}

static inline __attribute__((no_instrument_function)) bool __attribute__((warn_unused_result)) Model0_current_clr_polling_and_test(void)
{
 Model0___current_clr_polling();

 /*
	 * Polling state must be visible before we test NEED_RESCHED,
	 * paired by resched_curr()
	 */
 __asm__ __volatile__("": : :"memory");

 return __builtin_expect(!!(Model0_test_ti_thread_flag(Model0_current_thread_info(), 3)), 0);
}
static inline __attribute__((no_instrument_function)) void Model0_current_clr_polling(void)
{
 Model0___current_clr_polling();

 /*
	 * Ensure we check TIF_NEED_RESCHED after we clear the polling bit.
	 * Once the bit is cleared, we'll get IPIs with every new
	 * TIF_NEED_RESCHED and the IPI handler, scheduler_ipi(), will also
	 * fold.
	 */
 asm volatile("mfence":::"memory"); /* paired with resched_curr() */

 do { if (Model0_test_ti_thread_flag(Model0_current_thread_info(), 3)) Model0_set_preempt_need_resched(); } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model0_need_resched(void)
{
 return __builtin_expect(!!(Model0_test_ti_thread_flag(Model0_current_thread_info(), 3)), 0);
}

/*
 * Thread group CPU time accounting.
 */
void Model0_thread_group_cputime(struct Model0_task_struct *Model0_tsk, struct Model0_task_cputime *Model0_times);
void Model0_thread_group_cputimer(struct Model0_task_struct *Model0_tsk, struct Model0_task_cputime *Model0_times);

/*
 * Reevaluate whether the task has signals pending delivery.
 * Wake the task if so.
 * This is required every time the blocked sigset_t changes.
 * callers must hold sighand->siglock.
 */
extern void Model0_recalc_sigpending_and_wake(struct Model0_task_struct *Model0_t);
extern void Model0_recalc_sigpending(void);

extern void Model0_signal_wake_up_state(struct Model0_task_struct *Model0_t, unsigned int Model0_state);

static inline __attribute__((no_instrument_function)) void Model0_signal_wake_up(struct Model0_task_struct *Model0_t, bool Model0_resume)
{
 Model0_signal_wake_up_state(Model0_t, Model0_resume ? 128 : 0);
}
static inline __attribute__((no_instrument_function)) void Model0_ptrace_signal_wake_up(struct Model0_task_struct *Model0_t, bool Model0_resume)
{
 Model0_signal_wake_up_state(Model0_t, Model0_resume ? 8 : 0);
}

/*
 * Wrappers for p->thread_info->cpu access. No-op on UP.
 */


static inline __attribute__((no_instrument_function)) unsigned int Model0_task_cpu(const struct Model0_task_struct *Model0_p)
{
 return ((struct Model0_thread_info *)(Model0_p)->Model0_stack)->Model0_cpu;
}

static inline __attribute__((no_instrument_function)) int Model0_task_node(const struct Model0_task_struct *Model0_p)
{
 return Model0_cpu_to_node(Model0_task_cpu(Model0_p));
}

extern void Model0_set_task_cpu(struct Model0_task_struct *Model0_p, unsigned int Model0_cpu);
extern long Model0_sched_setaffinity(Model0_pid_t Model0_pid, const struct Model0_cpumask *Model0_new_mask);
extern long Model0_sched_getaffinity(Model0_pid_t Model0_pid, struct Model0_cpumask *Model0_mask);


extern struct Model0_task_group Model0_root_task_group;


extern int Model0_task_can_switch_user(struct Model0_user_struct *Model0_up,
     struct Model0_task_struct *Model0_tsk);


static inline __attribute__((no_instrument_function)) void Model0_add_rchar(struct Model0_task_struct *Model0_tsk, Model0_ssize_t Model0_amt)
{
 Model0_tsk->Model0_ioac.Model0_rchar += Model0_amt;
}

static inline __attribute__((no_instrument_function)) void Model0_add_wchar(struct Model0_task_struct *Model0_tsk, Model0_ssize_t Model0_amt)
{
 Model0_tsk->Model0_ioac.Model0_wchar += Model0_amt;
}

static inline __attribute__((no_instrument_function)) void Model0_inc_syscr(struct Model0_task_struct *Model0_tsk)
{
 Model0_tsk->Model0_ioac.Model0_syscr++;
}

static inline __attribute__((no_instrument_function)) void Model0_inc_syscw(struct Model0_task_struct *Model0_tsk)
{
 Model0_tsk->Model0_ioac.Model0_syscw++;
}
static inline __attribute__((no_instrument_function)) void Model0_mm_update_next_owner(struct Model0_mm_struct *Model0_mm)
{
}


static inline __attribute__((no_instrument_function)) unsigned long Model0_task_rlimit(const struct Model0_task_struct *Model0_tsk,
  unsigned int Model0_limit)
{
 return ({ union { typeof(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_cur) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_cur), Model0___u.Model0___c, sizeof(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_cur)); else Model0___read_once_size_nocheck(&(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_cur), Model0___u.Model0___c, sizeof(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_cur)); Model0___u.Model0___val; });
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_task_rlimit_max(const struct Model0_task_struct *Model0_tsk,
  unsigned int Model0_limit)
{
 return ({ union { typeof(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_max) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_max), Model0___u.Model0___c, sizeof(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_max)); else Model0___read_once_size_nocheck(&(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_max), Model0___u.Model0___c, sizeof(Model0_tsk->Model0_signal->Model0_rlim[Model0_limit].Model0_rlim_max)); Model0___u.Model0___val; });
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_rlimit(unsigned int Model0_limit)
{
 return Model0_task_rlimit(Model0_get_current(), Model0_limit);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_rlimit_max(unsigned int Model0_limit)
{
 return Model0_task_rlimit_max(Model0_get_current(), Model0_limit);
}


struct Model0_update_util_data {
 void (*func)(struct Model0_update_util_data *Model0_data,
       Model0_u64 Model0_time, unsigned long Model0_util, unsigned long Model0_max);
};

void Model0_cpufreq_add_update_util_hook(int Model0_cpu, struct Model0_update_util_data *Model0_data,
   void (*func)(struct Model0_update_util_data *Model0_data, Model0_u64 Model0_time,
         unsigned long Model0_util, unsigned long Model0_max));
void Model0_cpufreq_remove_update_util_hook(int Model0_cpu);


struct Model0_kmem_cache;
struct Model0_page;
struct Model0_vm_struct;
static inline __attribute__((no_instrument_function)) void Model0_kasan_unpoison_shadow(const void *Model0_address, Model0_size_t Model0_size) {}

static inline __attribute__((no_instrument_function)) void Model0_kasan_unpoison_task_stack(struct Model0_task_struct *Model0_task) {}

static inline __attribute__((no_instrument_function)) void Model0_kasan_enable_current(void) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_disable_current(void) {}

static inline __attribute__((no_instrument_function)) void Model0_kasan_alloc_pages(struct Model0_page *Model0_page, unsigned int Model0_order) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_free_pages(struct Model0_page *Model0_page, unsigned int Model0_order) {}

static inline __attribute__((no_instrument_function)) void Model0_kasan_cache_create(struct Model0_kmem_cache *Model0_cache,
          Model0_size_t *Model0_size,
          unsigned long *Model0_flags) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_cache_shrink(struct Model0_kmem_cache *Model0_cache) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_cache_destroy(struct Model0_kmem_cache *Model0_cache) {}

static inline __attribute__((no_instrument_function)) void Model0_kasan_poison_slab(struct Model0_page *Model0_page) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_unpoison_object_data(struct Model0_kmem_cache *Model0_cache,
     void *Model0_object) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_poison_object_data(struct Model0_kmem_cache *Model0_cache,
     void *Model0_object) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_init_slab_obj(struct Model0_kmem_cache *Model0_cache,
    const void *Model0_object) {}

static inline __attribute__((no_instrument_function)) void Model0_kasan_kmalloc_large(void *Model0_ptr, Model0_size_t Model0_size, Model0_gfp_t Model0_flags) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_kfree_large(const void *Model0_ptr) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_poison_kfree(void *Model0_ptr) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_kmalloc(struct Model0_kmem_cache *Model0_s, const void *Model0_object,
    Model0_size_t Model0_size, Model0_gfp_t Model0_flags) {}
static inline __attribute__((no_instrument_function)) void Model0_kasan_krealloc(const void *Model0_object, Model0_size_t Model0_new_size,
     Model0_gfp_t Model0_flags) {}

static inline __attribute__((no_instrument_function)) void Model0_kasan_slab_alloc(struct Model0_kmem_cache *Model0_s, void *Model0_object,
       Model0_gfp_t Model0_flags) {}
static inline __attribute__((no_instrument_function)) bool Model0_kasan_slab_free(struct Model0_kmem_cache *Model0_s, void *Model0_object)
{
 return false;
}

static inline __attribute__((no_instrument_function)) int Model0_kasan_module_alloc(void *Model0_addr, Model0_size_t Model0_size) { return 0; }
static inline __attribute__((no_instrument_function)) void Model0_kasan_free_shadow(const struct Model0_vm_struct *Model0_vm) {}

static inline __attribute__((no_instrument_function)) void Model0_kasan_unpoison_slab(const void *Model0_ptr) { }
static inline __attribute__((no_instrument_function)) Model0_size_t Model0_kasan_metadata_size(struct Model0_kmem_cache *Model0_cache) { return 0; }

struct Model0_mem_cgroup;
/*
 * struct kmem_cache related prototypes
 */
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_kmem_cache_init(void);
bool Model0_slab_is_available(void);

struct Model0_kmem_cache *Model0_kmem_cache_create(const char *, Model0_size_t, Model0_size_t,
   unsigned long,
   void (*)(void *));
void Model0_kmem_cache_destroy(struct Model0_kmem_cache *);
int Model0_kmem_cache_shrink(struct Model0_kmem_cache *);

void Model0_memcg_create_kmem_cache(struct Model0_mem_cgroup *, struct Model0_kmem_cache *);
void Model0_memcg_deactivate_kmem_caches(struct Model0_mem_cgroup *);
void Model0_memcg_destroy_kmem_caches(struct Model0_mem_cgroup *);

/*
 * Please use this macro to create slab caches. Simply specify the
 * name of the structure and maybe some flags that are listed above.
 *
 * The alignment of the struct determines object alignment. If you
 * f.e. add ____cacheline_aligned_in_smp to the struct declaration
 * then the objects will be properly aligned in SMP configurations.
 */




/*
 * Common kmalloc functions provided by all allocators
 */
void * __attribute__((warn_unused_result)) Model0___krealloc(const void *, Model0_size_t, Model0_gfp_t);
void * __attribute__((warn_unused_result)) Model0_krealloc(const void *, Model0_size_t, Model0_gfp_t);
void Model0_kfree(const void *);
void Model0_kzfree(const void *);
Model0_size_t Model0_ksize(const void *);


const char *Model0___check_heap_object(const void *Model0_ptr, unsigned long Model0_n,
    struct Model0_page *Model0_page);
/*
 * Some archs want to perform DMA into kmalloc caches and need a guaranteed
 * alignment larger than the alignment of a 64-bit integer.
 * Setting ARCH_KMALLOC_MINALIGN in arch headers allows that.
 */
/*
 * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
 * Intended for arches that get misalignment faults even for 64 bit integer
 * aligned buffers.
 */




/*
 * kmalloc and friends return ARCH_KMALLOC_MINALIGN aligned
 * pointers. kmem_cache_alloc and friends return ARCH_SLAB_MINALIGN
 * aligned pointers.
 */




/*
 * Kmalloc array related definitions
 */
/*
 * SLUB directly allocates requests fitting in to an order-1 page
 * (PAGE_SIZE*2).  Larger requests are passed to the page allocator.
 */
/* Maximum allocatable size */

/* Maximum size for which we actually use a slab cache */

/* Maximum order allocatable via the slab allocagtor */


/*
 * Kmalloc subsystem.
 */




/*
 * This restriction comes from byte sized index implementation.
 * Page size is normally 2^12 bytes and, in this case, if we want to use
 * byte sized index which can represent 2^8 entries, the size of the object
 * should be equal or greater to 2^12 / 2^8 = 2^4 = 16.
 * If minimum size of kmalloc is less than 16, we use it as minimum object
 * size and give up to use byte sized index.
 */




extern struct Model0_kmem_cache *Model0_kmalloc_caches[(12 + 1) + 1];

extern struct Model0_kmem_cache *Model0_kmalloc_dma_caches[(12 + 1) + 1];


/*
 * Figure out which kmalloc slab an allocation of a certain size
 * belongs to.
 * 0 = zero alloc
 * 1 =  65 .. 96 bytes
 * 2 = 129 .. 192 bytes
 * n = 2^(n-1)+1 .. 2^n
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_kmalloc_index(Model0_size_t Model0_size)
{
 if (!Model0_size)
  return 0;

 if (Model0_size <= (1 << 3))
  return 3;

 if ((1 << 3) <= 32 && Model0_size > 64 && Model0_size <= 96)
  return 1;
 if ((1 << 3) <= 64 && Model0_size > 128 && Model0_size <= 192)
  return 2;
 if (Model0_size <= 8) return 3;
 if (Model0_size <= 16) return 4;
 if (Model0_size <= 32) return 5;
 if (Model0_size <= 64) return 6;
 if (Model0_size <= 128) return 7;
 if (Model0_size <= 256) return 8;
 if (Model0_size <= 512) return 9;
 if (Model0_size <= 1024) return 10;
 if (Model0_size <= 2 * 1024) return 11;
 if (Model0_size <= 4 * 1024) return 12;
 if (Model0_size <= 8 * 1024) return 13;
 if (Model0_size <= 16 * 1024) return 14;
 if (Model0_size <= 32 * 1024) return 15;
 if (Model0_size <= 64 * 1024) return 16;
 if (Model0_size <= 128 * 1024) return 17;
 if (Model0_size <= 256 * 1024) return 18;
 if (Model0_size <= 512 * 1024) return 19;
 if (Model0_size <= 1024 * 1024) return 20;
 if (Model0_size <= 2 * 1024 * 1024) return 21;
 if (Model0_size <= 4 * 1024 * 1024) return 22;
 if (Model0_size <= 8 * 1024 * 1024) return 23;
 if (Model0_size <= 16 * 1024 * 1024) return 24;
 if (Model0_size <= 32 * 1024 * 1024) return 25;
 if (Model0_size <= 64 * 1024 * 1024) return 26;
 do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/slab.h"), "i" (323), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0);

 /* Will never be reached. Needed because the compiler may complain */
 return -1;
}


void *Model0___kmalloc(Model0_size_t Model0_size, Model0_gfp_t Model0_flags) __attribute__((__malloc__));
void *Model0_kmem_cache_alloc(struct Model0_kmem_cache *, Model0_gfp_t Model0_flags) __attribute__((__malloc__));
void Model0_kmem_cache_free(struct Model0_kmem_cache *, void *);

/*
 * Bulk allocation and freeing operations. These are accelerated in an
 * allocator specific way to avoid taking locks repeatedly or building
 * metadata structures unnecessarily.
 *
 * Note that interrupts must be enabled when calling these functions.
 */
void Model0_kmem_cache_free_bulk(struct Model0_kmem_cache *, Model0_size_t, void **);
int Model0_kmem_cache_alloc_bulk(struct Model0_kmem_cache *, Model0_gfp_t, Model0_size_t, void **);

/*
 * Caller must not use kfree_bulk() on memory not originally allocated
 * by kmalloc(), because the SLOB allocator cannot handle this.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_kfree_bulk(Model0_size_t Model0_size, void **Model0_p)
{
 Model0_kmem_cache_free_bulk(((void *)0), Model0_size, Model0_p);
}


void *Model0___kmalloc_node(Model0_size_t Model0_size, Model0_gfp_t Model0_flags, int Model0_node) __attribute__((__malloc__));
void *Model0_kmem_cache_alloc_node(struct Model0_kmem_cache *, Model0_gfp_t Model0_flags, int Model0_node) __attribute__((__malloc__));
extern void *Model0_kmem_cache_alloc_trace(struct Model0_kmem_cache *, Model0_gfp_t, Model0_size_t) __attribute__((__malloc__));


extern void *Model0_kmem_cache_alloc_node_trace(struct Model0_kmem_cache *Model0_s,
        Model0_gfp_t Model0_gfpflags,
        int Model0_node, Model0_size_t Model0_size) __attribute__((__malloc__));
extern void *Model0_kmalloc_order(Model0_size_t Model0_size, Model0_gfp_t Model0_flags, unsigned int Model0_order) __attribute__((__malloc__));


extern void *Model0_kmalloc_order_trace(Model0_size_t Model0_size, Model0_gfp_t Model0_flags, unsigned int Model0_order) __attribute__((__malloc__));
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model0_kmalloc_large(Model0_size_t Model0_size, Model0_gfp_t Model0_flags)
{
 unsigned int Model0_order = ( __builtin_constant_p(Model0_size) ? ( ((Model0_size) == 0UL) ? 64 - 12 : (((Model0_size) < (1UL << 12)) ? 0 : ( __builtin_constant_p((Model0_size) - 1) ? ( ((Model0_size) - 1) < 1 ? Model0_____ilog2_NaN() : ((Model0_size) - 1) & (1ULL << 63) ? 63 : ((Model0_size) - 1) & (1ULL << 62) ? 62 : ((Model0_size) - 1) & (1ULL << 61) ? 61 : ((Model0_size) - 1) & (1ULL << 60) ? 60 : ((Model0_size) - 1) & (1ULL << 59) ? 59 : ((Model0_size) - 1) & (1ULL << 58) ? 58 : ((Model0_size) - 1) & (1ULL << 57) ? 57 : ((Model0_size) - 1) & (1ULL << 56) ? 56 : ((Model0_size) - 1) & (1ULL << 55) ? 55 : ((Model0_size) - 1) & (1ULL << 54) ? 54 : ((Model0_size) - 1) & (1ULL << 53) ? 53 : ((Model0_size) - 1) & (1ULL << 52) ? 52 : ((Model0_size) - 1) & (1ULL << 51) ? 51 : ((Model0_size) - 1) & (1ULL << 50) ? 50 : ((Model0_size) - 1) & (1ULL << 49) ? 49 : ((Model0_size) - 1) & (1ULL << 48) ? 48 : ((Model0_size) - 1) & (1ULL << 47) ? 47 : ((Model0_size) - 1) & (1ULL << 46) ? 46 : ((Model0_size) - 1) & (1ULL << 45) ? 45 : ((Model0_size) - 1) & (1ULL << 44) ? 44 : ((Model0_size) - 1) & (1ULL << 43) ? 43 : ((Model0_size) - 1) & (1ULL << 42) ? 42 : ((Model0_size) - 1) & (1ULL << 41) ? 41 : ((Model0_size) - 1) & (1ULL << 40) ? 40 : ((Model0_size) - 1) & (1ULL << 39) ? 39 : ((Model0_size) - 1) & (1ULL << 38) ? 38 : ((Model0_size) - 1) & (1ULL << 37) ? 37 : ((Model0_size) - 1) & (1ULL << 36) ? 36 : ((Model0_size) - 1) & (1ULL << 35) ? 35 : ((Model0_size) - 1) & (1ULL << 34) ? 34 : ((Model0_size) - 1) & (1ULL << 33) ? 33 : ((Model0_size) - 1) & (1ULL << 32) ? 32 : ((Model0_size) - 1) & (1ULL << 31) ? 31 : ((Model0_size) - 1) & (1ULL << 30) ? 30 : ((Model0_size) - 1) & (1ULL << 29) ? 29 : ((Model0_size) - 1) & (1ULL << 28) ? 28 : ((Model0_size) - 1) & (1ULL << 27) ? 27 : ((Model0_size) - 1) & (1ULL << 26) ? 26 : ((Model0_size) - 1) & (1ULL << 25) ? 25 : ((Model0_size) - 1) & (1ULL << 24) ? 24 : ((Model0_size) - 1) & (1ULL << 23) ? 23 : ((Model0_size) - 1) & (1ULL << 22) ? 22 : ((Model0_size) - 1) & (1ULL << 21) ? 21 : ((Model0_size) - 1) & (1ULL << 20) ? 20 : ((Model0_size) - 1) & (1ULL << 19) ? 19 : ((Model0_size) - 1) & (1ULL << 18) ? 18 : ((Model0_size) - 1) & (1ULL << 17) ? 17 : ((Model0_size) - 1) & (1ULL << 16) ? 16 : ((Model0_size) - 1) & (1ULL << 15) ? 15 : ((Model0_size) - 1) & (1ULL << 14) ? 14 : ((Model0_size) - 1) & (1ULL << 13) ? 13 : ((Model0_size) - 1) & (1ULL << 12) ? 12 : ((Model0_size) - 1) & (1ULL << 11) ? 11 : ((Model0_size) - 1) & (1ULL << 10) ? 10 : ((Model0_size) - 1) & (1ULL << 9) ? 9 : ((Model0_size) - 1) & (1ULL << 8) ? 8 : ((Model0_size) - 1) & (1ULL << 7) ? 7 : ((Model0_size) - 1) & (1ULL << 6) ? 6 : ((Model0_size) - 1) & (1ULL << 5) ? 5 : ((Model0_size) - 1) & (1ULL << 4) ? 4 : ((Model0_size) - 1) & (1ULL << 3) ? 3 : ((Model0_size) - 1) & (1ULL << 2) ? 2 : ((Model0_size) - 1) & (1ULL << 1) ? 1 : ((Model0_size) - 1) & (1ULL << 0) ? 0 : Model0_____ilog2_NaN() ) : (sizeof((Model0_size) - 1) <= 4) ? Model0___ilog2_u32((Model0_size) - 1) : Model0___ilog2_u64((Model0_size) - 1) ) - 12 + 1) ) : Model0___get_order(Model0_size) );
 return Model0_kmalloc_order_trace(Model0_size, Model0_flags, Model0_order);
}

/**
 * kmalloc - allocate memory
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate.
 *
 * kmalloc is the normal method of allocating memory
 * for objects smaller than page size in the kernel.
 *
 * The @flags argument may be one of:
 *
 * %GFP_USER - Allocate memory on behalf of user.  May sleep.
 *
 * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
 *
 * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
 *   For example, use this inside interrupt handlers.
 *
 * %GFP_HIGHUSER - Allocate pages from high memory.
 *
 * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
 *
 * %GFP_NOFS - Do not make any fs calls while trying to get memory.
 *
 * %GFP_NOWAIT - Allocation will not sleep.
 *
 * %__GFP_THISNODE - Allocate node-local memory only.
 *
 * %GFP_DMA - Allocation suitable for DMA.
 *   Should only be used for kmalloc() caches. Otherwise, use a
 *   slab created with SLAB_DMA.
 *
 * Also it is possible to set different flags by OR'ing
 * in one or more of the following additional @flags:
 *
 * %__GFP_COLD - Request cache-cold pages instead of
 *   trying to return cache-warm pages.
 *
 * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
 *
 * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
 *   (think twice before using).
 *
 * %__GFP_NORETRY - If memory is not immediately available,
 *   then give up at once.
 *
 * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
 *
 * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
 *
 * There are other flags available as well, but these are not intended
 * for general use, and so are not documented here. For a full list of
 * potential flags, always refer to linux/gfp.h.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model0_kmalloc(Model0_size_t Model0_size, Model0_gfp_t Model0_flags)
{
 if (__builtin_constant_p(Model0_size)) {
  if (Model0_size > (1UL << (12 + 1)))
   return Model0_kmalloc_large(Model0_size, Model0_flags);

  if (!(Model0_flags & (( Model0_gfp_t)0x01u))) {
   int Model0_index = Model0_kmalloc_index(Model0_size);

   if (!Model0_index)
    return ((void *)16);

   return Model0_kmem_cache_alloc_trace(Model0_kmalloc_caches[Model0_index],
     Model0_flags, Model0_size);
  }

 }
 return Model0___kmalloc(Model0_size, Model0_flags);
}

/*
 * Determine size used for the nth kmalloc cache.
 * return size or 0 if a kmalloc cache for that
 * size does not exist
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model0_kmalloc_size(int Model0_n)
{

 if (Model0_n > 2)
  return 1 << Model0_n;

 if (Model0_n == 1 && (1 << 3) <= 32)
  return 96;

 if (Model0_n == 2 && (1 << 3) <= 64)
  return 192;

 return 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model0_kmalloc_node(Model0_size_t Model0_size, Model0_gfp_t Model0_flags, int Model0_node)
{

 if (__builtin_constant_p(Model0_size) &&
  Model0_size <= (1UL << (12 + 1)) && !(Model0_flags & (( Model0_gfp_t)0x01u))) {
  int Model0_i = Model0_kmalloc_index(Model0_size);

  if (!Model0_i)
   return ((void *)16);

  return Model0_kmem_cache_alloc_node_trace(Model0_kmalloc_caches[Model0_i],
      Model0_flags, Model0_node, Model0_size);
 }

 return Model0___kmalloc_node(Model0_size, Model0_flags, Model0_node);
}

struct Model0_memcg_cache_array {
 struct Model0_callback_head Model0_rcu;
 struct Model0_kmem_cache *Model0_entries[0];
};

/*
 * This is the main placeholder for memcg-related information in kmem caches.
 * Both the root cache and the child caches will have it. For the root cache,
 * this will hold a dynamically allocated array large enough to hold
 * information about the currently limited memcgs in the system. To allow the
 * array to be accessed without taking any locks, on relocation we free the old
 * version only after a grace period.
 *
 * Child caches will hold extra metadata needed for its operation. Fields are:
 *
 * @memcg: pointer to the memcg this cache belongs to
 * @root_cache: pointer to the global, root cache, this cache was derived from
 *
 * Both root and child caches of the same kind are linked into a list chained
 * through @list.
 */
struct Model0_memcg_cache_params {
 bool Model0_is_root_cache;
 struct Model0_list_head Model0_list;
 union {
  struct Model0_memcg_cache_array *Model0_memcg_caches;
  struct {
   struct Model0_mem_cgroup *Model0_memcg;
   struct Model0_kmem_cache *Model0_root_cache;
  };
 };
};

int Model0_memcg_update_all_caches(int Model0_num_memcgs);

/**
 * kmalloc_array - allocate memory for an array.
 * @n: number of elements.
 * @size: element size.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((no_instrument_function)) void *Model0_kmalloc_array(Model0_size_t Model0_n, Model0_size_t Model0_size, Model0_gfp_t Model0_flags)
{
 if (Model0_size != 0 && Model0_n > (~(Model0_size_t)0) / Model0_size)
  return ((void *)0);
 if (__builtin_constant_p(Model0_n) && __builtin_constant_p(Model0_size))
  return Model0_kmalloc(Model0_n * Model0_size, Model0_flags);
 return Model0___kmalloc(Model0_n * Model0_size, Model0_flags);
}

/**
 * kcalloc - allocate memory for an array. The memory is set to zero.
 * @n: number of elements.
 * @size: element size.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((no_instrument_function)) void *Model0_kcalloc(Model0_size_t Model0_n, Model0_size_t Model0_size, Model0_gfp_t Model0_flags)
{
 return Model0_kmalloc_array(Model0_n, Model0_size, Model0_flags | (( Model0_gfp_t)0x8000u));
}

/*
 * kmalloc_track_caller is a special version of kmalloc that records the
 * calling function of the routine calling it for slab leak tracking instead
 * of just the calling function (confusing, eh?).
 * It's useful when the call to kmalloc comes from a widely-used standard
 * allocator where we care about the real place the memory allocation
 * request comes from.
 */
extern void *Model0___kmalloc_track_caller(Model0_size_t, Model0_gfp_t, unsigned long);




extern void *Model0___kmalloc_node_track_caller(Model0_size_t, Model0_gfp_t, int, unsigned long);
/*
 * Shortcuts
 */
static inline __attribute__((no_instrument_function)) void *Model0_kmem_cache_zalloc(struct Model0_kmem_cache *Model0_k, Model0_gfp_t Model0_flags)
{
 return Model0_kmem_cache_alloc(Model0_k, Model0_flags | (( Model0_gfp_t)0x8000u));
}

/**
 * kzalloc - allocate memory. The memory is set to zero.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((no_instrument_function)) void *Model0_kzalloc(Model0_size_t Model0_size, Model0_gfp_t Model0_flags)
{
 return Model0_kmalloc(Model0_size, Model0_flags | (( Model0_gfp_t)0x8000u));
}

/**
 * kzalloc_node - allocate zeroed memory from a particular memory node.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 * @node: memory node from which to allocate
 */
static inline __attribute__((no_instrument_function)) void *Model0_kzalloc_node(Model0_size_t Model0_size, Model0_gfp_t Model0_flags, int Model0_node)
{
 return Model0_kmalloc_node(Model0_size, Model0_flags | (( Model0_gfp_t)0x8000u), Model0_node);
}

unsigned int Model0_kmem_cache_size(struct Model0_kmem_cache *Model0_s);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_kmem_cache_init_late(void);


/*
 * Dynamic loading of modules into the kernel.
 *
 * Rewritten by Richard Henderson <rth@tamu.edu> Dec 1996
 * Rewritten again by Rusty Russell, 2002
 */









struct Model0_stat {
 Model0___kernel_ulong_t Model0_st_dev;
 Model0___kernel_ulong_t Model0_st_ino;
 Model0___kernel_ulong_t Model0_st_nlink;

 unsigned int Model0_st_mode;
 unsigned int Model0_st_uid;
 unsigned int Model0_st_gid;
 unsigned int Model0___pad0;
 Model0___kernel_ulong_t Model0_st_rdev;
 Model0___kernel_long_t Model0_st_size;
 Model0___kernel_long_t Model0_st_blksize;
 Model0___kernel_long_t Model0_st_blocks; /* Number 512-byte blocks allocated. */

 Model0___kernel_ulong_t Model0_st_atime;
 Model0___kernel_ulong_t Model0_st_atime_nsec;
 Model0___kernel_ulong_t Model0_st_mtime;
 Model0___kernel_ulong_t Model0_st_mtime_nsec;
 Model0___kernel_ulong_t Model0_st_ctime;
 Model0___kernel_ulong_t Model0_st_ctime_nsec;
 Model0___kernel_long_t Model0___unused[3];
};

/* We don't need to memset the whole thing just to initialize the padding */
/* for 32bit emulation and 32 bit kernels */
struct Model0___old_kernel_stat {
 unsigned short Model0_st_dev;
 unsigned short Model0_st_ino;
 unsigned short Model0_st_mode;
 unsigned short Model0_st_nlink;
 unsigned short Model0_st_uid;
 unsigned short Model0_st_gid;
 unsigned short Model0_st_rdev;






 unsigned int Model0_st_size;
 unsigned int Model0_st_atime;
 unsigned int Model0_st_mtime;
 unsigned int Model0_st_ctime;

};
struct Model0_kstat {
 Model0_u64 Model0_ino;
 Model0_dev_t Model0_dev;
 Model0_umode_t Model0_mode;
 unsigned int Model0_nlink;
 Model0_kuid_t Model0_uid;
 Model0_kgid_t Model0_gid;
 Model0_dev_t Model0_rdev;
 Model0_loff_t Model0_size;
 struct Model0_timespec Model0_atime;
 struct Model0_timespec Model0_mtime;
 struct Model0_timespec Model0_ctime;
 unsigned long Model0_blksize;
 unsigned long long Model0_blocks;
};





/*
 *	include/linux/kmod.h
 *
 *      This program is free software; you can redistribute it and/or modify
 *      it under the terms of the GNU General Public License as published by
 *      the Free Software Foundation; either version 2 of the License, or
 *      (at your option) any later version.
 *
 *      This program is distributed in the hope that it will be useful,
 *      but WITHOUT ANY WARRANTY; without even the implied warranty of
 *      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *      GNU General Public License for more details.
 *
 *      You should have received a copy of the GNU General Public License
 *      along with this program; if not, write to the Free Software
 *      Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */
extern char Model0_modprobe_path[]; /* for sysctl */
/* modprobe exit status on success, -ve on error.  Return value
 * usually useless though. */
extern __attribute__((format(printf, 2, 3)))
int Model0___request_module(bool Model0_wait, const char *Model0_name, ...);
struct Model0_cred;
struct Model0_file;






struct Model0_subprocess_info {
 struct Model0_work_struct Model0_work;
 struct Model0_completion *Model0_complete;
 char *Model0_path;
 char **Model0_argv;
 char **Model0_envp;
 int Model0_wait;
 int Model0_retval;
 int (*Model0_init)(struct Model0_subprocess_info *Model0_info, struct Model0_cred *Model0_new);
 void (*Model0_cleanup)(struct Model0_subprocess_info *Model0_info);
 void *Model0_data;
};

extern int
Model0_call_usermodehelper(char *Model0_path, char **Model0_argv, char **Model0_envp, int Model0_wait);

extern struct Model0_subprocess_info *
Model0_call_usermodehelper_setup(char *Model0_path, char **Model0_argv, char **Model0_envp, Model0_gfp_t Model0_gfp_mask,
     int (*Model0_init)(struct Model0_subprocess_info *Model0_info, struct Model0_cred *Model0_new),
     void (*Model0_cleanup)(struct Model0_subprocess_info *), void *Model0_data);

extern int
Model0_call_usermodehelper_exec(struct Model0_subprocess_info *Model0_info, int Model0_wait);

extern struct Model0_ctl_table Model0_usermodehelper_table[];

enum Model0_umh_disable_depth {
 Model0_UMH_ENABLED = 0,
 Model0_UMH_FREEZING,
 Model0_UMH_DISABLED,
};

extern int Model0___usermodehelper_disable(enum Model0_umh_disable_depth Model0_depth);
extern void Model0___usermodehelper_set_disable_depth(enum Model0_umh_disable_depth Model0_depth);

static inline __attribute__((no_instrument_function)) int Model0_usermodehelper_disable(void)
{
 return Model0___usermodehelper_disable(Model0_UMH_DISABLED);
}

static inline __attribute__((no_instrument_function)) void Model0_usermodehelper_enable(void)
{
 Model0___usermodehelper_set_disable_depth(Model0_UMH_ENABLED);
}

extern int Model0_usermodehelper_read_trylock(void);
extern long Model0_usermodehelper_read_lock_wait(long Model0_timeout);
extern void Model0_usermodehelper_read_unlock(void);







/*
 * ELF register definitions..
 */















/* Core file format: The core file is written in such a way that gdb
   can understand it and provide useful information to the user.
   There are quite a number of obstacles to being able to view the
   contents of the floating point registers, and until these are
   solved you will not be able to view the contents of them.
   Actually, you can read in the core file and look at the contents of
   the user struct to find out what the floating point registers
   contain.

   The actual file contents are as follows:
   UPAGE: 1 page consisting of a user struct that tells gdb what is present
   in the file.  Directly after this is a copy of the task_struct, which
   is currently not used by gdb, but it may come in useful at some point.
   All of the registers are stored as part of the upage.  The upage should
   always be only one page.
   DATA: The data area is stored.  We use current->end_text to
   current->brk to pick up all of the user variables, plus any memory
   that may have been malloced.  No attempt is made to determine if a page
   is demand-zero or if a page is totally unused, we just cover the entire
   range.  All of the addresses are rounded in such a way that an integral
   number of pages is written.
   STACK: We need the stack information in order to get a meaningful
   backtrace.  We need to write the data from (esp) to
   current->start_stack, so we round each of these off in order to be able
   to write an integer number of pages.
   The minimum core file size is 3 pages, or 12288 bytes.  */

/*
 * Pentium III FXSR, SSE support
 *	Gareth Hughes <gareth@valinux.com>, May 2000
 *
 * Provide support for the GDB 5.0+ PTRACE_{GET|SET}FPXREGS requests for
 * interacting with the FXSR-format floating point environment.  Floating
 * point data can be accessed in the regular format in the usual manner,
 * and both the standard and SIMD floating point data can be accessed via
 * the new ptrace requests.  In either case, changes to the FPU environment
 * will be reflected in the task's state as expected.
 *
 * x86-64 support by Andi Kleen.
 */

/* This matches the 64bit FXSAVE format as defined by AMD. It is the same
   as the 32bit format defined by Intel, except that the selector:offset pairs
   for data and eip are replaced with flat 64bit pointers. */
struct Model0_user_i387_struct {
 unsigned short Model0_cwd;
 unsigned short Model0_swd;
 unsigned short Model0_twd; /* Note this is not the same as
				   the 32bit/x87/FSAVE twd */
 unsigned short Model0_fop;
 __u64 Model0_rip;
 __u64 Model0_rdp;
 __u32 Model0_mxcsr;
 __u32 Model0_mxcsr_mask;
 __u32 Model0_st_space[32]; /* 8*16 bytes for each FP-reg = 128 bytes */
 __u32 Model0_xmm_space[64]; /* 16*16 bytes for each XMM-reg = 256 bytes */
 __u32 Model0_padding[24];
};

/*
 * Segment register layout in coredumps.
 */
struct Model0_user_regs_struct {
 unsigned long Model0_r15;
 unsigned long Model0_r14;
 unsigned long Model0_r13;
 unsigned long Model0_r12;
 unsigned long Model0_bp;
 unsigned long Model0_bx;
 unsigned long Model0_r11;
 unsigned long Model0_r10;
 unsigned long Model0_r9;
 unsigned long Model0_r8;
 unsigned long Model0_ax;
 unsigned long Model0_cx;
 unsigned long Model0_dx;
 unsigned long Model0_si;
 unsigned long Model0_di;
 unsigned long Model0_orig_ax;
 unsigned long Model0_ip;
 unsigned long Model0_cs;
 unsigned long Model0_flags;
 unsigned long Model0_sp;
 unsigned long Model0_ss;
 unsigned long Model0_fs_base;
 unsigned long Model0_gs_base;
 unsigned long Model0_ds;
 unsigned long Model0_es;
 unsigned long Model0_fs;
 unsigned long Model0_gs;
};

/* When the kernel dumps core, it starts by dumping the user struct -
   this will be used by gdb to figure out where the data and stack segments
   are within the file, and what virtual addresses to use. */

struct Model0_user {
/* We start with the registers, to mimic the way that "memory" is returned
   from the ptrace(3,...) function.  */
  struct Model0_user_regs_struct Model0_regs; /* Where the registers are actually stored */
/* ptrace does not yet supply these.  Someday.... */
  int Model0_u_fpvalid; /* True if math co-processor being used. */
    /* for this mess. Not yet used. */
  int Model0_pad0;
  struct Model0_user_i387_struct Model0_i387; /* Math Co-processor registers. */
/* The rest of this junk is to help gdb figure out what goes where */
  unsigned long int Model0_u_tsize; /* Text segment size (pages). */
  unsigned long int Model0_u_dsize; /* Data segment size (pages). */
  unsigned long int Model0_u_ssize; /* Stack segment size (pages). */
  unsigned long Model0_start_code; /* Starting virtual address of text. */
  unsigned long Model0_start_stack; /* Starting virtual address of stack area.
				   This is actually the bottom of the stack,
				   the top of the stack is always found in the
				   esp register.  */
  long int Model0_signal; /* Signal that caused the core dump. */
  int Model0_reserved; /* No longer used */
  int Model0_pad1;
  unsigned long Model0_u_ar0; /* Used by gdb to help find the values for */
    /* the registers. */
  struct Model0_user_i387_struct *Model0_u_fpstate; /* Math Co-processor pointer. */
  unsigned long Model0_magic; /* To uniquely identify a core file */
  char Model0_u_comm[32]; /* User command that was responsible */
  unsigned long Model0_u_debugreg[8];
  unsigned long Model0_error_code; /* CPU error code or 0 */
  unsigned long Model0_fault_address; /* CR3 or 0 */
};




struct Model0_user_ymmh_regs {
 /* 16 * 16 bytes for each YMMH-reg */
 __u32 Model0_ymmh_space[64];
};

struct Model0_user_xstate_header {
 __u64 Model0_xfeatures;
 __u64 Model0_reserved1[2];
 __u64 Model0_reserved2[5];
};

/*
 * The structure layout of user_xstateregs, used for exporting the
 * extended register state through ptrace and core-dump (NT_X86_XSTATE note)
 * interfaces will be same as the memory layout of xsave used by the processor
 * (except for the bytes 464..511, which can be used by the software) and hence
 * the size of this structure varies depending on the features supported by the
 * processor and OS. The size of the structure that users need to use can be
 * obtained by doing:
 *     cpuid_count(0xd, 0, &eax, &ptrace_xstateregs_struct_size, &ecx, &edx);
 * i.e., cpuid.(eax=0xd,ecx=0).ebx will be the size that user (debuggers, etc.)
 * need to use.
 *
 * For now, only the first 8 bytes of the software usable bytes[464..471] will
 * be used and will be set to OS enabled xstate mask (which is same as the
 * 64bit mask returned by the xgetbv's xCR0).  Users (analyzing core dump
 * remotely, etc.) can use this mask as well as the mask saved in the
 * xstate_hdr bytes and interpret what states the processor/OS supports
 * and what states are in modified/initialized conditions for the
 * particular process/thread.
 *
 * Also when the user modifies certain state FP/SSE/etc through the
 * ptrace interface, they must ensure that the header.xfeatures
 * bytes[512..519] of the memory layout are updated correspondingly.
 * i.e., for example when FP state is modified to a non-init state,
 * header.xfeatures's bit 0 must be set to '1', when SSE is modified to
 * non-init state, header.xfeatures's bit 1 must to be set to '1', etc.
 */



struct Model0_user_xstateregs {
 struct {
  __u64 Model0_fpx_space[58];
  __u64 Model0_xstate_fx_sw[6];
 } Model0_i387;
 struct Model0_user_xstate_header Model0_header;
 struct Model0_user_ymmh_regs Model0_ymmh;
 /* further processor state extensions go here */
};


typedef unsigned long Model0_elf_greg_t;


typedef Model0_elf_greg_t Model0_elf_gregset_t[(sizeof(struct Model0_user_regs_struct) / sizeof(Model0_elf_greg_t))];

typedef struct Model0_user_i387_struct Model0_elf_fpregset_t;
/* x86-64 relocation types */
/*
 * These are used to set parameters in the core dumps.
 */







struct Model0_vdso_image {
 void *Model0_data;
 unsigned long Model0_size; /* Always a multiple of PAGE_SIZE */

 unsigned long Model0_alt, Model0_alt_len;

 long Model0_sym_vvar_start; /* Negative offset to the vvar area */

 long Model0_sym_vvar_page;
 long Model0_sym_hpet_page;
 long Model0_sym_pvclock_page;
 long Model0_sym_VDSO32_NOTE_MASK;
 long Model0_sym___kernel_sigreturn;
 long Model0_sym___kernel_rt_sigreturn;
 long Model0_sym___kernel_vsyscall;
 long Model0_sym_int80_landing_pad;
};


extern const struct Model0_vdso_image Model0_vdso_image_64;







extern const struct Model0_vdso_image Model0_vdso_image_32;


extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_init_vdso_image(const struct Model0_vdso_image *Model0_image);


extern unsigned int Model0_vdso64_enabled;


extern unsigned int Model0_vdso32_enabled;


/*
 * This is used to ensure we don't load something for the wrong architecture.
 */
/*
 * This is used to ensure we don't load something for the wrong architecture.
 */
static inline __attribute__((no_instrument_function)) void Model0_elf_common_init(struct Model0_thread_struct *Model0_t,
       struct Model0_pt_regs *Model0_regs, const Model0_u16 Model0_ds)
{
 /* ax gets execve's return value. */
 /*regs->ax = */ Model0_regs->Model0_bx = Model0_regs->Model0_cx = Model0_regs->Model0_dx = 0;
 Model0_regs->Model0_si = Model0_regs->Model0_di = Model0_regs->Model0_bp = 0;
 Model0_regs->Model0_r8 = Model0_regs->Model0_r9 = Model0_regs->Model0_r10 = Model0_regs->Model0_r11 = 0;
 Model0_regs->Model0_r12 = Model0_regs->Model0_r13 = Model0_regs->Model0_r14 = Model0_regs->Model0_r15 = 0;
 Model0_t->Model0_fsbase = Model0_t->Model0_gsbase = 0;
 Model0_t->Model0_fsindex = Model0_t->Model0_gsindex = 0;
 Model0_t->Model0_ds = Model0_t->Model0_es = Model0_ds;
}







void Model0_compat_start_thread(struct Model0_pt_regs *Model0_regs, Model0_u32 Model0_new_ip, Model0_u32 Model0_new_sp);


void Model0_set_personality_ia32(bool);





/*
 * regs is struct pt_regs, pr_reg is elf_gregset_t (which is
 * now struct_user_regs, they are different). Assumes current is the process
 * getting dumped.
 */
/* I'm not sure if we can use '-' here */

extern void Model0_set_personality_64bit(void);
extern unsigned int Model0_sysctl_vsyscall32;
extern int Model0_force_personality32;






/* This is the location that an ET_DYN program is loaded if exec'ed.  Typical
   use of this is to invoke "./ld.so someprog" to test out a new version of
   the loader.  We need to make sure that it is out of the way of the program
   that it will "exec", and that there is sufficient room for the brk.  */



/* This yields a mask that user programs can use to figure out what
   instruction set this CPU supports.  This could be done in user space,
   but it's not easy, and we've already done it here.  */



/* This yields a string that ld.so will use to load implementation
   specific libraries for optimization.  This is more specific in
   intent than poking at uname or /proc/cpuinfo.

   For the moment, we have only optimizations for the Intel generations,
   but that could change... */



/*
 * An executable for which elf_read_implies_exec() returns TRUE will
 * have the READ_IMPLIES_EXEC personality flag set automatically.
 */



struct Model0_task_struct;
/* 1GB for 64bit, 8MB for 32bit */
/* As a historical oddity, the x32 and x86_64 vDSOs are controlled together. */
struct Model0_linux_binprm;


extern int Model0_arch_setup_additional_pages(struct Model0_linux_binprm *Model0_bprm,
           int Model0_uses_interp);
extern int Model0_compat_arch_setup_additional_pages(struct Model0_linux_binprm *Model0_bprm,
           int Model0_uses_interp);


/*
 * True on X86_32 or when emulating IA32 on X86_64
 */
static inline __attribute__((no_instrument_function)) int Model0_mmap_is_ia32(void)
{
 return 0 ||
        (1 &&
  Model0_test_ti_thread_flag(Model0_current_thread_info(), 29));
}

/* Do not change the values. See get_align_mask() */
enum Model0_align_flags {
 Model0_ALIGN_VA_32 = (1UL << (0)),
 Model0_ALIGN_VA_64 = (1UL << (1)),
};

struct Model0_va_alignment {
 int Model0_flags;
 unsigned long Model0_mask;
 unsigned long Model0_bits;
} __attribute__((__aligned__((1 << (6)))));

extern struct Model0_va_alignment Model0_va_align;
extern unsigned long Model0_align_vdso_addr(unsigned long);







/* These constants define the various ELF target machines */
    /* Next two are historical and binaries and
				   modules of these types will be rejected by
				   Linux.  */
/*
 * This is an interim value that we will use until the committee comes
 * up with a final number.
 */


/* Bogus old m32r magic number, used by old tools. */

/* This is the old interim value for S/390 architecture */

/* Also Panasonic/MEI MN10300, AM33 */

/* 32-bit ELF base types. */
typedef __u32 Model0_Elf32_Addr;
typedef Model0___u16 Model0_Elf32_Half;
typedef __u32 Model0_Elf32_Off;
typedef Model0___s32 Model0_Elf32_Sword;
typedef __u32 Model0_Elf32_Word;

/* 64-bit ELF base types. */
typedef __u64 Model0_Elf64_Addr;
typedef Model0___u16 Model0_Elf64_Half;
typedef Model0___s16 Model0_Elf64_SHalf;
typedef __u64 Model0_Elf64_Off;
typedef Model0___s32 Model0_Elf64_Sword;
typedef __u32 Model0_Elf64_Word;
typedef __u64 Model0_Elf64_Xword;
typedef Model0___s64 Model0_Elf64_Sxword;

/* These constants are for the segment types stored in the image headers */
/*
 * Extended Numbering
 *
 * If the real number of program header table entries is larger than
 * or equal to PN_XNUM(0xffff), it is set to sh_info field of the
 * section header at index 0, and PN_XNUM is set to e_phnum
 * field. Otherwise, the section header at index 0 is zero
 * initialized, if it exists.
 *
 * Specifications are available in:
 *
 * - Oracle: Linker and Libraries.
 *   Part No: 817–1984–19, August 2011.
 *   http://docs.oracle.com/cd/E18752_01/pdf/817-1984.pdf
 *
 * - System V ABI AMD64 Architecture Processor Supplement
 *   Draft Version 0.99.4,
 *   January 13, 2010.
 *   http://www.cs.washington.edu/education/courses/cse351/12wi/supp-docs/abi.pdf
 */


/* These constants define the different elf file types */
/* This is the info that is needed to parse the dynamic section of the file */
/* This info is needed when parsing the symbol table */
typedef struct Model0_dynamic{
  Model0_Elf32_Sword Model0_d_tag;
  union{
    Model0_Elf32_Sword Model0_d_val;
    Model0_Elf32_Addr Model0_d_ptr;
  } Model0_d_un;
} Model0_Elf32_Dyn;

typedef struct {
  Model0_Elf64_Sxword Model0_d_tag; /* entry tag value */
  union {
    Model0_Elf64_Xword Model0_d_val;
    Model0_Elf64_Addr Model0_d_ptr;
  } Model0_d_un;
} Model0_Elf64_Dyn;

/* The following are used with relocations */






typedef struct Model0_elf32_rel {
  Model0_Elf32_Addr Model0_r_offset;
  Model0_Elf32_Word Model0_r_info;
} Model0_Elf32_Rel;

typedef struct Model0_elf64_rel {
  Model0_Elf64_Addr Model0_r_offset; /* Location at which to apply the action */
  Model0_Elf64_Xword Model0_r_info; /* index and type of relocation */
} Model0_Elf64_Rel;

typedef struct Model0_elf32_rela{
  Model0_Elf32_Addr Model0_r_offset;
  Model0_Elf32_Word Model0_r_info;
  Model0_Elf32_Sword Model0_r_addend;
} Model0_Elf32_Rela;

typedef struct Model0_elf64_rela {
  Model0_Elf64_Addr Model0_r_offset; /* Location at which to apply the action */
  Model0_Elf64_Xword Model0_r_info; /* index and type of relocation */
  Model0_Elf64_Sxword Model0_r_addend; /* Constant addend used to compute value */
} Model0_Elf64_Rela;

typedef struct Model0_elf32_sym{
  Model0_Elf32_Word Model0_st_name;
  Model0_Elf32_Addr Model0_st_value;
  Model0_Elf32_Word Model0_st_size;
  unsigned char Model0_st_info;
  unsigned char Model0_st_other;
  Model0_Elf32_Half Model0_st_shndx;
} Model0_Elf32_Sym;

typedef struct Model0_elf64_sym {
  Model0_Elf64_Word Model0_st_name; /* Symbol name, index in string tbl */
  unsigned char Model0_st_info; /* Type and binding attributes */
  unsigned char Model0_st_other; /* No defined meaning, 0 */
  Model0_Elf64_Half Model0_st_shndx; /* Associated section index */
  Model0_Elf64_Addr Model0_st_value; /* Value of the symbol */
  Model0_Elf64_Xword Model0_st_size; /* Associated symbol size */
} Model0_Elf64_Sym;




typedef struct Model0_elf32_hdr{
  unsigned char Model0_e_ident[16];
  Model0_Elf32_Half Model0_e_type;
  Model0_Elf32_Half Model0_e_machine;
  Model0_Elf32_Word Model0_e_version;
  Model0_Elf32_Addr Model0_e_entry; /* Entry point */
  Model0_Elf32_Off Model0_e_phoff;
  Model0_Elf32_Off Model0_e_shoff;
  Model0_Elf32_Word Model0_e_flags;
  Model0_Elf32_Half Model0_e_ehsize;
  Model0_Elf32_Half Model0_e_phentsize;
  Model0_Elf32_Half Model0_e_phnum;
  Model0_Elf32_Half Model0_e_shentsize;
  Model0_Elf32_Half Model0_e_shnum;
  Model0_Elf32_Half Model0_e_shstrndx;
} Model0_Elf32_Ehdr;

typedef struct Model0_elf64_hdr {
  unsigned char Model0_e_ident[16]; /* ELF "magic number" */
  Model0_Elf64_Half Model0_e_type;
  Model0_Elf64_Half Model0_e_machine;
  Model0_Elf64_Word Model0_e_version;
  Model0_Elf64_Addr Model0_e_entry; /* Entry point virtual address */
  Model0_Elf64_Off Model0_e_phoff; /* Program header table file offset */
  Model0_Elf64_Off Model0_e_shoff; /* Section header table file offset */
  Model0_Elf64_Word Model0_e_flags;
  Model0_Elf64_Half Model0_e_ehsize;
  Model0_Elf64_Half Model0_e_phentsize;
  Model0_Elf64_Half Model0_e_phnum;
  Model0_Elf64_Half Model0_e_shentsize;
  Model0_Elf64_Half Model0_e_shnum;
  Model0_Elf64_Half Model0_e_shstrndx;
} Model0_Elf64_Ehdr;

/* These constants define the permissions on sections in the program
   header, p_flags. */




typedef struct Model0_elf32_phdr{
  Model0_Elf32_Word Model0_p_type;
  Model0_Elf32_Off Model0_p_offset;
  Model0_Elf32_Addr Model0_p_vaddr;
  Model0_Elf32_Addr Model0_p_paddr;
  Model0_Elf32_Word Model0_p_filesz;
  Model0_Elf32_Word Model0_p_memsz;
  Model0_Elf32_Word Model0_p_flags;
  Model0_Elf32_Word Model0_p_align;
} Model0_Elf32_Phdr;

typedef struct Model0_elf64_phdr {
  Model0_Elf64_Word Model0_p_type;
  Model0_Elf64_Word Model0_p_flags;
  Model0_Elf64_Off Model0_p_offset; /* Segment file offset */
  Model0_Elf64_Addr Model0_p_vaddr; /* Segment virtual address */
  Model0_Elf64_Addr Model0_p_paddr; /* Segment physical address */
  Model0_Elf64_Xword Model0_p_filesz; /* Segment size in file */
  Model0_Elf64_Xword Model0_p_memsz; /* Segment size in memory */
  Model0_Elf64_Xword Model0_p_align; /* Segment alignment, file & memory */
} Model0_Elf64_Phdr;

/* sh_type */
/* sh_flags */







/* special section indexes */
typedef struct Model0_elf32_shdr {
  Model0_Elf32_Word Model0_sh_name;
  Model0_Elf32_Word Model0_sh_type;
  Model0_Elf32_Word Model0_sh_flags;
  Model0_Elf32_Addr Model0_sh_addr;
  Model0_Elf32_Off Model0_sh_offset;
  Model0_Elf32_Word Model0_sh_size;
  Model0_Elf32_Word Model0_sh_link;
  Model0_Elf32_Word Model0_sh_info;
  Model0_Elf32_Word Model0_sh_addralign;
  Model0_Elf32_Word Model0_sh_entsize;
} Model0_Elf32_Shdr;

typedef struct Model0_elf64_shdr {
  Model0_Elf64_Word Model0_sh_name; /* Section name, index in string tbl */
  Model0_Elf64_Word Model0_sh_type; /* Type of section */
  Model0_Elf64_Xword Model0_sh_flags; /* Miscellaneous section attributes */
  Model0_Elf64_Addr Model0_sh_addr; /* Section virtual addr at execution */
  Model0_Elf64_Off Model0_sh_offset; /* Section file offset */
  Model0_Elf64_Xword Model0_sh_size; /* Size of section in bytes */
  Model0_Elf64_Word Model0_sh_link; /* Index of another section */
  Model0_Elf64_Word Model0_sh_info; /* Additional section information */
  Model0_Elf64_Xword Model0_sh_addralign; /* Section alignment */
  Model0_Elf64_Xword Model0_sh_entsize; /* Entry size if section holds table */
} Model0_Elf64_Shdr;
/*
 * Notes used in ET_CORE. Architectures export some of the arch register sets
 * using the corresponding note types via the PTRACE_GETREGSET and
 * PTRACE_SETREGSET requests.
 */





/*
 * Note to userspace developers: size of NT_SIGINFO note may increase
 * in the future to accomodate more fields, don't assume it is fixed!
 */
/* Note header in a PT_NOTE section */
typedef struct Model0_elf32_note {
  Model0_Elf32_Word Model0_n_namesz; /* Name size */
  Model0_Elf32_Word Model0_n_descsz; /* Content size */
  Model0_Elf32_Word Model0_n_type; /* Content type */
} Model0_Elf32_Nhdr;

/* Note header in a PT_NOTE section */
typedef struct Model0_elf64_note {
  Model0_Elf64_Word Model0_n_namesz; /* Name size */
  Model0_Elf64_Word Model0_n_descsz; /* Content size */
  Model0_Elf64_Word Model0_n_type; /* Content type */
} Model0_Elf64_Nhdr;
extern Model0_Elf64_Dyn Model0__DYNAMIC [];
/* Optional callbacks to write extra ELF notes. */
struct Model0_file;
struct Model0_coredump_params;


static inline __attribute__((no_instrument_function)) int Model0_elf_coredump_extra_notes_size(void) { return 0; }
static inline __attribute__((no_instrument_function)) int Model0_elf_coredump_extra_notes_write(struct Model0_coredump_params *Model0_cprm) { return 0; }

/*
 * kobject.h - generic kernel object infrastructure.
 *
 * Copyright (c) 2002-2003 Patrick Mochel
 * Copyright (c) 2002-2003 Open Source Development Labs
 * Copyright (c) 2006-2008 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (c) 2006-2008 Novell Inc.
 *
 * This file is released under the GPLv2.
 *
 * Please read Documentation/kobject.txt before using the kobject
 * interface, ESPECIALLY the parts about reference counts and object
 * destructors.
 */







/*
 * sysfs.h - definitions for the device driver filesystem
 *
 * Copyright (c) 2001,2002 Patrick Mochel
 * Copyright (c) 2004 Silicon Graphics, Inc.
 * Copyright (c) 2007 SUSE Linux Products GmbH
 * Copyright (c) 2007 Tejun Heo <teheo@suse.de>
 *
 * Please see Documentation/filesystems/sysfs.txt for more information.
 */





/*
 * kernfs.h - pseudo filesystem decoupled from vfs locking
 *
 * This file is released under the GPLv2.
 */
struct Model0_file;
struct Model0_dentry;
struct Model0_iattr;
struct Model0_seq_file;
struct Model0_vm_area_struct;
struct Model0_super_block;
struct Model0_file_system_type;

struct Model0_kernfs_open_node;
struct Model0_kernfs_iattrs;

enum Model0_kernfs_node_type {
 Model0_KERNFS_DIR = 0x0001,
 Model0_KERNFS_FILE = 0x0002,
 Model0_KERNFS_LINK = 0x0004,
};




enum Model0_kernfs_node_flag {
 Model0_KERNFS_ACTIVATED = 0x0010,
 Model0_KERNFS_NS = 0x0020,
 Model0_KERNFS_HAS_SEQ_SHOW = 0x0040,
 Model0_KERNFS_HAS_MMAP = 0x0080,
 Model0_KERNFS_LOCKDEP = 0x0100,
 Model0_KERNFS_SUICIDAL = 0x0400,
 Model0_KERNFS_SUICIDED = 0x0800,
 Model0_KERNFS_EMPTY_DIR = 0x1000,
};

/* @flags for kernfs_create_root() */
enum Model0_kernfs_root_flag {
 /*
	 * kernfs_nodes are created in the deactivated state and invisible.
	 * They require explicit kernfs_activate() to become visible.  This
	 * can be used to make related nodes become visible atomically
	 * after all nodes are created successfully.
	 */
 Model0_KERNFS_ROOT_CREATE_DEACTIVATED = 0x0001,

 /*
	 * For regular flies, if the opener has CAP_DAC_OVERRIDE, open(2)
	 * succeeds regardless of the RW permissions.  sysfs had an extra
	 * layer of enforcement where open(2) fails with -EACCES regardless
	 * of CAP_DAC_OVERRIDE if the permission doesn't have the
	 * respective read or write access at all (none of S_IRUGO or
	 * S_IWUGO) or the respective operation isn't implemented.  The
	 * following flag enables that behavior.
	 */
 Model0_KERNFS_ROOT_EXTRA_OPEN_PERM_CHECK = 0x0002,
};

/* type-specific structures for kernfs_node union members */
struct Model0_kernfs_elem_dir {
 unsigned long Model0_subdirs;
 /* children rbtree starts here and goes through kn->rb */
 struct Model0_rb_root Model0_children;

 /*
	 * The kernfs hierarchy this directory belongs to.  This fits
	 * better directly in kernfs_node but is here to save space.
	 */
 struct Model0_kernfs_root *Model0_root;
};

struct Model0_kernfs_elem_symlink {
 struct Model0_kernfs_node *Model0_target_kn;
};

struct Model0_kernfs_elem_attr {
 const struct Model0_kernfs_ops *Model0_ops;
 struct Model0_kernfs_open_node *Model0_open;
 Model0_loff_t Model0_size;
 struct Model0_kernfs_node *Model0_notify_next; /* for kernfs_notify() */
};

/*
 * kernfs_node - the building block of kernfs hierarchy.  Each and every
 * kernfs node is represented by single kernfs_node.  Most fields are
 * private to kernfs and shouldn't be accessed directly by kernfs users.
 *
 * As long as s_count reference is held, the kernfs_node itself is
 * accessible.  Dereferencing elem or any other outer entity requires
 * active reference.
 */
struct Model0_kernfs_node {
 Model0_atomic_t Model0_count;
 Model0_atomic_t Model0_active;



 /*
	 * Use kernfs_get_parent() and kernfs_name/path() instead of
	 * accessing the following two fields directly.  If the node is
	 * never moved to a different parent, it is safe to access the
	 * parent directly.
	 */
 struct Model0_kernfs_node *Model0_parent;
 const char *Model0_name;

 struct Model0_rb_node Model0_rb;

 const void *Model0_ns; /* namespace tag */
 unsigned int Model0_hash; /* ns + name hash */
 union {
  struct Model0_kernfs_elem_dir Model0_dir;
  struct Model0_kernfs_elem_symlink Model0_symlink;
  struct Model0_kernfs_elem_attr Model0_attr;
 };

 void *Model0_priv;

 unsigned short Model0_flags;
 Model0_umode_t Model0_mode;
 unsigned int Model0_ino;
 struct Model0_kernfs_iattrs *Model0_iattr;
};

/*
 * kernfs_syscall_ops may be specified on kernfs_create_root() to support
 * syscalls.  These optional callbacks are invoked on the matching syscalls
 * and can perform any kernfs operations which don't necessarily have to be
 * the exact operation requested.  An active reference is held for each
 * kernfs_node parameter.
 */
struct Model0_kernfs_syscall_ops {
 int (*Model0_remount_fs)(struct Model0_kernfs_root *Model0_root, int *Model0_flags, char *Model0_data);
 int (*Model0_show_options)(struct Model0_seq_file *Model0_sf, struct Model0_kernfs_root *Model0_root);

 int (*Model0_mkdir)(struct Model0_kernfs_node *Model0_parent, const char *Model0_name,
       Model0_umode_t Model0_mode);
 int (*Model0_rmdir)(struct Model0_kernfs_node *Model0_kn);
 int (*Model0_rename)(struct Model0_kernfs_node *Model0_kn, struct Model0_kernfs_node *Model0_new_parent,
        const char *Model0_new_name);
 int (*Model0_show_path)(struct Model0_seq_file *Model0_sf, struct Model0_kernfs_node *Model0_kn,
    struct Model0_kernfs_root *Model0_root);
};

struct Model0_kernfs_root {
 /* published fields */
 struct Model0_kernfs_node *Model0_kn;
 unsigned int Model0_flags; /* KERNFS_ROOT_* flags */

 /* private fields, do not use outside kernfs proper */
 struct Model0_ida Model0_ino_ida;
 struct Model0_kernfs_syscall_ops *Model0_syscall_ops;

 /* list of kernfs_super_info of this root, protected by kernfs_mutex */
 struct Model0_list_head Model0_supers;

 Model0_wait_queue_head_t Model0_deactivate_waitq;
};

struct Model0_kernfs_open_file {
 /* published fields */
 struct Model0_kernfs_node *Model0_kn;
 struct Model0_file *Model0_file;
 void *Model0_priv;

 /* private fields, do not use outside kernfs proper */
 struct Model0_mutex Model0_mutex;
 struct Model0_mutex Model0_prealloc_mutex;
 int Model0_event;
 struct Model0_list_head Model0_list;
 char *Model0_prealloc_buf;

 Model0_size_t Model0_atomic_write_len;
 bool Model0_mmapped;
 const struct Model0_vm_operations_struct *Model0_vm_ops;
};

struct Model0_kernfs_ops {
 /*
	 * Read is handled by either seq_file or raw_read().
	 *
	 * If seq_show() is present, seq_file path is active.  Other seq
	 * operations are optional and if not implemented, the behavior is
	 * equivalent to single_open().  @sf->private points to the
	 * associated kernfs_open_file.
	 *
	 * read() is bounced through kernel buffer and a read larger than
	 * PAGE_SIZE results in partial operation of PAGE_SIZE.
	 */
 int (*Model0_seq_show)(struct Model0_seq_file *Model0_sf, void *Model0_v);

 void *(*Model0_seq_start)(struct Model0_seq_file *Model0_sf, Model0_loff_t *Model0_ppos);
 void *(*Model0_seq_next)(struct Model0_seq_file *Model0_sf, void *Model0_v, Model0_loff_t *Model0_ppos);
 void (*Model0_seq_stop)(struct Model0_seq_file *Model0_sf, void *Model0_v);

 Model0_ssize_t (*Model0_read)(struct Model0_kernfs_open_file *Model0_of, char *Model0_buf, Model0_size_t Model0_bytes,
   Model0_loff_t Model0_off);

 /*
	 * write() is bounced through kernel buffer.  If atomic_write_len
	 * is not set, a write larger than PAGE_SIZE results in partial
	 * operations of PAGE_SIZE chunks.  If atomic_write_len is set,
	 * writes upto the specified size are executed atomically but
	 * larger ones are rejected with -E2BIG.
	 */
 Model0_size_t Model0_atomic_write_len;
 /*
	 * "prealloc" causes a buffer to be allocated at open for
	 * all read/write requests.  As ->seq_show uses seq_read()
	 * which does its own allocation, it is incompatible with
	 * ->prealloc.  Provide ->read and ->write with ->prealloc.
	 */
 bool Model0_prealloc;
 Model0_ssize_t (*Model0_write)(struct Model0_kernfs_open_file *Model0_of, char *Model0_buf, Model0_size_t Model0_bytes,
    Model0_loff_t Model0_off);

 int (*Model0_mmap)(struct Model0_kernfs_open_file *Model0_of, struct Model0_vm_area_struct *Model0_vma);




};



static inline __attribute__((no_instrument_function)) enum Model0_kernfs_node_type Model0_kernfs_type(struct Model0_kernfs_node *Model0_kn)
{
 return Model0_kn->Model0_flags & 0x000f;
}

/**
 * kernfs_enable_ns - enable namespace under a directory
 * @kn: directory of interest, should be empty
 *
 * This is to be called right after @kn is created to enable namespace
 * under it.  All children of @kn must have non-NULL namespace tags and
 * only the ones which match the super_block's tag will be visible.
 */
static inline __attribute__((no_instrument_function)) void Model0_kernfs_enable_ns(struct Model0_kernfs_node *Model0_kn)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(Model0_kernfs_type(Model0_kn) != Model0_KERNFS_DIR); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/kernfs.h", 255); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(!(({ union { typeof((&Model0_kn->Model0_dir.Model0_children)->Model0_rb_node) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((&Model0_kn->Model0_dir.Model0_children)->Model0_rb_node), Model0___u.Model0___c, sizeof((&Model0_kn->Model0_dir.Model0_children)->Model0_rb_node)); else Model0___read_once_size_nocheck(&((&Model0_kn->Model0_dir.Model0_children)->Model0_rb_node), Model0___u.Model0___c, sizeof((&Model0_kn->Model0_dir.Model0_children)->Model0_rb_node)); Model0___u.Model0___val; }) == ((void *)0))); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/kernfs.h", 256); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });
 Model0_kn->Model0_flags |= Model0_KERNFS_NS;
}

/**
 * kernfs_ns_enabled - test whether namespace is enabled
 * @kn: the node to test
 *
 * Test whether namespace filtering is enabled for the children of @ns.
 */
static inline __attribute__((no_instrument_function)) bool Model0_kernfs_ns_enabled(struct Model0_kernfs_node *Model0_kn)
{
 return Model0_kn->Model0_flags & Model0_KERNFS_NS;
}

int Model0_kernfs_name(struct Model0_kernfs_node *Model0_kn, char *Model0_buf, Model0_size_t Model0_buflen);
Model0_size_t Model0_kernfs_path_len(struct Model0_kernfs_node *Model0_kn);
int Model0_kernfs_path_from_node(struct Model0_kernfs_node *Model0_root_kn, struct Model0_kernfs_node *Model0_kn,
     char *Model0_buf, Model0_size_t Model0_buflen);
char *Model0_kernfs_path(struct Model0_kernfs_node *Model0_kn, char *Model0_buf, Model0_size_t Model0_buflen);
void Model0_pr_cont_kernfs_name(struct Model0_kernfs_node *Model0_kn);
void Model0_pr_cont_kernfs_path(struct Model0_kernfs_node *Model0_kn);
struct Model0_kernfs_node *Model0_kernfs_get_parent(struct Model0_kernfs_node *Model0_kn);
struct Model0_kernfs_node *Model0_kernfs_find_and_get_ns(struct Model0_kernfs_node *Model0_parent,
        const char *Model0_name, const void *Model0_ns);
struct Model0_kernfs_node *Model0_kernfs_walk_and_get_ns(struct Model0_kernfs_node *Model0_parent,
        const char *Model0_path, const void *Model0_ns);
void Model0_kernfs_get(struct Model0_kernfs_node *Model0_kn);
void Model0_kernfs_put(struct Model0_kernfs_node *Model0_kn);

struct Model0_kernfs_node *Model0_kernfs_node_from_dentry(struct Model0_dentry *Model0_dentry);
struct Model0_kernfs_root *Model0_kernfs_root_from_sb(struct Model0_super_block *Model0_sb);
struct Model0_inode *Model0_kernfs_get_inode(struct Model0_super_block *Model0_sb, struct Model0_kernfs_node *Model0_kn);

struct Model0_dentry *Model0_kernfs_node_dentry(struct Model0_kernfs_node *Model0_kn,
      struct Model0_super_block *Model0_sb);
struct Model0_kernfs_root *Model0_kernfs_create_root(struct Model0_kernfs_syscall_ops *Model0_scops,
           unsigned int Model0_flags, void *Model0_priv);
void Model0_kernfs_destroy_root(struct Model0_kernfs_root *Model0_root);

struct Model0_kernfs_node *Model0_kernfs_create_dir_ns(struct Model0_kernfs_node *Model0_parent,
      const char *Model0_name, Model0_umode_t Model0_mode,
      void *Model0_priv, const void *Model0_ns);
struct Model0_kernfs_node *Model0_kernfs_create_empty_dir(struct Model0_kernfs_node *Model0_parent,
         const char *Model0_name);
struct Model0_kernfs_node *Model0___kernfs_create_file(struct Model0_kernfs_node *Model0_parent,
      const char *Model0_name,
      Model0_umode_t Model0_mode, Model0_loff_t Model0_size,
      const struct Model0_kernfs_ops *Model0_ops,
      void *Model0_priv, const void *Model0_ns,
      struct Model0_lock_class_key *Model0_key);
struct Model0_kernfs_node *Model0_kernfs_create_link(struct Model0_kernfs_node *Model0_parent,
           const char *Model0_name,
           struct Model0_kernfs_node *Model0_target);
void Model0_kernfs_activate(struct Model0_kernfs_node *Model0_kn);
void Model0_kernfs_remove(struct Model0_kernfs_node *Model0_kn);
void Model0_kernfs_break_active_protection(struct Model0_kernfs_node *Model0_kn);
void Model0_kernfs_unbreak_active_protection(struct Model0_kernfs_node *Model0_kn);
bool Model0_kernfs_remove_self(struct Model0_kernfs_node *Model0_kn);
int Model0_kernfs_remove_by_name_ns(struct Model0_kernfs_node *Model0_parent, const char *Model0_name,
        const void *Model0_ns);
int Model0_kernfs_rename_ns(struct Model0_kernfs_node *Model0_kn, struct Model0_kernfs_node *Model0_new_parent,
       const char *Model0_new_name, const void *Model0_new_ns);
int Model0_kernfs_setattr(struct Model0_kernfs_node *Model0_kn, const struct Model0_iattr *Model0_iattr);
void Model0_kernfs_notify(struct Model0_kernfs_node *Model0_kn);

const void *Model0_kernfs_super_ns(struct Model0_super_block *Model0_sb);
struct Model0_dentry *Model0_kernfs_mount_ns(struct Model0_file_system_type *Model0_fs_type, int Model0_flags,
          struct Model0_kernfs_root *Model0_root, unsigned long Model0_magic,
          bool *Model0_new_sb_created, const void *Model0_ns);
void Model0_kernfs_kill_sb(struct Model0_super_block *Model0_sb);
struct Model0_super_block *Model0_kernfs_pin_sb(struct Model0_kernfs_root *Model0_root, const void *Model0_ns);

void Model0_kernfs_init(void);
static inline __attribute__((no_instrument_function)) struct Model0_kernfs_node *
Model0_kernfs_find_and_get(struct Model0_kernfs_node *Model0_kn, const char *Model0_name)
{
 return Model0_kernfs_find_and_get_ns(Model0_kn, Model0_name, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model0_kernfs_node *
Model0_kernfs_walk_and_get(struct Model0_kernfs_node *Model0_kn, const char *Model0_path)
{
 return Model0_kernfs_walk_and_get_ns(Model0_kn, Model0_path, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model0_kernfs_node *
Model0_kernfs_create_dir(struct Model0_kernfs_node *Model0_parent, const char *Model0_name, Model0_umode_t Model0_mode,
    void *Model0_priv)
{
 return Model0_kernfs_create_dir_ns(Model0_parent, Model0_name, Model0_mode, Model0_priv, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model0_kernfs_node *
Model0_kernfs_create_file_ns(struct Model0_kernfs_node *Model0_parent, const char *Model0_name,
        Model0_umode_t Model0_mode, Model0_loff_t Model0_size, const struct Model0_kernfs_ops *Model0_ops,
        void *Model0_priv, const void *Model0_ns)
{
 struct Model0_lock_class_key *Model0_key = ((void *)0);




 return Model0___kernfs_create_file(Model0_parent, Model0_name, Model0_mode, Model0_size, Model0_ops, Model0_priv, Model0_ns,
        Model0_key);
}

static inline __attribute__((no_instrument_function)) struct Model0_kernfs_node *
Model0_kernfs_create_file(struct Model0_kernfs_node *Model0_parent, const char *Model0_name, Model0_umode_t Model0_mode,
     Model0_loff_t Model0_size, const struct Model0_kernfs_ops *Model0_ops, void *Model0_priv)
{
 return Model0_kernfs_create_file_ns(Model0_parent, Model0_name, Model0_mode, Model0_size, Model0_ops, Model0_priv, ((void *)0));
}

static inline __attribute__((no_instrument_function)) int Model0_kernfs_remove_by_name(struct Model0_kernfs_node *Model0_parent,
     const char *Model0_name)
{
 return Model0_kernfs_remove_by_name_ns(Model0_parent, Model0_name, ((void *)0));
}

static inline __attribute__((no_instrument_function)) int Model0_kernfs_rename(struct Model0_kernfs_node *Model0_kn,
    struct Model0_kernfs_node *Model0_new_parent,
    const char *Model0_new_name)
{
 return Model0_kernfs_rename_ns(Model0_kn, Model0_new_parent, Model0_new_name, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model0_dentry *
Model0_kernfs_mount(struct Model0_file_system_type *Model0_fs_type, int Model0_flags,
  struct Model0_kernfs_root *Model0_root, unsigned long Model0_magic,
  bool *Model0_new_sb_created)
{
 return Model0_kernfs_mount_ns(Model0_fs_type, Model0_flags, Model0_root,
    Model0_magic, Model0_new_sb_created, ((void *)0));
}




/* Kernel object name space definitions
 *
 * Copyright (c) 2002-2003 Patrick Mochel
 * Copyright (c) 2002-2003 Open Source Development Labs
 * Copyright (c) 2006-2008 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (c) 2006-2008 Novell Inc.
 *
 * Split from kobject.h by David Howells (dhowells@redhat.com)
 *
 * This file is released under the GPLv2.
 *
 * Please read Documentation/kobject.txt before using the kobject
 * interface, ESPECIALLY the parts about reference counts and object
 * destructors.
 */




struct Model0_sock;
struct Model0_kobject;

/*
 * Namespace types which are used to tag kobjects and sysfs entries.
 * Network namespace will likely be the first.
 */
enum Model0_kobj_ns_type {
 Model0_KOBJ_NS_TYPE_NONE = 0,
 Model0_KOBJ_NS_TYPE_NET,
 Model0_KOBJ_NS_TYPES
};

/*
 * Callbacks so sysfs can determine namespaces
 *   @grab_current_ns: return a new reference to calling task's namespace
 *   @netlink_ns: return namespace to which a sock belongs (right?)
 *   @initial_ns: return the initial namespace (i.e. init_net_ns)
 *   @drop_ns: drops a reference to namespace
 */
struct Model0_kobj_ns_type_operations {
 enum Model0_kobj_ns_type Model0_type;
 bool (*Model0_current_may_mount)(void);
 void *(*Model0_grab_current_ns)(void);
 const void *(*Model0_netlink_ns)(struct Model0_sock *Model0_sk);
 const void *(*Model0_initial_ns)(void);
 void (*Model0_drop_ns)(void *);
};

int Model0_kobj_ns_type_register(const struct Model0_kobj_ns_type_operations *Model0_ops);
int Model0_kobj_ns_type_registered(enum Model0_kobj_ns_type Model0_type);
const struct Model0_kobj_ns_type_operations *Model0_kobj_child_ns_ops(struct Model0_kobject *Model0_parent);
const struct Model0_kobj_ns_type_operations *Model0_kobj_ns_ops(struct Model0_kobject *Model0_kobj);

bool Model0_kobj_ns_current_may_mount(enum Model0_kobj_ns_type Model0_type);
void *Model0_kobj_ns_grab_current(enum Model0_kobj_ns_type Model0_type);
const void *Model0_kobj_ns_netlink(enum Model0_kobj_ns_type Model0_type, struct Model0_sock *Model0_sk);
const void *Model0_kobj_ns_initial(enum Model0_kobj_ns_type Model0_type);
void Model0_kobj_ns_drop(enum Model0_kobj_ns_type Model0_type, void *Model0_ns);



struct Model0_kobject;
struct Model0_module;
struct Model0_bin_attribute;
enum Model0_kobj_ns_type;

struct Model0_attribute {
 const char *Model0_name;
 Model0_umode_t Model0_mode;





};

/**
 *	sysfs_attr_init - initialize a dynamically allocated sysfs attribute
 *	@attr: struct attribute to initialize
 *
 *	Initialize a dynamically allocated struct attribute so we can
 *	make lockdep happy.  This is a new requirement for attributes
 *	and initially this is only needed when lockdep is enabled.
 *	Lockdep gives a nice error when your attribute is added to
 *	sysfs if you don't have this.
 */
/**
 * struct attribute_group - data structure used to declare an attribute group.
 * @name:	Optional: Attribute group name
 *		If specified, the attribute group will be created in
 *		a new subdirectory with this name.
 * @is_visible:	Optional: Function to return permissions associated with an
 *		attribute of the group. Will be called repeatedly for each
 *		non-binary attribute in the group. Only read/write
 *		permissions as well as SYSFS_PREALLOC are accepted. Must
 *		return 0 if an attribute is not visible. The returned value
 *		will replace static permissions defined in struct attribute.
 * @is_bin_visible:
 *		Optional: Function to return permissions associated with a
 *		binary attribute of the group. Will be called repeatedly
 *		for each binary attribute in the group. Only read/write
 *		permissions as well as SYSFS_PREALLOC are accepted. Must
 *		return 0 if a binary attribute is not visible. The returned
 *		value will replace static permissions defined in
 *		struct bin_attribute.
 * @attrs:	Pointer to NULL terminated list of attributes.
 * @bin_attrs:	Pointer to NULL terminated list of binary attributes.
 *		Either attrs or bin_attrs or both must be provided.
 */
struct Model0_attribute_group {
 const char *Model0_name;
 Model0_umode_t (*Model0_is_visible)(struct Model0_kobject *,
           struct Model0_attribute *, int);
 Model0_umode_t (*Model0_is_bin_visible)(struct Model0_kobject *,
        struct Model0_bin_attribute *, int);
 struct Model0_attribute **Model0_attrs;
 struct Model0_bin_attribute **Model0_bin_attrs;
};

/**
 * Use these macros to make defining attributes easier. See include/linux/device.h
 * for examples..
 */
struct Model0_file;
struct Model0_vm_area_struct;

struct Model0_bin_attribute {
 struct Model0_attribute Model0_attr;
 Model0_size_t Model0_size;
 void *Model0_private;
 Model0_ssize_t (*Model0_read)(struct Model0_file *, struct Model0_kobject *, struct Model0_bin_attribute *,
   char *, Model0_loff_t, Model0_size_t);
 Model0_ssize_t (*Model0_write)(struct Model0_file *, struct Model0_kobject *, struct Model0_bin_attribute *,
    char *, Model0_loff_t, Model0_size_t);
 int (*Model0_mmap)(struct Model0_file *, struct Model0_kobject *, struct Model0_bin_attribute *Model0_attr,
      struct Model0_vm_area_struct *Model0_vma);
};

/**
 *	sysfs_bin_attr_init - initialize a dynamically allocated bin_attribute
 *	@attr: struct bin_attribute to initialize
 *
 *	Initialize a dynamically allocated struct bin_attribute so we
 *	can make lockdep happy.  This is a new requirement for
 *	attributes and initially this is only needed when lockdep is
 *	enabled.  Lockdep gives a nice error when your attribute is
 *	added to sysfs if you don't have this.
 */


/* macros to create static binary attributes easier */
struct Model0_sysfs_ops {
 Model0_ssize_t (*Model0_show)(struct Model0_kobject *, struct Model0_attribute *, char *);
 Model0_ssize_t (*Model0_store)(struct Model0_kobject *, struct Model0_attribute *, const char *, Model0_size_t);
};



int __attribute__((warn_unused_result)) Model0_sysfs_create_dir_ns(struct Model0_kobject *Model0_kobj, const void *Model0_ns);
void Model0_sysfs_remove_dir(struct Model0_kobject *Model0_kobj);
int __attribute__((warn_unused_result)) Model0_sysfs_rename_dir_ns(struct Model0_kobject *Model0_kobj, const char *Model0_new_name,
         const void *Model0_new_ns);
int __attribute__((warn_unused_result)) Model0_sysfs_move_dir_ns(struct Model0_kobject *Model0_kobj,
       struct Model0_kobject *Model0_new_parent_kobj,
       const void *Model0_new_ns);
int __attribute__((warn_unused_result)) Model0_sysfs_create_mount_point(struct Model0_kobject *Model0_parent_kobj,
       const char *Model0_name);
void Model0_sysfs_remove_mount_point(struct Model0_kobject *Model0_parent_kobj,
         const char *Model0_name);

int __attribute__((warn_unused_result)) Model0_sysfs_create_file_ns(struct Model0_kobject *Model0_kobj,
          const struct Model0_attribute *Model0_attr,
          const void *Model0_ns);
int __attribute__((warn_unused_result)) Model0_sysfs_create_files(struct Model0_kobject *Model0_kobj,
       const struct Model0_attribute **Model0_attr);
int __attribute__((warn_unused_result)) Model0_sysfs_chmod_file(struct Model0_kobject *Model0_kobj,
      const struct Model0_attribute *Model0_attr, Model0_umode_t Model0_mode);
void Model0_sysfs_remove_file_ns(struct Model0_kobject *Model0_kobj, const struct Model0_attribute *Model0_attr,
     const void *Model0_ns);
bool Model0_sysfs_remove_file_self(struct Model0_kobject *Model0_kobj, const struct Model0_attribute *Model0_attr);
void Model0_sysfs_remove_files(struct Model0_kobject *Model0_kobj, const struct Model0_attribute **Model0_attr);

int __attribute__((warn_unused_result)) Model0_sysfs_create_bin_file(struct Model0_kobject *Model0_kobj,
           const struct Model0_bin_attribute *Model0_attr);
void Model0_sysfs_remove_bin_file(struct Model0_kobject *Model0_kobj,
      const struct Model0_bin_attribute *Model0_attr);

int __attribute__((warn_unused_result)) Model0_sysfs_create_link(struct Model0_kobject *Model0_kobj, struct Model0_kobject *Model0_target,
       const char *Model0_name);
int __attribute__((warn_unused_result)) Model0_sysfs_create_link_nowarn(struct Model0_kobject *Model0_kobj,
       struct Model0_kobject *Model0_target,
       const char *Model0_name);
void Model0_sysfs_remove_link(struct Model0_kobject *Model0_kobj, const char *Model0_name);

int Model0_sysfs_rename_link_ns(struct Model0_kobject *Model0_kobj, struct Model0_kobject *Model0_target,
    const char *Model0_old_name, const char *Model0_new_name,
    const void *Model0_new_ns);

void Model0_sysfs_delete_link(struct Model0_kobject *Model0_dir, struct Model0_kobject *Model0_targ,
   const char *Model0_name);

int __attribute__((warn_unused_result)) Model0_sysfs_create_group(struct Model0_kobject *Model0_kobj,
        const struct Model0_attribute_group *Model0_grp);
int __attribute__((warn_unused_result)) Model0_sysfs_create_groups(struct Model0_kobject *Model0_kobj,
         const struct Model0_attribute_group **Model0_groups);
int Model0_sysfs_update_group(struct Model0_kobject *Model0_kobj,
         const struct Model0_attribute_group *Model0_grp);
void Model0_sysfs_remove_group(struct Model0_kobject *Model0_kobj,
   const struct Model0_attribute_group *Model0_grp);
void Model0_sysfs_remove_groups(struct Model0_kobject *Model0_kobj,
    const struct Model0_attribute_group **Model0_groups);
int Model0_sysfs_add_file_to_group(struct Model0_kobject *Model0_kobj,
   const struct Model0_attribute *Model0_attr, const char *Model0_group);
void Model0_sysfs_remove_file_from_group(struct Model0_kobject *Model0_kobj,
   const struct Model0_attribute *Model0_attr, const char *Model0_group);
int Model0_sysfs_merge_group(struct Model0_kobject *Model0_kobj,
         const struct Model0_attribute_group *Model0_grp);
void Model0_sysfs_unmerge_group(struct Model0_kobject *Model0_kobj,
         const struct Model0_attribute_group *Model0_grp);
int Model0_sysfs_add_link_to_group(struct Model0_kobject *Model0_kobj, const char *Model0_group_name,
       struct Model0_kobject *Model0_target, const char *Model0_link_name);
void Model0_sysfs_remove_link_from_group(struct Model0_kobject *Model0_kobj, const char *Model0_group_name,
      const char *Model0_link_name);
int Model0___compat_only_sysfs_link_entry_to_kobj(struct Model0_kobject *Model0_kobj,
          struct Model0_kobject *Model0_target_kobj,
          const char *Model0_target_name);

void Model0_sysfs_notify(struct Model0_kobject *Model0_kobj, const char *Model0_dir, const char *Model0_attr);

int __attribute__((warn_unused_result)) Model0_sysfs_init(void);

static inline __attribute__((no_instrument_function)) void Model0_sysfs_enable_ns(struct Model0_kernfs_node *Model0_kn)
{
 return Model0_kernfs_enable_ns(Model0_kn);
}
static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_sysfs_create_file(struct Model0_kobject *Model0_kobj,
       const struct Model0_attribute *Model0_attr)
{
 return Model0_sysfs_create_file_ns(Model0_kobj, Model0_attr, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model0_sysfs_remove_file(struct Model0_kobject *Model0_kobj,
         const struct Model0_attribute *Model0_attr)
{
 Model0_sysfs_remove_file_ns(Model0_kobj, Model0_attr, ((void *)0));
}

static inline __attribute__((no_instrument_function)) int Model0_sysfs_rename_link(struct Model0_kobject *Model0_kobj, struct Model0_kobject *Model0_target,
        const char *Model0_old_name, const char *Model0_new_name)
{
 return Model0_sysfs_rename_link_ns(Model0_kobj, Model0_target, Model0_old_name, Model0_new_name, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model0_sysfs_notify_dirent(struct Model0_kernfs_node *Model0_kn)
{
 Model0_kernfs_notify(Model0_kn);
}

static inline __attribute__((no_instrument_function)) struct Model0_kernfs_node *Model0_sysfs_get_dirent(struct Model0_kernfs_node *Model0_parent,
         const unsigned char *Model0_name)
{
 return Model0_kernfs_find_and_get(Model0_parent, Model0_name);
}

static inline __attribute__((no_instrument_function)) struct Model0_kernfs_node *Model0_sysfs_get(struct Model0_kernfs_node *Model0_kn)
{
 Model0_kernfs_get(Model0_kn);
 return Model0_kn;
}

static inline __attribute__((no_instrument_function)) void Model0_sysfs_put(struct Model0_kernfs_node *Model0_kn)
{
 Model0_kernfs_put(Model0_kn);
}


/*
 * kref.h - library routines for handling generic reference counted objects
 *
 * Copyright (C) 2004 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (C) 2004 IBM Corp.
 *
 * based on kobject.h which was:
 * Copyright (C) 2002-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (C) 2002-2003 Open Source Development Labs
 *
 * This file is released under the GPLv2.
 *
 */
struct Model0_kref {
 Model0_atomic_t Model0_refcount;
};

/**
 * kref_init - initialize object.
 * @kref: object in question.
 */
static inline __attribute__((no_instrument_function)) void Model0_kref_init(struct Model0_kref *Model0_kref)
{
 Model0_atomic_set(&Model0_kref->Model0_refcount, 1);
}

/**
 * kref_get - increment refcount for object.
 * @kref: object.
 */
static inline __attribute__((no_instrument_function)) void Model0_kref_get(struct Model0_kref *Model0_kref)
{
 /* If refcount was 0 before incrementing then we have a race
	 * condition when this kref is freeing by some other thread right now.
	 * In this case one should use kref_get_unless_zero()
	 */
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!((Model0_atomic_add_return(1, &Model0_kref->Model0_refcount)) < 2); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/kref.h", 46); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });
}

/**
 * kref_sub - subtract a number of refcounts for object.
 * @kref: object.
 * @count: Number of recounts to subtract.
 * @release: pointer to the function that will clean up the object when the
 *	     last reference to the object is released.
 *	     This pointer is required, and it is not acceptable to pass kfree
 *	     in as this function.  If the caller does pass kfree to this
 *	     function, you will be publicly mocked mercilessly by the kref
 *	     maintainer, and anyone else who happens to notice it.  You have
 *	     been warned.
 *
 * Subtract @count from the refcount, and if 0, call release().
 * Return 1 if the object was removed, otherwise return 0.  Beware, if this
 * function returns 0, you still can not count on the kref from remaining in
 * memory.  Only use the return value if you want to see if the kref is now
 * gone, not present.
 */
static inline __attribute__((no_instrument_function)) int Model0_kref_sub(struct Model0_kref *Model0_kref, unsigned int Model0_count,
      void (*Model0_release)(struct Model0_kref *Model0_kref))
{
 ({ int Model0___ret_warn_on = !!(Model0_release == ((void *)0)); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/kref.h", 70); __builtin_expect(!!(Model0___ret_warn_on), 0); });

 if (Model0_atomic_sub_and_test((int) Model0_count, &Model0_kref->Model0_refcount)) {
  Model0_release(Model0_kref);
  return 1;
 }
 return 0;
}

/**
 * kref_put - decrement refcount for object.
 * @kref: object.
 * @release: pointer to the function that will clean up the object when the
 *	     last reference to the object is released.
 *	     This pointer is required, and it is not acceptable to pass kfree
 *	     in as this function.  If the caller does pass kfree to this
 *	     function, you will be publicly mocked mercilessly by the kref
 *	     maintainer, and anyone else who happens to notice it.  You have
 *	     been warned.
 *
 * Decrement the refcount, and if 0, call release().
 * Return 1 if the object was removed, otherwise return 0.  Beware, if this
 * function returns 0, you still can not count on the kref from remaining in
 * memory.  Only use the return value if you want to see if the kref is now
 * gone, not present.
 */
static inline __attribute__((no_instrument_function)) int Model0_kref_put(struct Model0_kref *Model0_kref, void (*Model0_release)(struct Model0_kref *Model0_kref))
{
 return Model0_kref_sub(Model0_kref, 1, Model0_release);
}

static inline __attribute__((no_instrument_function)) int Model0_kref_put_mutex(struct Model0_kref *Model0_kref,
     void (*Model0_release)(struct Model0_kref *Model0_kref),
     struct Model0_mutex *Model0_lock)
{
 ({ int Model0___ret_warn_on = !!(Model0_release == ((void *)0)); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/kref.h", 105); __builtin_expect(!!(Model0___ret_warn_on), 0); });
 if (__builtin_expect(!!(!Model0_atomic_add_unless(&Model0_kref->Model0_refcount, -1, 1)), 0)) {
  Model0_mutex_lock(Model0_lock);
  if (__builtin_expect(!!(!Model0_atomic_dec_and_test(&Model0_kref->Model0_refcount)), 0)) {
   Model0_mutex_unlock(Model0_lock);
   return 0;
  }
  Model0_release(Model0_kref);
  return 1;
 }
 return 0;
}

/**
 * kref_get_unless_zero - Increment refcount for object unless it is zero.
 * @kref: object.
 *
 * Return non-zero if the increment succeeded. Otherwise return 0.
 *
 * This function is intended to simplify locking around refcounting for
 * objects that can be looked up from a lookup structure, and which are
 * removed from that lookup structure in the object destructor.
 * Operations on such objects require at least a read lock around
 * lookup + kref_get, and a write lock around kref_put + remove from lookup
 * structure. Furthermore, RCU implementations become extremely tricky.
 * With a lookup followed by a kref_get_unless_zero *with return value check*
 * locking in the kref_put path can be deferred to the actual removal from
 * the lookup structure and RCU lookups become trivial.
 */
static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_kref_get_unless_zero(struct Model0_kref *Model0_kref)
{
 return Model0_atomic_add_unless(&Model0_kref->Model0_refcount, 1, 0);
}
/* path to the userspace helper executed on an event */
extern char Model0_uevent_helper[];


/* counter to tag the uevent, read only except for the kobject core */
extern Model0_u64 Model0_uevent_seqnum;

/*
 * The actions here must match the index to the string array
 * in lib/kobject_uevent.c
 *
 * Do not add new actions here without checking with the driver-core
 * maintainers. Action strings are not meant to express subsystem
 * or device specific properties. In most cases you want to send a
 * kobject_uevent_env(kobj, KOBJ_CHANGE, env) with additional event
 * specific variables added to the event environment.
 */
enum Model0_kobject_action {
 Model0_KOBJ_ADD,
 Model0_KOBJ_REMOVE,
 Model0_KOBJ_CHANGE,
 Model0_KOBJ_MOVE,
 Model0_KOBJ_ONLINE,
 Model0_KOBJ_OFFLINE,
 Model0_KOBJ_MAX
};

struct Model0_kobject {
 const char *Model0_name;
 struct Model0_list_head Model0_entry;
 struct Model0_kobject *Model0_parent;
 struct Model0_kset *Model0_kset;
 struct Model0_kobj_type *Model0_ktype;
 struct Model0_kernfs_node *Model0_sd; /* sysfs directory entry */
 struct Model0_kref Model0_kref;



 unsigned int Model0_state_initialized:1;
 unsigned int Model0_state_in_sysfs:1;
 unsigned int Model0_state_add_uevent_sent:1;
 unsigned int Model0_state_remove_uevent_sent:1;
 unsigned int Model0_uevent_suppress:1;
};

extern __attribute__((format(printf, 2, 3)))
int Model0_kobject_set_name(struct Model0_kobject *Model0_kobj, const char *Model0_name, ...);
extern __attribute__((format(printf, 2, 0)))
int Model0_kobject_set_name_vargs(struct Model0_kobject *Model0_kobj, const char *Model0_fmt,
      Model0_va_list Model0_vargs);

static inline __attribute__((no_instrument_function)) const char *Model0_kobject_name(const struct Model0_kobject *Model0_kobj)
{
 return Model0_kobj->Model0_name;
}

extern void Model0_kobject_init(struct Model0_kobject *Model0_kobj, struct Model0_kobj_type *Model0_ktype);
extern __attribute__((format(printf, 3, 4))) __attribute__((warn_unused_result))
int Model0_kobject_add(struct Model0_kobject *Model0_kobj, struct Model0_kobject *Model0_parent,
  const char *Model0_fmt, ...);
extern __attribute__((format(printf, 4, 5))) __attribute__((warn_unused_result))
int Model0_kobject_init_and_add(struct Model0_kobject *Model0_kobj,
    struct Model0_kobj_type *Model0_ktype, struct Model0_kobject *Model0_parent,
    const char *Model0_fmt, ...);

extern void Model0_kobject_del(struct Model0_kobject *Model0_kobj);

extern struct Model0_kobject * __attribute__((warn_unused_result)) Model0_kobject_create(void);
extern struct Model0_kobject * __attribute__((warn_unused_result)) Model0_kobject_create_and_add(const char *Model0_name,
      struct Model0_kobject *Model0_parent);

extern int __attribute__((warn_unused_result)) Model0_kobject_rename(struct Model0_kobject *, const char *Model0_new_name);
extern int __attribute__((warn_unused_result)) Model0_kobject_move(struct Model0_kobject *, struct Model0_kobject *);

extern struct Model0_kobject *Model0_kobject_get(struct Model0_kobject *Model0_kobj);
extern void Model0_kobject_put(struct Model0_kobject *Model0_kobj);

extern const void *Model0_kobject_namespace(struct Model0_kobject *Model0_kobj);
extern char *Model0_kobject_get_path(struct Model0_kobject *Model0_kobj, Model0_gfp_t Model0_flag);

struct Model0_kobj_type {
 void (*Model0_release)(struct Model0_kobject *Model0_kobj);
 const struct Model0_sysfs_ops *Model0_sysfs_ops;
 struct Model0_attribute **Model0_default_attrs;
 const struct Model0_kobj_ns_type_operations *(*Model0_child_ns_type)(struct Model0_kobject *Model0_kobj);
 const void *(*Model0_namespace)(struct Model0_kobject *Model0_kobj);
};

struct Model0_kobj_uevent_env {
 char *Model0_argv[3];
 char *Model0_envp[32];
 int Model0_envp_idx;
 char Model0_buf[2048];
 int Model0_buflen;
};

struct Model0_kset_uevent_ops {
 int (* const Model0_filter)(struct Model0_kset *Model0_kset, struct Model0_kobject *Model0_kobj);
 const char *(* const Model0_name)(struct Model0_kset *Model0_kset, struct Model0_kobject *Model0_kobj);
 int (* const Model0_uevent)(struct Model0_kset *Model0_kset, struct Model0_kobject *Model0_kobj,
        struct Model0_kobj_uevent_env *Model0_env);
};

struct Model0_kobj_attribute {
 struct Model0_attribute Model0_attr;
 Model0_ssize_t (*Model0_show)(struct Model0_kobject *Model0_kobj, struct Model0_kobj_attribute *Model0_attr,
   char *Model0_buf);
 Model0_ssize_t (*Model0_store)(struct Model0_kobject *Model0_kobj, struct Model0_kobj_attribute *Model0_attr,
    const char *Model0_buf, Model0_size_t Model0_count);
};

extern const struct Model0_sysfs_ops Model0_kobj_sysfs_ops;

struct Model0_sock;

/**
 * struct kset - a set of kobjects of a specific type, belonging to a specific subsystem.
 *
 * A kset defines a group of kobjects.  They can be individually
 * different "types" but overall these kobjects all want to be grouped
 * together and operated on in the same manner.  ksets are used to
 * define the attribute callbacks and other common events that happen to
 * a kobject.
 *
 * @list: the list of all kobjects for this kset
 * @list_lock: a lock for iterating over the kobjects
 * @kobj: the embedded kobject for this kset (recursion, isn't it fun...)
 * @uevent_ops: the set of uevent operations for this kset.  These are
 * called whenever a kobject has something happen to it so that the kset
 * can add new environment variables, or filter out the uevents if so
 * desired.
 */
struct Model0_kset {
 struct Model0_list_head Model0_list;
 Model0_spinlock_t Model0_list_lock;
 struct Model0_kobject Model0_kobj;
 const struct Model0_kset_uevent_ops *Model0_uevent_ops;
};

extern void Model0_kset_init(struct Model0_kset *Model0_kset);
extern int __attribute__((warn_unused_result)) Model0_kset_register(struct Model0_kset *Model0_kset);
extern void Model0_kset_unregister(struct Model0_kset *Model0_kset);
extern struct Model0_kset * __attribute__((warn_unused_result)) Model0_kset_create_and_add(const char *Model0_name,
      const struct Model0_kset_uevent_ops *Model0_u,
      struct Model0_kobject *Model0_parent_kobj);

static inline __attribute__((no_instrument_function)) struct Model0_kset *Model0_to_kset(struct Model0_kobject *Model0_kobj)
{
 return Model0_kobj ? ({ const typeof( ((struct Model0_kset *)0)->Model0_kobj ) *Model0___mptr = (Model0_kobj); (struct Model0_kset *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_kset, Model0_kobj) );}) : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_kset *Model0_kset_get(struct Model0_kset *Model0_k)
{
 return Model0_k ? Model0_to_kset(Model0_kobject_get(&Model0_k->Model0_kobj)) : ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_kset_put(struct Model0_kset *Model0_k)
{
 Model0_kobject_put(&Model0_k->Model0_kobj);
}

static inline __attribute__((no_instrument_function)) struct Model0_kobj_type *Model0_get_ktype(struct Model0_kobject *Model0_kobj)
{
 return Model0_kobj->Model0_ktype;
}

extern struct Model0_kobject *Model0_kset_find_obj(struct Model0_kset *, const char *);

/* The global /sys/kernel/ kobject for people to chain off of */
extern struct Model0_kobject *Model0_kernel_kobj;
/* The global /sys/kernel/mm/ kobject for people to chain off of */
extern struct Model0_kobject *Model0_mm_kobj;
/* The global /sys/hypervisor/ kobject for people to chain off of */
extern struct Model0_kobject *Model0_hypervisor_kobj;
/* The global /sys/power/ kobject for people to chain off of */
extern struct Model0_kobject *Model0_power_kobj;
/* The global /sys/firmware/ kobject for people to chain off of */
extern struct Model0_kobject *Model0_firmware_kobj;

int Model0_kobject_uevent(struct Model0_kobject *Model0_kobj, enum Model0_kobject_action Model0_action);
int Model0_kobject_uevent_env(struct Model0_kobject *Model0_kobj, enum Model0_kobject_action Model0_action,
   char *Model0_envp[]);

__attribute__((format(printf, 2, 3)))
int Model0_add_uevent_var(struct Model0_kobj_uevent_env *Model0_env, const char *format, ...);

int Model0_kobject_action_type(const char *Model0_buf, Model0_size_t Model0_count,
   enum Model0_kobject_action *Model0_type);


/* (C) Copyright 2001, 2002 Rusty Russell IBM Corporation */




/* You can override this manually, but generally this should match the
   module name. */






/* Chosen so that structs with an unsigned long line up. */
/* This struct is here for syntactic coherency, it is not used */






/* One for each parameter, describing how to use it.  Some files do
   multiple of these per line, so can't just use MODULE_INFO. */



struct Model0_kernel_param;

/*
 * Flags available for kernel_param_ops
 *
 * NOARG - the parameter allows for no argument (foo instead of foo=1)
 */
enum {
 Model0_KERNEL_PARAM_OPS_FL_NOARG = (1 << 0)
};

struct Model0_kernel_param_ops {
 /* How the ops should behave */
 unsigned int Model0_flags;
 /* Returns 0, or -errno.  arg is in kp->arg. */
 int (*Model0_set)(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
 /* Returns length written or -errno.  Buffer is 4k (ie. be short!) */
 int (*Model0_get)(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);
 /* Optional function to free kp->arg when module unloaded. */
 void (*Model0_free)(void *Model0_arg);
};

/*
 * Flags available for kernel_param
 *
 * UNSAFE - the parameter is dangerous and setting it will taint the kernel
 */
enum {
 Model0_KERNEL_PARAM_FL_UNSAFE = (1 << 0)
};

struct Model0_kernel_param {
 const char *Model0_name;
 struct Model0_module *Model0_mod;
 const struct Model0_kernel_param_ops *Model0_ops;
 const Model0_u16 Model0_perm;
 Model0_s8 Model0_level;
 Model0_u8 Model0_flags;
 union {
  void *Model0_arg;
  const struct Model0_kparam_string *Model0_str;
  const struct Model0_kparam_array *Model0_arr;
 };
};

extern const struct Model0_kernel_param Model0___start___param[], Model0___stop___param[];

/* Special one for strings we want to copy into */
struct Model0_kparam_string {
 unsigned int Model0_maxlen;
 char *Model0_string;
};

/* Special one for arrays */
struct Model0_kparam_array
{
 unsigned int Model0_max;
 unsigned int Model0_elemsize;
 unsigned int *Model0_num;
 const struct Model0_kernel_param_ops *Model0_ops;
 void *Model0_elem;
};

/**
 * module_param - typesafe helper for a module/cmdline parameter
 * @value: the variable to alter, and exposed parameter name.
 * @type: the type of the parameter
 * @perm: visibility in sysfs.
 *
 * @value becomes the module parameter, or (prefixed by KBUILD_MODNAME and a
 * ".") the kernel commandline parameter.  Note that - is changed to _, so
 * the user can use "foo-bar=1" even for variable "foo_bar".
 *
 * @perm is 0 if the the variable is not to appear in sysfs, or 0444
 * for world-readable, 0644 for root-writable, etc.  Note that if it
 * is writable, you may need to use kernel_param_lock() around
 * accesses (esp. charp, which can be kfreed when it changes).
 *
 * The @type is simply pasted to refer to a param_ops_##type and a
 * param_check_##type: for convenience many standard types are provided but
 * you can create your own by defining those variables.
 *
 * Standard types are:
 *	byte, short, ushort, int, uint, long, ulong
 *	charp: a character pointer
 *	bool: a bool, values 0/1, y/n, Y/N.
 *	invbool: the above, only sense-reversed (N = true).
 */



/**
 * module_param_unsafe - same as module_param but taints kernel
 */



/**
 * module_param_named - typesafe helper for a renamed module/cmdline parameter
 * @name: a valid C identifier which is the parameter name.
 * @value: the actual lvalue to alter.
 * @type: the type of the parameter
 * @perm: visibility in sysfs.
 *
 * Usually it's a good idea to have variable names and user-exposed names the
 * same, but that's harder if the variable must be non-static or is inside a
 * structure.  This allows exposure under a different name.
 */





/**
 * module_param_named_unsafe - same as module_param_named but taints kernel
 */





/**
 * module_param_cb - general callback for a module/cmdline parameter
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */







/**
 * <level>_param_cb - general callback for a module/cmdline parameter
 *                    to be evaluated before certain initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */
/* On alpha, ia64 and ppc64 relocations to global data cannot go into
   read-only sections (which is part of respective UNIX ABI on these
   platforms). So 'const' makes no sense and even causes compile failures
   with some compilers. */






/* This is the fundamental function for registering boot/module
   parameters. */
/* Obsolete - use module_param_cb() */







/* We don't get oldget: it's often a new-style param_get_uint, etc. */
static inline __attribute__((no_instrument_function)) int
Model0___check_old_set_param(int (*Model0_oldset)(const char *, struct Model0_kernel_param *))
{
 return 0;
}


extern void Model0_kernel_param_lock(struct Model0_module *Model0_mod);
extern void Model0_kernel_param_unlock(struct Model0_module *Model0_mod);
/**
 * core_param - define a historical core kernel parameter.
 * @name: the name of the cmdline and sysfs parameter (often the same as var)
 * @var: the variable
 * @type: the type of the parameter
 * @perm: visibility in sysfs
 *
 * core_param is just like module_param(), but cannot be modular and
 * doesn't add a prefix (such as "printk.").  This is for compatibility
 * with __setup(), and it makes sense as truly core parameters aren't
 * tied to the particular file they're in.
 */




/**
 * core_param_unsafe - same as core_param but taints kernel
 */







/**
 * module_param_string - a char array parameter
 * @name: the name of the parameter
 * @string: the string variable
 * @len: the maximum length of the string, incl. terminator
 * @perm: visibility in sysfs.
 *
 * This actually copies the string when it's set (unlike type charp).
 * @len is usually just sizeof(string).
 */
/**
 * parameq - checks if two parameter names match
 * @name1: parameter name 1
 * @name2: parameter name 2
 *
 * Returns true if the two parameter names are equal.
 * Dashes (-) are considered equal to underscores (_).
 */
extern bool Model0_parameq(const char *Model0_name1, const char *Model0_name2);

/**
 * parameqn - checks if two parameter names match
 * @name1: parameter name 1
 * @name2: parameter name 2
 * @n: the length to compare
 *
 * Similar to parameq(), except it compares @n characters.
 */
extern bool Model0_parameqn(const char *Model0_name1, const char *Model0_name2, Model0_size_t Model0_n);

/* Called on module insert or kernel boot */
extern char *Model0_parse_args(const char *Model0_name,
        char *Model0_args,
        const struct Model0_kernel_param *Model0_params,
        unsigned Model0_num,
        Model0_s16 Model0_level_min,
        Model0_s16 Model0_level_max,
        void *Model0_arg,
        int (*Model0_unknown)(char *Model0_param, char *Model0_val,
         const char *Model0_doing, void *Model0_arg));

/* Called by module remove. */

extern void Model0_destroy_params(const struct Model0_kernel_param *Model0_params, unsigned Model0_num);







/* All the helper functions */
/* The macros to do compile-time type checking stolen from Jakub
   Jelinek, who IIRC came up with this idea for the 2.4 module init code. */



extern const struct Model0_kernel_param_ops Model0_param_ops_byte;
extern int Model0_param_set_byte(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_byte(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_short;
extern int Model0_param_set_short(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_short(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_ushort;
extern int Model0_param_set_ushort(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_ushort(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_int;
extern int Model0_param_set_int(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_int(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_uint;
extern int Model0_param_set_uint(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_uint(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_long;
extern int Model0_param_set_long(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_long(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_ulong;
extern int Model0_param_set_ulong(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_ulong(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_ullong;
extern int Model0_param_set_ullong(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_ullong(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_charp;
extern int Model0_param_set_charp(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_charp(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);
extern void Model0_param_free_charp(void *Model0_arg);


/* We used to allow int as well as bool.  We're taking that away! */
extern const struct Model0_kernel_param_ops Model0_param_ops_bool;
extern int Model0_param_set_bool(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_bool(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


extern const struct Model0_kernel_param_ops Model0_param_ops_bool_enable_only;
extern int Model0_param_set_bool_enable_only(const char *Model0_val,
          const struct Model0_kernel_param *Model0_kp);
/* getter is the same as for the regular bool */


extern const struct Model0_kernel_param_ops Model0_param_ops_invbool;
extern int Model0_param_set_invbool(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);
extern int Model0_param_get_invbool(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);


/* An int, which can only be set like a bool (though it shows as an int). */
extern const struct Model0_kernel_param_ops Model0_param_ops_bint;
extern int Model0_param_set_bint(const char *Model0_val, const struct Model0_kernel_param *Model0_kp);



/**
 * module_param_array - a parameter which is an array of some type
 * @name: the name of the array variable
 * @type: the type, as per module_param()
 * @nump: optional pointer filled in with the number written
 * @perm: visibility in sysfs
 *
 * Input and output are as comma-separated values.  Commas inside values
 * don't work properly (eg. an array of charp).
 *
 * ARRAY_SIZE(@name) is used to determine the number of elements in the
 * array, so the definition must be visible.
 */



/**
 * module_param_array_named - renamed parameter which is an array of some type
 * @name: a valid C identifier which is the parameter name
 * @array: the name of the array variable
 * @type: the type, as per module_param()
 * @nump: optional pointer filled in with the number written
 * @perm: visibility in sysfs
 *
 * This exposes a different name than the actual variable name.  See
 * module_param_named() for why this might be necessary.
 */
extern const struct Model0_kernel_param_ops Model0_param_array_ops;

extern const struct Model0_kernel_param_ops Model0_param_ops_string;
extern int Model0_param_set_copystring(const char *Model0_val, const struct Model0_kernel_param *);
extern int Model0_param_get_string(char *Model0_buffer, const struct Model0_kernel_param *Model0_kp);

/* for exporting parameters in /sys/module/.../parameters */

struct Model0_module;


extern int Model0_module_param_sysfs_setup(struct Model0_module *Model0_mod,
        const struct Model0_kernel_param *Model0_kparam,
        unsigned int Model0_num_params);

extern void Model0_module_param_sysfs_remove(struct Model0_module *Model0_mod);







struct Model0_module;
struct Model0_exception_table_entry;

const struct Model0_exception_table_entry *
Model0_search_extable(const struct Model0_exception_table_entry *Model0_first,
        const struct Model0_exception_table_entry *Model0_last,
        unsigned long Model0_value);
void Model0_sort_extable(struct Model0_exception_table_entry *Model0_start,
    struct Model0_exception_table_entry *Model0_finish);
void Model0_sort_main_extable(void);
void Model0_trim_init_extable(struct Model0_module *Model0_m);

/* Given an address, look for it in the exception tables */
const struct Model0_exception_table_entry *Model0_search_exception_tables(unsigned long Model0_add);


/* For extable.c to search modules' exception tables. */
const struct Model0_exception_table_entry *Model0_search_module_extables(unsigned long Model0_addr);
/*
 * Latched RB-trees
 *
 * Copyright (C) 2015 Intel Corp., Peter Zijlstra <peterz@infradead.org>
 *
 * Since RB-trees have non-atomic modifications they're not immediately suited
 * for RCU/lockless queries. Even though we made RB-tree lookups non-fatal for
 * lockless lookups; we cannot guarantee they return a correct result.
 *
 * The simplest solution is a seqlock + RB-tree, this will allow lockless
 * lookups; but has the constraint (inherent to the seqlock) that read sides
 * cannot nest in write sides.
 *
 * If we need to allow unconditional lookups (say as required for NMI context
 * usage) we need a more complex setup; this data structure provides this by
 * employing the latch technique -- see @raw_write_seqcount_latch -- to
 * implement a latched RB-tree which does allow for unconditional lookups by
 * virtue of always having (at least) one stable copy of the tree.
 *
 * However, while we have the guarantee that there is at all times one stable
 * copy, this does not guarantee an iteration will not observe modifications.
 * What might have been a stable copy at the start of the iteration, need not
 * remain so for the duration of the iteration.
 *
 * Therefore, this does require a lockless RB-tree iteration to be non-fatal;
 * see the comment in lib/rbtree.c. Note however that we only require the first
 * condition -- not seeing partial stores -- because the latch thing isolates
 * us from loops. If we were to interrupt a modification the lookup would be
 * pointed at the stable tree and complete while the modification was halted.
 */







struct Model0_latch_tree_node {
 struct Model0_rb_node Model0_node[2];
};

struct Model0_latch_tree_root {
 Model0_seqcount_t Model0_seq;
 struct Model0_rb_root Model0_tree[2];
};

/**
 * latch_tree_ops - operators to define the tree order
 * @less: used for insertion; provides the (partial) order between two elements.
 * @comp: used for lookups; provides the order between the search key and an element.
 *
 * The operators are related like:
 *
 *	comp(a->key,b) < 0  := less(a,b)
 *	comp(a->key,b) > 0  := less(b,a)
 *	comp(a->key,b) == 0 := !less(a,b) && !less(b,a)
 *
 * If these operators define a partial order on the elements we make no
 * guarantee on which of the elements matching the key is found. See
 * latch_tree_find().
 */
struct Model0_latch_tree_ops {
 bool (*Model0_less)(struct Model0_latch_tree_node *Model0_a, struct Model0_latch_tree_node *Model0_b);
 int (*Model0_comp)(void *Model0_key, struct Model0_latch_tree_node *Model0_b);
};

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model0_latch_tree_node *
Model0___lt_from_rb(struct Model0_rb_node *Model0_node, int Model0_idx)
{
 return ({ const typeof( ((struct Model0_latch_tree_node *)0)->Model0_node[Model0_idx] ) *Model0___mptr = (Model0_node); (struct Model0_latch_tree_node *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_latch_tree_node, Model0_node[Model0_idx]) );});
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model0___lt_insert(struct Model0_latch_tree_node *Model0_ltn, struct Model0_latch_tree_root *Model0_ltr, int Model0_idx,
     bool (*Model0_less)(struct Model0_latch_tree_node *Model0_a, struct Model0_latch_tree_node *Model0_b))
{
 struct Model0_rb_root *Model0_root = &Model0_ltr->Model0_tree[Model0_idx];
 struct Model0_rb_node **Model0_link = &Model0_root->Model0_rb_node;
 struct Model0_rb_node *Model0_node = &Model0_ltn->Model0_node[Model0_idx];
 struct Model0_rb_node *Model0_parent = ((void *)0);
 struct Model0_latch_tree_node *Model0_ltp;

 while (*Model0_link) {
  Model0_parent = *Model0_link;
  Model0_ltp = Model0___lt_from_rb(Model0_parent, Model0_idx);

  if (Model0_less(Model0_ltn, Model0_ltp))
   Model0_link = &Model0_parent->Model0_rb_left;
  else
   Model0_link = &Model0_parent->Model0_rb_right;
 }

 Model0_rb_link_node_rcu(Model0_node, Model0_parent, Model0_link);
 Model0_rb_insert_color(Model0_node, Model0_root);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model0___lt_erase(struct Model0_latch_tree_node *Model0_ltn, struct Model0_latch_tree_root *Model0_ltr, int Model0_idx)
{
 Model0_rb_erase(&Model0_ltn->Model0_node[Model0_idx], &Model0_ltr->Model0_tree[Model0_idx]);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model0_latch_tree_node *
Model0___lt_find(void *Model0_key, struct Model0_latch_tree_root *Model0_ltr, int Model0_idx,
   int (*Model0_comp)(void *Model0_key, struct Model0_latch_tree_node *Model0_node))
{
 struct Model0_rb_node *Model0_node = ({ typeof(Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node) Model0_________p1 = ({ typeof(Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node) Model0__________p1 = ({ union { typeof(Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node), Model0___u.Model0___c, sizeof(Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node)); else Model0___read_once_size_nocheck(&(Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node), Model0___u.Model0___c, sizeof(Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node)); Model0___u.Model0___val; }); typeof(*(Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node)) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); ((typeof(*Model0_ltr->Model0_tree[Model0_idx].Model0_rb_node) *)(Model0_________p1)); });
 struct Model0_latch_tree_node *Model0_ltn;
 int Model0_c;

 while (Model0_node) {
  Model0_ltn = Model0___lt_from_rb(Model0_node, Model0_idx);
  Model0_c = Model0_comp(Model0_key, Model0_ltn);

  if (Model0_c < 0)
   Model0_node = ({ typeof(Model0_node->Model0_rb_left) Model0_________p1 = ({ typeof(Model0_node->Model0_rb_left) Model0__________p1 = ({ union { typeof(Model0_node->Model0_rb_left) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_node->Model0_rb_left), Model0___u.Model0___c, sizeof(Model0_node->Model0_rb_left)); else Model0___read_once_size_nocheck(&(Model0_node->Model0_rb_left), Model0___u.Model0___c, sizeof(Model0_node->Model0_rb_left)); Model0___u.Model0___val; }); typeof(*(Model0_node->Model0_rb_left)) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); ((typeof(*Model0_node->Model0_rb_left) *)(Model0_________p1)); });
  else if (Model0_c > 0)
   Model0_node = ({ typeof(Model0_node->Model0_rb_right) Model0_________p1 = ({ typeof(Model0_node->Model0_rb_right) Model0__________p1 = ({ union { typeof(Model0_node->Model0_rb_right) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_node->Model0_rb_right), Model0___u.Model0___c, sizeof(Model0_node->Model0_rb_right)); else Model0___read_once_size_nocheck(&(Model0_node->Model0_rb_right), Model0___u.Model0___c, sizeof(Model0_node->Model0_rb_right)); Model0___u.Model0___val; }); typeof(*(Model0_node->Model0_rb_right)) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); ((typeof(*Model0_node->Model0_rb_right) *)(Model0_________p1)); });
  else
   return Model0_ltn;
 }

 return ((void *)0);
}

/**
 * latch_tree_insert() - insert @node into the trees @root
 * @node: nodes to insert
 * @root: trees to insert @node into
 * @ops: operators defining the node order
 *
 * It inserts @node into @root in an ordered fashion such that we can always
 * observe one complete tree. See the comment for raw_write_seqcount_latch().
 *
 * The inserts use rcu_assign_pointer() to publish the element such that the
 * tree structure is stored before we can observe the new @node.
 *
 * All modifications (latch_tree_insert, latch_tree_remove) are assumed to be
 * serialized.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model0_latch_tree_insert(struct Model0_latch_tree_node *Model0_node,
    struct Model0_latch_tree_root *Model0_root,
    const struct Model0_latch_tree_ops *Model0_ops)
{
 Model0_raw_write_seqcount_latch(&Model0_root->Model0_seq);
 Model0___lt_insert(Model0_node, Model0_root, 0, Model0_ops->Model0_less);
 Model0_raw_write_seqcount_latch(&Model0_root->Model0_seq);
 Model0___lt_insert(Model0_node, Model0_root, 1, Model0_ops->Model0_less);
}

/**
 * latch_tree_erase() - removes @node from the trees @root
 * @node: nodes to remote
 * @root: trees to remove @node from
 * @ops: operators defining the node order
 *
 * Removes @node from the trees @root in an ordered fashion such that we can
 * always observe one complete tree. See the comment for
 * raw_write_seqcount_latch().
 *
 * It is assumed that @node will observe one RCU quiescent state before being
 * reused of freed.
 *
 * All modifications (latch_tree_insert, latch_tree_remove) are assumed to be
 * serialized.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model0_latch_tree_erase(struct Model0_latch_tree_node *Model0_node,
   struct Model0_latch_tree_root *Model0_root,
   const struct Model0_latch_tree_ops *Model0_ops)
{
 Model0_raw_write_seqcount_latch(&Model0_root->Model0_seq);
 Model0___lt_erase(Model0_node, Model0_root, 0);
 Model0_raw_write_seqcount_latch(&Model0_root->Model0_seq);
 Model0___lt_erase(Model0_node, Model0_root, 1);
}

/**
 * latch_tree_find() - find the node matching @key in the trees @root
 * @key: search key
 * @root: trees to search for @key
 * @ops: operators defining the node order
 *
 * Does a lockless lookup in the trees @root for the node matching @key.
 *
 * It is assumed that this is called while holding the appropriate RCU read
 * side lock.
 *
 * If the operators define a partial order on the elements (there are multiple
 * elements which have the same key value) it is undefined which of these
 * elements will be found. Nor is it possible to iterate the tree to find
 * further elements with the same key value.
 *
 * Returns: a pointer to the node matching @key or NULL.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model0_latch_tree_node *
Model0_latch_tree_find(void *Model0_key, struct Model0_latch_tree_root *Model0_root,
  const struct Model0_latch_tree_ops *Model0_ops)
{
 struct Model0_latch_tree_node *Model0_node;
 unsigned int Model0_seq;

 do {
  Model0_seq = Model0_raw_read_seqcount_latch(&Model0_root->Model0_seq);
  Model0_node = Model0___lt_find(Model0_key, Model0_root, Model0_seq & 1, Model0_ops->Model0_comp);
 } while (Model0_read_seqcount_retry(&Model0_root->Model0_seq, Model0_seq));

 return Model0_node;
}








/*
 * Many architectures just need a simple module
 * loader without arch specific data.
 */

struct Model0_mod_arch_specific
{
};


/* X86_64 does not define MODULE_PROC_FAMILY */

/* In stripped ARM and x86-64 modules, ~ is surprisingly rare. */


/* Not Yet Implemented */




struct Model0_modversion_info {
 unsigned long Model0_crc;
 char Model0_name[(64 - sizeof(unsigned long))];
};

struct Model0_module;
struct Model0_exception_table_entry;

struct Model0_module_kobject {
 struct Model0_kobject Model0_kobj;
 struct Model0_module *Model0_mod;
 struct Model0_kobject *Model0_drivers_dir;
 struct Model0_module_param_attrs *Model0_mp;
 struct Model0_completion *Model0_kobj_completion;
};

struct Model0_module_attribute {
 struct Model0_attribute Model0_attr;
 Model0_ssize_t (*Model0_show)(struct Model0_module_attribute *, struct Model0_module_kobject *,
   char *);
 Model0_ssize_t (*Model0_store)(struct Model0_module_attribute *, struct Model0_module_kobject *,
    const char *, Model0_size_t Model0_count);
 void (*Model0_setup)(struct Model0_module *, const char *);
 int (*Model0_test)(struct Model0_module *);
 void (*Model0_free)(struct Model0_module *);
};

struct Model0_module_version_attribute {
 struct Model0_module_attribute Model0_mattr;
 const char *Model0_module_name;
 const char *Model0_version;
} __attribute__ ((__aligned__(sizeof(void *))));

extern Model0_ssize_t Model0___modver_version_show(struct Model0_module_attribute *,
         struct Model0_module_kobject *, char *);

extern struct Model0_module_attribute Model0_module_uevent;

/* These are either module local, or the kernel's dummy ones. */
extern int Model0_init_module(void);
extern void Model0_cleanup_module(void);


/**
 * module_init() - driver initialization entry point
 * @x: function to be run at kernel boot time or module insertion
 *
 * module_init() will either be called during do_initcalls() (if
 * builtin) or at module insertion time (if a module).  There can only
 * be one per module.
 */


/**
 * module_exit() - driver exit entry point
 * @x: function to be run when driver is removed
 *
 * module_exit() will wrap the driver clean-up code
 * with cleanup_module() when used with rmmod when
 * the driver is a module.  If the driver is statically
 * compiled into the kernel, module_exit() has no effect.
 * There can only be one per module.
 */
/* This means "can be init if no module support, otherwise module load
   may call it." */
/* Generic info of form tag = "info" */


/* For userspace: you can also call me... */


/* Soft module dependencies. See man modprobe.d for details.
 * Example: MODULE_SOFTDEP("pre: module-foo module-bar post: module-baz")
 */


/*
 * The following license idents are currently accepted as indicating free
 * software modules
 *
 *	"GPL"				[GNU Public License v2 or later]
 *	"GPL v2"			[GNU Public License v2]
 *	"GPL and additional rights"	[GNU Public License v2 rights and more]
 *	"Dual BSD/GPL"			[GNU Public License v2
 *					 or BSD license choice]
 *	"Dual MIT/GPL"			[GNU Public License v2
 *					 or MIT license choice]
 *	"Dual MPL/GPL"			[GNU Public License v2
 *					 or Mozilla license choice]
 *
 * The following other idents are available
 *
 *	"Proprietary"			[Non free products]
 *
 * There are dual licensed components, but when running with Linux it is the
 * GPL that is relevant so this is a non issue. Similarly LGPL linked with GPL
 * is a GPL combined work.
 *
 * This exists for several reasons
 * 1.	So modinfo can show license info for users wanting to vet their setup
 *	is free
 * 2.	So the community can ignore bug reports including proprietary modules
 * 3.	So vendors can do likewise based on their own policies
 */


/*
 * Author(s), use "Name <email>" or just "Name", for multiple
 * authors use multiple MODULE_AUTHOR() statements/lines.
 */


/* What your module does. */
/* Version of form [<epoch>:]<version>[-<extra-version>].
 * Or for CVS/RCS ID version, everything but the number is stripped.
 * <epoch>: A (small) unsigned integer which allows you to start versions
 * anew. If not mentioned, it's zero.  eg. "2:1.0" is after
 * "1:2.0".

 * <version>: The <version> may contain only alphanumerics and the
 * character `.'.  Ordered by numeric sort for numeric parts,
 * ascii sort for ascii parts (as per RPM or DEB algorithm).

 * <extraversion>: Like <version>, but inserted for local
 * customizations, eg "rh3" or "rusty1".

 * Using this automatically adds a checksum of the .c files and the
 * local headers in "srcversion".
 */
/* Optional firmware file (or files) needed by the module
 * format is simply firmware file name.  Multiple firmware
 * files require multiple MODULE_FIRMWARE() specifiers */


struct Model0_notifier_block;



extern int Model0_modules_disabled; /* for sysctl */
/* Get/put a kernel symbol (calls must be symmetric) */
void *Model0___symbol_get(const char *Model0_symbol);
void *Model0___symbol_get_gpl(const char *Model0_symbol);


/* modules using other modules: kdb wants to see this. */
struct Model0_module_use {
 struct Model0_list_head Model0_source_list;
 struct Model0_list_head Model0_target_list;
 struct Model0_module *Model0_source, *Model0_target;
};

enum Model0_module_state {
 Model0_MODULE_STATE_LIVE, /* Normal state. */
 Model0_MODULE_STATE_COMING, /* Full formed, running module_init. */
 Model0_MODULE_STATE_GOING, /* Going away. */
 Model0_MODULE_STATE_UNFORMED, /* Still setting it up. */
};

struct Model0_module;

struct Model0_mod_tree_node {
 struct Model0_module *Model0_mod;
 struct Model0_latch_tree_node Model0_node;
};

struct Model0_module_layout {
 /* The actual code + data. */
 void *Model0_base;
 /* Total size. */
 unsigned int Model0_size;
 /* The size of the executable code.  */
 unsigned int Model0_text_size;
 /* Size of RO section of the module (text+rodata) */
 unsigned int Model0_ro_size;
 /* Size of RO after init section */
 unsigned int Model0_ro_after_init_size;


 struct Model0_mod_tree_node Model0_mtn;

};


/* Only touch one cacheline for common rbtree-for-core-layout case. */





struct Model0_mod_kallsyms {
 Model0_Elf64_Sym *Model0_symtab;
 unsigned int Model0_num_symtab;
 char *Model0_strtab;
};
struct Model0_module {
 enum Model0_module_state Model0_state;

 /* Member of list of modules */
 struct Model0_list_head Model0_list;

 /* Unique handle for this module */
 char Model0_name[(64 - sizeof(unsigned long))];

 /* Sysfs stuff. */
 struct Model0_module_kobject Model0_mkobj;
 struct Model0_module_attribute *Model0_modinfo_attrs;
 const char *Model0_version;
 const char *Model0_srcversion;
 struct Model0_kobject *Model0_holders_dir;

 /* Exported symbols */
 const struct Model0_kernel_symbol *Model0_syms;
 const unsigned long *Model0_crcs;
 unsigned int Model0_num_syms;

 /* Kernel parameters. */

 struct Model0_mutex Model0_param_lock;

 struct Model0_kernel_param *Model0_kp;
 unsigned int Model0_num_kp;

 /* GPL-only exported symbols. */
 unsigned int Model0_num_gpl_syms;
 const struct Model0_kernel_symbol *Model0_gpl_syms;
 const unsigned long *Model0_gpl_crcs;
 bool Model0_async_probe_requested;

 /* symbols that will be GPL-only in the near future. */
 const struct Model0_kernel_symbol *Model0_gpl_future_syms;
 const unsigned long *Model0_gpl_future_crcs;
 unsigned int Model0_num_gpl_future_syms;

 /* Exception table */
 unsigned int Model0_num_exentries;
 struct Model0_exception_table_entry *Model0_extable;

 /* Startup function. */
 int (*Model0_init)(void);

 /* Core layout: rbtree is accessed frequently, so keep together. */
 struct Model0_module_layout Model0_core_layout __attribute__((__aligned__((1 << (6)))));
 struct Model0_module_layout Model0_init_layout;

 /* Arch-specific module values */
 struct Model0_mod_arch_specific Model0_arch;

 unsigned int Model0_taints; /* same bits as kernel:tainted */


 /* Support for BUG */
 unsigned Model0_num_bugs;
 struct Model0_list_head Model0_bug_list;
 struct Model0_bug_entry *Model0_bug_table;



 /* Protected by RCU and/or module_mutex: use rcu_dereference() */
 struct Model0_mod_kallsyms *Model0_kallsyms;
 struct Model0_mod_kallsyms Model0_core_kallsyms;

 /* Section attributes */
 struct Model0_module_sect_attrs *Model0_sect_attrs;

 /* Notes attributes */
 struct Model0_module_notes_attrs *Model0_notes_attrs;


 /* The command line arguments (may be mangled).  People like
	   keeping pointers to this stuff */
 char *Model0_args;


 /* Per-cpu data. */
 void *Model0_percpu;
 unsigned int Model0_percpu_size;



 unsigned int Model0_num_tracepoints;
 struct Model0_tracepoint * const *Model0_tracepoints_ptrs;






 unsigned int Model0_num_trace_bprintk_fmt;
 const char **Model0_trace_bprintk_fmt_start;


 struct Model0_trace_event_call **Model0_trace_events;
 unsigned int Model0_num_trace_events;
 struct Model0_trace_enum_map **Model0_trace_enums;
 unsigned int Model0_num_trace_enums;
 /* What modules depend on me? */
 struct Model0_list_head Model0_source_list;
 /* What modules do I depend on? */
 struct Model0_list_head Model0_target_list;

 /* Destruction function. */
 void (*Model0_exit)(void);

 Model0_atomic_t Model0_refcnt;







} __attribute__((__aligned__((1 << (6)))));




extern struct Model0_mutex Model0_module_mutex;

/* FIXME: It'd be nice to isolate modules during init, too, so they
   aren't used before they (may) fail.  But presently too much code
   (IDE & SCSI) require entry into the module during init.*/
static inline __attribute__((no_instrument_function)) int Model0_module_is_live(struct Model0_module *Model0_mod)
{
 return Model0_mod->Model0_state != Model0_MODULE_STATE_GOING;
}

struct Model0_module *Model0___module_text_address(unsigned long Model0_addr);
struct Model0_module *Model0___module_address(unsigned long Model0_addr);
bool Model0_is_module_address(unsigned long Model0_addr);
bool Model0_is_module_percpu_address(unsigned long Model0_addr);
bool Model0_is_module_text_address(unsigned long Model0_addr);

static inline __attribute__((no_instrument_function)) bool Model0_within_module_core(unsigned long Model0_addr,
          const struct Model0_module *Model0_mod)
{
 return (unsigned long)Model0_mod->Model0_core_layout.Model0_base <= Model0_addr &&
        Model0_addr < (unsigned long)Model0_mod->Model0_core_layout.Model0_base + Model0_mod->Model0_core_layout.Model0_size;
}

static inline __attribute__((no_instrument_function)) bool Model0_within_module_init(unsigned long Model0_addr,
          const struct Model0_module *Model0_mod)
{
 return (unsigned long)Model0_mod->Model0_init_layout.Model0_base <= Model0_addr &&
        Model0_addr < (unsigned long)Model0_mod->Model0_init_layout.Model0_base + Model0_mod->Model0_init_layout.Model0_size;
}

static inline __attribute__((no_instrument_function)) bool Model0_within_module(unsigned long Model0_addr, const struct Model0_module *Model0_mod)
{
 return Model0_within_module_init(Model0_addr, Model0_mod) || Model0_within_module_core(Model0_addr, Model0_mod);
}

/* Search for module by name: must hold module_mutex. */
struct Model0_module *Model0_find_module(const char *Model0_name);

struct Model0_symsearch {
 const struct Model0_kernel_symbol *Model0_start, *Model0_stop;
 const unsigned long *Model0_crcs;
 enum {
  Model0_NOT_GPL_ONLY,
  Model0_GPL_ONLY,
  Model0_WILL_BE_GPL_ONLY,
 } Model0_licence;
 bool unused;
};

/*
 * Search for an exported symbol by name.
 *
 * Must be called with module_mutex held or preemption disabled.
 */
const struct Model0_kernel_symbol *Model0_find_symbol(const char *Model0_name,
     struct Model0_module **Model0_owner,
     const unsigned long **Model0_crc,
     bool Model0_gplok,
     bool Model0_warn);

/*
 * Walk the exported symbol table
 *
 * Must be called with module_mutex held or preemption disabled.
 */
bool Model0_each_symbol_section(bool (*Model0_fn)(const struct Model0_symsearch *Model0_arr,
        struct Model0_module *Model0_owner,
        void *Model0_data), void *Model0_data);

/* Returns 0 and fills in value, defined and namebuf, or -ERANGE if
   symnum out of range. */
int Model0_module_get_kallsym(unsigned int Model0_symnum, unsigned long *Model0_value, char *Model0_type,
   char *Model0_name, char *Model0_module_name, int *Model0_exported);

/* Look for this name: can be of form module:name. */
unsigned long Model0_module_kallsyms_lookup_name(const char *Model0_name);

int Model0_module_kallsyms_on_each_symbol(int (*Model0_fn)(void *, const char *,
          struct Model0_module *, unsigned long),
       void *Model0_data);

extern void __attribute__((noreturn)) Model0___module_put_and_exit(struct Model0_module *Model0_mod,
   long Model0_code);



int Model0_module_refcount(struct Model0_module *Model0_mod);
void Model0___symbol_put(const char *Model0_symbol);

void Model0_symbol_put_addr(void *Model0_addr);

/* Sometimes we know we already have a refcount, and it's easier not
   to handle the error case (which only happens with rmmod --wait). */
extern void Model0___module_get(struct Model0_module *Model0_module);

/* This is the Right Way to get a module: if it fails, it's being removed,
 * so pretend it's not there. */
extern bool Model0_try_module_get(struct Model0_module *Model0_module);

extern void Model0_module_put(struct Model0_module *Model0_module);
int Model0_ref_module(struct Model0_module *Model0_a, struct Model0_module *Model0_b);

/* This is a #define so the string doesn't get put in every .o file */






/* For kallsyms to ask for address resolution.  namebuf should be at
 * least KSYM_NAME_LEN long: a pointer to namebuf is returned if
 * found, otherwise NULL. */
const char *Model0_module_address_lookup(unsigned long Model0_addr,
       unsigned long *Model0_symbolsize,
       unsigned long *Model0_offset,
       char **Model0_modname,
       char *Model0_namebuf);
int Model0_lookup_module_symbol_name(unsigned long Model0_addr, char *Model0_symname);
int Model0_lookup_module_symbol_attrs(unsigned long Model0_addr, unsigned long *Model0_size, unsigned long *Model0_offset, char *Model0_modname, char *Model0_name);

int Model0_register_module_notifier(struct Model0_notifier_block *Model0_nb);
int Model0_unregister_module_notifier(struct Model0_notifier_block *Model0_nb);

extern void Model0_print_modules(void);

static inline __attribute__((no_instrument_function)) bool Model0_module_requested_async_probing(struct Model0_module *Model0_module)
{
 return Model0_module && Model0_module->Model0_async_probe_requested;
}







static inline __attribute__((no_instrument_function)) bool Model0_is_livepatch_module(struct Model0_module *Model0_mod)
{
 return false;
}
extern struct Model0_kset *Model0_module_kset;
extern struct Model0_kobj_type Model0_module_ktype;
extern int Model0_module_sysfs_initialized;




/* BELOW HERE ALL THESE ARE OBSOLETE AND WILL VANISH */
static inline __attribute__((no_instrument_function)) void Model0_set_all_modules_text_rw(void) { }
static inline __attribute__((no_instrument_function)) void Model0_set_all_modules_text_ro(void) { }
static inline __attribute__((no_instrument_function)) void Model0_module_enable_ro(const struct Model0_module *Model0_mod, bool Model0_after_init) { }
static inline __attribute__((no_instrument_function)) void Model0_module_disable_ro(const struct Model0_module *Model0_mod) { }



void Model0_module_bug_finalize(const Model0_Elf64_Ehdr *, const Model0_Elf64_Shdr *,
    struct Model0_module *);
void Model0_module_bug_cleanup(struct Model0_module *);
static inline __attribute__((no_instrument_function)) bool Model0_module_sig_ok(struct Model0_module *Model0_module)
{
 return true;
}


/*
 *  Generic cache management functions. Everything is arch-specific,  
 *  but this header exists to make sure the defines/functions can be
 *  used in a generic way.
 *
 *  2000-11-13  Arjan van de Ven   <arjan@fenrus.demon.nl>
 *
 */
/*
	prefetch(x) attempts to pre-emptively get the memory pointed to
	by address "x" into the CPU L1 cache. 
	prefetch(x) should not cause any kind of exception, prefetch(0) is
	specifically ok.

	prefetch() should be defined by the architecture, if not, the 
	#define below provides a no-op define.	
	
	There are 3 prefetch() macros:
	
	prefetch(x)  	- prefetches the cacheline at "x" for read
	prefetchw(x)	- prefetches the cacheline at "x" for write
	spin_lock_prefetch(x) - prefetches the spinlock *x for taking
	
	there is also PREFETCH_STRIDE which is the architecure-preferred 
	"lookahead" size for prefetching streamed operations.
	
*/
static inline __attribute__((no_instrument_function)) void Model0_prefetch_range(void *Model0_addr, Model0_size_t Model0_len)
{







}
/*
 * net/dst.h	Protocol independent destination cache definitions.
 *
 * Authors:	Alexey Kuznetsov, <kuznet@ms2.inr.ac.ru>
 *
 */










/*
 * A simple "approximate counter" for use in ext2 and ext3 superblocks.
 *
 * WARNING: these things are HUGE.  4 kbytes per counter on 32-way P4.
 */
struct Model0_percpu_counter {
 Model0_raw_spinlock_t Model0_lock;
 Model0_s64 Model0_count;

 struct Model0_list_head Model0_list; /* All percpu_counters are on a list */

 Model0_s32 *Model0_counters;
};

extern int Model0_percpu_counter_batch;

int Model0___percpu_counter_init(struct Model0_percpu_counter *Model0_fbc, Model0_s64 Model0_amount, Model0_gfp_t Model0_gfp,
     struct Model0_lock_class_key *Model0_key);
void Model0_percpu_counter_destroy(struct Model0_percpu_counter *Model0_fbc);
void Model0_percpu_counter_set(struct Model0_percpu_counter *Model0_fbc, Model0_s64 Model0_amount);
void Model0___percpu_counter_add(struct Model0_percpu_counter *Model0_fbc, Model0_s64 Model0_amount, Model0_s32 Model0_batch);
Model0_s64 Model0___percpu_counter_sum(struct Model0_percpu_counter *Model0_fbc);
int Model0___percpu_counter_compare(struct Model0_percpu_counter *Model0_fbc, Model0_s64 Model0_rhs, Model0_s32 Model0_batch);

static inline __attribute__((no_instrument_function)) int Model0_percpu_counter_compare(struct Model0_percpu_counter *Model0_fbc, Model0_s64 Model0_rhs)
{
 return Model0___percpu_counter_compare(Model0_fbc, Model0_rhs, Model0_percpu_counter_batch);
}

static inline __attribute__((no_instrument_function)) void Model0_percpu_counter_add(struct Model0_percpu_counter *Model0_fbc, Model0_s64 Model0_amount)
{
#if CY_ABSTRACT1/* !CONFIG_SMP */
    //preempt_disable();
    Model0_fbc->Model0_count += Model0_amount;
    //preempt_enable();
#else
 Model0___percpu_counter_add(Model0_fbc, Model0_amount, Model0_percpu_counter_batch);
#endif
}

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_percpu_counter_sum_positive(struct Model0_percpu_counter *Model0_fbc)
{
 Model0_s64 Model0_ret = Model0___percpu_counter_sum(Model0_fbc);
 return Model0_ret < 0 ? 0 : Model0_ret;
}

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_percpu_counter_sum(struct Model0_percpu_counter *Model0_fbc)
{
 return Model0___percpu_counter_sum(Model0_fbc);
}

static inline __attribute__((no_instrument_function)) Model0_s64 Model0_percpu_counter_read(struct Model0_percpu_counter *Model0_fbc)
{
 return Model0_fbc->Model0_count;
}

/*
 * It is possible for the percpu_counter_read() to return a small negative
 * number for some counter which should never be negative.
 *
 */
static inline __attribute__((no_instrument_function)) Model0_s64 Model0_percpu_counter_read_positive(struct Model0_percpu_counter *Model0_fbc)
{
 Model0_s64 Model0_ret = Model0_fbc->Model0_count;

 __asm__ __volatile__("": : :"memory"); /* Prevent reloads of fbc->count */
 if (Model0_ret >= 0)
  return Model0_ret;
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_percpu_counter_initialized(struct Model0_percpu_counter *Model0_fbc)
{
 return (Model0_fbc->Model0_counters != ((void *)0));
}
static inline __attribute__((no_instrument_function)) void Model0_percpu_counter_inc(struct Model0_percpu_counter *Model0_fbc)
{
 Model0_percpu_counter_add(Model0_fbc, 1);
}

static inline __attribute__((no_instrument_function)) void Model0_percpu_counter_dec(struct Model0_percpu_counter *Model0_fbc)
{
 Model0_percpu_counter_add(Model0_fbc, -1);
}

static inline __attribute__((no_instrument_function)) void Model0_percpu_counter_sub(struct Model0_percpu_counter *Model0_fbc, Model0_s64 Model0_amount)
{
 Model0_percpu_counter_add(Model0_fbc, -Model0_amount);
}


struct Model0_dst_entry;
struct Model0_kmem_cachep;
struct Model0_net_device;
struct Model0_sk_buff;
struct Model0_sock;
struct Model0_net;

struct Model0_dst_ops {
 unsigned short Model0_family;
 unsigned int Model0_gc_thresh;

 int (*Model0_gc)(struct Model0_dst_ops *Model0_ops);
 struct Model0_dst_entry * (*Model0_check)(struct Model0_dst_entry *, __u32 Model0_cookie);
 unsigned int (*Model0_default_advmss)(const struct Model0_dst_entry *);
 unsigned int (*Model0_mtu)(const struct Model0_dst_entry *);
 Model0_u32 * (*Model0_cow_metrics)(struct Model0_dst_entry *, unsigned long);
 void (*Model0_destroy)(struct Model0_dst_entry *);
 void (*Model0_ifdown)(struct Model0_dst_entry *,
       struct Model0_net_device *Model0_dev, int Model0_how);
 struct Model0_dst_entry * (*Model0_negative_advice)(struct Model0_dst_entry *);
 void (*Model0_link_failure)(struct Model0_sk_buff *);
 void (*Model0_update_pmtu)(struct Model0_dst_entry *Model0_dst, struct Model0_sock *Model0_sk,
            struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_mtu);
 void (*Model0_redirect)(struct Model0_dst_entry *Model0_dst, struct Model0_sock *Model0_sk,
         struct Model0_sk_buff *Model0_skb);
 int (*Model0_local_out)(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
 struct Model0_neighbour * (*Model0_neigh_lookup)(const struct Model0_dst_entry *Model0_dst,
      struct Model0_sk_buff *Model0_skb,
      const void *Model0_daddr);

 struct Model0_kmem_cache *Model0_kmem_cachep;

 struct Model0_percpu_counter Model0_pcpuc_entries __attribute__((__aligned__((1 << (6)))));
};

static inline __attribute__((no_instrument_function)) int Model0_dst_entries_get_fast(struct Model0_dst_ops *Model0_dst)
{
 return Model0_percpu_counter_read_positive(&Model0_dst->Model0_pcpuc_entries);
}

static inline __attribute__((no_instrument_function)) int Model0_dst_entries_get_slow(struct Model0_dst_ops *Model0_dst)
{
 int Model0_res;

 Model0_local_bh_disable();
 Model0_res = Model0_percpu_counter_sum_positive(&Model0_dst->Model0_pcpuc_entries);
 Model0_local_bh_enable();
 return Model0_res;
}

static inline __attribute__((no_instrument_function)) void Model0_dst_entries_add(struct Model0_dst_ops *Model0_dst, int Model0_val)
{
 Model0_local_bh_disable();
 Model0_percpu_counter_add(&Model0_dst->Model0_pcpuc_entries, Model0_val);
 Model0_local_bh_enable();
}

static inline __attribute__((no_instrument_function)) int Model0_dst_entries_init(struct Model0_dst_ops *Model0_dst)
{
 return ({ static struct Model0_lock_class_key Model0___key; Model0___percpu_counter_init(&Model0_dst->Model0_pcpuc_entries, 0, ((( Model0_gfp_t)(0x400000u|0x2000000u)) | (( Model0_gfp_t)0x40u) | (( Model0_gfp_t)0x80u)), &Model0___key); });
}

static inline __attribute__((no_instrument_function)) void Model0_dst_entries_destroy(struct Model0_dst_ops *Model0_dst)
{
 Model0_percpu_counter_destroy(&Model0_dst->Model0_pcpuc_entries);
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the Interfaces handler.
 *
 * Version:	@(#)dev.h	1.0.10	08/12/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Corey Minyard <wf-rch!minyard@relay.EU.net>
 *		Donald J. Becker, <becker@cesdis.gsfc.nasa.gov>
 *		Alan Cox, <alan@lxorguk.ukuu.org.uk>
 *		Bjorn Ekwall. <bj0rn@blox.se>
 *              Pekka Riikonen <priikone@poseidon.pspt.fi>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 *
 *		Moved to /usr/include/linux for NET3
 */









/*
 * Copyright (C) 1993 Linus Torvalds
 *
 * Delay routines, using a pre-computed "loops_per_jiffy" value.
 */



extern unsigned long Model0_loops_per_jiffy;








/* Undefined functions to get compile-time errors */
extern void Model0___bad_udelay(void);
extern void Model0___bad_ndelay(void);

extern void Model0___udelay(unsigned long Model0_usecs);
extern void Model0___ndelay(unsigned long Model0_nsecs);
extern void Model0___const_udelay(unsigned long Model0_xloops);
extern void Model0___delay(unsigned long Model0_loops);

/*
 * The weird n/20000 thing suppresses a "comparison is always false due to
 * limited range of data type" warning with non-const 8-bit arguments.
 */

/* 0x10c7 is 2**32 / 1000000 (rounded up) */
/* 0x5 is 2**32 / 1000000000 (rounded up) */

void Model0_use_tsc_delay(void);
void Model0_use_mwaitx_delay(void);

/*
 * Using udelay() for intervals greater than a few milliseconds can
 * risk overflow for high loops_per_jiffy (high bogomips) machines. The
 * mdelay() provides a wrapper to prevent this.  For delays greater
 * than MAX_UDELAY_MS milliseconds, the wrapper is used.  Architecture
 * specific values can be defined in asm-???/delay.h as an override.
 * The 2nd mdelay() definition ensures GCC will optimize away the 
 * while loop for the common cases where n <= MAX_UDELAY_MS  --  Paul G.
 */
extern unsigned long Model0_lpj_fine;
void Model0_calibrate_delay(void);
void Model0_msleep(unsigned int Model0_msecs);
unsigned long Model0_msleep_interruptible(unsigned int Model0_msecs);
void Model0_usleep_range(unsigned long Model0_min, unsigned long Model0_max);

static inline __attribute__((no_instrument_function)) void Model0_ssleep(unsigned int Model0_seconds)
{
 Model0_msleep(Model0_seconds * 1000);
}







/*
 * Copyright(c) 2004 - 2006 Intel Corporation. All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 * Software Foundation; either version 2 of the License, or (at your option)
 * any later version.
 *
 * This program is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
 * more details.
 *
 * The full GNU General Public License is included in this distribution in the
 * file called COPYING.
 */




/*
 * device.h - generic, centralized driver model
 *
 * Copyright (c) 2001-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (c) 2004-2009 Greg Kroah-Hartman <gregkh@suse.de>
 * Copyright (c) 2008-2009 Novell Inc.
 *
 * This file is released under the GPLv2
 *
 * See Documentation/driver-model/ for more information.
 */







/*
 *	klist.h - Some generic list helpers, extending struct list_head a bit.
 *
 *	Implementations are found in lib/klist.c
 *
 *
 *	Copyright (C) 2005 Patrick Mochel
 *
 *	This file is rleased under the GPL v2.
 */
struct Model0_klist_node;
struct Model0_klist {
 Model0_spinlock_t Model0_k_lock;
 struct Model0_list_head Model0_k_list;
 void (*Model0_get)(struct Model0_klist_node *);
 void (*Model0_put)(struct Model0_klist_node *);
} __attribute__ ((aligned (sizeof(void *))));
extern void Model0_klist_init(struct Model0_klist *Model0_k, void (*Model0_get)(struct Model0_klist_node *),
         void (*Model0_put)(struct Model0_klist_node *));

struct Model0_klist_node {
 void *Model0_n_klist; /* never access directly */
 struct Model0_list_head Model0_n_node;
 struct Model0_kref Model0_n_ref;
};

extern void Model0_klist_add_tail(struct Model0_klist_node *Model0_n, struct Model0_klist *Model0_k);
extern void Model0_klist_add_head(struct Model0_klist_node *Model0_n, struct Model0_klist *Model0_k);
extern void Model0_klist_add_behind(struct Model0_klist_node *Model0_n, struct Model0_klist_node *Model0_pos);
extern void Model0_klist_add_before(struct Model0_klist_node *Model0_n, struct Model0_klist_node *Model0_pos);

extern void Model0_klist_del(struct Model0_klist_node *Model0_n);
extern void Model0_klist_remove(struct Model0_klist_node *Model0_n);

extern int Model0_klist_node_attached(struct Model0_klist_node *Model0_n);


struct Model0_klist_iter {
 struct Model0_klist *Model0_i_klist;
 struct Model0_klist_node *Model0_i_cur;
};


extern void Model0_klist_iter_init(struct Model0_klist *Model0_k, struct Model0_klist_iter *Model0_i);
extern void Model0_klist_iter_init_node(struct Model0_klist *Model0_k, struct Model0_klist_iter *Model0_i,
     struct Model0_klist_node *Model0_n);
extern void Model0_klist_iter_exit(struct Model0_klist_iter *Model0_i);
extern struct Model0_klist_node *Model0_klist_prev(struct Model0_klist_iter *Model0_i);
extern struct Model0_klist_node *Model0_klist_next(struct Model0_klist_iter *Model0_i);





/*
 * Per-device information from the pin control system.
 * This is the stuff that get included into the device
 * core.
 *
 * Copyright (C) 2012 ST-Ericsson SA
 * Written on behalf of Linaro for ST-Ericsson
 * This interface is used in the core to keep track of pins.
 *
 * Author: Linus Walleij <linus.walleij@linaro.org>
 *
 * License terms: GNU General Public License (GPL) version 2
 */
/* Stubs if we're not using pinctrl */

static inline __attribute__((no_instrument_function)) int Model0_pinctrl_bind_pins(struct Model0_device *Model0_dev)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_pinctrl_init_done(struct Model0_device *Model0_dev)
{
 return 0;
}


/* issue num suppressed message on exit */


struct Model0_ratelimit_state {
 Model0_raw_spinlock_t Model0_lock; /* protect the state */

 int Model0_interval;
 int Model0_burst;
 int Model0_printed;
 int Model0_missed;
 unsigned long Model0_begin;
 unsigned long Model0_flags;
};
static inline __attribute__((no_instrument_function)) void Model0_ratelimit_state_init(struct Model0_ratelimit_state *Model0_rs,
     int Model0_interval, int Model0_burst)
{
 memset(Model0_rs, 0, sizeof(*Model0_rs));

 do { *(&Model0_rs->Model0_lock) = (Model0_raw_spinlock_t) { .Model0_raw_lock = { { (0) } }, }; } while (0);
 Model0_rs->Model0_interval = Model0_interval;
 Model0_rs->Model0_burst = Model0_burst;
}

static inline __attribute__((no_instrument_function)) void Model0_ratelimit_default_init(struct Model0_ratelimit_state *Model0_rs)
{
 return Model0_ratelimit_state_init(Model0_rs, (5 * 1000),
     10);
}

static inline __attribute__((no_instrument_function)) void Model0_ratelimit_state_exit(struct Model0_ratelimit_state *Model0_rs)
{
 if (!(Model0_rs->Model0_flags & (1UL << (0))))
  return;

 if (Model0_rs->Model0_missed) {
  Model0_printk("\001" "4" "TCP: " "%s: %d output lines suppressed due to ratelimiting\n", Model0_get_current()->Model0_comm, Model0_rs->Model0_missed);

  Model0_rs->Model0_missed = 0;
 }
}

static inline __attribute__((no_instrument_function)) void
Model0_ratelimit_set_flags(struct Model0_ratelimit_state *Model0_rs, unsigned long Model0_flags)
{
 Model0_rs->Model0_flags = Model0_flags;
}

extern struct Model0_ratelimit_state Model0_printk_ratelimit_state;

extern int Model0____ratelimit(struct Model0_ratelimit_state *Model0_rs, const char *func);





struct Model0_dev_archdata {

 struct Model0_dma_map_ops *Model0_dma_ops;


 void *Model0_iommu; /* hook for IOMMU specific extension */

};


struct Model0_dma_domain {
 struct Model0_list_head Model0_node;
 struct Model0_dma_map_ops *Model0_dma_ops;
 int Model0_domain_nr;
};
void Model0_add_dma_domain(struct Model0_dma_domain *Model0_domain);
void Model0_del_dma_domain(struct Model0_dma_domain *Model0_domain);


struct Model0_pdev_archdata {
};

struct Model0_device;
struct Model0_device_private;
struct Model0_device_driver;
struct Model0_driver_private;
struct Model0_module;
struct Model0_class;
struct Model0_subsys_private;
struct Model0_bus_type;
struct Model0_device_node;
struct Model0_fwnode_handle;
struct Model0_iommu_ops;
struct Model0_iommu_group;

struct Model0_bus_attribute {
 struct Model0_attribute Model0_attr;
 Model0_ssize_t (*Model0_show)(struct Model0_bus_type *Model0_bus, char *Model0_buf);
 Model0_ssize_t (*Model0_store)(struct Model0_bus_type *Model0_bus, const char *Model0_buf, Model0_size_t Model0_count);
};
extern int __attribute__((warn_unused_result)) Model0_bus_create_file(struct Model0_bus_type *,
     struct Model0_bus_attribute *);
extern void Model0_bus_remove_file(struct Model0_bus_type *, struct Model0_bus_attribute *);

/**
 * struct bus_type - The bus type of the device
 *
 * @name:	The name of the bus.
 * @dev_name:	Used for subsystems to enumerate devices like ("foo%u", dev->id).
 * @dev_root:	Default device to use as the parent.
 * @dev_attrs:	Default attributes of the devices on the bus.
 * @bus_groups:	Default attributes of the bus.
 * @dev_groups:	Default attributes of the devices on the bus.
 * @drv_groups: Default attributes of the device drivers on the bus.
 * @match:	Called, perhaps multiple times, whenever a new device or driver
 *		is added for this bus. It should return a positive value if the
 *		given device can be handled by the given driver and zero
 *		otherwise. It may also return error code if determining that
 *		the driver supports the device is not possible. In case of
 *		-EPROBE_DEFER it will queue the device for deferred probing.
 * @uevent:	Called when a device is added, removed, or a few other things
 *		that generate uevents to add the environment variables.
 * @probe:	Called when a new device or driver add to this bus, and callback
 *		the specific driver's probe to initial the matched device.
 * @remove:	Called when a device removed from this bus.
 * @shutdown:	Called at shut-down time to quiesce the device.
 *
 * @online:	Called to put the device back online (after offlining it).
 * @offline:	Called to put the device offline for hot-removal. May fail.
 *
 * @suspend:	Called when a device on this bus wants to go to sleep mode.
 * @resume:	Called to bring a device on this bus out of sleep mode.
 * @pm:		Power management operations of this bus, callback the specific
 *		device driver's pm-ops.
 * @iommu_ops:  IOMMU specific operations for this bus, used to attach IOMMU
 *              driver implementations to a bus and allow the driver to do
 *              bus-specific setup
 * @p:		The private data of the driver core, only the driver core can
 *		touch this.
 * @lock_key:	Lock class key for use by the lock validator
 *
 * A bus is a channel between the processor and one or more devices. For the
 * purposes of the device model, all devices are connected via a bus, even if
 * it is an internal, virtual, "platform" bus. Buses can plug into each other.
 * A USB controller is usually a PCI device, for example. The device model
 * represents the actual connections between buses and the devices they control.
 * A bus is represented by the bus_type structure. It contains the name, the
 * default attributes, the bus' methods, PM operations, and the driver core's
 * private data.
 */
struct Model0_bus_type {
 const char *Model0_name;
 const char *Model0_dev_name;
 struct Model0_device *Model0_dev_root;
 struct Model0_device_attribute *Model0_dev_attrs; /* use dev_groups instead */
 const struct Model0_attribute_group **Model0_bus_groups;
 const struct Model0_attribute_group **Model0_dev_groups;
 const struct Model0_attribute_group **Model0_drv_groups;

 int (*Model0_match)(struct Model0_device *Model0_dev, struct Model0_device_driver *Model0_drv);
 int (*Model0_uevent)(struct Model0_device *Model0_dev, struct Model0_kobj_uevent_env *Model0_env);
 int (*Model0_probe)(struct Model0_device *Model0_dev);
 int (*Model0_remove)(struct Model0_device *Model0_dev);
 void (*Model0_shutdown)(struct Model0_device *Model0_dev);

 int (*Model0_online)(struct Model0_device *Model0_dev);
 int (*Model0_offline)(struct Model0_device *Model0_dev);

 int (*Model0_suspend)(struct Model0_device *Model0_dev, Model0_pm_message_t Model0_state);
 int (*Model0_resume)(struct Model0_device *Model0_dev);

 const struct Model0_dev_pm_ops *Model0_pm;

 const struct Model0_iommu_ops *Model0_iommu_ops;

 struct Model0_subsys_private *Model0_p;
 struct Model0_lock_class_key Model0_lock_key;
};

extern int __attribute__((warn_unused_result)) Model0_bus_register(struct Model0_bus_type *Model0_bus);

extern void Model0_bus_unregister(struct Model0_bus_type *Model0_bus);

extern int __attribute__((warn_unused_result)) Model0_bus_rescan_devices(struct Model0_bus_type *Model0_bus);

/* iterator helpers for buses */
struct Model0_subsys_dev_iter {
 struct Model0_klist_iter Model0_ki;
 const struct Model0_device_type *Model0_type;
};
void Model0_subsys_dev_iter_init(struct Model0_subsys_dev_iter *Model0_iter,
    struct Model0_bus_type *Model0_subsys,
    struct Model0_device *Model0_start,
    const struct Model0_device_type *Model0_type);
struct Model0_device *Model0_subsys_dev_iter_next(struct Model0_subsys_dev_iter *Model0_iter);
void Model0_subsys_dev_iter_exit(struct Model0_subsys_dev_iter *Model0_iter);

int Model0_bus_for_each_dev(struct Model0_bus_type *Model0_bus, struct Model0_device *Model0_start, void *Model0_data,
       int (*Model0_fn)(struct Model0_device *Model0_dev, void *Model0_data));
struct Model0_device *Model0_bus_find_device(struct Model0_bus_type *Model0_bus, struct Model0_device *Model0_start,
          void *Model0_data,
          int (*Model0_match)(struct Model0_device *Model0_dev, void *Model0_data));
struct Model0_device *Model0_bus_find_device_by_name(struct Model0_bus_type *Model0_bus,
           struct Model0_device *Model0_start,
           const char *Model0_name);
struct Model0_device *Model0_subsys_find_device_by_id(struct Model0_bus_type *Model0_bus, unsigned int Model0_id,
     struct Model0_device *Model0_hint);
int Model0_bus_for_each_drv(struct Model0_bus_type *Model0_bus, struct Model0_device_driver *Model0_start,
       void *Model0_data, int (*Model0_fn)(struct Model0_device_driver *, void *));
void Model0_bus_sort_breadthfirst(struct Model0_bus_type *Model0_bus,
      int (*Model0_compare)(const struct Model0_device *Model0_a,
       const struct Model0_device *Model0_b));
/*
 * Bus notifiers: Get notified of addition/removal of devices
 * and binding/unbinding of drivers to devices.
 * In the long run, it should be a replacement for the platform
 * notify hooks.
 */
struct Model0_notifier_block;

extern int Model0_bus_register_notifier(struct Model0_bus_type *Model0_bus,
     struct Model0_notifier_block *Model0_nb);
extern int Model0_bus_unregister_notifier(struct Model0_bus_type *Model0_bus,
       struct Model0_notifier_block *Model0_nb);

/* All 4 notifers below get called with the target struct device *
 * as an argument. Note that those functions are likely to be called
 * with the device lock held in the core, so be careful.
 */
extern struct Model0_kset *Model0_bus_get_kset(struct Model0_bus_type *Model0_bus);
extern struct Model0_klist *Model0_bus_get_device_klist(struct Model0_bus_type *Model0_bus);

/**
 * enum probe_type - device driver probe type to try
 *	Device drivers may opt in for special handling of their
 *	respective probe routines. This tells the core what to
 *	expect and prefer.
 *
 * @PROBE_DEFAULT_STRATEGY: Used by drivers that work equally well
 *	whether probed synchronously or asynchronously.
 * @PROBE_PREFER_ASYNCHRONOUS: Drivers for "slow" devices which
 *	probing order is not essential for booting the system may
 *	opt into executing their probes asynchronously.
 * @PROBE_FORCE_SYNCHRONOUS: Use this to annotate drivers that need
 *	their probe routines to run synchronously with driver and
 *	device registration (with the exception of -EPROBE_DEFER
 *	handling - re-probing always ends up being done asynchronously).
 *
 * Note that the end goal is to switch the kernel to use asynchronous
 * probing by default, so annotating drivers with
 * %PROBE_PREFER_ASYNCHRONOUS is a temporary measure that allows us
 * to speed up boot process while we are validating the rest of the
 * drivers.
 */
enum Model0_probe_type {
 Model0_PROBE_DEFAULT_STRATEGY,
 Model0_PROBE_PREFER_ASYNCHRONOUS,
 Model0_PROBE_FORCE_SYNCHRONOUS,
};

/**
 * struct device_driver - The basic device driver structure
 * @name:	Name of the device driver.
 * @bus:	The bus which the device of this driver belongs to.
 * @owner:	The module owner.
 * @mod_name:	Used for built-in modules.
 * @suppress_bind_attrs: Disables bind/unbind via sysfs.
 * @probe_type:	Type of the probe (synchronous or asynchronous) to use.
 * @of_match_table: The open firmware table.
 * @acpi_match_table: The ACPI match table.
 * @probe:	Called to query the existence of a specific device,
 *		whether this driver can work with it, and bind the driver
 *		to a specific device.
 * @remove:	Called when the device is removed from the system to
 *		unbind a device from this driver.
 * @shutdown:	Called at shut-down time to quiesce the device.
 * @suspend:	Called to put the device to sleep mode. Usually to a
 *		low power state.
 * @resume:	Called to bring a device from sleep mode.
 * @groups:	Default attributes that get created by the driver core
 *		automatically.
 * @pm:		Power management operations of the device which matched
 *		this driver.
 * @p:		Driver core's private data, no one other than the driver
 *		core can touch this.
 *
 * The device driver-model tracks all of the drivers known to the system.
 * The main reason for this tracking is to enable the driver core to match
 * up drivers with new devices. Once drivers are known objects within the
 * system, however, a number of other things become possible. Device drivers
 * can export information and configuration variables that are independent
 * of any specific device.
 */
struct Model0_device_driver {
 const char *Model0_name;
 struct Model0_bus_type *Model0_bus;

 struct Model0_module *Model0_owner;
 const char *Model0_mod_name; /* used for built-in modules */

 bool Model0_suppress_bind_attrs; /* disables bind/unbind via sysfs */
 enum Model0_probe_type Model0_probe_type;

 const struct Model0_of_device_id *Model0_of_match_table;
 const struct Model0_acpi_device_id *Model0_acpi_match_table;

 int (*Model0_probe) (struct Model0_device *Model0_dev);
 int (*Model0_remove) (struct Model0_device *Model0_dev);
 void (*Model0_shutdown) (struct Model0_device *Model0_dev);
 int (*Model0_suspend) (struct Model0_device *Model0_dev, Model0_pm_message_t Model0_state);
 int (*Model0_resume) (struct Model0_device *Model0_dev);
 const struct Model0_attribute_group **Model0_groups;

 const struct Model0_dev_pm_ops *Model0_pm;

 struct Model0_driver_private *Model0_p;
};


extern int __attribute__((warn_unused_result)) Model0_driver_register(struct Model0_device_driver *Model0_drv);
extern void Model0_driver_unregister(struct Model0_device_driver *Model0_drv);

extern struct Model0_device_driver *Model0_driver_find(const char *Model0_name,
      struct Model0_bus_type *Model0_bus);
extern int Model0_driver_probe_done(void);
extern void Model0_wait_for_device_probe(void);


/* sysfs interface for exporting driver attributes */

struct Model0_driver_attribute {
 struct Model0_attribute Model0_attr;
 Model0_ssize_t (*Model0_show)(struct Model0_device_driver *Model0_driver, char *Model0_buf);
 Model0_ssize_t (*Model0_store)(struct Model0_device_driver *Model0_driver, const char *Model0_buf,
    Model0_size_t Model0_count);
};
extern int __attribute__((warn_unused_result)) Model0_driver_create_file(struct Model0_device_driver *Model0_driver,
     const struct Model0_driver_attribute *Model0_attr);
extern void Model0_driver_remove_file(struct Model0_device_driver *Model0_driver,
          const struct Model0_driver_attribute *Model0_attr);

extern int __attribute__((warn_unused_result)) Model0_driver_for_each_device(struct Model0_device_driver *Model0_drv,
            struct Model0_device *Model0_start,
            void *Model0_data,
            int (*Model0_fn)(struct Model0_device *Model0_dev,
        void *));
struct Model0_device *Model0_driver_find_device(struct Model0_device_driver *Model0_drv,
      struct Model0_device *Model0_start, void *Model0_data,
      int (*Model0_match)(struct Model0_device *Model0_dev, void *Model0_data));

/**
 * struct subsys_interface - interfaces to device functions
 * @name:       name of the device function
 * @subsys:     subsytem of the devices to attach to
 * @node:       the list of functions registered at the subsystem
 * @add_dev:    device hookup to device function handler
 * @remove_dev: device hookup to device function handler
 *
 * Simple interfaces attached to a subsystem. Multiple interfaces can
 * attach to a subsystem and its devices. Unlike drivers, they do not
 * exclusively claim or control devices. Interfaces usually represent
 * a specific functionality of a subsystem/class of devices.
 */
struct Model0_subsys_interface {
 const char *Model0_name;
 struct Model0_bus_type *Model0_subsys;
 struct Model0_list_head Model0_node;
 int (*Model0_add_dev)(struct Model0_device *Model0_dev, struct Model0_subsys_interface *Model0_sif);
 void (*Model0_remove_dev)(struct Model0_device *Model0_dev, struct Model0_subsys_interface *Model0_sif);
};

int Model0_subsys_interface_register(struct Model0_subsys_interface *Model0_sif);
void Model0_subsys_interface_unregister(struct Model0_subsys_interface *Model0_sif);

int Model0_subsys_system_register(struct Model0_bus_type *Model0_subsys,
      const struct Model0_attribute_group **Model0_groups);
int Model0_subsys_virtual_register(struct Model0_bus_type *Model0_subsys,
       const struct Model0_attribute_group **Model0_groups);

/**
 * struct class - device classes
 * @name:	Name of the class.
 * @owner:	The module owner.
 * @class_attrs: Default attributes of this class.
 * @dev_groups:	Default attributes of the devices that belong to the class.
 * @dev_kobj:	The kobject that represents this class and links it into the hierarchy.
 * @dev_uevent:	Called when a device is added, removed from this class, or a
 *		few other things that generate uevents to add the environment
 *		variables.
 * @devnode:	Callback to provide the devtmpfs.
 * @class_release: Called to release this class.
 * @dev_release: Called to release the device.
 * @suspend:	Used to put the device to sleep mode, usually to a low power
 *		state.
 * @resume:	Used to bring the device from the sleep mode.
 * @ns_type:	Callbacks so sysfs can detemine namespaces.
 * @namespace:	Namespace of the device belongs to this class.
 * @pm:		The default device power management operations of this class.
 * @p:		The private data of the driver core, no one other than the
 *		driver core can touch this.
 *
 * A class is a higher-level view of a device that abstracts out low-level
 * implementation details. Drivers may see a SCSI disk or an ATA disk, but,
 * at the class level, they are all simply disks. Classes allow user space
 * to work with devices based on what they do, rather than how they are
 * connected or how they work.
 */
struct Model0_class {
 const char *Model0_name;
 struct Model0_module *Model0_owner;

 struct Model0_class_attribute *Model0_class_attrs;
 const struct Model0_attribute_group **Model0_dev_groups;
 struct Model0_kobject *Model0_dev_kobj;

 int (*Model0_dev_uevent)(struct Model0_device *Model0_dev, struct Model0_kobj_uevent_env *Model0_env);
 char *(*Model0_devnode)(struct Model0_device *Model0_dev, Model0_umode_t *Model0_mode);

 void (*Model0_class_release)(struct Model0_class *Model0_class);
 void (*Model0_dev_release)(struct Model0_device *Model0_dev);

 int (*Model0_suspend)(struct Model0_device *Model0_dev, Model0_pm_message_t Model0_state);
 int (*Model0_resume)(struct Model0_device *Model0_dev);

 const struct Model0_kobj_ns_type_operations *Model0_ns_type;
 const void *(*Model0_namespace)(struct Model0_device *Model0_dev);

 const struct Model0_dev_pm_ops *Model0_pm;

 struct Model0_subsys_private *Model0_p;
};

struct Model0_class_dev_iter {
 struct Model0_klist_iter Model0_ki;
 const struct Model0_device_type *Model0_type;
};

extern struct Model0_kobject *Model0_sysfs_dev_block_kobj;
extern struct Model0_kobject *Model0_sysfs_dev_char_kobj;
extern int __attribute__((warn_unused_result)) Model0___class_register(struct Model0_class *Model0_class,
      struct Model0_lock_class_key *Model0_key);
extern void Model0_class_unregister(struct Model0_class *Model0_class);

/* This is a #define to keep the compiler from merging different
 * instances of the __key variable */






struct Model0_class_compat;
struct Model0_class_compat *Model0_class_compat_register(const char *Model0_name);
void Model0_class_compat_unregister(struct Model0_class_compat *Model0_cls);
int Model0_class_compat_create_link(struct Model0_class_compat *Model0_cls, struct Model0_device *Model0_dev,
        struct Model0_device *Model0_device_link);
void Model0_class_compat_remove_link(struct Model0_class_compat *Model0_cls, struct Model0_device *Model0_dev,
         struct Model0_device *Model0_device_link);

extern void Model0_class_dev_iter_init(struct Model0_class_dev_iter *Model0_iter,
    struct Model0_class *Model0_class,
    struct Model0_device *Model0_start,
    const struct Model0_device_type *Model0_type);
extern struct Model0_device *Model0_class_dev_iter_next(struct Model0_class_dev_iter *Model0_iter);
extern void Model0_class_dev_iter_exit(struct Model0_class_dev_iter *Model0_iter);

extern int Model0_class_for_each_device(struct Model0_class *Model0_class, struct Model0_device *Model0_start,
     void *Model0_data,
     int (*Model0_fn)(struct Model0_device *Model0_dev, void *Model0_data));
extern struct Model0_device *Model0_class_find_device(struct Model0_class *Model0_class,
     struct Model0_device *Model0_start, const void *Model0_data,
     int (*Model0_match)(struct Model0_device *, const void *));

struct Model0_class_attribute {
 struct Model0_attribute Model0_attr;
 Model0_ssize_t (*Model0_show)(struct Model0_class *Model0_class, struct Model0_class_attribute *Model0_attr,
   char *Model0_buf);
 Model0_ssize_t (*Model0_store)(struct Model0_class *Model0_class, struct Model0_class_attribute *Model0_attr,
   const char *Model0_buf, Model0_size_t Model0_count);
};
extern int __attribute__((warn_unused_result)) Model0_class_create_file_ns(struct Model0_class *Model0_class,
          const struct Model0_class_attribute *Model0_attr,
          const void *Model0_ns);
extern void Model0_class_remove_file_ns(struct Model0_class *Model0_class,
     const struct Model0_class_attribute *Model0_attr,
     const void *Model0_ns);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model0_class_create_file(struct Model0_class *Model0_class,
     const struct Model0_class_attribute *Model0_attr)
{
 return Model0_class_create_file_ns(Model0_class, Model0_attr, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model0_class_remove_file(struct Model0_class *Model0_class,
         const struct Model0_class_attribute *Model0_attr)
{
 return Model0_class_remove_file_ns(Model0_class, Model0_attr, ((void *)0));
}

/* Simple class attribute that is just a static string */
struct Model0_class_attribute_string {
 struct Model0_class_attribute Model0_attr;
 char *Model0_str;
};

/* Currently read-only only */






extern Model0_ssize_t Model0_show_class_attr_string(struct Model0_class *Model0_class, struct Model0_class_attribute *Model0_attr,
                        char *Model0_buf);

struct Model0_class_interface {
 struct Model0_list_head Model0_node;
 struct Model0_class *Model0_class;

 int (*Model0_add_dev) (struct Model0_device *, struct Model0_class_interface *);
 void (*Model0_remove_dev) (struct Model0_device *, struct Model0_class_interface *);
};

extern int __attribute__((warn_unused_result)) Model0_class_interface_register(struct Model0_class_interface *);
extern void Model0_class_interface_unregister(struct Model0_class_interface *);

extern struct Model0_class * __attribute__((warn_unused_result)) Model0___class_create(struct Model0_module *Model0_owner,
        const char *Model0_name,
        struct Model0_lock_class_key *Model0_key);
extern void Model0_class_destroy(struct Model0_class *Model0_cls);

/* This is a #define to keep the compiler from merging different
 * instances of the __key variable */






/*
 * The type of device, "struct device" is embedded in. A class
 * or bus can contain devices of different types
 * like "partitions" and "disks", "mouse" and "event".
 * This identifies the device type and carries type-specific
 * information, equivalent to the kobj_type of a kobject.
 * If "name" is specified, the uevent will contain it in
 * the DEVTYPE variable.
 */
struct Model0_device_type {
 const char *Model0_name;
 const struct Model0_attribute_group **Model0_groups;
 int (*Model0_uevent)(struct Model0_device *Model0_dev, struct Model0_kobj_uevent_env *Model0_env);
 char *(*Model0_devnode)(struct Model0_device *Model0_dev, Model0_umode_t *Model0_mode,
    Model0_kuid_t *Model0_uid, Model0_kgid_t *Model0_gid);
 void (*Model0_release)(struct Model0_device *Model0_dev);

 const struct Model0_dev_pm_ops *Model0_pm;
};

/* interface for exporting device attributes */
struct Model0_device_attribute {
 struct Model0_attribute Model0_attr;
 Model0_ssize_t (*Model0_show)(struct Model0_device *Model0_dev, struct Model0_device_attribute *Model0_attr,
   char *Model0_buf);
 Model0_ssize_t (*Model0_store)(struct Model0_device *Model0_dev, struct Model0_device_attribute *Model0_attr,
    const char *Model0_buf, Model0_size_t Model0_count);
};

struct Model0_dev_ext_attribute {
 struct Model0_device_attribute Model0_attr;
 void *Model0_var;
};

Model0_ssize_t Model0_device_show_ulong(struct Model0_device *Model0_dev, struct Model0_device_attribute *Model0_attr,
     char *Model0_buf);
Model0_ssize_t Model0_device_store_ulong(struct Model0_device *Model0_dev, struct Model0_device_attribute *Model0_attr,
      const char *Model0_buf, Model0_size_t Model0_count);
Model0_ssize_t Model0_device_show_int(struct Model0_device *Model0_dev, struct Model0_device_attribute *Model0_attr,
   char *Model0_buf);
Model0_ssize_t Model0_device_store_int(struct Model0_device *Model0_dev, struct Model0_device_attribute *Model0_attr,
    const char *Model0_buf, Model0_size_t Model0_count);
Model0_ssize_t Model0_device_show_bool(struct Model0_device *Model0_dev, struct Model0_device_attribute *Model0_attr,
   char *Model0_buf);
Model0_ssize_t Model0_device_store_bool(struct Model0_device *Model0_dev, struct Model0_device_attribute *Model0_attr,
    const char *Model0_buf, Model0_size_t Model0_count);
extern int Model0_device_create_file(struct Model0_device *Model0_device,
         const struct Model0_device_attribute *Model0_entry);
extern void Model0_device_remove_file(struct Model0_device *Model0_dev,
          const struct Model0_device_attribute *Model0_attr);
extern bool Model0_device_remove_file_self(struct Model0_device *Model0_dev,
        const struct Model0_device_attribute *Model0_attr);
extern int __attribute__((warn_unused_result)) Model0_device_create_bin_file(struct Model0_device *Model0_dev,
     const struct Model0_bin_attribute *Model0_attr);
extern void Model0_device_remove_bin_file(struct Model0_device *Model0_dev,
       const struct Model0_bin_attribute *Model0_attr);

/* device resource management */
typedef void (*Model0_dr_release_t)(struct Model0_device *Model0_dev, void *Model0_res);
typedef int (*Model0_dr_match_t)(struct Model0_device *Model0_dev, void *Model0_res, void *Model0_match_data);


extern void *Model0___devres_alloc_node(Model0_dr_release_t Model0_release, Model0_size_t Model0_size, Model0_gfp_t Model0_gfp,
     int Model0_nid, const char *Model0_name) __attribute__((__malloc__));
extern void Model0_devres_for_each_res(struct Model0_device *Model0_dev, Model0_dr_release_t Model0_release,
    Model0_dr_match_t Model0_match, void *Model0_match_data,
    void (*Model0_fn)(struct Model0_device *, void *, void *),
    void *Model0_data);
extern void Model0_devres_free(void *Model0_res);
extern void Model0_devres_add(struct Model0_device *Model0_dev, void *Model0_res);
extern void *Model0_devres_find(struct Model0_device *Model0_dev, Model0_dr_release_t Model0_release,
    Model0_dr_match_t Model0_match, void *Model0_match_data);
extern void *Model0_devres_get(struct Model0_device *Model0_dev, void *Model0_new_res,
   Model0_dr_match_t Model0_match, void *Model0_match_data);
extern void *Model0_devres_remove(struct Model0_device *Model0_dev, Model0_dr_release_t Model0_release,
      Model0_dr_match_t Model0_match, void *Model0_match_data);
extern int Model0_devres_destroy(struct Model0_device *Model0_dev, Model0_dr_release_t Model0_release,
     Model0_dr_match_t Model0_match, void *Model0_match_data);
extern int Model0_devres_release(struct Model0_device *Model0_dev, Model0_dr_release_t Model0_release,
     Model0_dr_match_t Model0_match, void *Model0_match_data);

/* devres group */
extern void * __attribute__((warn_unused_result)) Model0_devres_open_group(struct Model0_device *Model0_dev, void *Model0_id,
          Model0_gfp_t Model0_gfp);
extern void Model0_devres_close_group(struct Model0_device *Model0_dev, void *Model0_id);
extern void Model0_devres_remove_group(struct Model0_device *Model0_dev, void *Model0_id);
extern int Model0_devres_release_group(struct Model0_device *Model0_dev, void *Model0_id);

/* managed devm_k.alloc/kfree for device drivers */
extern void *Model0_devm_kmalloc(struct Model0_device *Model0_dev, Model0_size_t Model0_size, Model0_gfp_t Model0_gfp) __attribute__((__malloc__));
extern __attribute__((format(printf, 3, 0)))
char *Model0_devm_kvasprintf(struct Model0_device *Model0_dev, Model0_gfp_t Model0_gfp, const char *Model0_fmt,
        Model0_va_list Model0_ap) __attribute__((__malloc__));
extern __attribute__((format(printf, 3, 4)))
char *Model0_devm_kasprintf(struct Model0_device *Model0_dev, Model0_gfp_t Model0_gfp, const char *Model0_fmt, ...) __attribute__((__malloc__));
static inline __attribute__((no_instrument_function)) void *Model0_devm_kzalloc(struct Model0_device *Model0_dev, Model0_size_t Model0_size, Model0_gfp_t Model0_gfp)
{
 return Model0_devm_kmalloc(Model0_dev, Model0_size, Model0_gfp | (( Model0_gfp_t)0x8000u));
}
static inline __attribute__((no_instrument_function)) void *Model0_devm_kmalloc_array(struct Model0_device *Model0_dev,
           Model0_size_t Model0_n, Model0_size_t Model0_size, Model0_gfp_t Model0_flags)
{
 if (Model0_size != 0 && Model0_n > (~(Model0_size_t)0) / Model0_size)
  return ((void *)0);
 return Model0_devm_kmalloc(Model0_dev, Model0_n * Model0_size, Model0_flags);
}
static inline __attribute__((no_instrument_function)) void *Model0_devm_kcalloc(struct Model0_device *Model0_dev,
     Model0_size_t Model0_n, Model0_size_t Model0_size, Model0_gfp_t Model0_flags)
{
 return Model0_devm_kmalloc_array(Model0_dev, Model0_n, Model0_size, Model0_flags | (( Model0_gfp_t)0x8000u));
}
extern void Model0_devm_kfree(struct Model0_device *Model0_dev, void *Model0_p);
extern char *Model0_devm_kstrdup(struct Model0_device *Model0_dev, const char *Model0_s, Model0_gfp_t Model0_gfp) __attribute__((__malloc__));
extern void *Model0_devm_kmemdup(struct Model0_device *Model0_dev, const void *Model0_src, Model0_size_t Model0_len,
     Model0_gfp_t Model0_gfp);

extern unsigned long Model0_devm_get_free_pages(struct Model0_device *Model0_dev,
      Model0_gfp_t Model0_gfp_mask, unsigned int Model0_order);
extern void Model0_devm_free_pages(struct Model0_device *Model0_dev, unsigned long Model0_addr);

void *Model0_devm_ioremap_resource(struct Model0_device *Model0_dev, struct Model0_resource *Model0_res);

/* allows to add/remove a custom action to devres stack */
int Model0_devm_add_action(struct Model0_device *Model0_dev, void (*Model0_action)(void *), void *Model0_data);
void Model0_devm_remove_action(struct Model0_device *Model0_dev, void (*Model0_action)(void *), void *Model0_data);

static inline __attribute__((no_instrument_function)) int Model0_devm_add_action_or_reset(struct Model0_device *Model0_dev,
        void (*Model0_action)(void *), void *Model0_data)
{
 int Model0_ret;

 Model0_ret = Model0_devm_add_action(Model0_dev, Model0_action, Model0_data);
 if (Model0_ret)
  Model0_action(Model0_data);

 return Model0_ret;
}

struct Model0_device_dma_parameters {
 /*
	 * a low level driver may set these to teach IOMMU code about
	 * sg limitations.
	 */
 unsigned int Model0_max_segment_size;
 unsigned long Model0_segment_boundary_mask;
};

/**
 * struct device - The basic device structure
 * @parent:	The device's "parent" device, the device to which it is attached.
 * 		In most cases, a parent device is some sort of bus or host
 * 		controller. If parent is NULL, the device, is a top-level device,
 * 		which is not usually what you want.
 * @p:		Holds the private data of the driver core portions of the device.
 * 		See the comment of the struct device_private for detail.
 * @kobj:	A top-level, abstract class from which other classes are derived.
 * @init_name:	Initial name of the device.
 * @type:	The type of device.
 * 		This identifies the device type and carries type-specific
 * 		information.
 * @mutex:	Mutex to synchronize calls to its driver.
 * @bus:	Type of bus device is on.
 * @driver:	Which driver has allocated this
 * @platform_data: Platform data specific to the device.
 * 		Example: For devices on custom boards, as typical of embedded
 * 		and SOC based hardware, Linux often uses platform_data to point
 * 		to board-specific structures describing devices and how they
 * 		are wired.  That can include what ports are available, chip
 * 		variants, which GPIO pins act in what additional roles, and so
 * 		on.  This shrinks the "Board Support Packages" (BSPs) and
 * 		minimizes board-specific #ifdefs in drivers.
 * @driver_data: Private pointer for driver specific info.
 * @power:	For device power management.
 * 		See Documentation/power/devices.txt for details.
 * @pm_domain:	Provide callbacks that are executed during system suspend,
 * 		hibernation, system resume and during runtime PM transitions
 * 		along with subsystem-level and driver-level callbacks.
 * @pins:	For device pin management.
 *		See Documentation/pinctrl.txt for details.
 * @msi_list:	Hosts MSI descriptors
 * @msi_domain: The generic MSI domain this device is using.
 * @numa_node:	NUMA node this device is close to.
 * @dma_mask:	Dma mask (if dma'ble device).
 * @coherent_dma_mask: Like dma_mask, but for alloc_coherent mapping as not all
 * 		hardware supports 64-bit addresses for consistent allocations
 * 		such descriptors.
 * @dma_pfn_offset: offset of DMA memory range relatively of RAM
 * @dma_parms:	A low level driver may set these to teach IOMMU code about
 * 		segment limitations.
 * @dma_pools:	Dma pools (if dma'ble device).
 * @dma_mem:	Internal for coherent mem override.
 * @cma_area:	Contiguous memory area for dma allocations
 * @archdata:	For arch-specific additions.
 * @of_node:	Associated device tree node.
 * @fwnode:	Associated device node supplied by platform firmware.
 * @devt:	For creating the sysfs "dev".
 * @id:		device instance
 * @devres_lock: Spinlock to protect the resource of the device.
 * @devres_head: The resources list of the device.
 * @knode_class: The node used to add the device to the class list.
 * @class:	The class of the device.
 * @groups:	Optional attribute groups.
 * @release:	Callback to free the device after all references have
 * 		gone away. This should be set by the allocator of the
 * 		device (i.e. the bus driver that discovered the device).
 * @iommu_group: IOMMU group the device belongs to.
 *
 * @offline_disabled: If set, the device is permanently online.
 * @offline:	Set after successful invocation of bus type's .offline().
 *
 * At the lowest level, every device in a Linux system is represented by an
 * instance of struct device. The device structure contains the information
 * that the device model core needs to model the system. Most subsystems,
 * however, track additional information about the devices they host. As a
 * result, it is rare for devices to be represented by bare device structures;
 * instead, that structure, like kobject structures, is usually embedded within
 * a higher-level representation of the device.
 */
struct Model0_device {
 struct Model0_device *Model0_parent;

 struct Model0_device_private *Model0_p;

 struct Model0_kobject Model0_kobj;
 const char *Model0_init_name; /* initial name of the device */
 const struct Model0_device_type *Model0_type;

 struct Model0_mutex Model0_mutex; /* mutex to synchronize calls to
					 * its driver.
					 */

 struct Model0_bus_type *Model0_bus; /* type of bus device is on */
 struct Model0_device_driver *Model0_driver; /* which driver has allocated this
					   device */
 void *Model0_platform_data; /* Platform specific data, device
					   core doesn't touch it */
 void *Model0_driver_data; /* Driver data, set and get with
					   dev_set/get_drvdata */
 struct Model0_dev_pm_info Model0_power;
 struct Model0_dev_pm_domain *Model0_pm_domain;


 struct Model0_irq_domain *Model0_msi_domain;





 struct Model0_list_head Model0_msi_list;



 int Model0_numa_node; /* NUMA node this device is close to */

 Model0_u64 *Model0_dma_mask; /* dma mask (if dma'able device) */
 Model0_u64 Model0_coherent_dma_mask;/* Like dma_mask, but for
					     alloc_coherent mappings as
					     not all hardware supports
					     64 bit addresses for consistent
					     allocations such descriptors. */
 unsigned long Model0_dma_pfn_offset;

 struct Model0_device_dma_parameters *Model0_dma_parms;

 struct Model0_list_head Model0_dma_pools; /* dma pools (if dma'ble) */

 struct Model0_dma_coherent_mem *Model0_dma_mem; /* internal for coherent mem
					     override */




 /* arch specific additions */
 struct Model0_dev_archdata Model0_archdata;

 struct Model0_device_node *Model0_of_node; /* associated device tree node */
 struct Model0_fwnode_handle *Model0_fwnode; /* firmware device node */

 Model0_dev_t Model0_devt; /* dev_t, creates the sysfs "dev" */
 Model0_u32 Model0_id; /* device instance */

 Model0_spinlock_t Model0_devres_lock;
 struct Model0_list_head Model0_devres_head;

 struct Model0_klist_node Model0_knode_class;
 struct Model0_class *Model0_class;
 const struct Model0_attribute_group **Model0_groups; /* optional groups */

 void (*Model0_release)(struct Model0_device *Model0_dev);
 struct Model0_iommu_group *Model0_iommu_group;

 bool Model0_offline_disabled:1;
 bool Model0_offline:1;
};

static inline __attribute__((no_instrument_function)) struct Model0_device *Model0_kobj_to_dev(struct Model0_kobject *Model0_kobj)
{
 return ({ const typeof( ((struct Model0_device *)0)->Model0_kobj ) *Model0___mptr = (Model0_kobj); (struct Model0_device *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_device, Model0_kobj) );});
}

/* Get the wakeup routines, which depend on struct device */

/*
 *  pm_wakeup.h - Power management wakeup interface
 *
 *  Copyright (C) 2008 Alan Stern
 *  Copyright (C) 2010 Rafael J. Wysocki, Novell Inc.
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */
struct Model0_wake_irq;

/**
 * struct wakeup_source - Representation of wakeup sources
 *
 * @name: Name of the wakeup source
 * @entry: Wakeup source list entry
 * @lock: Wakeup source lock
 * @wakeirq: Optional device specific wakeirq
 * @timer: Wakeup timer list
 * @timer_expires: Wakeup timer expiration
 * @total_time: Total time this wakeup source has been active.
 * @max_time: Maximum time this wakeup source has been continuously active.
 * @last_time: Monotonic clock when the wakeup source's was touched last time.
 * @prevent_sleep_time: Total time this source has been preventing autosleep.
 * @event_count: Number of signaled wakeup events.
 * @active_count: Number of times the wakeup source was activated.
 * @relax_count: Number of times the wakeup source was deactivated.
 * @expire_count: Number of times the wakeup source's timeout has expired.
 * @wakeup_count: Number of times the wakeup source might abort suspend.
 * @active: Status of the wakeup source.
 * @has_timeout: The wakeup source has been activated with a timeout.
 */
struct Model0_wakeup_source {
 const char *Model0_name;
 struct Model0_list_head Model0_entry;
 Model0_spinlock_t Model0_lock;
 struct Model0_wake_irq *Model0_wakeirq;
 struct Model0_timer_list Model0_timer;
 unsigned long Model0_timer_expires;
 Model0_ktime_t Model0_total_time;
 Model0_ktime_t Model0_max_time;
 Model0_ktime_t Model0_last_time;
 Model0_ktime_t Model0_start_prevent_time;
 Model0_ktime_t Model0_prevent_sleep_time;
 unsigned long Model0_event_count;
 unsigned long Model0_active_count;
 unsigned long Model0_relax_count;
 unsigned long Model0_expire_count;
 unsigned long Model0_wakeup_count;
 bool Model0_active:1;
 bool Model0_autosleep_enabled:1;
};



/*
 * Changes to device_may_wakeup take effect on the next pm state change.
 */

static inline __attribute__((no_instrument_function)) bool Model0_device_can_wakeup(struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_power.Model0_can_wakeup;
}

static inline __attribute__((no_instrument_function)) bool Model0_device_may_wakeup(struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_power.Model0_can_wakeup && !!Model0_dev->Model0_power.Model0_wakeup;
}

/* drivers/base/power/wakeup.c */
extern void Model0_wakeup_source_prepare(struct Model0_wakeup_source *Model0_ws, const char *Model0_name);
extern struct Model0_wakeup_source *Model0_wakeup_source_create(const char *Model0_name);
extern void Model0_wakeup_source_drop(struct Model0_wakeup_source *Model0_ws);
extern void Model0_wakeup_source_destroy(struct Model0_wakeup_source *Model0_ws);
extern void Model0_wakeup_source_add(struct Model0_wakeup_source *Model0_ws);
extern void Model0_wakeup_source_remove(struct Model0_wakeup_source *Model0_ws);
extern struct Model0_wakeup_source *Model0_wakeup_source_register(const char *Model0_name);
extern void Model0_wakeup_source_unregister(struct Model0_wakeup_source *Model0_ws);
extern int Model0_device_wakeup_enable(struct Model0_device *Model0_dev);
extern int Model0_device_wakeup_disable(struct Model0_device *Model0_dev);
extern void Model0_device_set_wakeup_capable(struct Model0_device *Model0_dev, bool Model0_capable);
extern int Model0_device_init_wakeup(struct Model0_device *Model0_dev, bool Model0_val);
extern int Model0_device_set_wakeup_enable(struct Model0_device *Model0_dev, bool Model0_enable);
extern void Model0___pm_stay_awake(struct Model0_wakeup_source *Model0_ws);
extern void Model0_pm_stay_awake(struct Model0_device *Model0_dev);
extern void Model0___pm_relax(struct Model0_wakeup_source *Model0_ws);
extern void Model0_pm_relax(struct Model0_device *Model0_dev);
extern void Model0___pm_wakeup_event(struct Model0_wakeup_source *Model0_ws, unsigned int Model0_msec);
extern void Model0_pm_wakeup_event(struct Model0_device *Model0_dev, unsigned int Model0_msec);
static inline __attribute__((no_instrument_function)) void Model0_wakeup_source_init(struct Model0_wakeup_source *Model0_ws,
          const char *Model0_name)
{
 Model0_wakeup_source_prepare(Model0_ws, Model0_name);
 Model0_wakeup_source_add(Model0_ws);
}

static inline __attribute__((no_instrument_function)) void Model0_wakeup_source_trash(struct Model0_wakeup_source *Model0_ws)
{
 Model0_wakeup_source_remove(Model0_ws);
 Model0_wakeup_source_drop(Model0_ws);
}

static inline __attribute__((no_instrument_function)) const char *Model0_dev_name(const struct Model0_device *Model0_dev)
{
 /* Use the init name until the kobject becomes available */
 if (Model0_dev->Model0_init_name)
  return Model0_dev->Model0_init_name;

 return Model0_kobject_name(&Model0_dev->Model0_kobj);
}

extern __attribute__((format(printf, 2, 3)))
int Model0_dev_set_name(struct Model0_device *Model0_dev, const char *Model0_name, ...);


static inline __attribute__((no_instrument_function)) int Model0_dev_to_node(struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_numa_node;
}
static inline __attribute__((no_instrument_function)) void Model0_set_dev_node(struct Model0_device *Model0_dev, int Model0_node)
{
 Model0_dev->Model0_numa_node = Model0_node;
}
static inline __attribute__((no_instrument_function)) struct Model0_irq_domain *Model0_dev_get_msi_domain(const struct Model0_device *Model0_dev)
{

 return Model0_dev->Model0_msi_domain;



}

static inline __attribute__((no_instrument_function)) void Model0_dev_set_msi_domain(struct Model0_device *Model0_dev, struct Model0_irq_domain *Model0_d)
{

 Model0_dev->Model0_msi_domain = Model0_d;

}

static inline __attribute__((no_instrument_function)) void *Model0_dev_get_drvdata(const struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_driver_data;
}

static inline __attribute__((no_instrument_function)) void Model0_dev_set_drvdata(struct Model0_device *Model0_dev, void *Model0_data)
{
 Model0_dev->Model0_driver_data = Model0_data;
}

static inline __attribute__((no_instrument_function)) struct Model0_pm_subsys_data *Model0_dev_to_psd(struct Model0_device *Model0_dev)
{
 return Model0_dev ? Model0_dev->Model0_power.Model0_subsys_data : ((void *)0);
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_dev_get_uevent_suppress(const struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_kobj.Model0_uevent_suppress;
}

static inline __attribute__((no_instrument_function)) void Model0_dev_set_uevent_suppress(struct Model0_device *Model0_dev, int Model0_val)
{
 Model0_dev->Model0_kobj.Model0_uevent_suppress = Model0_val;
}

static inline __attribute__((no_instrument_function)) int Model0_device_is_registered(struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_kobj.Model0_state_in_sysfs;
}

static inline __attribute__((no_instrument_function)) void Model0_device_enable_async_suspend(struct Model0_device *Model0_dev)
{
 if (!Model0_dev->Model0_power.Model0_is_prepared)
  Model0_dev->Model0_power.Model0_async_suspend = true;
}

static inline __attribute__((no_instrument_function)) void Model0_device_disable_async_suspend(struct Model0_device *Model0_dev)
{
 if (!Model0_dev->Model0_power.Model0_is_prepared)
  Model0_dev->Model0_power.Model0_async_suspend = false;
}

static inline __attribute__((no_instrument_function)) bool Model0_device_async_suspend_enabled(struct Model0_device *Model0_dev)
{
 return !!Model0_dev->Model0_power.Model0_async_suspend;
}

static inline __attribute__((no_instrument_function)) void Model0_dev_pm_syscore_device(struct Model0_device *Model0_dev, bool Model0_val)
{

 Model0_dev->Model0_power.Model0_syscore = Model0_val;

}

static inline __attribute__((no_instrument_function)) void Model0_device_lock(struct Model0_device *Model0_dev)
{
 Model0_mutex_lock(&Model0_dev->Model0_mutex);
}

static inline __attribute__((no_instrument_function)) int Model0_device_lock_interruptible(struct Model0_device *Model0_dev)
{
 return Model0_mutex_lock_interruptible(&Model0_dev->Model0_mutex);
}

static inline __attribute__((no_instrument_function)) int Model0_device_trylock(struct Model0_device *Model0_dev)
{
 return Model0_mutex_trylock(&Model0_dev->Model0_mutex);
}

static inline __attribute__((no_instrument_function)) void Model0_device_unlock(struct Model0_device *Model0_dev)
{
 Model0_mutex_unlock(&Model0_dev->Model0_mutex);
}

static inline __attribute__((no_instrument_function)) void Model0_device_lock_assert(struct Model0_device *Model0_dev)
{
 do { (void)(&Model0_dev->Model0_mutex); } while (0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_dev_of_node(struct Model0_device *Model0_dev)
{
 if (!0)
  return ((void *)0);
 return Model0_dev->Model0_of_node;
}

void Model0_driver_init(void);

/*
 * High level routines for use by the bus drivers
 */
extern int __attribute__((warn_unused_result)) Model0_device_register(struct Model0_device *Model0_dev);
extern void Model0_device_unregister(struct Model0_device *Model0_dev);
extern void Model0_device_initialize(struct Model0_device *Model0_dev);
extern int __attribute__((warn_unused_result)) Model0_device_add(struct Model0_device *Model0_dev);
extern void Model0_device_del(struct Model0_device *Model0_dev);
extern int Model0_device_for_each_child(struct Model0_device *Model0_dev, void *Model0_data,
       int (*Model0_fn)(struct Model0_device *Model0_dev, void *Model0_data));
extern int Model0_device_for_each_child_reverse(struct Model0_device *Model0_dev, void *Model0_data,
       int (*Model0_fn)(struct Model0_device *Model0_dev, void *Model0_data));
extern struct Model0_device *Model0_device_find_child(struct Model0_device *Model0_dev, void *Model0_data,
    int (*Model0_match)(struct Model0_device *Model0_dev, void *Model0_data));
extern int Model0_device_rename(struct Model0_device *Model0_dev, const char *Model0_new_name);
extern int Model0_device_move(struct Model0_device *Model0_dev, struct Model0_device *Model0_new_parent,
         enum Model0_dpm_order Model0_dpm_order);
extern const char *Model0_device_get_devnode(struct Model0_device *Model0_dev,
          Model0_umode_t *Model0_mode, Model0_kuid_t *Model0_uid, Model0_kgid_t *Model0_gid,
          const char **Model0_tmp);

static inline __attribute__((no_instrument_function)) bool Model0_device_supports_offline(struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_bus && Model0_dev->Model0_bus->Model0_offline && Model0_dev->Model0_bus->Model0_online;
}

extern void Model0_lock_device_hotplug(void);
extern void Model0_unlock_device_hotplug(void);
extern int Model0_lock_device_hotplug_sysfs(void);
extern int Model0_device_offline(struct Model0_device *Model0_dev);
extern int Model0_device_online(struct Model0_device *Model0_dev);
extern void Model0_set_primary_fwnode(struct Model0_device *Model0_dev, struct Model0_fwnode_handle *Model0_fwnode);
extern void Model0_set_secondary_fwnode(struct Model0_device *Model0_dev, struct Model0_fwnode_handle *Model0_fwnode);

/*
 * Root device objects for grouping under /sys/devices
 */
extern struct Model0_device *Model0___root_device_register(const char *Model0_name,
          struct Model0_module *Model0_owner);

/* This is a macro to avoid include problems with THIS_MODULE */



extern void Model0_root_device_unregister(struct Model0_device *Model0_root);

static inline __attribute__((no_instrument_function)) void *Model0_dev_get_platdata(const struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_platform_data;
}

/*
 * Manual binding of a device to driver. See drivers/base/bus.c
 * for information on use.
 */
extern int __attribute__((warn_unused_result)) Model0_device_bind_driver(struct Model0_device *Model0_dev);
extern void Model0_device_release_driver(struct Model0_device *Model0_dev);
extern int __attribute__((warn_unused_result)) Model0_device_attach(struct Model0_device *Model0_dev);
extern int __attribute__((warn_unused_result)) Model0_driver_attach(struct Model0_device_driver *Model0_drv);
extern void Model0_device_initial_probe(struct Model0_device *Model0_dev);
extern int __attribute__((warn_unused_result)) Model0_device_reprobe(struct Model0_device *Model0_dev);

extern bool Model0_device_is_bound(struct Model0_device *Model0_dev);

/*
 * Easy functions for dynamically creating devices on the fly
 */
extern __attribute__((format(printf, 5, 0)))
struct Model0_device *Model0_device_create_vargs(struct Model0_class *Model0_cls, struct Model0_device *Model0_parent,
       Model0_dev_t Model0_devt, void *Model0_drvdata,
       const char *Model0_fmt, Model0_va_list Model0_vargs);
extern __attribute__((format(printf, 5, 6)))
struct Model0_device *Model0_device_create(struct Model0_class *Model0_cls, struct Model0_device *Model0_parent,
        Model0_dev_t Model0_devt, void *Model0_drvdata,
        const char *Model0_fmt, ...);
extern __attribute__((format(printf, 6, 7)))
struct Model0_device *Model0_device_create_with_groups(struct Model0_class *Model0_cls,
        struct Model0_device *Model0_parent, Model0_dev_t Model0_devt, void *Model0_drvdata,
        const struct Model0_attribute_group **Model0_groups,
        const char *Model0_fmt, ...);
extern void Model0_device_destroy(struct Model0_class *Model0_cls, Model0_dev_t Model0_devt);

/*
 * Platform "fixup" functions - allow the platform to have their say
 * about devices and actions that the general device layer doesn't
 * know about.
 */
/* Notify platform of device discovery */
extern int (*Model0_platform_notify)(struct Model0_device *Model0_dev);

extern int (*Model0_platform_notify_remove)(struct Model0_device *Model0_dev);


/*
 * get_device - atomically increment the reference count for the device.
 *
 */
extern struct Model0_device *Model0_get_device(struct Model0_device *Model0_dev);
extern void Model0_put_device(struct Model0_device *Model0_dev);


extern int Model0_devtmpfs_create_node(struct Model0_device *Model0_dev);
extern int Model0_devtmpfs_delete_node(struct Model0_device *Model0_dev);
extern int Model0_devtmpfs_mount(const char *Model0_mntdir);






/* drivers/base/power/shutdown.c */
extern void Model0_device_shutdown(void);

/* debugging and troubleshooting/diagnostic helpers. */
extern const char *Model0_dev_driver_string(const struct Model0_device *Model0_dev);




extern __attribute__((format(printf, 3, 0)))
int Model0_dev_vprintk_emit(int Model0_level, const struct Model0_device *Model0_dev,
       const char *Model0_fmt, Model0_va_list Model0_args);
extern __attribute__((format(printf, 3, 4)))
int Model0_dev_printk_emit(int Model0_level, const struct Model0_device *Model0_dev, const char *Model0_fmt, ...);

extern __attribute__((format(printf, 3, 4)))
void Model0_dev_printk(const char *Model0_level, const struct Model0_device *Model0_dev,
  const char *Model0_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model0_dev_emerg(const struct Model0_device *Model0_dev, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model0_dev_alert(const struct Model0_device *Model0_dev, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model0_dev_crit(const struct Model0_device *Model0_dev, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model0_dev_err(const struct Model0_device *Model0_dev, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model0_dev_warn(const struct Model0_device *Model0_dev, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model0_dev_notice(const struct Model0_device *Model0_dev, const char *Model0_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model0__dev_info(const struct Model0_device *Model0_dev, const char *Model0_fmt, ...);
/*
 * Stupid hackaround for existing uses of non-printk uses dev_info
 *
 * Note that the definition of dev_info below is actually _dev_info
 * and a macro is used to avoid redefining dev_info
 */
/*
 * dev_WARN*() acts like dev_printk(), but with the key difference of
 * using WARN/WARN_ONCE to include file/line information and a backtrace.
 */







/* Create alias, so I can be autoloaded. */
/**
 * module_driver() - Helper macro for drivers that don't do anything
 * special in module init/exit. This eliminates a lot of boilerplate.
 * Each module may only use this macro once, and calling it replaces
 * module_init() and module_exit().
 *
 * @__driver: driver name
 * @__register: register function for this driver type
 * @__unregister: unregister function for this driver type
 * @...: Additional arguments to be passed to __register and __unregister.
 *
 * Use this macro to construct bus specific macros for registering
 * drivers, and do not use it on its own.
 */
/**
 * builtin_driver() - Helper macro for drivers that don't do anything
 * special in init and have no exit. This eliminates some boilerplate.
 * Each driver may only use this macro once, and calling it replaces
 * device_initcall (or in some cases, the legacy __initcall).  This is
 * meant to be a direct parallel of module_driver() above but without
 * the __exit stuff that is not used for builtin cases.
 *
 * @__driver: driver name
 * @__register: register function for this driver type
 * @...: Additional arguments to be passed to __register
 *
 * Use this macro to construct bus specific macros for registering
 * drivers, and do not use it on its own.
 */

/*
 *	Berkeley style UIO structures	-	Alan Cox 1994.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 *	Berkeley style UIO structures	-	Alan Cox 1994.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







struct Model0_iovec
{
 void *Model0_iov_base; /* BSD uses caddr_t (1003.1g requires void *) */
 Model0___kernel_size_t Model0_iov_len; /* Must be size_t (1003.1g) */
};

/*
 *	UIO_MAXIOV shall be at least 16 1003.1g (5.4.1.1)
 */

struct Model0_page;

struct Model0_kvec {
 void *Model0_iov_base; /* and that should *never* hold a userland pointer */
 Model0_size_t Model0_iov_len;
};

enum {
 Model0_ITER_IOVEC = 0,
 Model0_ITER_KVEC = 2,
 Model0_ITER_BVEC = 4,
};

struct Model0_iov_iter {
 int Model0_type;
 Model0_size_t Model0_iov_offset;
 Model0_size_t Model0_count;
 union {
  const struct Model0_iovec *Model0_iov;
  const struct Model0_kvec *Model0_kvec;
  const struct Model0_bio_vec *Model0_bvec;
 };
 unsigned long Model0_nr_segs;
};

/*
 * Total number of bytes covered by an iovec.
 *
 * NOTE that it is not safe to use this function until all the iovec's
 * segment lengths have been validated.  Because the individual lengths can
 * overflow a size_t when added together.
 */
static inline __attribute__((no_instrument_function)) Model0_size_t Model0_iov_length(const struct Model0_iovec *Model0_iov, unsigned long Model0_nr_segs)
{
 unsigned long Model0_seg;
 Model0_size_t Model0_ret = 0;

 for (Model0_seg = 0; Model0_seg < Model0_nr_segs; Model0_seg++)
  Model0_ret += Model0_iov[Model0_seg].Model0_iov_len;
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) struct Model0_iovec Model0_iov_iter_iovec(const struct Model0_iov_iter *Model0_iter)
{
 return (struct Model0_iovec) {
  .Model0_iov_base = Model0_iter->Model0_iov->Model0_iov_base + Model0_iter->Model0_iov_offset,
  .Model0_iov_len = ({ typeof(Model0_iter->Model0_count) Model0__min1 = (Model0_iter->Model0_count); typeof(Model0_iter->Model0_iov->Model0_iov_len - Model0_iter->Model0_iov_offset) Model0__min2 = (Model0_iter->Model0_iov->Model0_iov_len - Model0_iter->Model0_iov_offset); (void) (&Model0__min1 == &Model0__min2); Model0__min1 < Model0__min2 ? Model0__min1 : Model0__min2; }),

 };
}
unsigned long Model0_iov_shorten(struct Model0_iovec *Model0_iov, unsigned long Model0_nr_segs, Model0_size_t Model0_to);

Model0_size_t Model0_iov_iter_copy_from_user_atomic(struct Model0_page *Model0_page,
  struct Model0_iov_iter *Model0_i, unsigned long Model0_offset, Model0_size_t Model0_bytes);
void Model0_iov_iter_advance(struct Model0_iov_iter *Model0_i, Model0_size_t Model0_bytes);
int Model0_iov_iter_fault_in_readable(struct Model0_iov_iter *Model0_i, Model0_size_t Model0_bytes);

Model0_size_t Model0_iov_iter_single_seg_count(const struct Model0_iov_iter *Model0_i);
Model0_size_t Model0_copy_page_to_iter(struct Model0_page *Model0_page, Model0_size_t Model0_offset, Model0_size_t Model0_bytes,
    struct Model0_iov_iter *Model0_i);
Model0_size_t Model0_copy_page_from_iter(struct Model0_page *Model0_page, Model0_size_t Model0_offset, Model0_size_t Model0_bytes,
    struct Model0_iov_iter *Model0_i);
Model0_size_t Model0_copy_to_iter(const void *Model0_addr, Model0_size_t Model0_bytes, struct Model0_iov_iter *Model0_i);
Model0_size_t Model0_copy_from_iter(void *Model0_addr, Model0_size_t Model0_bytes, struct Model0_iov_iter *Model0_i);
Model0_size_t Model0_copy_from_iter_nocache(void *Model0_addr, Model0_size_t Model0_bytes, struct Model0_iov_iter *Model0_i);
Model0_size_t Model0_iov_iter_zero(Model0_size_t Model0_bytes, struct Model0_iov_iter *);
unsigned long Model0_iov_iter_alignment(const struct Model0_iov_iter *Model0_i);
unsigned long Model0_iov_iter_gap_alignment(const struct Model0_iov_iter *Model0_i);
void Model0_iov_iter_init(struct Model0_iov_iter *Model0_i, int Model0_direction, const struct Model0_iovec *Model0_iov,
   unsigned long Model0_nr_segs, Model0_size_t Model0_count);
void Model0_iov_iter_kvec(struct Model0_iov_iter *Model0_i, int Model0_direction, const struct Model0_kvec *Model0_kvec,
   unsigned long Model0_nr_segs, Model0_size_t Model0_count);
void Model0_iov_iter_bvec(struct Model0_iov_iter *Model0_i, int Model0_direction, const struct Model0_bio_vec *Model0_bvec,
   unsigned long Model0_nr_segs, Model0_size_t Model0_count);
Model0_ssize_t Model0_iov_iter_get_pages(struct Model0_iov_iter *Model0_i, struct Model0_page **Model0_pages,
   Model0_size_t Model0_maxsize, unsigned Model0_maxpages, Model0_size_t *Model0_start);
Model0_ssize_t Model0_iov_iter_get_pages_alloc(struct Model0_iov_iter *Model0_i, struct Model0_page ***Model0_pages,
   Model0_size_t Model0_maxsize, Model0_size_t *Model0_start);
int Model0_iov_iter_npages(const struct Model0_iov_iter *Model0_i, int Model0_maxpages);

const void *Model0_dup_iter(struct Model0_iov_iter *Model0_new, struct Model0_iov_iter *old, Model0_gfp_t Model0_flags);

static inline __attribute__((no_instrument_function)) Model0_size_t Model0_iov_iter_count(struct Model0_iov_iter *Model0_i)
{
 return Model0_i->Model0_count;
}

static inline __attribute__((no_instrument_function)) bool Model0_iter_is_iovec(struct Model0_iov_iter *Model0_i)
{
 return !(Model0_i->Model0_type & (Model0_ITER_BVEC | Model0_ITER_KVEC));
}

/*
 * Get one of READ or WRITE out of iter->type without any other flags OR'd in
 * with it.
 *
 * The ?: is just for type safety.
 */


/*
 * Cap the iov_iter by given limit; note that the second argument is
 * *not* the new size - it's upper limit for such.  Passing it a value
 * greater than the amount of data in iov_iter is fine - it'll just do
 * nothing in that case.
 */
static inline __attribute__((no_instrument_function)) void Model0_iov_iter_truncate(struct Model0_iov_iter *Model0_i, Model0_u64 Model0_count)
{
 /*
	 * count doesn't have to fit in size_t - comparison extends both
	 * operands to u64 here and any value that would be truncated by
	 * conversion in assignement is by definition greater than all
	 * values of size_t, including old i->count.
	 */
 if (Model0_i->Model0_count > Model0_count)
  Model0_i->Model0_count = Model0_count;
}

/*
 * reexpand a previously truncated iterator; count must be no more than how much
 * we had shrunk it.
 */
static inline __attribute__((no_instrument_function)) void Model0_iov_iter_reexpand(struct Model0_iov_iter *Model0_i, Model0_size_t Model0_count)
{
 Model0_i->Model0_count = Model0_count;
}
Model0_size_t Model0_csum_and_copy_to_iter(const void *Model0_addr, Model0_size_t Model0_bytes, Model0___wsum *Model0_csum, struct Model0_iov_iter *Model0_i);
Model0_size_t Model0_csum_and_copy_from_iter(void *Model0_addr, Model0_size_t Model0_bytes, Model0___wsum *Model0_csum, struct Model0_iov_iter *Model0_i);

int Model0_import_iovec(int Model0_type, const struct Model0_iovec * Model0_uvector,
   unsigned Model0_nr_segs, unsigned Model0_fast_segs,
   struct Model0_iovec **Model0_iov, struct Model0_iov_iter *Model0_i);


struct Model0_compat_iovec;
int Model0_compat_import_iovec(int Model0_type, const struct Model0_compat_iovec * Model0_uvector,
   unsigned Model0_nr_segs, unsigned Model0_fast_segs,
   struct Model0_iovec **Model0_iov, struct Model0_iov_iter *Model0_i);


int Model0_import_single_range(int Model0_type, void *Model0_buf, Model0_size_t Model0_len,
   struct Model0_iovec *Model0_iov, struct Model0_iov_iter *Model0_i);

struct Model0_scatterlist {



 unsigned long Model0_page_link;
 unsigned int Model0_offset;
 unsigned int Model0_length;
 Model0_dma_addr_t Model0_dma_address;

 unsigned int Model0_dma_length;

};

/*
 * These macros should be used after a dma_map_sg call has been done
 * to get bus addresses of each of the SG entries and their lengths.
 * You should only work with the number of sg entries dma_map_sg
 * returns, or alternatively stop on the first sg_dma_len(sg) which
 * is 0.
 */
struct Model0_sg_table {
 struct Model0_scatterlist *Model0_sgl; /* the list */
 unsigned int Model0_nents; /* number of mapped entries */
 unsigned int Model0_orig_nents; /* original size of list */
};

/*
 * Notes on SG table design.
 *
 * We use the unsigned long page_link field in the scatterlist struct to place
 * the page pointer AND encode information about the sg table as well. The two
 * lower bits are reserved for this information.
 *
 * If bit 0 is set, then the page_link contains a pointer to the next sg
 * table list. Otherwise the next entry is at sg + 1.
 *
 * If bit 1 is set, then this sg entry is the last element in a list.
 *
 * See sg_next().
 *
 */



/*
 * We overload the LSB of the page pointer to indicate whether it's
 * a valid sg entry, or whether it points to the start of a new scatterlist.
 * Those low bits are there for everyone! (thanks mason :-)
 */





/**
 * sg_assign_page - Assign a given page to an SG entry
 * @sg:		    SG entry
 * @page:	    The page
 *
 * Description:
 *   Assign page to sg entry. Also see sg_set_page(), the most commonly used
 *   variant.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model0_sg_assign_page(struct Model0_scatterlist *Model0_sg, struct Model0_page *Model0_page)
{
 unsigned long Model0_page_link = Model0_sg->Model0_page_link & 0x3;

 /*
	 * In order for the low bit stealing approach to work, pages
	 * must be aligned at a 32-bit boundary as a minimum.
	 */
 do { if (__builtin_expect(!!((unsigned long) Model0_page & 0x03), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/scatterlist.h"), "i" (90), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);




 Model0_sg->Model0_page_link = Model0_page_link | (unsigned long) Model0_page;
}

/**
 * sg_set_page - Set sg entry to point at given page
 * @sg:		 SG entry
 * @page:	 The page
 * @len:	 Length of data
 * @offset:	 Offset into page
 *
 * Description:
 *   Use this function to set an sg entry pointing at a page, never assign
 *   the page directly. We encode sg table information in the lower bits
 *   of the page pointer. See sg_page() for looking up the page belonging
 *   to an sg entry.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model0_sg_set_page(struct Model0_scatterlist *Model0_sg, struct Model0_page *Model0_page,
          unsigned int Model0_len, unsigned int Model0_offset)
{
 Model0_sg_assign_page(Model0_sg, Model0_page);
 Model0_sg->Model0_offset = Model0_offset;
 Model0_sg->Model0_length = Model0_len;
}

static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_sg_page(struct Model0_scatterlist *Model0_sg)
{




 return (struct Model0_page *)((Model0_sg)->Model0_page_link & ~0x3);
}

/**
 * sg_set_buf - Set sg entry to point at given data
 * @sg:		 SG entry
 * @buf:	 Data
 * @buflen:	 Data length
 *
 **/
static inline __attribute__((no_instrument_function)) void Model0_sg_set_buf(struct Model0_scatterlist *Model0_sg, const void *Model0_buf,
         unsigned int Model0_buflen)
{



 Model0_sg_set_page(Model0_sg, (((struct Model0_page *)(0xffffea0000000000UL)) + (Model0___phys_addr_nodebug((unsigned long)(Model0_buf)) >> 12)), Model0_buflen, ((unsigned long)(Model0_buf) & ~(~(((1UL) << 12)-1))));
}

/*
 * Loop over each sg element, following the pointer to a new list if necessary
 */



/**
 * sg_chain - Chain two sglists together
 * @prv:	First scatterlist
 * @prv_nents:	Number of entries in prv
 * @sgl:	Second scatterlist
 *
 * Description:
 *   Links @prv@ and @sgl@ together, to form a longer scatterlist.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model0_sg_chain(struct Model0_scatterlist *Model0_prv, unsigned int Model0_prv_nents,
       struct Model0_scatterlist *Model0_sgl)
{
 /*
	 * offset and length are unused for chain entry.  Clear them.
	 */
 Model0_prv[Model0_prv_nents - 1].Model0_offset = 0;
 Model0_prv[Model0_prv_nents - 1].Model0_length = 0;

 /*
	 * Set lowest bit to indicate a link pointer, and make sure to clear
	 * the termination bit if it happens to be set.
	 */
 Model0_prv[Model0_prv_nents - 1].Model0_page_link = ((unsigned long) Model0_sgl | 0x01) & ~0x02;
}

/**
 * sg_mark_end - Mark the end of the scatterlist
 * @sg:		 SG entryScatterlist
 *
 * Description:
 *   Marks the passed in sg entry as the termination point for the sg
 *   table. A call to sg_next() on this entry will return NULL.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model0_sg_mark_end(struct Model0_scatterlist *Model0_sg)
{



 /*
	 * Set termination bit, clear potential chain bit
	 */
 Model0_sg->Model0_page_link |= 0x02;
 Model0_sg->Model0_page_link &= ~0x01;
}

/**
 * sg_unmark_end - Undo setting the end of the scatterlist
 * @sg:		 SG entryScatterlist
 *
 * Description:
 *   Removes the termination marker from the given entry of the scatterlist.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model0_sg_unmark_end(struct Model0_scatterlist *Model0_sg)
{



 Model0_sg->Model0_page_link &= ~0x02;
}

/**
 * sg_phys - Return physical address of an sg entry
 * @sg:	     SG entry
 *
 * Description:
 *   This calls page_to_phys() on the page in this sg entry, and adds the
 *   sg offset. The caller must know that it is legal to call page_to_phys()
 *   on the sg page.
 *
 **/
static inline __attribute__((no_instrument_function)) Model0_dma_addr_t Model0_sg_phys(struct Model0_scatterlist *Model0_sg)
{
 return ((Model0_dma_addr_t)(unsigned long)((Model0_sg_page(Model0_sg)) - ((struct Model0_page *)(0xffffea0000000000UL))) << 12) + Model0_sg->Model0_offset;
}

/**
 * sg_virt - Return virtual address of an sg entry
 * @sg:      SG entry
 *
 * Description:
 *   This calls page_address() on the page in this sg entry, and adds the
 *   sg offset. The caller must know that the sg page has a valid virtual
 *   mapping.
 *
 **/
static inline __attribute__((no_instrument_function)) void *Model0_sg_virt(struct Model0_scatterlist *Model0_sg)
{
 return Model0_lowmem_page_address(Model0_sg_page(Model0_sg)) + Model0_sg->Model0_offset;
}

int Model0_sg_nents(struct Model0_scatterlist *Model0_sg);
int Model0_sg_nents_for_len(struct Model0_scatterlist *Model0_sg, Model0_u64 Model0_len);
struct Model0_scatterlist *Model0_sg_next(struct Model0_scatterlist *);
struct Model0_scatterlist *Model0_sg_last(struct Model0_scatterlist *Model0_s, unsigned int);
void Model0_sg_init_table(struct Model0_scatterlist *, unsigned int);
void Model0_sg_init_one(struct Model0_scatterlist *, const void *, unsigned int);
int Model0_sg_split(struct Model0_scatterlist *Model0_in, const int Model0_in_mapped_nents,
      const Model0_off_t Model0_skip, const int Model0_nb_splits,
      const Model0_size_t *Model0_split_sizes,
      struct Model0_scatterlist **Model0_out, int *Model0_out_mapped_nents,
      Model0_gfp_t Model0_gfp_mask);

typedef struct Model0_scatterlist *(Model0_sg_alloc_fn)(unsigned int, Model0_gfp_t);
typedef void (Model0_sg_free_fn)(struct Model0_scatterlist *, unsigned int);

void Model0___sg_free_table(struct Model0_sg_table *, unsigned int, bool, Model0_sg_free_fn *);
void Model0_sg_free_table(struct Model0_sg_table *);
int Model0___sg_alloc_table(struct Model0_sg_table *, unsigned int, unsigned int,
       struct Model0_scatterlist *, Model0_gfp_t, Model0_sg_alloc_fn *);
int Model0_sg_alloc_table(struct Model0_sg_table *, unsigned int, Model0_gfp_t);
int Model0_sg_alloc_table_from_pages(struct Model0_sg_table *Model0_sgt,
 struct Model0_page **Model0_pages, unsigned int Model0_n_pages,
 unsigned long Model0_offset, unsigned long Model0_size,
 Model0_gfp_t Model0_gfp_mask);

Model0_size_t Model0_sg_copy_buffer(struct Model0_scatterlist *Model0_sgl, unsigned int Model0_nents, void *Model0_buf,
        Model0_size_t Model0_buflen, Model0_off_t Model0_skip, bool Model0_to_buffer);

Model0_size_t Model0_sg_copy_from_buffer(struct Model0_scatterlist *Model0_sgl, unsigned int Model0_nents,
      const void *Model0_buf, Model0_size_t Model0_buflen);
Model0_size_t Model0_sg_copy_to_buffer(struct Model0_scatterlist *Model0_sgl, unsigned int Model0_nents,
    void *Model0_buf, Model0_size_t Model0_buflen);

Model0_size_t Model0_sg_pcopy_from_buffer(struct Model0_scatterlist *Model0_sgl, unsigned int Model0_nents,
       const void *Model0_buf, Model0_size_t Model0_buflen, Model0_off_t Model0_skip);
Model0_size_t Model0_sg_pcopy_to_buffer(struct Model0_scatterlist *Model0_sgl, unsigned int Model0_nents,
     void *Model0_buf, Model0_size_t Model0_buflen, Model0_off_t Model0_skip);

/*
 * Maximum number of entries that will be allocated in one piece, if
 * a list larger than this is required then chaining will be utilized.
 */


/*
 * The maximum number of SG segments that we will put inside a
 * scatterlist (unless chaining is used). Should ideally fit inside a
 * single page, to avoid a higher order allocation.  We could define this
 * to SG_MAX_SINGLE_ALLOC to pack correctly at the highest order.  The
 * minimum value is 32
 */


/*
 * Like SG_CHUNK_SIZE, but for archs that have sg chaining. This limit
 * is totally arbitrary, a setting of 2048 will get you at least 8mb ios.
 */







void Model0_sg_free_table_chained(struct Model0_sg_table *Model0_table, bool Model0_first_chunk);
int Model0_sg_alloc_table_chained(struct Model0_sg_table *Model0_table, int Model0_nents,
      struct Model0_scatterlist *Model0_first_chunk);


/*
 * sg page iterator
 *
 * Iterates over sg entries page-by-page.  On each successful iteration,
 * you can call sg_page_iter_page(@piter) and sg_page_iter_dma_address(@piter)
 * to get the current page and its dma address. @piter->sg will point to the
 * sg holding this page and @piter->sg_pgoffset to the page's page offset
 * within the sg. The iteration will stop either when a maximum number of sg
 * entries was reached or a terminating sg (sg_last(sg) == true) was reached.
 */
struct Model0_sg_page_iter {
 struct Model0_scatterlist *Model0_sg; /* sg holding the page */
 unsigned int Model0_sg_pgoffset; /* page offset within the sg */

 /* these are internal states, keep away */
 unsigned int Model0___nents; /* remaining sg entries */
 int Model0___pg_advance; /* nr pages to advance at the
						 * next step */
};

bool Model0___sg_page_iter_next(struct Model0_sg_page_iter *Model0_piter);
void Model0___sg_page_iter_start(struct Model0_sg_page_iter *Model0_piter,
     struct Model0_scatterlist *Model0_sglist, unsigned int Model0_nents,
     unsigned long Model0_pgoffset);
/**
 * sg_page_iter_page - get the current page held by the page iterator
 * @piter:	page iterator holding the page
 */
static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_sg_page_iter_page(struct Model0_sg_page_iter *Model0_piter)
{
 return (((struct Model0_page *)(0xffffea0000000000UL)) + ((unsigned long)(((Model0_sg_page(Model0_piter->Model0_sg))) - ((struct Model0_page *)(0xffffea0000000000UL))) + (Model0_piter->Model0_sg_pgoffset)));
}

/**
 * sg_page_iter_dma_address - get the dma address of the current page held by
 * the page iterator.
 * @piter:	page iterator holding the page
 */
static inline __attribute__((no_instrument_function)) Model0_dma_addr_t Model0_sg_page_iter_dma_address(struct Model0_sg_page_iter *Model0_piter)
{
 return ((Model0_piter->Model0_sg)->Model0_dma_address) + (Model0_piter->Model0_sg_pgoffset << 12);
}

/**
 * for_each_sg_page - iterate over the pages of the given sg list
 * @sglist:	sglist to iterate over
 * @piter:	page iterator to hold current page, sg, sg_pgoffset
 * @nents:	maximum number of sg entries to iterate over
 * @pgoffset:	starting page offset
 */




/*
 * Mapping sg iterator
 *
 * Iterates over sg entries mapping page-by-page.  On each successful
 * iteration, @miter->page points to the mapped page and
 * @miter->length bytes of data can be accessed at @miter->addr.  As
 * long as an interation is enclosed between start and stop, the user
 * is free to choose control structure and when to stop.
 *
 * @miter->consumed is set to @miter->length on each iteration.  It
 * can be adjusted if the user can't consume all the bytes in one go.
 * Also, a stopped iteration can be resumed by calling next on it.
 * This is useful when iteration needs to release all resources and
 * continue later (e.g. at the next interrupt).
 */





struct Model0_sg_mapping_iter {
 /* the following three fields can be accessed directly */
 struct Model0_page *Model0_page; /* currently mapped page */
 void *Model0_addr; /* pointer to the mapped area */
 Model0_size_t Model0_length; /* length of the mapped area */
 Model0_size_t Model0_consumed; /* number of consumed bytes */
 struct Model0_sg_page_iter Model0_piter; /* page iterator */

 /* these are internal states, keep away */
 unsigned int Model0___offset; /* offset within page */
 unsigned int Model0___remaining; /* remaining bytes on page */
 unsigned int Model0___flags;
};

void Model0_sg_miter_start(struct Model0_sg_mapping_iter *Model0_miter, struct Model0_scatterlist *Model0_sgl,
      unsigned int Model0_nents, unsigned int Model0_flags);
bool Model0_sg_miter_skip(struct Model0_sg_mapping_iter *Model0_miter, Model0_off_t Model0_offset);
bool Model0_sg_miter_next(struct Model0_sg_mapping_iter *Model0_miter);
void Model0_sg_miter_stop(struct Model0_sg_mapping_iter *Model0_miter);




/**
 * typedef dma_cookie_t - an opaque DMA cookie
 *
 * if dma_cookie_t is >0 it's a DMA request cookie, <0 it's an error code
 */
typedef Model0_s32 Model0_dma_cookie_t;


static inline __attribute__((no_instrument_function)) int Model0_dma_submit_error(Model0_dma_cookie_t Model0_cookie)
{
 return Model0_cookie < 0 ? Model0_cookie : 0;
}

/**
 * enum dma_status - DMA transaction status
 * @DMA_COMPLETE: transaction completed
 * @DMA_IN_PROGRESS: transaction not yet processed
 * @DMA_PAUSED: transaction is paused
 * @DMA_ERROR: transaction failed
 */
enum Model0_dma_status {
 Model0_DMA_COMPLETE,
 Model0_DMA_IN_PROGRESS,
 Model0_DMA_PAUSED,
 Model0_DMA_ERROR,
};

/**
 * enum dma_transaction_type - DMA transaction types/indexes
 *
 * Note: The DMA_ASYNC_TX capability is not to be set by drivers.  It is
 * automatically set as dma devices are registered.
 */
enum Model0_dma_transaction_type {
 Model0_DMA_MEMCPY,
 Model0_DMA_XOR,
 Model0_DMA_PQ,
 Model0_DMA_XOR_VAL,
 Model0_DMA_PQ_VAL,
 Model0_DMA_MEMSET,
 Model0_DMA_MEMSET_SG,
 Model0_DMA_INTERRUPT,
 Model0_DMA_SG,
 Model0_DMA_PRIVATE,
 Model0_DMA_ASYNC_TX,
 Model0_DMA_SLAVE,
 Model0_DMA_CYCLIC,
 Model0_DMA_INTERLEAVE,
/* last transaction type for creation of the capabilities mask */
 Model0_DMA_TX_TYPE_END,
};

/**
 * enum dma_transfer_direction - dma transfer mode and direction indicator
 * @DMA_MEM_TO_MEM: Async/Memcpy mode
 * @DMA_MEM_TO_DEV: Slave mode & From Memory to Device
 * @DMA_DEV_TO_MEM: Slave mode & From Device to Memory
 * @DMA_DEV_TO_DEV: Slave mode & From Device to Device
 */
enum Model0_dma_transfer_direction {
 Model0_DMA_MEM_TO_MEM,
 Model0_DMA_MEM_TO_DEV,
 Model0_DMA_DEV_TO_MEM,
 Model0_DMA_DEV_TO_DEV,
 Model0_DMA_TRANS_NONE,
};

/**
 * Interleaved Transfer Request
 * ----------------------------
 * A chunk is collection of contiguous bytes to be transfered.
 * The gap(in bytes) between two chunks is called inter-chunk-gap(ICG).
 * ICGs may or maynot change between chunks.
 * A FRAME is the smallest series of contiguous {chunk,icg} pairs,
 *  that when repeated an integral number of times, specifies the transfer.
 * A transfer template is specification of a Frame, the number of times
 *  it is to be repeated and other per-transfer attributes.
 *
 * Practically, a client driver would have ready a template for each
 *  type of transfer it is going to need during its lifetime and
 *  set only 'src_start' and 'dst_start' before submitting the requests.
 *
 *
 *  |      Frame-1        |       Frame-2       | ~ |       Frame-'numf'  |
 *  |====....==.===...=...|====....==.===...=...| ~ |====....==.===...=...|
 *
 *    ==  Chunk size
 *    ... ICG
 */

/**
 * struct data_chunk - Element of scatter-gather list that makes a frame.
 * @size: Number of bytes to read from source.
 *	  size_dst := fn(op, size_src), so doesn't mean much for destination.
 * @icg: Number of bytes to jump after last src/dst address of this
 *	 chunk and before first src/dst address for next chunk.
 *	 Ignored for dst(assumed 0), if dst_inc is true and dst_sgl is false.
 *	 Ignored for src(assumed 0), if src_inc is true and src_sgl is false.
 * @dst_icg: Number of bytes to jump after last dst address of this
 *	 chunk and before the first dst address for next chunk.
 *	 Ignored if dst_inc is true and dst_sgl is false.
 * @src_icg: Number of bytes to jump after last src address of this
 *	 chunk and before the first src address for next chunk.
 *	 Ignored if src_inc is true and src_sgl is false.
 */
struct Model0_data_chunk {
 Model0_size_t Model0_size;
 Model0_size_t Model0_icg;
 Model0_size_t Model0_dst_icg;
 Model0_size_t Model0_src_icg;
};

/**
 * struct dma_interleaved_template - Template to convey DMAC the transfer pattern
 *	 and attributes.
 * @src_start: Bus address of source for the first chunk.
 * @dst_start: Bus address of destination for the first chunk.
 * @dir: Specifies the type of Source and Destination.
 * @src_inc: If the source address increments after reading from it.
 * @dst_inc: If the destination address increments after writing to it.
 * @src_sgl: If the 'icg' of sgl[] applies to Source (scattered read).
 *		Otherwise, source is read contiguously (icg ignored).
 *		Ignored if src_inc is false.
 * @dst_sgl: If the 'icg' of sgl[] applies to Destination (scattered write).
 *		Otherwise, destination is filled contiguously (icg ignored).
 *		Ignored if dst_inc is false.
 * @numf: Number of frames in this template.
 * @frame_size: Number of chunks in a frame i.e, size of sgl[].
 * @sgl: Array of {chunk,icg} pairs that make up a frame.
 */
struct Model0_dma_interleaved_template {
 Model0_dma_addr_t Model0_src_start;
 Model0_dma_addr_t Model0_dst_start;
 enum Model0_dma_transfer_direction Model0_dir;
 bool Model0_src_inc;
 bool Model0_dst_inc;
 bool Model0_src_sgl;
 bool Model0_dst_sgl;
 Model0_size_t Model0_numf;
 Model0_size_t Model0_frame_size;
 struct Model0_data_chunk Model0_sgl[0];
};

/**
 * enum dma_ctrl_flags - DMA flags to augment operation preparation,
 *  control completion, and communicate status.
 * @DMA_PREP_INTERRUPT - trigger an interrupt (callback) upon completion of
 *  this transaction
 * @DMA_CTRL_ACK - if clear, the descriptor cannot be reused until the client
 *  acknowledges receipt, i.e. has has a chance to establish any dependency
 *  chains
 * @DMA_PREP_PQ_DISABLE_P - prevent generation of P while generating Q
 * @DMA_PREP_PQ_DISABLE_Q - prevent generation of Q while generating P
 * @DMA_PREP_CONTINUE - indicate to a driver that it is reusing buffers as
 *  sources that were the result of a previous operation, in the case of a PQ
 *  operation it continues the calculation with new sources
 * @DMA_PREP_FENCE - tell the driver that subsequent operations depend
 *  on the result of this operation
 * @DMA_CTRL_REUSE: client can reuse the descriptor and submit again till
 *  cleared or freed
 */
enum Model0_dma_ctrl_flags {
 Model0_DMA_PREP_INTERRUPT = (1 << 0),
 Model0_DMA_CTRL_ACK = (1 << 1),
 Model0_DMA_PREP_PQ_DISABLE_P = (1 << 2),
 Model0_DMA_PREP_PQ_DISABLE_Q = (1 << 3),
 Model0_DMA_PREP_CONTINUE = (1 << 4),
 Model0_DMA_PREP_FENCE = (1 << 5),
 Model0_DMA_CTRL_REUSE = (1 << 6),
};

/**
 * enum sum_check_bits - bit position of pq_check_flags
 */
enum Model0_sum_check_bits {
 Model0_SUM_CHECK_P = 0,
 Model0_SUM_CHECK_Q = 1,
};

/**
 * enum pq_check_flags - result of async_{xor,pq}_zero_sum operations
 * @SUM_CHECK_P_RESULT - 1 if xor zero sum error, 0 otherwise
 * @SUM_CHECK_Q_RESULT - 1 if reed-solomon zero sum error, 0 otherwise
 */
enum Model0_sum_check_flags {
 Model0_SUM_CHECK_P_RESULT = (1 << Model0_SUM_CHECK_P),
 Model0_SUM_CHECK_Q_RESULT = (1 << Model0_SUM_CHECK_Q),
};


/**
 * dma_cap_mask_t - capabilities bitmap modeled after cpumask_t.
 * See linux/cpumask.h
 */
typedef struct { unsigned long Model0_bits[(((Model0_DMA_TX_TYPE_END) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } Model0_dma_cap_mask_t;

/**
 * struct dma_chan_percpu - the per-CPU part of struct dma_chan
 * @memcpy_count: transaction counter
 * @bytes_transferred: byte counter
 */

struct Model0_dma_chan_percpu {
 /* stats */
 unsigned long Model0_memcpy_count;
 unsigned long Model0_bytes_transferred;
};

/**
 * struct dma_router - DMA router structure
 * @dev: pointer to the DMA router device
 * @route_free: function to be called when the route can be disconnected
 */
struct Model0_dma_router {
 struct Model0_device *Model0_dev;
 void (*Model0_route_free)(struct Model0_device *Model0_dev, void *Model0_route_data);
};

/**
 * struct dma_chan - devices supply DMA channels, clients use them
 * @device: ptr to the dma device who supplies this channel, always !%NULL
 * @cookie: last cookie value returned to client
 * @completed_cookie: last completed cookie for this channel
 * @chan_id: channel ID for sysfs
 * @dev: class device for sysfs
 * @device_node: used to add this to the device chan list
 * @local: per-cpu pointer to a struct dma_chan_percpu
 * @client_count: how many clients are using this channel
 * @table_count: number of appearances in the mem-to-mem allocation table
 * @router: pointer to the DMA router structure
 * @route_data: channel specific data for the router
 * @private: private data for certain client-channel associations
 */
struct Model0_dma_chan {
 struct Model0_dma_device *Model0_device;
 Model0_dma_cookie_t Model0_cookie;
 Model0_dma_cookie_t Model0_completed_cookie;

 /* sysfs */
 int Model0_chan_id;
 struct Model0_dma_chan_dev *Model0_dev;

 struct Model0_list_head Model0_device_node;
 struct Model0_dma_chan_percpu *Model0_local;
 int Model0_client_count;
 int Model0_table_count;

 /* DMA router */
 struct Model0_dma_router *Model0_router;
 void *Model0_route_data;

 void *Model0_private;
};

/**
 * struct dma_chan_dev - relate sysfs device node to backing channel device
 * @chan: driver channel device
 * @device: sysfs device
 * @dev_id: parent dma_device dev_id
 * @idr_ref: reference count to gate release of dma_device dev_id
 */
struct Model0_dma_chan_dev {
 struct Model0_dma_chan *Model0_chan;
 struct Model0_device Model0_device;
 int Model0_dev_id;
 Model0_atomic_t *Model0_idr_ref;
};

/**
 * enum dma_slave_buswidth - defines bus width of the DMA slave
 * device, source or target buses
 */
enum Model0_dma_slave_buswidth {
 Model0_DMA_SLAVE_BUSWIDTH_UNDEFINED = 0,
 Model0_DMA_SLAVE_BUSWIDTH_1_BYTE = 1,
 Model0_DMA_SLAVE_BUSWIDTH_2_BYTES = 2,
 Model0_DMA_SLAVE_BUSWIDTH_3_BYTES = 3,
 Model0_DMA_SLAVE_BUSWIDTH_4_BYTES = 4,
 Model0_DMA_SLAVE_BUSWIDTH_8_BYTES = 8,
 Model0_DMA_SLAVE_BUSWIDTH_16_BYTES = 16,
 Model0_DMA_SLAVE_BUSWIDTH_32_BYTES = 32,
 Model0_DMA_SLAVE_BUSWIDTH_64_BYTES = 64,
};

/**
 * struct dma_slave_config - dma slave channel runtime config
 * @direction: whether the data shall go in or out on this slave
 * channel, right now. DMA_MEM_TO_DEV and DMA_DEV_TO_MEM are
 * legal values. DEPRECATED, drivers should use the direction argument
 * to the device_prep_slave_sg and device_prep_dma_cyclic functions or
 * the dir field in the dma_interleaved_template structure.
 * @src_addr: this is the physical address where DMA slave data
 * should be read (RX), if the source is memory this argument is
 * ignored.
 * @dst_addr: this is the physical address where DMA slave data
 * should be written (TX), if the source is memory this argument
 * is ignored.
 * @src_addr_width: this is the width in bytes of the source (RX)
 * register where DMA data shall be read. If the source
 * is memory this may be ignored depending on architecture.
 * Legal values: 1, 2, 4, 8.
 * @dst_addr_width: same as src_addr_width but for destination
 * target (TX) mutatis mutandis.
 * @src_maxburst: the maximum number of words (note: words, as in
 * units of the src_addr_width member, not bytes) that can be sent
 * in one burst to the device. Typically something like half the
 * FIFO depth on I/O peripherals so you don't overflow it. This
 * may or may not be applicable on memory sources.
 * @dst_maxburst: same as src_maxburst but for destination target
 * mutatis mutandis.
 * @device_fc: Flow Controller Settings. Only valid for slave channels. Fill
 * with 'true' if peripheral should be flow controller. Direction will be
 * selected at Runtime.
 * @slave_id: Slave requester id. Only valid for slave channels. The dma
 * slave peripheral will have unique id as dma requester which need to be
 * pass as slave config.
 *
 * This struct is passed in as configuration data to a DMA engine
 * in order to set up a certain channel for DMA transport at runtime.
 * The DMA device/engine has to provide support for an additional
 * callback in the dma_device structure, device_config and this struct
 * will then be passed in as an argument to the function.
 *
 * The rationale for adding configuration information to this struct is as
 * follows: if it is likely that more than one DMA slave controllers in
 * the world will support the configuration option, then make it generic.
 * If not: if it is fixed so that it be sent in static from the platform
 * data, then prefer to do that.
 */
struct Model0_dma_slave_config {
 enum Model0_dma_transfer_direction Model0_direction;
 Model0_phys_addr_t Model0_src_addr;
 Model0_phys_addr_t Model0_dst_addr;
 enum Model0_dma_slave_buswidth Model0_src_addr_width;
 enum Model0_dma_slave_buswidth Model0_dst_addr_width;
 Model0_u32 Model0_src_maxburst;
 Model0_u32 Model0_dst_maxburst;
 bool Model0_device_fc;
 unsigned int Model0_slave_id;
};

/**
 * enum dma_residue_granularity - Granularity of the reported transfer residue
 * @DMA_RESIDUE_GRANULARITY_DESCRIPTOR: Residue reporting is not support. The
 *  DMA channel is only able to tell whether a descriptor has been completed or
 *  not, which means residue reporting is not supported by this channel. The
 *  residue field of the dma_tx_state field will always be 0.
 * @DMA_RESIDUE_GRANULARITY_SEGMENT: Residue is updated after each successfully
 *  completed segment of the transfer (For cyclic transfers this is after each
 *  period). This is typically implemented by having the hardware generate an
 *  interrupt after each transferred segment and then the drivers updates the
 *  outstanding residue by the size of the segment. Another possibility is if
 *  the hardware supports scatter-gather and the segment descriptor has a field
 *  which gets set after the segment has been completed. The driver then counts
 *  the number of segments without the flag set to compute the residue.
 * @DMA_RESIDUE_GRANULARITY_BURST: Residue is updated after each transferred
 *  burst. This is typically only supported if the hardware has a progress
 *  register of some sort (E.g. a register with the current read/write address
 *  or a register with the amount of bursts/beats/bytes that have been
 *  transferred or still need to be transferred).
 */
enum Model0_dma_residue_granularity {
 Model0_DMA_RESIDUE_GRANULARITY_DESCRIPTOR = 0,
 Model0_DMA_RESIDUE_GRANULARITY_SEGMENT = 1,
 Model0_DMA_RESIDUE_GRANULARITY_BURST = 2,
};

/* struct dma_slave_caps - expose capabilities of a slave channel only
 *
 * @src_addr_widths: bit mask of src addr widths the channel supports
 * @dst_addr_widths: bit mask of dstn addr widths the channel supports
 * @directions: bit mask of slave direction the channel supported
 * 	since the enum dma_transfer_direction is not defined as bits for each
 * 	type of direction, the dma controller should fill (1 << <TYPE>) and same
 * 	should be checked by controller as well
 * @max_burst: max burst capability per-transfer
 * @cmd_pause: true, if pause and thereby resume is supported
 * @cmd_terminate: true, if terminate cmd is supported
 * @residue_granularity: granularity of the reported transfer residue
 * @descriptor_reuse: if a descriptor can be reused by client and
 * resubmitted multiple times
 */
struct Model0_dma_slave_caps {
 Model0_u32 Model0_src_addr_widths;
 Model0_u32 Model0_dst_addr_widths;
 Model0_u32 Model0_directions;
 Model0_u32 Model0_max_burst;
 bool Model0_cmd_pause;
 bool Model0_cmd_terminate;
 enum Model0_dma_residue_granularity Model0_residue_granularity;
 bool Model0_descriptor_reuse;
};

static inline __attribute__((no_instrument_function)) const char *Model0_dma_chan_name(struct Model0_dma_chan *Model0_chan)
{
 return Model0_dev_name(&Model0_chan->Model0_dev->Model0_device);
}

void Model0_dma_chan_cleanup(struct Model0_kref *Model0_kref);

/**
 * typedef dma_filter_fn - callback filter for dma_request_channel
 * @chan: channel to be reviewed
 * @filter_param: opaque parameter passed through dma_request_channel
 *
 * When this optional parameter is specified in a call to dma_request_channel a
 * suitable channel is passed to this routine for further dispositioning before
 * being returned.  Where 'suitable' indicates a non-busy channel that
 * satisfies the given capability mask.  It returns 'true' to indicate that the
 * channel is suitable.
 */
typedef bool (*Model0_dma_filter_fn)(struct Model0_dma_chan *Model0_chan, void *Model0_filter_param);

typedef void (*Model0_dma_async_tx_callback)(void *Model0_dma_async_param);

struct Model0_dmaengine_unmap_data {
 Model0_u8 Model0_map_cnt;
 Model0_u8 Model0_to_cnt;
 Model0_u8 Model0_from_cnt;
 Model0_u8 Model0_bidi_cnt;
 struct Model0_device *Model0_dev;
 struct Model0_kref Model0_kref;
 Model0_size_t Model0_len;
 Model0_dma_addr_t Model0_addr[0];
};

/**
 * struct dma_async_tx_descriptor - async transaction descriptor
 * ---dma generic offload fields---
 * @cookie: tracking cookie for this transaction, set to -EBUSY if
 *	this tx is sitting on a dependency list
 * @flags: flags to augment operation preparation, control completion, and
 * 	communicate status
 * @phys: physical address of the descriptor
 * @chan: target channel for this operation
 * @tx_submit: accept the descriptor, assign ordered cookie and mark the
 * descriptor pending. To be pushed on .issue_pending() call
 * @callback: routine to call after this operation is complete
 * @callback_param: general parameter to pass to the callback routine
 * ---async_tx api specific fields---
 * @next: at completion submit this descriptor
 * @parent: pointer to the next level up in the dependency chain
 * @lock: protect the parent and next pointers
 */
struct Model0_dma_async_tx_descriptor {
 Model0_dma_cookie_t Model0_cookie;
 enum Model0_dma_ctrl_flags Model0_flags; /* not a 'long' to pack with cookie */
 Model0_dma_addr_t Model0_phys;
 struct Model0_dma_chan *Model0_chan;
 Model0_dma_cookie_t (*Model0_tx_submit)(struct Model0_dma_async_tx_descriptor *Model0_tx);
 int (*Model0_desc_free)(struct Model0_dma_async_tx_descriptor *Model0_tx);
 Model0_dma_async_tx_callback Model0_callback;
 void *Model0_callback_param;
 struct Model0_dmaengine_unmap_data *Model0_unmap;





};


static inline __attribute__((no_instrument_function)) void Model0_dma_set_unmap(struct Model0_dma_async_tx_descriptor *Model0_tx,
     struct Model0_dmaengine_unmap_data *Model0_unmap)
{
 Model0_kref_get(&Model0_unmap->Model0_kref);
 Model0_tx->Model0_unmap = Model0_unmap;
}

struct Model0_dmaengine_unmap_data *
Model0_dmaengine_get_unmap_data(struct Model0_device *Model0_dev, int Model0_nr, Model0_gfp_t Model0_flags);
void Model0_dmaengine_unmap_put(struct Model0_dmaengine_unmap_data *Model0_unmap);
static inline __attribute__((no_instrument_function)) void Model0_dma_descriptor_unmap(struct Model0_dma_async_tx_descriptor *Model0_tx)
{
 if (Model0_tx->Model0_unmap) {
  Model0_dmaengine_unmap_put(Model0_tx->Model0_unmap);
  Model0_tx->Model0_unmap = ((void *)0);
 }
}


static inline __attribute__((no_instrument_function)) void Model0_txd_lock(struct Model0_dma_async_tx_descriptor *Model0_txd)
{
}
static inline __attribute__((no_instrument_function)) void Model0_txd_unlock(struct Model0_dma_async_tx_descriptor *Model0_txd)
{
}
static inline __attribute__((no_instrument_function)) void Model0_txd_chain(struct Model0_dma_async_tx_descriptor *Model0_txd, struct Model0_dma_async_tx_descriptor *Model0_next)
{
 do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dmaengine.h"), "i" (533), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0);
}
static inline __attribute__((no_instrument_function)) void Model0_txd_clear_parent(struct Model0_dma_async_tx_descriptor *Model0_txd)
{
}
static inline __attribute__((no_instrument_function)) void Model0_txd_clear_next(struct Model0_dma_async_tx_descriptor *Model0_txd)
{
}
static inline __attribute__((no_instrument_function)) struct Model0_dma_async_tx_descriptor *Model0_txd_next(struct Model0_dma_async_tx_descriptor *Model0_txd)
{
 return ((void *)0);
}
static inline __attribute__((no_instrument_function)) struct Model0_dma_async_tx_descriptor *Model0_txd_parent(struct Model0_dma_async_tx_descriptor *Model0_txd)
{
 return ((void *)0);
}
/**
 * struct dma_tx_state - filled in to report the status of
 * a transfer.
 * @last: last completed DMA cookie
 * @used: last issued DMA cookie (i.e. the one in progress)
 * @residue: the remaining number of bytes left to transmit
 *	on the selected transfer for states DMA_IN_PROGRESS and
 *	DMA_PAUSED if this is implemented in the driver, else 0
 */
struct Model0_dma_tx_state {
 Model0_dma_cookie_t Model0_last;
 Model0_dma_cookie_t Model0_used;
 Model0_u32 Model0_residue;
};

/**
 * enum dmaengine_alignment - defines alignment of the DMA async tx
 * buffers
 */
enum Model0_dmaengine_alignment {
 Model0_DMAENGINE_ALIGN_1_BYTE = 0,
 Model0_DMAENGINE_ALIGN_2_BYTES = 1,
 Model0_DMAENGINE_ALIGN_4_BYTES = 2,
 Model0_DMAENGINE_ALIGN_8_BYTES = 3,
 Model0_DMAENGINE_ALIGN_16_BYTES = 4,
 Model0_DMAENGINE_ALIGN_32_BYTES = 5,
 Model0_DMAENGINE_ALIGN_64_BYTES = 6,
};

/**
 * struct dma_slave_map - associates slave device and it's slave channel with
 * parameter to be used by a filter function
 * @devname: name of the device
 * @slave: slave channel name
 * @param: opaque parameter to pass to struct dma_filter.fn
 */
struct Model0_dma_slave_map {
 const char *Model0_devname;
 const char *Model0_slave;
 void *Model0_param;
};

/**
 * struct dma_filter - information for slave device/channel to filter_fn/param
 * mapping
 * @fn: filter function callback
 * @mapcnt: number of slave device/channel in the map
 * @map: array of channel to filter mapping data
 */
struct Model0_dma_filter {
 Model0_dma_filter_fn Model0_fn;
 int Model0_mapcnt;
 const struct Model0_dma_slave_map *Model0_map;
};

/**
 * struct dma_device - info on the entity supplying DMA services
 * @chancnt: how many DMA channels are supported
 * @privatecnt: how many DMA channels are requested by dma_request_channel
 * @channels: the list of struct dma_chan
 * @global_node: list_head for global dma_device_list
 * @filter: information for device/slave to filter function/param mapping
 * @cap_mask: one or more dma_capability flags
 * @max_xor: maximum number of xor sources, 0 if no capability
 * @max_pq: maximum number of PQ sources and PQ-continue capability
 * @copy_align: alignment shift for memcpy operations
 * @xor_align: alignment shift for xor operations
 * @pq_align: alignment shift for pq operations
 * @fill_align: alignment shift for memset operations
 * @dev_id: unique device ID
 * @dev: struct device reference for dma mapping api
 * @src_addr_widths: bit mask of src addr widths the device supports
 * @dst_addr_widths: bit mask of dst addr widths the device supports
 * @directions: bit mask of slave direction the device supports since
 * 	the enum dma_transfer_direction is not defined as bits for
 * 	each type of direction, the dma controller should fill (1 <<
 * 	<TYPE>) and same should be checked by controller as well
 * @max_burst: max burst capability per-transfer
 * @residue_granularity: granularity of the transfer residue reported
 *	by tx_status
 * @device_alloc_chan_resources: allocate resources and return the
 *	number of allocated descriptors
 * @device_free_chan_resources: release DMA channel's resources
 * @device_prep_dma_memcpy: prepares a memcpy operation
 * @device_prep_dma_xor: prepares a xor operation
 * @device_prep_dma_xor_val: prepares a xor validation operation
 * @device_prep_dma_pq: prepares a pq operation
 * @device_prep_dma_pq_val: prepares a pqzero_sum operation
 * @device_prep_dma_memset: prepares a memset operation
 * @device_prep_dma_memset_sg: prepares a memset operation over a scatter list
 * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
 * @device_prep_slave_sg: prepares a slave dma operation
 * @device_prep_dma_cyclic: prepare a cyclic dma operation suitable for audio.
 *	The function takes a buffer of size buf_len. The callback function will
 *	be called after period_len bytes have been transferred.
 * @device_prep_interleaved_dma: Transfer expression in a generic way.
 * @device_prep_dma_imm_data: DMA's 8 byte immediate data to the dst address
 * @device_config: Pushes a new configuration to a channel, return 0 or an error
 *	code
 * @device_pause: Pauses any transfer happening on a channel. Returns
 *	0 or an error code
 * @device_resume: Resumes any transfer on a channel previously
 *	paused. Returns 0 or an error code
 * @device_terminate_all: Aborts all transfers on a channel. Returns 0
 *	or an error code
 * @device_synchronize: Synchronizes the termination of a transfers to the
 *  current context.
 * @device_tx_status: poll for transaction completion, the optional
 *	txstate parameter can be supplied with a pointer to get a
 *	struct with auxiliary transfer status information, otherwise the call
 *	will just return a simple status code
 * @device_issue_pending: push pending transactions to hardware
 * @descriptor_reuse: a submitted transfer can be resubmitted after completion
 */
struct Model0_dma_device {

 unsigned int Model0_chancnt;
 unsigned int Model0_privatecnt;
 struct Model0_list_head Model0_channels;
 struct Model0_list_head Model0_global_node;
 struct Model0_dma_filter Model0_filter;
 Model0_dma_cap_mask_t Model0_cap_mask;
 unsigned short Model0_max_xor;
 unsigned short Model0_max_pq;
 enum Model0_dmaengine_alignment Model0_copy_align;
 enum Model0_dmaengine_alignment Model0_xor_align;
 enum Model0_dmaengine_alignment Model0_pq_align;
 enum Model0_dmaengine_alignment Model0_fill_align;


 int Model0_dev_id;
 struct Model0_device *Model0_dev;

 Model0_u32 Model0_src_addr_widths;
 Model0_u32 Model0_dst_addr_widths;
 Model0_u32 Model0_directions;
 Model0_u32 Model0_max_burst;
 bool Model0_descriptor_reuse;
 enum Model0_dma_residue_granularity Model0_residue_granularity;

 int (*Model0_device_alloc_chan_resources)(struct Model0_dma_chan *Model0_chan);
 void (*Model0_device_free_chan_resources)(struct Model0_dma_chan *Model0_chan);

 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_memcpy)(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t Model0_dst, Model0_dma_addr_t Model0_src,
  Model0_size_t Model0_len, unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_xor)(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t Model0_dst, Model0_dma_addr_t *Model0_src,
  unsigned int Model0_src_cnt, Model0_size_t Model0_len, unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_xor_val)(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t *Model0_src, unsigned int Model0_src_cnt,
  Model0_size_t Model0_len, enum Model0_sum_check_flags *Model0_result, unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_pq)(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t *Model0_dst, Model0_dma_addr_t *Model0_src,
  unsigned int Model0_src_cnt, const unsigned char *Model0_scf,
  Model0_size_t Model0_len, unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_pq_val)(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t *Model0_pq, Model0_dma_addr_t *Model0_src,
  unsigned int Model0_src_cnt, const unsigned char *Model0_scf, Model0_size_t Model0_len,
  enum Model0_sum_check_flags *Model0_pqres, unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_memset)(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t Model0_dest, int Model0_value, Model0_size_t Model0_len,
  unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_memset_sg)(
  struct Model0_dma_chan *Model0_chan, struct Model0_scatterlist *Model0_sg,
  unsigned int Model0_nents, int Model0_value, unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_interrupt)(
  struct Model0_dma_chan *Model0_chan, unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_sg)(
  struct Model0_dma_chan *Model0_chan,
  struct Model0_scatterlist *Model0_dst_sg, unsigned int Model0_dst_nents,
  struct Model0_scatterlist *Model0_src_sg, unsigned int Model0_src_nents,
  unsigned long Model0_flags);

 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_slave_sg)(
  struct Model0_dma_chan *Model0_chan, struct Model0_scatterlist *Model0_sgl,
  unsigned int Model0_sg_len, enum Model0_dma_transfer_direction Model0_direction,
  unsigned long Model0_flags, void *Model0_context);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_cyclic)(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t Model0_buf_addr, Model0_size_t Model0_buf_len,
  Model0_size_t Model0_period_len, enum Model0_dma_transfer_direction Model0_direction,
  unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_interleaved_dma)(
  struct Model0_dma_chan *Model0_chan, struct Model0_dma_interleaved_template *Model0_xt,
  unsigned long Model0_flags);
 struct Model0_dma_async_tx_descriptor *(*Model0_device_prep_dma_imm_data)(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t Model0_dst, Model0_u64 Model0_data,
  unsigned long Model0_flags);

 int (*Model0_device_config)(struct Model0_dma_chan *Model0_chan,
        struct Model0_dma_slave_config *Model0_config);
 int (*Model0_device_pause)(struct Model0_dma_chan *Model0_chan);
 int (*Model0_device_resume)(struct Model0_dma_chan *Model0_chan);
 int (*Model0_device_terminate_all)(struct Model0_dma_chan *Model0_chan);
 void (*Model0_device_synchronize)(struct Model0_dma_chan *Model0_chan);

 enum Model0_dma_status (*Model0_device_tx_status)(struct Model0_dma_chan *Model0_chan,
         Model0_dma_cookie_t Model0_cookie,
         struct Model0_dma_tx_state *Model0_txstate);
 void (*Model0_device_issue_pending)(struct Model0_dma_chan *Model0_chan);
};

static inline __attribute__((no_instrument_function)) int Model0_dmaengine_slave_config(struct Model0_dma_chan *Model0_chan,
       struct Model0_dma_slave_config *Model0_config)
{
 if (Model0_chan->Model0_device->Model0_device_config)
  return Model0_chan->Model0_device->Model0_device_config(Model0_chan, Model0_config);

 return -38;
}

static inline __attribute__((no_instrument_function)) bool Model0_is_slave_direction(enum Model0_dma_transfer_direction Model0_direction)
{
 return (Model0_direction == Model0_DMA_MEM_TO_DEV) || (Model0_direction == Model0_DMA_DEV_TO_MEM);
}

static inline __attribute__((no_instrument_function)) struct Model0_dma_async_tx_descriptor *Model0_dmaengine_prep_slave_single(
 struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t Model0_buf, Model0_size_t Model0_len,
 enum Model0_dma_transfer_direction Model0_dir, unsigned long Model0_flags)
{
 struct Model0_scatterlist Model0_sg;
 Model0_sg_init_table(&Model0_sg, 1);
 ((&Model0_sg)->Model0_dma_address) = Model0_buf;
 ((&Model0_sg)->Model0_dma_length) = Model0_len;

 if (!Model0_chan || !Model0_chan->Model0_device || !Model0_chan->Model0_device->Model0_device_prep_slave_sg)
  return ((void *)0);

 return Model0_chan->Model0_device->Model0_device_prep_slave_sg(Model0_chan, &Model0_sg, 1,
        Model0_dir, Model0_flags, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model0_dma_async_tx_descriptor *Model0_dmaengine_prep_slave_sg(
 struct Model0_dma_chan *Model0_chan, struct Model0_scatterlist *Model0_sgl, unsigned int Model0_sg_len,
 enum Model0_dma_transfer_direction Model0_dir, unsigned long Model0_flags)
{
 if (!Model0_chan || !Model0_chan->Model0_device || !Model0_chan->Model0_device->Model0_device_prep_slave_sg)
  return ((void *)0);

 return Model0_chan->Model0_device->Model0_device_prep_slave_sg(Model0_chan, Model0_sgl, Model0_sg_len,
        Model0_dir, Model0_flags, ((void *)0));
}
static inline __attribute__((no_instrument_function)) struct Model0_dma_async_tx_descriptor *Model0_dmaengine_prep_dma_cyclic(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t Model0_buf_addr, Model0_size_t Model0_buf_len,
  Model0_size_t Model0_period_len, enum Model0_dma_transfer_direction Model0_dir,
  unsigned long Model0_flags)
{
 if (!Model0_chan || !Model0_chan->Model0_device || !Model0_chan->Model0_device->Model0_device_prep_dma_cyclic)
  return ((void *)0);

 return Model0_chan->Model0_device->Model0_device_prep_dma_cyclic(Model0_chan, Model0_buf_addr, Model0_buf_len,
      Model0_period_len, Model0_dir, Model0_flags);
}

static inline __attribute__((no_instrument_function)) struct Model0_dma_async_tx_descriptor *Model0_dmaengine_prep_interleaved_dma(
  struct Model0_dma_chan *Model0_chan, struct Model0_dma_interleaved_template *Model0_xt,
  unsigned long Model0_flags)
{
 if (!Model0_chan || !Model0_chan->Model0_device || !Model0_chan->Model0_device->Model0_device_prep_interleaved_dma)
  return ((void *)0);

 return Model0_chan->Model0_device->Model0_device_prep_interleaved_dma(Model0_chan, Model0_xt, Model0_flags);
}

static inline __attribute__((no_instrument_function)) struct Model0_dma_async_tx_descriptor *Model0_dmaengine_prep_dma_memset(
  struct Model0_dma_chan *Model0_chan, Model0_dma_addr_t Model0_dest, int Model0_value, Model0_size_t Model0_len,
  unsigned long Model0_flags)
{
 if (!Model0_chan || !Model0_chan->Model0_device || !Model0_chan->Model0_device->Model0_device_prep_dma_memset)
  return ((void *)0);

 return Model0_chan->Model0_device->Model0_device_prep_dma_memset(Model0_chan, Model0_dest, Model0_value,
          Model0_len, Model0_flags);
}

static inline __attribute__((no_instrument_function)) struct Model0_dma_async_tx_descriptor *Model0_dmaengine_prep_dma_sg(
  struct Model0_dma_chan *Model0_chan,
  struct Model0_scatterlist *Model0_dst_sg, unsigned int Model0_dst_nents,
  struct Model0_scatterlist *Model0_src_sg, unsigned int Model0_src_nents,
  unsigned long Model0_flags)
{
 if (!Model0_chan || !Model0_chan->Model0_device || !Model0_chan->Model0_device->Model0_device_prep_dma_sg)
  return ((void *)0);

 return Model0_chan->Model0_device->Model0_device_prep_dma_sg(Model0_chan, Model0_dst_sg, Model0_dst_nents,
   Model0_src_sg, Model0_src_nents, Model0_flags);
}

/**
 * dmaengine_terminate_all() - Terminate all active DMA transfers
 * @chan: The channel for which to terminate the transfers
 *
 * This function is DEPRECATED use either dmaengine_terminate_sync() or
 * dmaengine_terminate_async() instead.
 */
static inline __attribute__((no_instrument_function)) int Model0_dmaengine_terminate_all(struct Model0_dma_chan *Model0_chan)
{
 if (Model0_chan->Model0_device->Model0_device_terminate_all)
  return Model0_chan->Model0_device->Model0_device_terminate_all(Model0_chan);

 return -38;
}

/**
 * dmaengine_terminate_async() - Terminate all active DMA transfers
 * @chan: The channel for which to terminate the transfers
 *
 * Calling this function will terminate all active and pending descriptors
 * that have previously been submitted to the channel. It is not guaranteed
 * though that the transfer for the active descriptor has stopped when the
 * function returns. Furthermore it is possible the complete callback of a
 * submitted transfer is still running when this function returns.
 *
 * dmaengine_synchronize() needs to be called before it is safe to free
 * any memory that is accessed by previously submitted descriptors or before
 * freeing any resources accessed from within the completion callback of any
 * perviously submitted descriptors.
 *
 * This function can be called from atomic context as well as from within a
 * complete callback of a descriptor submitted on the same channel.
 *
 * If none of the two conditions above apply consider using
 * dmaengine_terminate_sync() instead.
 */
static inline __attribute__((no_instrument_function)) int Model0_dmaengine_terminate_async(struct Model0_dma_chan *Model0_chan)
{
 if (Model0_chan->Model0_device->Model0_device_terminate_all)
  return Model0_chan->Model0_device->Model0_device_terminate_all(Model0_chan);

 return -22;
}

/**
 * dmaengine_synchronize() - Synchronize DMA channel termination
 * @chan: The channel to synchronize
 *
 * Synchronizes to the DMA channel termination to the current context. When this
 * function returns it is guaranteed that all transfers for previously issued
 * descriptors have stopped and and it is safe to free the memory assoicated
 * with them. Furthermore it is guaranteed that all complete callback functions
 * for a previously submitted descriptor have finished running and it is safe to
 * free resources accessed from within the complete callbacks.
 *
 * The behavior of this function is undefined if dma_async_issue_pending() has
 * been called between dmaengine_terminate_async() and this function.
 *
 * This function must only be called from non-atomic context and must not be
 * called from within a complete callback of a descriptor submitted on the same
 * channel.
 */
static inline __attribute__((no_instrument_function)) void Model0_dmaengine_synchronize(struct Model0_dma_chan *Model0_chan)
{
 do { Model0__cond_resched(); } while (0);

 if (Model0_chan->Model0_device->Model0_device_synchronize)
  Model0_chan->Model0_device->Model0_device_synchronize(Model0_chan);
}

/**
 * dmaengine_terminate_sync() - Terminate all active DMA transfers
 * @chan: The channel for which to terminate the transfers
 *
 * Calling this function will terminate all active and pending transfers
 * that have previously been submitted to the channel. It is similar to
 * dmaengine_terminate_async() but guarantees that the DMA transfer has actually
 * stopped and that all complete callbacks have finished running when the
 * function returns.
 *
 * This function must only be called from non-atomic context and must not be
 * called from within a complete callback of a descriptor submitted on the same
 * channel.
 */
static inline __attribute__((no_instrument_function)) int Model0_dmaengine_terminate_sync(struct Model0_dma_chan *Model0_chan)
{
 int Model0_ret;

 Model0_ret = Model0_dmaengine_terminate_async(Model0_chan);
 if (Model0_ret)
  return Model0_ret;

 Model0_dmaengine_synchronize(Model0_chan);

 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_dmaengine_pause(struct Model0_dma_chan *Model0_chan)
{
 if (Model0_chan->Model0_device->Model0_device_pause)
  return Model0_chan->Model0_device->Model0_device_pause(Model0_chan);

 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_dmaengine_resume(struct Model0_dma_chan *Model0_chan)
{
 if (Model0_chan->Model0_device->Model0_device_resume)
  return Model0_chan->Model0_device->Model0_device_resume(Model0_chan);

 return -38;
}

static inline __attribute__((no_instrument_function)) enum Model0_dma_status Model0_dmaengine_tx_status(struct Model0_dma_chan *Model0_chan,
 Model0_dma_cookie_t Model0_cookie, struct Model0_dma_tx_state *Model0_state)
{
 return Model0_chan->Model0_device->Model0_device_tx_status(Model0_chan, Model0_cookie, Model0_state);
}

static inline __attribute__((no_instrument_function)) Model0_dma_cookie_t Model0_dmaengine_submit(struct Model0_dma_async_tx_descriptor *Model0_desc)
{
 return Model0_desc->Model0_tx_submit(Model0_desc);
}

static inline __attribute__((no_instrument_function)) bool Model0_dmaengine_check_align(enum Model0_dmaengine_alignment Model0_align,
      Model0_size_t Model0_off1, Model0_size_t Model0_off2, Model0_size_t Model0_len)
{
 Model0_size_t Model0_mask;

 if (!Model0_align)
  return true;
 Model0_mask = (1 << Model0_align) - 1;
 if (Model0_mask & (Model0_off1 | Model0_off2 | Model0_len))
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_is_dma_copy_aligned(struct Model0_dma_device *Model0_dev, Model0_size_t Model0_off1,
           Model0_size_t Model0_off2, Model0_size_t Model0_len)
{
 return Model0_dmaengine_check_align(Model0_dev->Model0_copy_align, Model0_off1, Model0_off2, Model0_len);
}

static inline __attribute__((no_instrument_function)) bool Model0_is_dma_xor_aligned(struct Model0_dma_device *Model0_dev, Model0_size_t Model0_off1,
          Model0_size_t Model0_off2, Model0_size_t Model0_len)
{
 return Model0_dmaengine_check_align(Model0_dev->Model0_xor_align, Model0_off1, Model0_off2, Model0_len);
}

static inline __attribute__((no_instrument_function)) bool Model0_is_dma_pq_aligned(struct Model0_dma_device *Model0_dev, Model0_size_t Model0_off1,
         Model0_size_t Model0_off2, Model0_size_t Model0_len)
{
 return Model0_dmaengine_check_align(Model0_dev->Model0_pq_align, Model0_off1, Model0_off2, Model0_len);
}

static inline __attribute__((no_instrument_function)) bool Model0_is_dma_fill_aligned(struct Model0_dma_device *Model0_dev, Model0_size_t Model0_off1,
           Model0_size_t Model0_off2, Model0_size_t Model0_len)
{
 return Model0_dmaengine_check_align(Model0_dev->Model0_fill_align, Model0_off1, Model0_off2, Model0_len);
}

static inline __attribute__((no_instrument_function)) void
Model0_dma_set_maxpq(struct Model0_dma_device *Model0_dma, int Model0_maxpq, int Model0_has_pq_continue)
{
 Model0_dma->Model0_max_pq = Model0_maxpq;
 if (Model0_has_pq_continue)
  Model0_dma->Model0_max_pq |= (1 << 15);
}

static inline __attribute__((no_instrument_function)) bool Model0_dmaf_continue(enum Model0_dma_ctrl_flags Model0_flags)
{
 return (Model0_flags & Model0_DMA_PREP_CONTINUE) == Model0_DMA_PREP_CONTINUE;
}

static inline __attribute__((no_instrument_function)) bool Model0_dmaf_p_disabled_continue(enum Model0_dma_ctrl_flags Model0_flags)
{
 enum Model0_dma_ctrl_flags Model0_mask = Model0_DMA_PREP_CONTINUE | Model0_DMA_PREP_PQ_DISABLE_P;

 return (Model0_flags & Model0_mask) == Model0_mask;
}

static inline __attribute__((no_instrument_function)) bool Model0_dma_dev_has_pq_continue(struct Model0_dma_device *Model0_dma)
{
 return (Model0_dma->Model0_max_pq & (1 << 15)) == (1 << 15);
}

static inline __attribute__((no_instrument_function)) unsigned short Model0_dma_dev_to_maxpq(struct Model0_dma_device *Model0_dma)
{
 return Model0_dma->Model0_max_pq & ~(1 << 15);
}

/* dma_maxpq - reduce maxpq in the face of continued operations
 * @dma - dma device with PQ capability
 * @flags - to check if DMA_PREP_CONTINUE and DMA_PREP_PQ_DISABLE_P are set
 *
 * When an engine does not support native continuation we need 3 extra
 * source slots to reuse P and Q with the following coefficients:
 * 1/ {00} * P : remove P from Q', but use it as a source for P'
 * 2/ {01} * Q : use Q to continue Q' calculation
 * 3/ {00} * Q : subtract Q from P' to cancel (2)
 *
 * In the case where P is disabled we only need 1 extra source:
 * 1/ {01} * Q : use Q to continue Q' calculation
 */
static inline __attribute__((no_instrument_function)) int Model0_dma_maxpq(struct Model0_dma_device *Model0_dma, enum Model0_dma_ctrl_flags Model0_flags)
{
 if (Model0_dma_dev_has_pq_continue(Model0_dma) || !Model0_dmaf_continue(Model0_flags))
  return Model0_dma_dev_to_maxpq(Model0_dma);
 else if (Model0_dmaf_p_disabled_continue(Model0_flags))
  return Model0_dma_dev_to_maxpq(Model0_dma) - 1;
 else if (Model0_dmaf_continue(Model0_flags))
  return Model0_dma_dev_to_maxpq(Model0_dma) - 3;
 do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dmaengine.h"), "i" (1098), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0);
}

static inline __attribute__((no_instrument_function)) Model0_size_t Model0_dmaengine_get_icg(bool Model0_inc, bool Model0_sgl, Model0_size_t Model0_icg,
          Model0_size_t Model0_dir_icg)
{
 if (Model0_inc) {
  if (Model0_dir_icg)
   return Model0_dir_icg;
  else if (Model0_sgl)
   return Model0_icg;
 }

 return 0;
}

static inline __attribute__((no_instrument_function)) Model0_size_t Model0_dmaengine_get_dst_icg(struct Model0_dma_interleaved_template *Model0_xt,
        struct Model0_data_chunk *Model0_chunk)
{
 return Model0_dmaengine_get_icg(Model0_xt->Model0_dst_inc, Model0_xt->Model0_dst_sgl,
     Model0_chunk->Model0_icg, Model0_chunk->Model0_dst_icg);
}

static inline __attribute__((no_instrument_function)) Model0_size_t Model0_dmaengine_get_src_icg(struct Model0_dma_interleaved_template *Model0_xt,
        struct Model0_data_chunk *Model0_chunk)
{
 return Model0_dmaengine_get_icg(Model0_xt->Model0_src_inc, Model0_xt->Model0_src_sgl,
     Model0_chunk->Model0_icg, Model0_chunk->Model0_src_icg);
}

/* --- public DMA engine API --- */


void Model0_dmaengine_get(void);
void Model0_dmaengine_put(void);
static inline __attribute__((no_instrument_function)) void Model0_async_dmaengine_get(void)
{
}
static inline __attribute__((no_instrument_function)) void Model0_async_dmaengine_put(void)
{
}
static inline __attribute__((no_instrument_function)) struct Model0_dma_chan *
Model0_async_dma_find_channel(enum Model0_dma_transaction_type Model0_type)
{
 return ((void *)0);
}

void Model0_dma_async_tx_descriptor_init(struct Model0_dma_async_tx_descriptor *Model0_tx,
      struct Model0_dma_chan *Model0_chan);

static inline __attribute__((no_instrument_function)) void Model0_async_tx_ack(struct Model0_dma_async_tx_descriptor *Model0_tx)
{
 Model0_tx->Model0_flags |= Model0_DMA_CTRL_ACK;
}

static inline __attribute__((no_instrument_function)) void Model0_async_tx_clear_ack(struct Model0_dma_async_tx_descriptor *Model0_tx)
{
 Model0_tx->Model0_flags &= ~Model0_DMA_CTRL_ACK;
}

static inline __attribute__((no_instrument_function)) bool Model0_async_tx_test_ack(struct Model0_dma_async_tx_descriptor *Model0_tx)
{
 return (Model0_tx->Model0_flags & Model0_DMA_CTRL_ACK) == Model0_DMA_CTRL_ACK;
}


static inline __attribute__((no_instrument_function)) void
Model0___dma_cap_set(enum Model0_dma_transaction_type Model0_tx_type, Model0_dma_cap_mask_t *Model0_dstp)
{
 Model0_set_bit(Model0_tx_type, Model0_dstp->Model0_bits);
}


static inline __attribute__((no_instrument_function)) void
Model0___dma_cap_clear(enum Model0_dma_transaction_type Model0_tx_type, Model0_dma_cap_mask_t *Model0_dstp)
{
 Model0_clear_bit(Model0_tx_type, Model0_dstp->Model0_bits);
}


static inline __attribute__((no_instrument_function)) void Model0___dma_cap_zero(Model0_dma_cap_mask_t *Model0_dstp)
{
 Model0_bitmap_zero(Model0_dstp->Model0_bits, Model0_DMA_TX_TYPE_END);
}


static inline __attribute__((no_instrument_function)) int
Model0___dma_has_cap(enum Model0_dma_transaction_type Model0_tx_type, Model0_dma_cap_mask_t *Model0_srcp)
{
 return (__builtin_constant_p((Model0_tx_type)) ? Model0_constant_test_bit((Model0_tx_type), (Model0_srcp->Model0_bits)) : Model0_variable_test_bit((Model0_tx_type), (Model0_srcp->Model0_bits)));
}




/**
 * dma_async_issue_pending - flush pending transactions to HW
 * @chan: target DMA channel
 *
 * This allows drivers to push copies to HW in batches,
 * reducing MMIO writes where possible.
 */
static inline __attribute__((no_instrument_function)) void Model0_dma_async_issue_pending(struct Model0_dma_chan *Model0_chan)
{
 Model0_chan->Model0_device->Model0_device_issue_pending(Model0_chan);
}

/**
 * dma_async_is_tx_complete - poll for transaction completion
 * @chan: DMA channel
 * @cookie: transaction identifier to check status of
 * @last: returns last completed cookie, can be NULL
 * @used: returns last issued cookie, can be NULL
 *
 * If @last and @used are passed in, upon return they reflect the driver
 * internal state and can be used with dma_async_is_complete() to check
 * the status of multiple cookies without re-checking hardware state.
 */
static inline __attribute__((no_instrument_function)) enum Model0_dma_status Model0_dma_async_is_tx_complete(struct Model0_dma_chan *Model0_chan,
 Model0_dma_cookie_t Model0_cookie, Model0_dma_cookie_t *Model0_last, Model0_dma_cookie_t *Model0_used)
{
 struct Model0_dma_tx_state Model0_state;
 enum Model0_dma_status Model0_status;

 Model0_status = Model0_chan->Model0_device->Model0_device_tx_status(Model0_chan, Model0_cookie, &Model0_state);
 if (Model0_last)
  *Model0_last = Model0_state.Model0_last;
 if (Model0_used)
  *Model0_used = Model0_state.Model0_used;
 return Model0_status;
}

/**
 * dma_async_is_complete - test a cookie against chan state
 * @cookie: transaction identifier to test status of
 * @last_complete: last know completed transaction
 * @last_used: last cookie value handed out
 *
 * dma_async_is_complete() is used in dma_async_is_tx_complete()
 * the test logic is separated for lightweight testing of multiple cookies
 */
static inline __attribute__((no_instrument_function)) enum Model0_dma_status Model0_dma_async_is_complete(Model0_dma_cookie_t Model0_cookie,
   Model0_dma_cookie_t Model0_last_complete, Model0_dma_cookie_t Model0_last_used)
{
 if (Model0_last_complete <= Model0_last_used) {
  if ((Model0_cookie <= Model0_last_complete) || (Model0_cookie > Model0_last_used))
   return Model0_DMA_COMPLETE;
 } else {
  if ((Model0_cookie <= Model0_last_complete) && (Model0_cookie > Model0_last_used))
   return Model0_DMA_COMPLETE;
 }
 return Model0_DMA_IN_PROGRESS;
}

static inline __attribute__((no_instrument_function)) void
Model0_dma_set_tx_state(struct Model0_dma_tx_state *Model0_st, Model0_dma_cookie_t Model0_last, Model0_dma_cookie_t Model0_used, Model0_u32 Model0_residue)
{
 if (Model0_st) {
  Model0_st->Model0_last = Model0_last;
  Model0_st->Model0_used = Model0_used;
  Model0_st->Model0_residue = Model0_residue;
 }
}


struct Model0_dma_chan *Model0_dma_find_channel(enum Model0_dma_transaction_type Model0_tx_type);
enum Model0_dma_status Model0_dma_sync_wait(struct Model0_dma_chan *Model0_chan, Model0_dma_cookie_t Model0_cookie);
enum Model0_dma_status Model0_dma_wait_for_async_tx(struct Model0_dma_async_tx_descriptor *Model0_tx);
void Model0_dma_issue_pending_all(void);
struct Model0_dma_chan *Model0___dma_request_channel(const Model0_dma_cap_mask_t *Model0_mask,
     Model0_dma_filter_fn Model0_fn, void *Model0_fn_param);
struct Model0_dma_chan *Model0_dma_request_slave_channel(struct Model0_device *Model0_dev, const char *Model0_name);

struct Model0_dma_chan *Model0_dma_request_chan(struct Model0_device *Model0_dev, const char *Model0_name);
struct Model0_dma_chan *Model0_dma_request_chan_by_mask(const Model0_dma_cap_mask_t *Model0_mask);

void Model0_dma_release_channel(struct Model0_dma_chan *Model0_chan);
int Model0_dma_get_slave_caps(struct Model0_dma_chan *Model0_chan, struct Model0_dma_slave_caps *Model0_caps);
static inline __attribute__((no_instrument_function)) int Model0_dmaengine_desc_set_reuse(struct Model0_dma_async_tx_descriptor *Model0_tx)
{
 struct Model0_dma_slave_caps Model0_caps;

 Model0_dma_get_slave_caps(Model0_tx->Model0_chan, &Model0_caps);

 if (Model0_caps.Model0_descriptor_reuse) {
  Model0_tx->Model0_flags |= Model0_DMA_CTRL_REUSE;
  return 0;
 } else {
  return -1;
 }
}

static inline __attribute__((no_instrument_function)) void Model0_dmaengine_desc_clear_reuse(struct Model0_dma_async_tx_descriptor *Model0_tx)
{
 Model0_tx->Model0_flags &= ~Model0_DMA_CTRL_REUSE;
}

static inline __attribute__((no_instrument_function)) bool Model0_dmaengine_desc_test_reuse(struct Model0_dma_async_tx_descriptor *Model0_tx)
{
 return (Model0_tx->Model0_flags & Model0_DMA_CTRL_REUSE) == Model0_DMA_CTRL_REUSE;
}

static inline __attribute__((no_instrument_function)) int Model0_dmaengine_desc_free(struct Model0_dma_async_tx_descriptor *Model0_desc)
{
 /* this is supported for reusable desc, so check that */
 if (Model0_dmaengine_desc_test_reuse(Model0_desc))
  return Model0_desc->Model0_desc_free(Model0_desc);
 else
  return -1;
}

/* --- DMA device --- */

int Model0_dma_async_device_register(struct Model0_dma_device *Model0_device);
void Model0_dma_async_device_unregister(struct Model0_dma_device *Model0_device);
void Model0_dma_run_dependencies(struct Model0_dma_async_tx_descriptor *Model0_tx);
struct Model0_dma_chan *Model0_dma_get_slave_channel(struct Model0_dma_chan *Model0_chan);
struct Model0_dma_chan *Model0_dma_get_any_slave_channel(struct Model0_dma_device *Model0_device);




static inline __attribute__((no_instrument_function)) struct Model0_dma_chan
*Model0___dma_request_slave_channel_compat(const Model0_dma_cap_mask_t *Model0_mask,
      Model0_dma_filter_fn Model0_fn, void *Model0_fn_param,
      struct Model0_device *Model0_dev, const char *Model0_name)
{
 struct Model0_dma_chan *Model0_chan;

 Model0_chan = Model0_dma_request_slave_channel(Model0_dev, Model0_name);
 if (Model0_chan)
  return Model0_chan;

 if (!Model0_fn || !Model0_fn_param)
  return ((void *)0);

 return Model0___dma_request_channel(Model0_mask, Model0_fn, Model0_fn_param);
}

/*
 * Dynamic queue limits (dql) - Definitions
 *
 * Copyright (c) 2011, Tom Herbert <therbert@google.com>
 *
 * This header file contains the definitions for dynamic queue limits (dql).
 * dql would be used in conjunction with a producer/consumer type queue
 * (possibly a HW queue).  Such a queue would have these general properties:
 *
 *   1) Objects are queued up to some limit specified as number of objects.
 *   2) Periodically a completion process executes which retires consumed
 *      objects.
 *   3) Starvation occurs when limit has been reached, all queued data has
 *      actually been consumed, but completion processing has not yet run
 *      so queuing new data is blocked.
 *   4) Minimizing the amount of queued data is desirable.
 *
 * The goal of dql is to calculate the limit as the minimum number of objects
 * needed to prevent starvation.
 *
 * The primary functions of dql are:
 *    dql_queued - called when objects are enqueued to record number of objects
 *    dql_avail - returns how many objects are available to be queued based
 *      on the object limit and how many objects are already enqueued
 *    dql_completed - called at completion time to indicate how many objects
 *      were retired from the queue
 *
 * The dql implementation does not implement any locking for the dql data
 * structures, the higher layer should provide this.  dql_queued should
 * be serialized to prevent concurrent execution of the function; this
 * is also true for  dql_completed.  However, dql_queued and dlq_completed  can
 * be executed concurrently (i.e. they can be protected by different locks).
 */






struct Model0_dql {
 /* Fields accessed in enqueue path (dql_queued) */
 unsigned int Model0_num_queued; /* Total ever queued */
 unsigned int Model0_adj_limit; /* limit + num_completed */
 unsigned int Model0_last_obj_cnt; /* Count at last queuing */

 /* Fields accessed only by completion path (dql_completed) */

 unsigned int Model0_limit __attribute__((__aligned__((1 << (6))))); /* Current limit */
 unsigned int Model0_num_completed; /* Total ever completed */

 unsigned int Model0_prev_ovlimit; /* Previous over limit */
 unsigned int Model0_prev_num_queued; /* Previous queue total */
 unsigned int Model0_prev_last_obj_cnt; /* Previous queuing cnt */

 unsigned int Model0_lowest_slack; /* Lowest slack found */
 unsigned long Model0_slack_start_time; /* Time slacks seen */

 /* Configuration */
 unsigned int Model0_max_limit; /* Max limit */
 unsigned int Model0_min_limit; /* Minimum limit */
 unsigned int Model0_slack_hold_time; /* Time to measure slack */
};

/* Set some static maximums */



/*
 * Record number of objects queued. Assumes that caller has already checked
 * availability in the queue with dql_avail.
 */
static inline __attribute__((no_instrument_function)) void Model0_dql_queued(struct Model0_dql *Model0_dql, unsigned int Model0_count)
{
 do { if (__builtin_expect(!!(Model0_count > ((~0U) / 16)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dynamic_queue_limits.h"), "i" (74), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);

 Model0_dql->Model0_last_obj_cnt = Model0_count;

 /* We want to force a write first, so that cpu do not attempt
	 * to get cache line containing last_obj_cnt, num_queued, adj_limit
	 * in Shared state, but directly does a Request For Ownership
	 * It is only a hint, we use barrier() only.
	 */
 __asm__ __volatile__("": : :"memory");

 Model0_dql->Model0_num_queued += Model0_count;
}

/* Returns how many objects can be queued, < 0 indicates over limit. */
static inline __attribute__((no_instrument_function)) int Model0_dql_avail(const struct Model0_dql *Model0_dql)
{
 return (*({ __attribute__((unused)) typeof(Model0_dql->Model0_adj_limit) Model0___var = ( typeof(Model0_dql->Model0_adj_limit)) 0; (volatile typeof(Model0_dql->Model0_adj_limit) *)&(Model0_dql->Model0_adj_limit); })) - (*({ __attribute__((unused)) typeof(Model0_dql->Model0_num_queued) Model0___var = ( typeof(Model0_dql->Model0_num_queued)) 0; (volatile typeof(Model0_dql->Model0_num_queued) *)&(Model0_dql->Model0_num_queued); }));
}

/* Record number of completed objects and recalculate the limit. */
void Model0_dql_completed(struct Model0_dql *Model0_dql, unsigned int Model0_count);

/* Reset dql state */
void Model0_dql_reset(struct Model0_dql *Model0_dql);

/* Initialize dql state */
int Model0_dql_init(struct Model0_dql *Model0_dql, unsigned Model0_hold_time);

/*
 * ethtool.h: Defines for Linux ethtool.
 *
 * Copyright (C) 1998 David S. Miller (davem@redhat.com)
 * Copyright 2001 Jeff Garzik <jgarzik@pobox.com>
 * Portions Copyright 2001 Sun Microsystems (thockin@sun.com)
 * Portions Copyright 2002 Intel (eli.kupermann@intel.com,
 *                                christopher.leech@intel.com,
 *                                scott.feldman@intel.com)
 * Portions Copyright (C) Sun Microsystems 2008
 */







/*
 * These are the type definitions for the architecture specific
 * syscall compatibility layer.
 */










/* Socket-level I/O control calls. */

/* For setsockopt(2) */
/* Security levels - as per NRL IPv6 - don't actually do anything */






/* Socket filtering */
/* Instruct lower device to use last 4-bytes of skb data as FCS */
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions of the socket-level I/O control calls.
 *
 * Version:	@(#)sockios.h	1.0.2	03/09/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/* Linux-specific socket ioctls */



/* Routing table calls. */




/* Socket configuration controls. */
/* SIOCGIFDIVERT was:	0x8944		Frame diversion support */
/* SIOCSIFDIVERT was:	0x8945		Set frame diversion options */
/* ARP cache control calls. */
      /*  0x8950 - 0x8952  * obsolete calls, don't re-use */




/* RARP cache control calls. */




/* Driver configuration calls */




/* DLCI configuration calls */







/* bonding calls */
/* bridge calls */





/* hardware time stamping: parameters in linux/net_tstamp.h */



/* Device private ioctl calls */

/*
 *	These 16 ioctls are available to devices via the do_ioctl() device
 *	vector. Each device should include this file and redefine these names
 *	as their own. Because these are device dependent it is a good idea
 *	_NOT_ to issue them to random objects and hope.
 *
 *	THESE IOCTLS ARE _DEPRECATED_ AND WILL DISAPPEAR IN 2.5.X -DaveM
 */



/*
 *	These 16 ioctl calls are protocol private
 */






/*
 * Desired design of maximum size and alignment (see RFC2553)
 */


    /* Implementation specific desired alignment */

typedef unsigned short Model0___kernel_sa_family_t;

struct Model0___kernel_sockaddr_storage {
 Model0___kernel_sa_family_t Model0_ss_family; /* address family */
 /* Following field(s) are implementation specific */
 char Model0___data[128 - sizeof(unsigned short)];
    /* space to achieve desired size, */
    /* _SS_MAXSIZE value minus size of ss_family */
} __attribute__ ((aligned((__alignof__ (struct Model0_sockaddr *))))); /* force desired alignment */

struct Model0_pid;
struct Model0_cred;





struct Model0_seq_file;
extern void Model0_socket_seq_show(struct Model0_seq_file *Model0_seq);


typedef Model0___kernel_sa_family_t Model0_sa_family_t;

/*
 *	1003.1g requires sa_family_t and that sa_data is char.
 */

struct Model0_sockaddr {
 Model0_sa_family_t Model0_sa_family; /* address family, AF_xxx	*/
 char Model0_sa_data[14]; /* 14 bytes of protocol address	*/
};

struct Model0_linger {
 int Model0_l_onoff; /* Linger active		*/
 int Model0_l_linger; /* How long to linger for	*/
};



/*
 *	As we do 4.4BSD message passing we use a 4.4BSD message passing
 *	system, not 4.3. Thus msg_accrights(len) are now missing. They
 *	belong in an obscure libc emulation or the bin.
 */

struct Model0_msghdr {
 void *Model0_msg_name; /* ptr to socket address structure */
 int Model0_msg_namelen; /* size of socket address structure */
 struct Model0_iov_iter Model0_msg_iter; /* data */
 void *Model0_msg_control; /* ancillary data */
 Model0___kernel_size_t Model0_msg_controllen; /* ancillary data buffer length */
 unsigned int Model0_msg_flags; /* flags on received message */
 struct Model0_kiocb *Model0_msg_iocb; /* ptr to iocb for async requests */
};

struct Model0_user_msghdr {
 void *Model0_msg_name; /* ptr to socket address structure */
 int Model0_msg_namelen; /* size of socket address structure */
 struct Model0_iovec *Model0_msg_iov; /* scatter/gather array */
 Model0___kernel_size_t Model0_msg_iovlen; /* # elements in msg_iov */
 void *Model0_msg_control; /* ancillary data */
 Model0___kernel_size_t Model0_msg_controllen; /* ancillary data buffer length */
 unsigned int Model0_msg_flags; /* flags on received message */
};

/* For recvmmsg/sendmmsg */
struct Model0_mmsghdr {
 struct Model0_user_msghdr Model0_msg_hdr;
 unsigned int Model0_msg_len;
};

/*
 *	POSIX 1003.1g - ancillary data object information
 *	Ancillary data consits of a sequence of pairs of
 *	(cmsghdr, cmsg_data[])
 */

struct Model0_cmsghdr {
 Model0___kernel_size_t Model0_cmsg_len; /* data byte count, including hdr */
        int Model0_cmsg_level; /* originating protocol */
        int Model0_cmsg_type; /* protocol-specific type */
};

/*
 *	Ancillary data object information MACROS
 *	Table 5-14 of POSIX 1003.1g
 */
/*
 *	Get the next cmsg header
 *
 *	PLEASE, do not touch this function. If you think, that it is
 *	incorrect, grep kernel sources and think about consequences
 *	before trying to improve it.
 *
 *	Now it always returns valid, not truncated ancillary object
 *	HEADER. But caller still MUST check, that cmsg->cmsg_len is
 *	inside range, given by msg->msg_controllen before using
 *	ancillary object DATA.				--ANK (980731)
 */

static inline __attribute__((no_instrument_function)) struct Model0_cmsghdr * Model0___cmsg_nxthdr(void *Model0___ctl, Model0___kernel_size_t Model0___size,
            struct Model0_cmsghdr *Model0___cmsg)
{
 struct Model0_cmsghdr * Model0___ptr;

 Model0___ptr = (struct Model0_cmsghdr*)(((unsigned char *) Model0___cmsg) + ( ((Model0___cmsg->Model0_cmsg_len)+sizeof(long)-1) & ~(sizeof(long)-1) ));
 if ((unsigned long)((char*)(Model0___ptr+1) - (char *) Model0___ctl) > Model0___size)
  return (struct Model0_cmsghdr *)0;

 return Model0___ptr;
}

static inline __attribute__((no_instrument_function)) struct Model0_cmsghdr * Model0_cmsg_nxthdr (struct Model0_msghdr *Model0___msg, struct Model0_cmsghdr *Model0___cmsg)
{
 return Model0___cmsg_nxthdr(Model0___msg->Model0_msg_control, Model0___msg->Model0_msg_controllen, Model0___cmsg);
}

static inline __attribute__((no_instrument_function)) Model0_size_t Model0_msg_data_left(struct Model0_msghdr *Model0_msg)
{
 return Model0_iov_iter_count(&Model0_msg->Model0_msg_iter);
}

/* "Socket"-level control message types: */





struct Model0_ucred {
 __u32 Model0_pid;
 __u32 Model0_uid;
 __u32 Model0_gid;
};

/* Supported address families. */
/* Protocol families, same as address families. */
/* Maximum queue length specifiable by listen.  */


/* Flags we can use with send/ and recv. 
   Added those for 1003.1g not all are supported yet
 */
/* Setsockoptions(2) level. Thanks to BSD these must match IPPROTO_xxx */

/* #define SOL_ICMP	1	No-no-no! Due to Linux :-) we cannot use SOL_ICMP=1 */
/* IPX options */


extern int Model0_move_addr_to_kernel(void *Model0_uaddr, int Model0_ulen, struct Model0___kernel_sockaddr_storage *Model0_kaddr);
extern int Model0_put_cmsg(struct Model0_msghdr*, int Model0_level, int Model0_type, int Model0_len, void *Model0_data);

struct Model0_timespec;

/* The __sys_...msg variants allow MSG_CMSG_COMPAT */
extern long Model0___sys_recvmsg(int Model0_fd, struct Model0_user_msghdr *Model0_msg, unsigned Model0_flags);
extern long Model0___sys_sendmsg(int Model0_fd, struct Model0_user_msghdr *Model0_msg, unsigned Model0_flags);
extern int Model0___sys_recvmmsg(int Model0_fd, struct Model0_mmsghdr *Model0_mmsg, unsigned int Model0_vlen,
     unsigned int Model0_flags, struct Model0_timespec *Model0_timeout);
extern int Model0___sys_sendmmsg(int Model0_fd, struct Model0_mmsghdr *Model0_mmsg,
     unsigned int Model0_vlen, unsigned int Model0_flags);
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the INET interface module.
 *
 * Version:	@(#)if.h	1.0.2	04/18/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1982-1988
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */




/*
 * Compatibility interface for userspace libc header coordination:
 *
 * Define compatibility macros that are used to control the inclusion or
 * exclusion of UAPI structures and definitions in coordination with another
 * userspace C library.
 *
 * This header is intended to solve the problem of UAPI definitions that
 * conflict with userspace definitions. If a UAPI header has such conflicting
 * definitions then the solution is as follows:
 *
 * * Synchronize the UAPI header and the libc headers so either one can be
 *   used and such that the ABI is preserved. If this is not possible then
 *   no simple compatibility interface exists (you need to write translating
 *   wrappers and rename things) and you can't use this interface.
 *
 * Then follow this process:
 *
 * (a) Include libc-compat.h in the UAPI header.
 *      e.g. #include <linux/libc-compat.h>
 *     This include must be as early as possible.
 *
 * (b) In libc-compat.h add enough code to detect that the comflicting
 *     userspace libc header has been included first.
 *
 * (c) If the userspace libc header has been included first define a set of
 *     guard macros of the form __UAPI_DEF_FOO and set their values to 1, else
 *     set their values to 0.
 *
 * (d) Back in the UAPI header with the conflicting definitions, guard the
 *     definitions with:
 *     #if __UAPI_DEF_FOO
 *       ...
 *     #endif
 *
 * This fixes the situation where the linux headers are included *after* the
 * libc headers. To fix the problem with the inclusion in the other order the
 * userspace libc headers must be fixed like this:
 *
 * * For all definitions that conflict with kernel definitions wrap those
 *   defines in the following:
 *   #if !__UAPI_DEF_FOO
 *     ...
 *   #endif
 *
 * This prevents the redefinition of a construct already defined by the kernel.
 */



/* We have included glibc headers... */
/* Definitions for if.h */




/* Everything up to IFF_DYNAMIC, matches net/if.h until glibc 2.23 */

/* For the future if glibc adds IFF_LOWER_UP, IFF_DORMANT and IFF_ECHO */


/* Definitions for in.h */







/* Definitions for in6.h */
/* Definitions for ipx.h */






/* Definitions for xattr.h */








typedef struct {
 unsigned int Model0_clock_rate; /* bits per second */
 unsigned int Model0_clock_type; /* internal, external, TX-internal etc. */
 unsigned short Model0_loopback;
} Model0_sync_serial_settings; /* V.35, V.24, X.21 */

typedef struct {
 unsigned int Model0_clock_rate; /* bits per second */
 unsigned int Model0_clock_type; /* internal, external, TX-internal etc. */
 unsigned short Model0_loopback;
 unsigned int Model0_slot_map;
} Model0_te1_settings; /* T1, E1 */

typedef struct {
 unsigned short Model0_encoding;
 unsigned short Model0_parity;
} Model0_raw_hdlc_proto;

typedef struct {
 unsigned int Model0_t391;
 unsigned int Model0_t392;
 unsigned int Model0_n391;
 unsigned int Model0_n392;
 unsigned int Model0_n393;
 unsigned short Model0_lmi;
 unsigned short Model0_dce; /* 1 for DCE (network side) operation */
} Model0_fr_proto;

typedef struct {
 unsigned int Model0_dlci;
} Model0_fr_proto_pvc; /* for creating/deleting FR PVCs */

typedef struct {
 unsigned int Model0_dlci;
 char Model0_master[16]; /* Name of master FRAD device */
}Model0_fr_proto_pvc_info; /* for returning PVC information only */

typedef struct {
    unsigned int Model0_interval;
    unsigned int Model0_timeout;
} Model0_cisco_proto;

/* PPP doesn't need any info now - supply length = 0 to ioctl */

/* For glibc compatibility. An empty enum does not compile. */


/**
 * enum net_device_flags - &struct net_device flags
 *
 * These are the &struct net_device flags, they can be set by drivers, the
 * kernel and some can be triggered by userspace. Userspace can query and
 * set these flags using userspace utilities but there is also a sysfs
 * entry available for all dev flags which can be queried and set. These flags
 * are shared for all types of net_devices. The sysfs entries are available
 * via /sys/class/net/<dev>/flags. Flags which can be toggled through sysfs
 * are annotated below, note that only a few flags can be toggled and some
 * other flags are always preserved from the original net_device flags
 * even if you try to set them via sysfs. Flags which are always preserved
 * are kept under the flag grouping @IFF_VOLATILE. Flags which are volatile
 * are annotated below as such.
 *
 * You should have a pretty good reason to be extending these flags.
 *
 * @IFF_UP: interface is up. Can be toggled through sysfs.
 * @IFF_BROADCAST: broadcast address valid. Volatile.
 * @IFF_DEBUG: turn on debugging. Can be toggled through sysfs.
 * @IFF_LOOPBACK: is a loopback net. Volatile.
 * @IFF_POINTOPOINT: interface is has p-p link. Volatile.
 * @IFF_NOTRAILERS: avoid use of trailers. Can be toggled through sysfs.
 *	Volatile.
 * @IFF_RUNNING: interface RFC2863 OPER_UP. Volatile.
 * @IFF_NOARP: no ARP protocol. Can be toggled through sysfs. Volatile.
 * @IFF_PROMISC: receive all packets. Can be toggled through sysfs.
 * @IFF_ALLMULTI: receive all multicast packets. Can be toggled through
 *	sysfs.
 * @IFF_MASTER: master of a load balancer. Volatile.
 * @IFF_SLAVE: slave of a load balancer. Volatile.
 * @IFF_MULTICAST: Supports multicast. Can be toggled through sysfs.
 * @IFF_PORTSEL: can set media type. Can be toggled through sysfs.
 * @IFF_AUTOMEDIA: auto media select active. Can be toggled through sysfs.
 * @IFF_DYNAMIC: dialup device with changing addresses. Can be toggled
 *	through sysfs.
 * @IFF_LOWER_UP: driver signals L1 up. Volatile.
 * @IFF_DORMANT: driver signals dormant. Volatile.
 * @IFF_ECHO: echo sent packets. Volatile.
 */
enum Model0_net_device_flags {
/* for compatibility with glibc net/if.h */

 Model0_IFF_UP = 1<<0, /* sysfs */
 Model0_IFF_BROADCAST = 1<<1, /* volatile */
 Model0_IFF_DEBUG = 1<<2, /* sysfs */
 Model0_IFF_LOOPBACK = 1<<3, /* volatile */
 Model0_IFF_POINTOPOINT = 1<<4, /* volatile */
 Model0_IFF_NOTRAILERS = 1<<5, /* sysfs */
 Model0_IFF_RUNNING = 1<<6, /* volatile */
 Model0_IFF_NOARP = 1<<7, /* sysfs */
 Model0_IFF_PROMISC = 1<<8, /* sysfs */
 Model0_IFF_ALLMULTI = 1<<9, /* sysfs */
 Model0_IFF_MASTER = 1<<10, /* volatile */
 Model0_IFF_SLAVE = 1<<11, /* volatile */
 Model0_IFF_MULTICAST = 1<<12, /* sysfs */
 Model0_IFF_PORTSEL = 1<<13, /* sysfs */
 Model0_IFF_AUTOMEDIA = 1<<14, /* sysfs */
 Model0_IFF_DYNAMIC = 1<<15, /* sysfs */


 Model0_IFF_LOWER_UP = 1<<16, /* volatile */
 Model0_IFF_DORMANT = 1<<17, /* volatile */
 Model0_IFF_ECHO = 1<<18, /* volatile */

};


/* for compatibility with glibc net/if.h */
/* For definitions see hdlc.h */
/* For definitions see hdlc.h */
/* RFC 2863 operational status */
enum {
 Model0_IF_OPER_UNKNOWN,
 Model0_IF_OPER_NOTPRESENT,
 Model0_IF_OPER_DOWN,
 Model0_IF_OPER_LOWERLAYERDOWN,
 Model0_IF_OPER_TESTING,
 Model0_IF_OPER_DORMANT,
 Model0_IF_OPER_UP,
};

/* link modes */
enum {
 Model0_IF_LINK_MODE_DEFAULT,
 Model0_IF_LINK_MODE_DORMANT, /* limit upward transition to dormant */
};

/*
 *	Device mapping structure. I'd just gone off and designed a 
 *	beautiful scheme using only loadable modules with arguments
 *	for driver options and along come the PCMCIA people 8)
 *
 *	Ah well. The get() side of this is good for WDSETUP, and it'll
 *	be handy for debugging things. The set side is fine for now and
 *	being very small might be worth keeping for clean configuration.
 */

/* for compatibility with glibc net/if.h */

struct Model0_ifmap {
 unsigned long Model0_mem_start;
 unsigned long Model0_mem_end;
 unsigned short Model0_base_addr;
 unsigned char Model0_irq;
 unsigned char Model0_dma;
 unsigned char Model0_port;
 /* 3 bytes spare */
};


struct Model0_if_settings {
 unsigned int Model0_type; /* Type of physical device or protocol */
 unsigned int Model0_size; /* Size of the data allocated by the caller */
 union {
  /* {atm/eth/dsl}_settings anyone ? */
  Model0_raw_hdlc_proto *Model0_raw_hdlc;
  Model0_cisco_proto *Model0_cisco;
  Model0_fr_proto *Model0_fr;
  Model0_fr_proto_pvc *Model0_fr_pvc;
  Model0_fr_proto_pvc_info *Model0_fr_pvc_info;

  /* interface settings */
  Model0_sync_serial_settings *Model0_sync;
  Model0_te1_settings *Model0_te1;
 } Model0_ifs_ifsu;
};

/*
 * Interface request structure used for socket
 * ioctl's.  All interface ioctl's must have parameter
 * definitions which begin with ifr_name.  The
 * remainder may be interface specific.
 */

/* for compatibility with glibc net/if.h */

struct Model0_ifreq {

 union
 {
  char Model0_ifrn_name[16]; /* if name, e.g. "en0" */
 } Model0_ifr_ifrn;

 union {
  struct Model0_sockaddr Model0_ifru_addr;
  struct Model0_sockaddr Model0_ifru_dstaddr;
  struct Model0_sockaddr Model0_ifru_broadaddr;
  struct Model0_sockaddr Model0_ifru_netmask;
  struct Model0_sockaddr Model0_ifru_hwaddr;
  short Model0_ifru_flags;
  int Model0_ifru_ivalue;
  int Model0_ifru_mtu;
  struct Model0_ifmap Model0_ifru_map;
  char Model0_ifru_slave[16]; /* Just fits the size */
  char Model0_ifru_newname[16];
  void * Model0_ifru_data;
  struct Model0_if_settings Model0_ifru_settings;
 } Model0_ifr_ifru;
};
/*
 * Structure used in SIOCGIFCONF request.
 * Used to retrieve interface configuration
 * for machine (useful for programs which
 * must know all networks accessible).
 */

/* for compatibility with glibc net/if.h */

struct Model0_ifconf {
 int Model0_ifc_len; /* size of buffer	*/
 union {
  char *Model0_ifcu_buf;
  struct Model0_ifreq *Model0_ifcu_req;
 } Model0_ifc_ifcu;
};








/* acceptable for old filesystems */
static inline __attribute__((no_instrument_function)) bool Model0_old_valid_dev(Model0_dev_t Model0_dev)
{
 return ((unsigned int) ((Model0_dev) >> 20)) < 256 && ((unsigned int) ((Model0_dev) & ((1U << 20) - 1))) < 256;
}

static inline __attribute__((no_instrument_function)) Model0_u16 Model0_old_encode_dev(Model0_dev_t Model0_dev)
{
 return (((unsigned int) ((Model0_dev) >> 20)) << 8) | ((unsigned int) ((Model0_dev) & ((1U << 20) - 1)));
}

static inline __attribute__((no_instrument_function)) Model0_dev_t Model0_old_decode_dev(Model0_u16 Model0_val)
{
 return ((((Model0_val >> 8) & 255) << 20) | (Model0_val & 255));
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_new_encode_dev(Model0_dev_t Model0_dev)
{
 unsigned Model0_major = ((unsigned int) ((Model0_dev) >> 20));
 unsigned Model0_minor = ((unsigned int) ((Model0_dev) & ((1U << 20) - 1)));
 return (Model0_minor & 0xff) | (Model0_major << 8) | ((Model0_minor & ~0xff) << 12);
}

static inline __attribute__((no_instrument_function)) Model0_dev_t Model0_new_decode_dev(Model0_u32 Model0_dev)
{
 unsigned Model0_major = (Model0_dev & 0xfff00) >> 8;
 unsigned Model0_minor = (Model0_dev & 0xff) | ((Model0_dev >> 12) & 0xfff00);
 return (((Model0_major) << 20) | (Model0_minor));
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_huge_encode_dev(Model0_dev_t Model0_dev)
{
 return Model0_new_encode_dev(Model0_dev);
}

static inline __attribute__((no_instrument_function)) Model0_dev_t Model0_huge_decode_dev(Model0_u64 Model0_dev)
{
 return Model0_new_decode_dev(Model0_dev);
}

static inline __attribute__((no_instrument_function)) int Model0_sysv_valid_dev(Model0_dev_t Model0_dev)
{
 return ((unsigned int) ((Model0_dev) >> 20)) < (1<<14) && ((unsigned int) ((Model0_dev) & ((1U << 20) - 1))) < (1<<18);
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_sysv_encode_dev(Model0_dev_t Model0_dev)
{
 return ((unsigned int) ((Model0_dev) & ((1U << 20) - 1))) | (((unsigned int) ((Model0_dev) >> 20)) << 18);
}

static inline __attribute__((no_instrument_function)) unsigned Model0_sysv_major(Model0_u32 Model0_dev)
{
 return (Model0_dev >> 18) & 0x3fff;
}

static inline __attribute__((no_instrument_function)) unsigned Model0_sysv_minor(Model0_u32 Model0_dev)
{
 return Model0_dev & 0x3ffff;
}









/*
 * RCU-protected bl list version. See include/linux/list_bl.h.
 */







/*
 * Special version of lists, where head of the list has a lock in the lowest
 * bit. This is useful for scalable hash tables without increasing memory
 * footprint overhead.
 *
 * For modification operations, the 0 bit of hlist_bl_head->first
 * pointer must be set.
 *
 * With some small modifications, this can easily be adapted to store several
 * arbitrary bits (not just a single lock bit), if the need arises to store
 * some fast and compact auxiliary data.
 */
struct Model0_hlist_bl_head {
 struct Model0_hlist_bl_node *Model0_first;
};

struct Model0_hlist_bl_node {
 struct Model0_hlist_bl_node *Model0_next, **Model0_pprev;
};



static inline __attribute__((no_instrument_function)) void Model0_INIT_HLIST_BL_NODE(struct Model0_hlist_bl_node *Model0_h)
{
 Model0_h->Model0_next = ((void *)0);
 Model0_h->Model0_pprev = ((void *)0);
}



static inline __attribute__((no_instrument_function)) bool Model0_hlist_bl_unhashed(const struct Model0_hlist_bl_node *Model0_h)
{
 return !Model0_h->Model0_pprev;
}

static inline __attribute__((no_instrument_function)) struct Model0_hlist_bl_node *Model0_hlist_bl_first(struct Model0_hlist_bl_head *Model0_h)
{
 return (struct Model0_hlist_bl_node *)
  ((unsigned long)Model0_h->Model0_first & ~1UL);
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_set_first(struct Model0_hlist_bl_head *Model0_h,
     struct Model0_hlist_bl_node *Model0_n)
{
                                                    ;

                        ;
 Model0_h->Model0_first = (struct Model0_hlist_bl_node *)((unsigned long)Model0_n | 1UL);
}

static inline __attribute__((no_instrument_function)) bool Model0_hlist_bl_empty(const struct Model0_hlist_bl_head *Model0_h)
{
 return !((unsigned long)({ union { typeof(Model0_h->Model0_first) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_h->Model0_first), Model0___u.Model0___c, sizeof(Model0_h->Model0_first)); else Model0___read_once_size_nocheck(&(Model0_h->Model0_first), Model0___u.Model0___c, sizeof(Model0_h->Model0_first)); Model0___u.Model0___val; }) & ~1UL);
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_add_head(struct Model0_hlist_bl_node *Model0_n,
     struct Model0_hlist_bl_head *Model0_h)
{
 struct Model0_hlist_bl_node *Model0_first = Model0_hlist_bl_first(Model0_h);

 Model0_n->Model0_next = Model0_first;
 if (Model0_first)
  Model0_first->Model0_pprev = &Model0_n->Model0_next;
 Model0_n->Model0_pprev = &Model0_h->Model0_first;
 Model0_hlist_bl_set_first(Model0_h, Model0_n);
}

static inline __attribute__((no_instrument_function)) void Model0___hlist_bl_del(struct Model0_hlist_bl_node *Model0_n)
{
 struct Model0_hlist_bl_node *Model0_next = Model0_n->Model0_next;
 struct Model0_hlist_bl_node **Model0_pprev = Model0_n->Model0_pprev;

                                                    ;

 /* pprev may be `first`, so be careful not to lose the lock bit */
 ({ union { typeof(*Model0_pprev) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*Model0_pprev)) ((struct Model0_hlist_bl_node *) ((unsigned long)Model0_next | ((unsigned long)*Model0_pprev & 1UL))) }; Model0___write_once_size(&(*Model0_pprev), Model0___u.Model0___c, sizeof(*Model0_pprev)); Model0___u.Model0___val; });



 if (Model0_next)
  Model0_next->Model0_pprev = Model0_pprev;
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_del(struct Model0_hlist_bl_node *Model0_n)
{
 Model0___hlist_bl_del(Model0_n);
 Model0_n->Model0_next = ((void *) 0x100 + (0xdead000000000000UL));
 Model0_n->Model0_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_del_init(struct Model0_hlist_bl_node *Model0_n)
{
 if (!Model0_hlist_bl_unhashed(Model0_n)) {
  Model0___hlist_bl_del(Model0_n);
  Model0_INIT_HLIST_BL_NODE(Model0_n);
 }
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_lock(struct Model0_hlist_bl_head *Model0_b)
{
 Model0_bit_spin_lock(0, (unsigned long *)Model0_b);
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_unlock(struct Model0_hlist_bl_head *Model0_b)
{
 Model0___bit_spin_unlock(0, (unsigned long *)Model0_b);
}

static inline __attribute__((no_instrument_function)) bool Model0_hlist_bl_is_locked(struct Model0_hlist_bl_head *Model0_b)
{
 return Model0_bit_spin_is_locked(0, (unsigned long *)Model0_b);
}

/**
 * hlist_bl_for_each_entry	- iterate over list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 */






/**
 * hlist_bl_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @n:		another &struct hlist_node to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */


static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_set_first_rcu(struct Model0_hlist_bl_head *Model0_h,
     struct Model0_hlist_bl_node *Model0_n)
{
                                                    ;

                        ;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)((struct Model0_hlist_bl_node *)((unsigned long)Model0_n | 1UL)); if (__builtin_constant_p((struct Model0_hlist_bl_node *)((unsigned long)Model0_n | 1UL)) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof((Model0_h->Model0_first)) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof((Model0_h->Model0_first))) ((typeof(Model0_h->Model0_first))(Model0__r_a_p__v)) }; Model0___write_once_size(&((Model0_h->Model0_first)), Model0___u.Model0___c, sizeof((Model0_h->Model0_first))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&Model0_h->Model0_first) == sizeof(char) || sizeof(*&Model0_h->Model0_first) == sizeof(short) || sizeof(*&Model0_h->Model0_first) == sizeof(int) || sizeof(*&Model0_h->Model0_first) == sizeof(long))); extern void Model0___compiletime_assert_17(void) ; if (Model0___cond) Model0___compiletime_assert_17(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&Model0_h->Model0_first) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&Model0_h->Model0_first)) ((typeof(*((typeof(Model0_h->Model0_first))Model0__r_a_p__v)) *)((typeof(Model0_h->Model0_first))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&Model0_h->Model0_first), Model0___u.Model0___c, sizeof(*&Model0_h->Model0_first)); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });

}

static inline __attribute__((no_instrument_function)) struct Model0_hlist_bl_node *Model0_hlist_bl_first_rcu(struct Model0_hlist_bl_head *Model0_h)
{
 return (struct Model0_hlist_bl_node *)
  ((unsigned long)({ typeof(*(Model0_h->Model0_first)) *Model0_________p1 = (typeof(*(Model0_h->Model0_first)) *)({ typeof((Model0_h->Model0_first)) Model0__________p1 = ({ union { typeof((Model0_h->Model0_first)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_h->Model0_first)), Model0___u.Model0___c, sizeof((Model0_h->Model0_first))); else Model0___read_once_size_nocheck(&((Model0_h->Model0_first)), Model0___u.Model0___c, sizeof((Model0_h->Model0_first))); Model0___u.Model0___val; }); typeof(*((Model0_h->Model0_first))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_h->Model0_first)) *)(Model0_________p1)); }) & ~1UL);
}

/**
 * hlist_bl_del_init_rcu - deletes entry from hash list with re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_bl_unhashed() on the node returns true after this. It is
 * useful for RCU based read lockfree traversal if the writer side
 * must know if the list entry is still hashed or already unhashed.
 *
 * In particular, it means that we can not poison the forward pointers
 * that may still be used for walking the hash list and we can only
 * zero the pprev pointer so list_unhashed() will return true after
 * this.
 *
 * The caller must take whatever precautions are necessary (such as
 * holding appropriate locks) to avoid racing with another
 * list-mutation primitive, such as hlist_bl_add_head_rcu() or
 * hlist_bl_del_rcu(), running on this same list.  However, it is
 * perfectly legal to run concurrently with the _rcu list-traversal
 * primitives, such as hlist_bl_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_del_init_rcu(struct Model0_hlist_bl_node *Model0_n)
{
 if (!Model0_hlist_bl_unhashed(Model0_n)) {
  Model0___hlist_bl_del(Model0_n);
  Model0_n->Model0_pprev = ((void *)0);
 }
}

/**
 * hlist_bl_del_rcu - deletes entry from hash list without re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_bl_unhashed() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the hash list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_bl_add_head_rcu()
 * or hlist_bl_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_bl_for_each_entry().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_del_rcu(struct Model0_hlist_bl_node *Model0_n)
{
 Model0___hlist_bl_del(Model0_n);
 Model0_n->Model0_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_bl_add_head_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist_bl,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_bl_add_head_rcu()
 * or hlist_bl_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_bl_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_bl_add_head_rcu(struct Model0_hlist_bl_node *Model0_n,
     struct Model0_hlist_bl_head *Model0_h)
{
 struct Model0_hlist_bl_node *Model0_first;

 /* don't need hlist_bl_first_rcu because we're under lock */
 Model0_first = Model0_hlist_bl_first(Model0_h);

 Model0_n->Model0_next = Model0_first;
 if (Model0_first)
  Model0_first->Model0_pprev = &Model0_n->Model0_next;
 Model0_n->Model0_pprev = &Model0_h->Model0_first;

 /* need _rcu because we can have concurrent lock free readers */
 Model0_hlist_bl_set_first_rcu(Model0_h, Model0_n);
}
/**
 * hlist_bl_for_each_entry_rcu - iterate over rcu list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_bl_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_bl_node within the struct.
 *
 */







/*
 * Locked reference counts.
 *
 * These are different from just plain atomic refcounts in that they
 * are atomic with respect to the spinlock that goes with them.  In
 * particular, there can be implementations that don't actually get
 * the spinlock for the common decrement/increment operations, but they
 * still have to check that the operation is done semantically as if
 * the spinlock had been taken (using a cmpxchg operation that covers
 * both the lock and the count word, or using memory transactions, for
 * example).
 */
struct Model0_lockref {
 union {

  __u64 __attribute__((aligned(8))) Model0_lock_count;

  struct {
   Model0_spinlock_t Model0_lock;
   int Model0_count;
  };
 };
};

extern void Model0_lockref_get(struct Model0_lockref *);
extern int Model0_lockref_put_return(struct Model0_lockref *);
extern int Model0_lockref_get_not_zero(struct Model0_lockref *);
extern int Model0_lockref_get_or_lock(struct Model0_lockref *);
extern int Model0_lockref_put_or_lock(struct Model0_lockref *);

extern void Model0_lockref_mark_dead(struct Model0_lockref *);
extern int Model0_lockref_get_not_dead(struct Model0_lockref *);

/* Must be called under spinlock for reliable results */
static inline __attribute__((no_instrument_function)) int Model0___lockref_is_dead(const struct Model0_lockref *Model0_l)
{
 return ((int)Model0_l->Model0_count < 0);
}







/* Fast hashing routine for ints,  longs and pointers.
   (C) 2002 Nadia Yvette Chambers, IBM */




/*
 * The "GOLDEN_RATIO_PRIME" is used in ifs/btrfs/brtfs_inode.h and
 * fs/inode.c.  It's not actually prime any more (the previous primes
 * were actively bad for hashing), but the name remains.
 */
/*
 * This hash multiplies the input by a large odd number and takes the
 * high bits.  Since multiplication propagates changes to the most
 * significant end only, it is essential that the high bits of the
 * product be used for the hash value.
 *
 * Chuck Lever verified the effectiveness of this technique:
 * http://www.citi.umich.edu/techreports/reports/citi-tr-00-1.pdf
 *
 * Although a random odd number will do, it turns out that the golden
 * ratio phi = (sqrt(5)-1)/2, or its negative, has particularly nice
 * properties.  (See Knuth vol 3, section 6.4, exercise 9.)
 *
 * These are the negative, (1 - phi) = phi**2 = (3 - sqrt(5))/2,
 * which is very slightly easier to multiply by and makes no
 * difference to the hash distribution.
 */
/*
 * The _generic versions exist only so lib/test_hash.c can compare
 * the arch-optimized versions with the generic.
 *
 * Note that if you change these, any <asm/hash.h> that aren't updated
 * to match need to have their HAVE_ARCH_* define values updated so the
 * self-test will not false-positive.
 */



static inline __attribute__((no_instrument_function)) Model0_u32 Model0___hash_32_generic(Model0_u32 Model0_val)
{
 return Model0_val * 0x61C88647;
}




static inline __attribute__((no_instrument_function)) Model0_u32 Model0_hash_32_generic(Model0_u32 Model0_val, unsigned int Model0_bits)
{
 /* High bits are more random, so use them. */
 return Model0___hash_32_generic(Model0_val) >> (32 - Model0_bits);
}




static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_u32 Model0_hash_64_generic(Model0_u64 Model0_val, unsigned int Model0_bits)
{

 /* 64x64-bit multiply is efficient on all 64-bit processors */
 return Model0_val * 0x61C8864680B583EBull >> (64 - Model0_bits);




}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_hash_ptr(const void *Model0_ptr, unsigned int Model0_bits)
{
 return Model0_hash_64_generic((unsigned long)Model0_ptr, Model0_bits);
}

/* This really should be called fold32_ptr; it does no hashing to speak of. */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_hash32_ptr(const void *Model0_ptr)
{
 unsigned long Model0_val = (unsigned long)Model0_ptr;


 Model0_val ^= (Model0_val >> 32);

 return (Model0_u32)Model0_val;
}

/*
 * Routines for hashing strings of bytes to a 32-bit hash value.
 *
 * These hash functions are NOT GUARANTEED STABLE between kernel
 * versions, architectures, or even repeated boots of the same kernel.
 * (E.g. they may depend on boot-time hardware detection or be
 * deliberately randomized.)
 *
 * They are also not intended to be secure against collisions caused by
 * malicious inputs; much slower hash functions are required for that.
 *
 * They are optimized for pathname components, meaning short strings.
 * Even if a majority of files have longer names, the dynamic profile of
 * pathname components skews short due to short directory names.
 * (E.g. /usr/lib/libsesquipedalianism.so.3.141.)
 */

/*
 * Version 1: one byte at a time.  Example of use:
 *
 * unsigned long hash = init_name_hash;
 * while (*p)
 *	hash = partial_name_hash(tolower(*p++), hash);
 * hash = end_name_hash(hash);
 *
 * Although this is designed for bytes, fs/hfsplus/unicode.c
 * abuses it to hash 16-bit values.
 */

/* Hash courtesy of the R5 hash in reiserfs modulo sign bits */


/* partial hash update function. Assume roughly 4 bits per character */
static inline __attribute__((no_instrument_function)) unsigned long
Model0_partial_name_hash(unsigned long Model0_c, unsigned long Model0_prevhash)
{
 return (Model0_prevhash + (Model0_c << 4) + (Model0_c >> 4)) * 11;
}

/*
 * Finally: cut down the number of bits to a int value (and try to avoid
 * losing bits).  This also has the property (wanted by the dcache)
 * that the msbits make a good hash table index.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_end_name_hash(unsigned long Model0_hash)
{
 return Model0___hash_32_generic((unsigned int)Model0_hash);
}

/*
 * Version 2: One word (32 or 64 bits) at a time.
 * If CONFIG_DCACHE_WORD_ACCESS is defined (meaning <asm/word-at-a-time.h>
 * exists, which describes major Linux platforms like x86 and ARM), then
 * this computes a different hash function much faster.
 *
 * If not set, this falls back to a wrapper around the preceding.
 */
extern unsigned int __attribute__((pure)) Model0_full_name_hash(const void *Model0_salt, const char *, unsigned int);

/*
 * A hash_len is a u64 with the hash of a string in the low
 * half and the length in the high half.
 */




/* Return the "hash_len" (hash and length) of a null-terminated string */
extern Model0_u64 __attribute__((pure)) Model0_hashlen_string(const void *Model0_salt, const char *Model0_name);

struct Model0_path;
struct Model0_vfsmount;

/*
 * linux/include/linux/dcache.h
 *
 * Dirent cache data structures
 *
 * (C) Copyright 1997 Thomas Schoebel-Theuer,
 * with heavy changes by Linus Torvalds
 */



/* The hash is always the low bits of hash_len */
/*
 * "quick string" -- eases parameter passing, but more importantly
 * saves "metadata" about the string (ie length and the hash).
 *
 * hash comes first so it snuggles against d_parent in the
 * dentry.
 */
struct Model0_qstr {
 union {
  struct {
   Model0_u32 Model0_hash; Model0_u32 Model0_len;
  };
  Model0_u64 Model0_hash_len;
 };
 const unsigned char *Model0_name;
};



struct Model0_dentry_stat_t {
 long Model0_nr_dentry;
 long Model0_nr_unused;
 long Model0_age_limit; /* age in seconds */
 long Model0_want_pages; /* pages requested by system */
 long Model0_dummy[2];
};
extern struct Model0_dentry_stat_t Model0_dentry_stat;

/*
 * Try to keep struct dentry aligned on 64 byte cachelines (this will
 * give reasonable cacheline footprint with larger lines without the
 * large memory footprint increase).
 */
struct Model0_dentry {
 /* RCU lookup touched fields */
 unsigned int Model0_d_flags; /* protected by d_lock */
 Model0_seqcount_t Model0_d_seq; /* per dentry seqlock */
 struct Model0_hlist_bl_node Model0_d_hash; /* lookup hash list */
 struct Model0_dentry *Model0_d_parent; /* parent directory */
 struct Model0_qstr Model0_d_name;
 struct Model0_inode *Model0_d_inode; /* Where the name belongs to - NULL is
					 * negative */
 unsigned char Model0_d_iname[32]; /* small names */

 /* Ref lookup also touches following */
 struct Model0_lockref Model0_d_lockref; /* per-dentry lock and refcount */
 const struct Model0_dentry_operations *Model0_d_op;
 struct Model0_super_block *Model0_d_sb; /* The root of the dentry tree */
 unsigned long Model0_d_time; /* used by d_revalidate */
 void *Model0_d_fsdata; /* fs-specific data */

 union {
  struct Model0_list_head Model0_d_lru; /* LRU list */
  Model0_wait_queue_head_t *Model0_d_wait; /* in-lookup ones only */
 };
 struct Model0_list_head Model0_d_child; /* child of parent list */
 struct Model0_list_head Model0_d_subdirs; /* our children */
 /*
	 * d_alias and d_rcu can share memory
	 */
 union {
  struct Model0_hlist_node Model0_d_alias; /* inode alias list */
  struct Model0_hlist_bl_node Model0_d_in_lookup_hash; /* only for in-lookup ones */
   struct Model0_callback_head Model0_d_rcu;
 } Model0_d_u;
};

/*
 * dentry->d_lock spinlock nesting subclasses:
 *
 * 0: normal
 * 1: nested
 */
enum Model0_dentry_d_lock_class
{
 Model0_DENTRY_D_LOCK_NORMAL, /* implicitly used by plain spin_lock() APIs. */
 Model0_DENTRY_D_LOCK_NESTED
};

struct Model0_dentry_operations {
 int (*Model0_d_revalidate)(struct Model0_dentry *, unsigned int);
 int (*Model0_d_weak_revalidate)(struct Model0_dentry *, unsigned int);
 int (*Model0_d_hash)(const struct Model0_dentry *, struct Model0_qstr *);
 int (*Model0_d_compare)(const struct Model0_dentry *,
   unsigned int, const char *, const struct Model0_qstr *);
 int (*Model0_d_delete)(const struct Model0_dentry *);
 int (*Model0_d_init)(struct Model0_dentry *);
 void (*Model0_d_release)(struct Model0_dentry *);
 void (*Model0_d_prune)(struct Model0_dentry *);
 void (*Model0_d_iput)(struct Model0_dentry *, struct Model0_inode *);
 char *(*Model0_d_dname)(struct Model0_dentry *, char *, int);
 struct Model0_vfsmount *(*Model0_d_automount)(struct Model0_path *);
 int (*Model0_d_manage)(struct Model0_dentry *, bool);
 struct Model0_dentry *(*Model0_d_real)(struct Model0_dentry *, const struct Model0_inode *,
     unsigned int);
} __attribute__((__aligned__((1 << (6)))));

/*
 * Locking rules for dentry_operations callbacks are to be found in
 * Documentation/filesystems/Locking. Keep it updated!
 *
 * FUrther descriptions are found in Documentation/filesystems/vfs.txt.
 * Keep it updated too!
 */

/* d_flags entries */







     /* This dentry is possibly not currently connected to the dcache tree, in
      * which case its parent will either be itself, or will have this flag as
      * well.  nfsd will not use a dentry with this bit set, but will first
      * endeavour to clear the bit either by discovering that it is connected,
      * or by performing lookup operations.   Any filesystem which supports
      * nfsd_operations MUST have a lookup function which, if it finds a
      * directory inode with a DCACHE_DISCONNECTED dentry, will d_move that
      * dentry into place and return that dentry rather than the passed one,
      * typically using d_splice_alias. */
     /* this dentry has been "silly renamed" and has to be deleted on the last
      * dput() */


     /* Parent inode is watched by some fsnotify listener */
extern Model0_seqlock_t Model0_rename_lock;

/*
 * These are the low-level FS interfaces to the dcache..
 */
extern void Model0_d_instantiate(struct Model0_dentry *, struct Model0_inode *);
extern struct Model0_dentry * Model0_d_instantiate_unique(struct Model0_dentry *, struct Model0_inode *);
extern int Model0_d_instantiate_no_diralias(struct Model0_dentry *, struct Model0_inode *);
extern void Model0___d_drop(struct Model0_dentry *Model0_dentry);
extern void Model0_d_drop(struct Model0_dentry *Model0_dentry);
extern void Model0_d_delete(struct Model0_dentry *);
extern void Model0_d_set_d_op(struct Model0_dentry *Model0_dentry, const struct Model0_dentry_operations *Model0_op);

/* allocate/de-allocate */
extern struct Model0_dentry * Model0_d_alloc(struct Model0_dentry *, const struct Model0_qstr *);
extern struct Model0_dentry * Model0_d_alloc_pseudo(struct Model0_super_block *, const struct Model0_qstr *);
extern struct Model0_dentry * Model0_d_alloc_parallel(struct Model0_dentry *, const struct Model0_qstr *,
     Model0_wait_queue_head_t *);
extern struct Model0_dentry * Model0_d_splice_alias(struct Model0_inode *, struct Model0_dentry *);
extern struct Model0_dentry * Model0_d_add_ci(struct Model0_dentry *, struct Model0_inode *, struct Model0_qstr *);
extern struct Model0_dentry * Model0_d_exact_alias(struct Model0_dentry *, struct Model0_inode *);
extern struct Model0_dentry *Model0_d_find_any_alias(struct Model0_inode *Model0_inode);
extern struct Model0_dentry * Model0_d_obtain_alias(struct Model0_inode *);
extern struct Model0_dentry * Model0_d_obtain_root(struct Model0_inode *);
extern void Model0_shrink_dcache_sb(struct Model0_super_block *);
extern void Model0_shrink_dcache_parent(struct Model0_dentry *);
extern void Model0_shrink_dcache_for_umount(struct Model0_super_block *);
extern void Model0_d_invalidate(struct Model0_dentry *);

/* only used at mount-time */
extern struct Model0_dentry * Model0_d_make_root(struct Model0_inode *);

/* <clickety>-<click> the ramfs-type tree */
extern void Model0_d_genocide(struct Model0_dentry *);

extern void Model0_d_tmpfile(struct Model0_dentry *, struct Model0_inode *);

extern struct Model0_dentry *Model0_d_find_alias(struct Model0_inode *);
extern void Model0_d_prune_aliases(struct Model0_inode *);

/* test whether we have any submounts in a subdir tree */
extern int Model0_have_submounts(struct Model0_dentry *);

/*
 * This adds the entry to the hash queues.
 */
extern void Model0_d_rehash(struct Model0_dentry *);

extern void Model0_d_add(struct Model0_dentry *, struct Model0_inode *);

extern void Model0_dentry_update_name_case(struct Model0_dentry *, const struct Model0_qstr *);

/* used for rename() and baskets */
extern void Model0_d_move(struct Model0_dentry *, struct Model0_dentry *);
extern void Model0_d_exchange(struct Model0_dentry *, struct Model0_dentry *);
extern struct Model0_dentry *Model0_d_ancestor(struct Model0_dentry *, struct Model0_dentry *);

/* appendix may either be NULL or be used for transname suffixes */
extern struct Model0_dentry *Model0_d_lookup(const struct Model0_dentry *, const struct Model0_qstr *);
extern struct Model0_dentry *Model0_d_hash_and_lookup(struct Model0_dentry *, struct Model0_qstr *);
extern struct Model0_dentry *Model0___d_lookup(const struct Model0_dentry *, const struct Model0_qstr *);
extern struct Model0_dentry *Model0___d_lookup_rcu(const struct Model0_dentry *Model0_parent,
    const struct Model0_qstr *Model0_name, unsigned *Model0_seq);

static inline __attribute__((no_instrument_function)) unsigned Model0_d_count(const struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_lockref.Model0_count;
}

/*
 * helper function for dentry_operations.d_dname() members
 */
extern __attribute__((format(printf, 4, 5)))
char *Model0_dynamic_dname(struct Model0_dentry *, char *, int, const char *, ...);
extern char *Model0_simple_dname(struct Model0_dentry *, char *, int);

extern char *Model0___d_path(const struct Model0_path *, const struct Model0_path *, char *, int);
extern char *Model0_d_absolute_path(const struct Model0_path *, char *, int);
extern char *Model0_d_path(const struct Model0_path *, char *, int);
extern char *Model0_dentry_path_raw(struct Model0_dentry *, char *, int);
extern char *Model0_dentry_path(struct Model0_dentry *, char *, int);

/* Allocation counts.. */

/**
 *	dget, dget_dlock -	get a reference to a dentry
 *	@dentry: dentry to get a reference to
 *
 *	Given a dentry or %NULL pointer increment the reference count
 *	if appropriate and return the dentry. A dentry will not be 
 *	destroyed when it has references.
 */
static inline __attribute__((no_instrument_function)) struct Model0_dentry *Model0_dget_dlock(struct Model0_dentry *Model0_dentry)
{
 if (Model0_dentry)
  Model0_dentry->Model0_d_lockref.Model0_count++;
 return Model0_dentry;
}

static inline __attribute__((no_instrument_function)) struct Model0_dentry *Model0_dget(struct Model0_dentry *Model0_dentry)
{
 if (Model0_dentry)
  Model0_lockref_get(&Model0_dentry->Model0_d_lockref);
 return Model0_dentry;
}

extern struct Model0_dentry *Model0_dget_parent(struct Model0_dentry *Model0_dentry);

/**
 *	d_unhashed -	is dentry hashed
 *	@dentry: entry to check
 *
 *	Returns true if the dentry passed is not currently hashed.
 */

static inline __attribute__((no_instrument_function)) int Model0_d_unhashed(const struct Model0_dentry *Model0_dentry)
{
 return Model0_hlist_bl_unhashed(&Model0_dentry->Model0_d_hash);
}

static inline __attribute__((no_instrument_function)) int Model0_d_unlinked(const struct Model0_dentry *Model0_dentry)
{
 return Model0_d_unhashed(Model0_dentry) && !((Model0_dentry) == (Model0_dentry)->Model0_d_parent);
}

static inline __attribute__((no_instrument_function)) int Model0_cant_mount(const struct Model0_dentry *Model0_dentry)
{
 return (Model0_dentry->Model0_d_flags & 0x00000100);
}

static inline __attribute__((no_instrument_function)) void Model0_dont_mount(struct Model0_dentry *Model0_dentry)
{
 Model0_spin_lock(&Model0_dentry->Model0_d_lockref.Model0_lock);
 Model0_dentry->Model0_d_flags |= 0x00000100;
 Model0_spin_unlock(&Model0_dentry->Model0_d_lockref.Model0_lock);
}

extern void Model0___d_lookup_done(struct Model0_dentry *);

static inline __attribute__((no_instrument_function)) int Model0_d_in_lookup(struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_flags & 0x10000000;
}

static inline __attribute__((no_instrument_function)) void Model0_d_lookup_done(struct Model0_dentry *Model0_dentry)
{
 if (__builtin_expect(!!(Model0_d_in_lookup(Model0_dentry)), 0)) {
  Model0_spin_lock(&Model0_dentry->Model0_d_lockref.Model0_lock);
  Model0___d_lookup_done(Model0_dentry);
  Model0_spin_unlock(&Model0_dentry->Model0_d_lockref.Model0_lock);
 }
}

extern void Model0_dput(struct Model0_dentry *);

static inline __attribute__((no_instrument_function)) bool Model0_d_managed(const struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_flags & (0x00010000|0x00020000|0x00040000);
}

static inline __attribute__((no_instrument_function)) bool Model0_d_mountpoint(const struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_flags & 0x00010000;
}

/*
 * Directory cache entry type accessor functions.
 */
static inline __attribute__((no_instrument_function)) unsigned Model0___d_entry_type(const struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_flags & 0x00700000;
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_miss(const struct Model0_dentry *Model0_dentry)
{
 return Model0___d_entry_type(Model0_dentry) == 0x00000000;
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_whiteout(const struct Model0_dentry *Model0_dentry)
{
 return Model0___d_entry_type(Model0_dentry) == 0x00100000;
}

static inline __attribute__((no_instrument_function)) bool Model0_d_can_lookup(const struct Model0_dentry *Model0_dentry)
{
 return Model0___d_entry_type(Model0_dentry) == 0x00200000;
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_autodir(const struct Model0_dentry *Model0_dentry)
{
 return Model0___d_entry_type(Model0_dentry) == 0x00300000;
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_dir(const struct Model0_dentry *Model0_dentry)
{
 return Model0_d_can_lookup(Model0_dentry) || Model0_d_is_autodir(Model0_dentry);
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_symlink(const struct Model0_dentry *Model0_dentry)
{
 return Model0___d_entry_type(Model0_dentry) == 0x00600000;
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_reg(const struct Model0_dentry *Model0_dentry)
{
 return Model0___d_entry_type(Model0_dentry) == 0x00400000;
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_special(const struct Model0_dentry *Model0_dentry)
{
 return Model0___d_entry_type(Model0_dentry) == 0x00500000;
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_file(const struct Model0_dentry *Model0_dentry)
{
 return Model0_d_is_reg(Model0_dentry) || Model0_d_is_special(Model0_dentry);
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_negative(const struct Model0_dentry *Model0_dentry)
{
 // TODO: check d_is_whiteout(dentry) also.
 return Model0_d_is_miss(Model0_dentry);
}

static inline __attribute__((no_instrument_function)) bool Model0_d_is_positive(const struct Model0_dentry *Model0_dentry)
{
 return !Model0_d_is_negative(Model0_dentry);
}

/**
 * d_really_is_negative - Determine if a dentry is really negative (ignoring fallthroughs)
 * @dentry: The dentry in question
 *
 * Returns true if the dentry represents either an absent name or a name that
 * doesn't map to an inode (ie. ->d_inode is NULL).  The dentry could represent
 * a true miss, a whiteout that isn't represented by a 0,0 chardev or a
 * fallthrough marker in an opaque directory.
 *
 * Note!  (1) This should be used *only* by a filesystem to examine its own
 * dentries.  It should not be used to look at some other filesystem's
 * dentries.  (2) It should also be used in combination with d_inode() to get
 * the inode.  (3) The dentry may have something attached to ->d_lower and the
 * type field of the flags may be set to something other than miss or whiteout.
 */
static inline __attribute__((no_instrument_function)) bool Model0_d_really_is_negative(const struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_inode == ((void *)0);
}

/**
 * d_really_is_positive - Determine if a dentry is really positive (ignoring fallthroughs)
 * @dentry: The dentry in question
 *
 * Returns true if the dentry represents a name that maps to an inode
 * (ie. ->d_inode is not NULL).  The dentry might still represent a whiteout if
 * that is represented on medium as a 0,0 chardev.
 *
 * Note!  (1) This should be used *only* by a filesystem to examine its own
 * dentries.  It should not be used to look at some other filesystem's
 * dentries.  (2) It should also be used in combination with d_inode() to get
 * the inode.
 */
static inline __attribute__((no_instrument_function)) bool Model0_d_really_is_positive(const struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_inode != ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_simple_positive(struct Model0_dentry *Model0_dentry)
{
 return Model0_d_really_is_positive(Model0_dentry) && !Model0_d_unhashed(Model0_dentry);
}

extern void Model0_d_set_fallthru(struct Model0_dentry *Model0_dentry);

static inline __attribute__((no_instrument_function)) bool Model0_d_is_fallthru(const struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_flags & 0x01000000;
}


extern int Model0_sysctl_vfs_cache_pressure;

static inline __attribute__((no_instrument_function)) unsigned long Model0_vfs_pressure_ratio(unsigned long Model0_val)
{
 return ( { typeof(Model0_val) Model0_quot = (Model0_val) / (100); typeof(Model0_val) Model0_rem = (Model0_val) % (100); (Model0_quot * (Model0_sysctl_vfs_cache_pressure)) + ((Model0_rem * (Model0_sysctl_vfs_cache_pressure)) / (100)); } );
}

/**
 * d_inode - Get the actual inode of this dentry
 * @dentry: The dentry to query
 *
 * This is the helper normal filesystems should use to get at their own inodes
 * in their own dentries and ignore the layering superimposed upon them.
 */
static inline __attribute__((no_instrument_function)) struct Model0_inode *Model0_d_inode(const struct Model0_dentry *Model0_dentry)
{
 return Model0_dentry->Model0_d_inode;
}

/**
 * d_inode_rcu - Get the actual inode of this dentry with ACCESS_ONCE()
 * @dentry: The dentry to query
 *
 * This is the helper normal filesystems should use to get at their own inodes
 * in their own dentries and ignore the layering superimposed upon them.
 */
static inline __attribute__((no_instrument_function)) struct Model0_inode *Model0_d_inode_rcu(const struct Model0_dentry *Model0_dentry)
{
 return (*({ __attribute__((unused)) typeof(Model0_dentry->Model0_d_inode) Model0___var = ( typeof(Model0_dentry->Model0_d_inode)) 0; (volatile typeof(Model0_dentry->Model0_d_inode) *)&(Model0_dentry->Model0_d_inode); }));
}

/**
 * d_backing_inode - Get upper or lower inode we should be using
 * @upper: The upper layer
 *
 * This is the helper that should be used to get at the inode that will be used
 * if this dentry were to be opened as a file.  The inode may be on the upper
 * dentry or it may be on a lower dentry pinned by the upper.
 *
 * Normal filesystems should not use this to access their own inodes.
 */
static inline __attribute__((no_instrument_function)) struct Model0_inode *Model0_d_backing_inode(const struct Model0_dentry *Model0_upper)
{
 struct Model0_inode *Model0_inode = Model0_upper->Model0_d_inode;

 return Model0_inode;
}

/**
 * d_backing_dentry - Get upper or lower dentry we should be using
 * @upper: The upper layer
 *
 * This is the helper that should be used to get the dentry of the inode that
 * will be used if this dentry were opened as a file.  It may be the upper
 * dentry or it may be a lower dentry pinned by the upper.
 *
 * Normal filesystems should not use this to access their own dentries.
 */
static inline __attribute__((no_instrument_function)) struct Model0_dentry *Model0_d_backing_dentry(struct Model0_dentry *Model0_upper)
{
 return Model0_upper;
}

/**
 * d_real - Return the real dentry
 * @dentry: the dentry to query
 * @inode: inode to select the dentry from multiple layers (can be NULL)
 * @flags: open flags to control copy-up behavior
 *
 * If dentry is on an union/overlay, then return the underlying, real dentry.
 * Otherwise return the dentry itself.
 *
 * See also: Documentation/filesystems/vfs.txt
 */
static inline __attribute__((no_instrument_function)) struct Model0_dentry *Model0_d_real(struct Model0_dentry *Model0_dentry,
        const struct Model0_inode *Model0_inode,
        unsigned int Model0_flags)
{
 if (__builtin_expect(!!(Model0_dentry->Model0_d_flags & 0x04000000), 0))
  return Model0_dentry->Model0_d_op->Model0_d_real(Model0_dentry, Model0_inode, Model0_flags);
 else
  return Model0_dentry;
}

/**
 * d_real_inode - Return the real inode
 * @dentry: The dentry to query
 *
 * If dentry is on an union/overlay, then return the underlying, real inode.
 * Otherwise return d_inode().
 */
static inline __attribute__((no_instrument_function)) struct Model0_inode *Model0_d_real_inode(struct Model0_dentry *Model0_dentry)
{
 return Model0_d_backing_inode(Model0_d_real(Model0_dentry, ((void *)0), 0));
}



struct Model0_dentry;
struct Model0_vfsmount;

struct Model0_path {
 struct Model0_vfsmount *Model0_mnt;
 struct Model0_dentry *Model0_dentry;
};

extern void Model0_path_get(const struct Model0_path *);
extern void Model0_path_put(const struct Model0_path *);

static inline __attribute__((no_instrument_function)) int Model0_path_equal(const struct Model0_path *Model0_path1, const struct Model0_path *Model0_path2)
{
 return Model0_path1->Model0_mnt == Model0_path2->Model0_mnt && Model0_path1->Model0_dentry == Model0_path2->Model0_dentry;
}



/*
 * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.
 * Authors: David Chinner and Glauber Costa
 *
 * Generic LRU infrastructure
 */







struct Model0_mem_cgroup;

/* list_lru_walk_cb has to always return one of those */
enum Model0_lru_status {
 Model0_LRU_REMOVED, /* item removed from list */
 Model0_LRU_REMOVED_RETRY, /* item removed, but lock has been
				   dropped and reacquired */
 Model0_LRU_ROTATE, /* item referenced, give another pass */
 Model0_LRU_SKIP, /* item cannot be locked, skip */
 Model0_LRU_RETRY, /* item not freeable. May drop the lock
				   internally, but has to return locked. */
};

struct Model0_list_lru_one {
 struct Model0_list_head Model0_list;
 /* may become negative during memcg reparenting */
 long Model0_nr_items;
};

struct Model0_list_lru_memcg {
 /* array of per cgroup lists, indexed by memcg_cache_id */
 struct Model0_list_lru_one *Model0_lru[0];
};

struct Model0_list_lru_node {
 /* protects all lists on the node, including per cgroup */
 Model0_spinlock_t Model0_lock;
 /* global list, used for the root cgroup in cgroup aware lrus */
 struct Model0_list_lru_one Model0_lru;




} __attribute__((__aligned__((1 << (6)))));

struct Model0_list_lru {
 struct Model0_list_lru_node *Model0_node;



};

void Model0_list_lru_destroy(struct Model0_list_lru *Model0_lru);
int Model0___list_lru_init(struct Model0_list_lru *Model0_lru, bool Model0_memcg_aware,
      struct Model0_lock_class_key *Model0_key);





int Model0_memcg_update_all_list_lrus(int Model0_num_memcgs);
void Model0_memcg_drain_all_list_lrus(int Model0_src_idx, int Model0_dst_idx);

/**
 * list_lru_add: add an element to the lru list's tail
 * @list_lru: the lru pointer
 * @item: the item to be added.
 *
 * If the element is already part of a list, this function returns doing
 * nothing. Therefore the caller does not need to keep state about whether or
 * not the element already belongs in the list and is allowed to lazy update
 * it. Note however that this is valid for *a* list, not *this* list. If
 * the caller organize itself in a way that elements can be in more than
 * one type of list, it is up to the caller to fully remove the item from
 * the previous list (with list_lru_del() for instance) before moving it
 * to @list_lru
 *
 * Return value: true if the list was updated, false otherwise
 */
bool Model0_list_lru_add(struct Model0_list_lru *Model0_lru, struct Model0_list_head *Model0_item);

/**
 * list_lru_del: delete an element to the lru list
 * @list_lru: the lru pointer
 * @item: the item to be deleted.
 *
 * This function works analogously as list_lru_add in terms of list
 * manipulation. The comments about an element already pertaining to
 * a list are also valid for list_lru_del.
 *
 * Return value: true if the list was updated, false otherwise
 */
bool Model0_list_lru_del(struct Model0_list_lru *Model0_lru, struct Model0_list_head *Model0_item);

/**
 * list_lru_count_one: return the number of objects currently held by @lru
 * @lru: the lru pointer.
 * @nid: the node id to count from.
 * @memcg: the cgroup to count from.
 *
 * Always return a non-negative number, 0 for empty lists. There is no
 * guarantee that the list is not updated while the count is being computed.
 * Callers that want such a guarantee need to provide an outer lock.
 */
unsigned long Model0_list_lru_count_one(struct Model0_list_lru *Model0_lru,
     int Model0_nid, struct Model0_mem_cgroup *Model0_memcg);
unsigned long Model0_list_lru_count_node(struct Model0_list_lru *Model0_lru, int Model0_nid);

static inline __attribute__((no_instrument_function)) unsigned long Model0_list_lru_shrink_count(struct Model0_list_lru *Model0_lru,
        struct Model0_shrink_control *Model0_sc)
{
 return Model0_list_lru_count_one(Model0_lru, Model0_sc->Model0_nid, Model0_sc->Model0_memcg);
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_list_lru_count(struct Model0_list_lru *Model0_lru)
{
 long Model0_count = 0;
 int Model0_nid;

 for (((Model0_nid)) = Model0___first_node(&(Model0_node_states[Model0_N_NORMAL_MEMORY])); ((Model0_nid)) < (1 << 6); ((Model0_nid)) = Model0___next_node((((Model0_nid))), &((Model0_node_states[Model0_N_NORMAL_MEMORY]))))
  Model0_count += Model0_list_lru_count_node(Model0_lru, Model0_nid);

 return Model0_count;
}

void Model0_list_lru_isolate(struct Model0_list_lru_one *Model0_list, struct Model0_list_head *Model0_item);
void Model0_list_lru_isolate_move(struct Model0_list_lru_one *Model0_list, struct Model0_list_head *Model0_item,
      struct Model0_list_head *Model0_head);

typedef enum Model0_lru_status (*Model0_list_lru_walk_cb)(struct Model0_list_head *Model0_item,
  struct Model0_list_lru_one *Model0_list, Model0_spinlock_t *Model0_lock, void *Model0_cb_arg);

/**
 * list_lru_walk_one: walk a list_lru, isolating and disposing freeable items.
 * @lru: the lru pointer.
 * @nid: the node id to scan from.
 * @memcg: the cgroup to scan from.
 * @isolate: callback function that is resposible for deciding what to do with
 *  the item currently being scanned
 * @cb_arg: opaque type that will be passed to @isolate
 * @nr_to_walk: how many items to scan.
 *
 * This function will scan all elements in a particular list_lru, calling the
 * @isolate callback for each of those items, along with the current list
 * spinlock and a caller-provided opaque. The @isolate callback can choose to
 * drop the lock internally, but *must* return with the lock held. The callback
 * will return an enum lru_status telling the list_lru infrastructure what to
 * do with the object being scanned.
 *
 * Please note that nr_to_walk does not mean how many objects will be freed,
 * just how many objects will be scanned.
 *
 * Return value: the number of objects effectively removed from the LRU.
 */
unsigned long Model0_list_lru_walk_one(struct Model0_list_lru *Model0_lru,
    int Model0_nid, struct Model0_mem_cgroup *Model0_memcg,
    Model0_list_lru_walk_cb Model0_isolate, void *Model0_cb_arg,
    unsigned long *Model0_nr_to_walk);
unsigned long Model0_list_lru_walk_node(struct Model0_list_lru *Model0_lru, int Model0_nid,
     Model0_list_lru_walk_cb Model0_isolate, void *Model0_cb_arg,
     unsigned long *Model0_nr_to_walk);

static inline __attribute__((no_instrument_function)) unsigned long
Model0_list_lru_shrink_walk(struct Model0_list_lru *Model0_lru, struct Model0_shrink_control *Model0_sc,
       Model0_list_lru_walk_cb Model0_isolate, void *Model0_cb_arg)
{
 return Model0_list_lru_walk_one(Model0_lru, Model0_sc->Model0_nid, Model0_sc->Model0_memcg, Model0_isolate, Model0_cb_arg,
     &Model0_sc->Model0_nr_to_scan);
}

static inline __attribute__((no_instrument_function)) unsigned long
Model0_list_lru_walk(struct Model0_list_lru *Model0_lru, Model0_list_lru_walk_cb Model0_isolate,
       void *Model0_cb_arg, unsigned long Model0_nr_to_walk)
{
 long Model0_isolated = 0;
 int Model0_nid;

 for (((Model0_nid)) = Model0___first_node(&(Model0_node_states[Model0_N_NORMAL_MEMORY])); ((Model0_nid)) < (1 << 6); ((Model0_nid)) = Model0___next_node((((Model0_nid))), &((Model0_node_states[Model0_N_NORMAL_MEMORY])))) {
  Model0_isolated += Model0_list_lru_walk_node(Model0_lru, Model0_nid, Model0_isolate,
            Model0_cb_arg, &Model0_nr_to_walk);
  if (Model0_nr_to_walk <= 0)
   break;
 }
 return Model0_isolated;
}

/*
 * Copyright (C) 2001 Momchil Velikov
 * Portions Copyright (C) 2001 Christoph Hellwig
 * Copyright (C) 2006 Nick Piggin
 * Copyright (C) 2012 Konstantin Khlebnikov
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License as
 * published by the Free Software Foundation; either version 2, or (at
 * your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */
/*
 * The bottom two bits of the slot determine how the remaining bits in the
 * slot are interpreted:
 *
 * 00 - data pointer
 * 01 - internal entry
 * 10 - exceptional entry
 * 11 - this bit combination is currently unused/reserved
 *
 * The internal entry may be a pointer to the next level in the tree, a
 * sibling entry, or an indicator that the entry in this slot has been moved
 * to another location in the tree and the lookup should be restarted.  While
 * NULL fits the 'data pointer' pattern, it means that there is no entry in
 * the tree for this index (no matter what level of the tree it is found at).
 * This means that you cannot store NULL in the tree as a value for the index.
 */



/*
 * Most users of the radix tree store pointers but shmem/tmpfs stores swap
 * entries in the same tree.  They are marked as exceptional entries to
 * distinguish them from pointers to struct page.
 * EXCEPTIONAL_ENTRY tests the bit, EXCEPTIONAL_SHIFT shifts content past it.
 */



static inline __attribute__((no_instrument_function)) bool Model0_radix_tree_is_internal_node(void *Model0_ptr)
{
 return ((unsigned long)Model0_ptr & 3UL) ==
    1UL;
}

/*** radix-tree API starts here ***/
/* Internally used bits of node->count */



struct Model0_radix_tree_node {
 unsigned char Model0_shift; /* Bits remaining in each slot */
 unsigned char Model0_offset; /* Slot offset in parent */
 unsigned int Model0_count;
 union {
  struct {
   /* Used when ascending tree */
   struct Model0_radix_tree_node *Model0_parent;
   /* For tree user */
   void *Model0_private_data;
  };
  /* Used when freeing node */
  struct Model0_callback_head Model0_callback_head;
 };
 /* For tree user */
 struct Model0_list_head Model0_private_list;
 void *Model0_slots[(1UL << (0 ? 4 : 6))];
 unsigned long Model0_tags[3][(((1UL << (0 ? 4 : 6)) + 64 - 1) / 64)];
};

/* root tags are stored in gfp_mask, shifted by __GFP_BITS_SHIFT */
struct Model0_radix_tree_root {
 Model0_gfp_t Model0_gfp_mask;
 struct Model0_radix_tree_node *Model0_rnode;
};
static inline __attribute__((no_instrument_function)) bool Model0_radix_tree_empty(struct Model0_radix_tree_root *Model0_root)
{
 return Model0_root->Model0_rnode == ((void *)0);
}

/**
 * Radix-tree synchronization
 *
 * The radix-tree API requires that users provide all synchronisation (with
 * specific exceptions, noted below).
 *
 * Synchronization of access to the data items being stored in the tree, and
 * management of their lifetimes must be completely managed by API users.
 *
 * For API usage, in general,
 * - any function _modifying_ the tree or tags (inserting or deleting
 *   items, setting or clearing tags) must exclude other modifications, and
 *   exclude any functions reading the tree.
 * - any function _reading_ the tree or tags (looking up items or tags,
 *   gang lookups) must exclude modifications to the tree, but may occur
 *   concurrently with other readers.
 *
 * The notable exceptions to this rule are the following functions:
 * __radix_tree_lookup
 * radix_tree_lookup
 * radix_tree_lookup_slot
 * radix_tree_tag_get
 * radix_tree_gang_lookup
 * radix_tree_gang_lookup_slot
 * radix_tree_gang_lookup_tag
 * radix_tree_gang_lookup_tag_slot
 * radix_tree_tagged
 *
 * The first 8 functions are able to be called locklessly, using RCU. The
 * caller must ensure calls to these functions are made within rcu_read_lock()
 * regions. Other readers (lock-free or otherwise) and modifications may be
 * running concurrently.
 *
 * It is still required that the caller manage the synchronization and lifetimes
 * of the items. So if RCU lock-free lookups are used, typically this would mean
 * that the items have their own locks, or are amenable to lock-free access; and
 * that the items are freed by RCU (or only freed after having been deleted from
 * the radix tree *and* a synchronize_rcu() grace period).
 *
 * (Note, rcu_assign_pointer and rcu_dereference are not needed to control
 * access to data items when inserting into or looking up from the radix tree)
 *
 * Note that the value returned by radix_tree_tag_get() may not be relied upon
 * if only the RCU read lock is held.  Functions to set/clear tags and to
 * delete nodes running concurrently with it may affect its result such that
 * two consecutive reads in the same locked section may return different
 * values.  If reliability is required, modification functions must also be
 * excluded from concurrency.
 *
 * radix_tree_tagged is able to be called without locking or RCU.
 */

/**
 * radix_tree_deref_slot	- dereference a slot
 * @pslot:	pointer to slot, returned by radix_tree_lookup_slot
 * Returns:	item that was stored in that slot with any direct pointer flag
 *		removed.
 *
 * For use with radix_tree_lookup_slot().  Caller must hold tree at least read
 * locked across slot lookup and dereference. Not required if write lock is
 * held (ie. items cannot be concurrently inserted).
 *
 * radix_tree_deref_retry must be used to confirm validity of the pointer if
 * only the read lock is held.
 */
static inline __attribute__((no_instrument_function)) void *Model0_radix_tree_deref_slot(void **Model0_pslot)
{
 return ({ typeof(*(*Model0_pslot)) *Model0_________p1 = (typeof(*(*Model0_pslot)) *)({ typeof((*Model0_pslot)) Model0__________p1 = ({ union { typeof((*Model0_pslot)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((*Model0_pslot)), Model0___u.Model0___c, sizeof((*Model0_pslot))); else Model0___read_once_size_nocheck(&((*Model0_pslot)), Model0___u.Model0___c, sizeof((*Model0_pslot))); Model0___u.Model0___val; }); typeof(*((*Model0_pslot))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(*Model0_pslot)) *)(Model0_________p1)); });
}

/**
 * radix_tree_deref_slot_protected	- dereference a slot without RCU lock but with tree lock held
 * @pslot:	pointer to slot, returned by radix_tree_lookup_slot
 * Returns:	item that was stored in that slot with any direct pointer flag
 *		removed.
 *
 * Similar to radix_tree_deref_slot but only used during migration when a pages
 * mapping is being moved. The caller does not hold the RCU read lock but it
 * must hold the tree lock to prevent parallel updates.
 */
static inline __attribute__((no_instrument_function)) void *Model0_radix_tree_deref_slot_protected(void **Model0_pslot,
       Model0_spinlock_t *Model0_treelock)
{
 return ({ do { } while (0); ; ((typeof(*(*Model0_pslot)) *)((*Model0_pslot))); });
}

/**
 * radix_tree_deref_retry	- check radix_tree_deref_slot
 * @arg:	pointer returned by radix_tree_deref_slot
 * Returns:	0 if retry is not required, otherwise retry is required
 *
 * radix_tree_deref_retry must be used with radix_tree_deref_slot.
 */
static inline __attribute__((no_instrument_function)) int Model0_radix_tree_deref_retry(void *Model0_arg)
{
 return __builtin_expect(!!(Model0_radix_tree_is_internal_node(Model0_arg)), 0);
}

/**
 * radix_tree_exceptional_entry	- radix_tree_deref_slot gave exceptional entry?
 * @arg:	value returned by radix_tree_deref_slot
 * Returns:	0 if well-aligned pointer, non-0 if exceptional entry.
 */
static inline __attribute__((no_instrument_function)) int Model0_radix_tree_exceptional_entry(void *Model0_arg)
{
 /* Not unlikely because radix_tree_exception often tested first */
 return (unsigned long)Model0_arg & 2;
}

/**
 * radix_tree_exception	- radix_tree_deref_slot returned either exception?
 * @arg:	value returned by radix_tree_deref_slot
 * Returns:	0 if well-aligned pointer, non-0 if either kind of exception.
 */
static inline __attribute__((no_instrument_function)) int Model0_radix_tree_exception(void *Model0_arg)
{
 return __builtin_expect(!!((unsigned long)Model0_arg & 3UL), 0);
}

/**
 * radix_tree_replace_slot	- replace item in a slot
 * @pslot:	pointer to slot, returned by radix_tree_lookup_slot
 * @item:	new item to store in the slot.
 *
 * For use with radix_tree_lookup_slot().  Caller must hold tree write locked
 * across slot lookup and replacement.
 */
static inline __attribute__((no_instrument_function)) void Model0_radix_tree_replace_slot(void **Model0_pslot, void *Model0_item)
{
 do { if (__builtin_expect(!!(Model0_radix_tree_is_internal_node(Model0_item)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/radix-tree.h"), "i" (261), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_item); if (__builtin_constant_p(Model0_item) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof((*Model0_pslot)) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof((*Model0_pslot))) ((typeof(*Model0_pslot))(Model0__r_a_p__v)) }; Model0___write_once_size(&((*Model0_pslot)), Model0___u.Model0___c, sizeof((*Model0_pslot))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&*Model0_pslot) == sizeof(char) || sizeof(*&*Model0_pslot) == sizeof(short) || sizeof(*&*Model0_pslot) == sizeof(int) || sizeof(*&*Model0_pslot) == sizeof(long))); extern void Model0___compiletime_assert_262(void) ; if (Model0___cond) Model0___compiletime_assert_262(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&*Model0_pslot) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&*Model0_pslot)) ((typeof(*((typeof(*Model0_pslot))Model0__r_a_p__v)) *)((typeof(*Model0_pslot))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&*Model0_pslot), Model0___u.Model0___c, sizeof(*&*Model0_pslot)); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
}

int Model0___radix_tree_create(struct Model0_radix_tree_root *Model0_root, unsigned long Model0_index,
   unsigned Model0_order, struct Model0_radix_tree_node **Model0_nodep,
   void ***Model0_slotp);
int Model0___radix_tree_insert(struct Model0_radix_tree_root *, unsigned long Model0_index,
   unsigned Model0_order, void *);
static inline __attribute__((no_instrument_function)) int Model0_radix_tree_insert(struct Model0_radix_tree_root *Model0_root,
   unsigned long Model0_index, void *Model0_entry)
{
 return Model0___radix_tree_insert(Model0_root, Model0_index, 0, Model0_entry);
}
void *Model0___radix_tree_lookup(struct Model0_radix_tree_root *Model0_root, unsigned long Model0_index,
     struct Model0_radix_tree_node **Model0_nodep, void ***Model0_slotp);
void *Model0_radix_tree_lookup(struct Model0_radix_tree_root *, unsigned long);
void **Model0_radix_tree_lookup_slot(struct Model0_radix_tree_root *, unsigned long);
bool Model0___radix_tree_delete_node(struct Model0_radix_tree_root *Model0_root,
         struct Model0_radix_tree_node *Model0_node);
void *Model0_radix_tree_delete_item(struct Model0_radix_tree_root *, unsigned long, void *);
void *Model0_radix_tree_delete(struct Model0_radix_tree_root *, unsigned long);
struct Model0_radix_tree_node *Model0_radix_tree_replace_clear_tags(
    struct Model0_radix_tree_root *Model0_root,
    unsigned long Model0_index, void *Model0_entry);
unsigned int Model0_radix_tree_gang_lookup(struct Model0_radix_tree_root *Model0_root,
   void **Model0_results, unsigned long Model0_first_index,
   unsigned int Model0_max_items);
unsigned int Model0_radix_tree_gang_lookup_slot(struct Model0_radix_tree_root *Model0_root,
   void ***Model0_results, unsigned long *Model0_indices,
   unsigned long Model0_first_index, unsigned int Model0_max_items);
int Model0_radix_tree_preload(Model0_gfp_t Model0_gfp_mask);
int Model0_radix_tree_maybe_preload(Model0_gfp_t Model0_gfp_mask);
int Model0_radix_tree_maybe_preload_order(Model0_gfp_t Model0_gfp_mask, int Model0_order);
void Model0_radix_tree_init(void);
void *Model0_radix_tree_tag_set(struct Model0_radix_tree_root *Model0_root,
   unsigned long Model0_index, unsigned int Model0_tag);
void *Model0_radix_tree_tag_clear(struct Model0_radix_tree_root *Model0_root,
   unsigned long Model0_index, unsigned int Model0_tag);
int Model0_radix_tree_tag_get(struct Model0_radix_tree_root *Model0_root,
   unsigned long Model0_index, unsigned int Model0_tag);
unsigned int
Model0_radix_tree_gang_lookup_tag(struct Model0_radix_tree_root *Model0_root, void **Model0_results,
  unsigned long Model0_first_index, unsigned int Model0_max_items,
  unsigned int Model0_tag);
unsigned int
Model0_radix_tree_gang_lookup_tag_slot(struct Model0_radix_tree_root *Model0_root, void ***Model0_results,
  unsigned long Model0_first_index, unsigned int Model0_max_items,
  unsigned int Model0_tag);
unsigned long Model0_radix_tree_range_tag_if_tagged(struct Model0_radix_tree_root *Model0_root,
  unsigned long *Model0_first_indexp, unsigned long Model0_last_index,
  unsigned long Model0_nr_to_tag,
  unsigned int Model0_fromtag, unsigned int Model0_totag);
int Model0_radix_tree_tagged(struct Model0_radix_tree_root *Model0_root, unsigned int Model0_tag);
unsigned long Model0_radix_tree_locate_item(struct Model0_radix_tree_root *Model0_root, void *Model0_item);

static inline __attribute__((no_instrument_function)) void Model0_radix_tree_preload_end(void)
{
 __asm__ __volatile__("": : :"memory");
}

/**
 * struct radix_tree_iter - radix tree iterator state
 *
 * @index:	index of current slot
 * @next_index:	one beyond the last index for this chunk
 * @tags:	bit-mask for tag-iterating
 * @shift:	shift for the node that holds our slots
 *
 * This radix tree iterator works in terms of "chunks" of slots.  A chunk is a
 * subinterval of slots contained within one radix tree leaf node.  It is
 * described by a pointer to its first slot and a struct radix_tree_iter
 * which holds the chunk's position in the tree and its size.  For tagged
 * iteration radix_tree_iter also holds the slots' bit-mask for one chosen
 * radix tree tag.
 */
struct Model0_radix_tree_iter {
 unsigned long Model0_index;
 unsigned long Model0_next_index;
 unsigned long Model0_tags;



};

static inline __attribute__((no_instrument_function)) unsigned int Model0_iter_shift(struct Model0_radix_tree_iter *Model0_iter)
{



 return 0;

}





/**
 * radix_tree_iter_init - initialize radix tree iterator
 *
 * @iter:	pointer to iterator state
 * @start:	iteration starting index
 * Returns:	NULL
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void **
Model0_radix_tree_iter_init(struct Model0_radix_tree_iter *Model0_iter, unsigned long Model0_start)
{
 /*
	 * Leave iter->tags uninitialized. radix_tree_next_chunk() will fill it
	 * in the case of a successful tagged chunk lookup.  If the lookup was
	 * unsuccessful or non-tagged then nobody cares about ->tags.
	 *
	 * Set index to zero to bypass next_index overflow protection.
	 * See the comment in radix_tree_next_chunk() for details.
	 */
 Model0_iter->Model0_index = 0;
 Model0_iter->Model0_next_index = Model0_start;
 return ((void *)0);
}

/**
 * radix_tree_next_chunk - find next chunk of slots for iteration
 *
 * @root:	radix tree root
 * @iter:	iterator state
 * @flags:	RADIX_TREE_ITER_* flags and tag index
 * Returns:	pointer to chunk first slot, or NULL if there no more left
 *
 * This function looks up the next chunk in the radix tree starting from
 * @iter->next_index.  It returns a pointer to the chunk's first slot.
 * Also it fills @iter with data about chunk: position in the tree (index),
 * its end (next_index), and constructs a bit mask for tagged iterating (tags).
 */
void **Model0_radix_tree_next_chunk(struct Model0_radix_tree_root *Model0_root,
        struct Model0_radix_tree_iter *Model0_iter, unsigned Model0_flags);

/**
 * radix_tree_iter_retry - retry this chunk of the iteration
 * @iter:	iterator state
 *
 * If we iterate over a tree protected only by the RCU lock, a race
 * against deletion or creation may result in seeing a slot for which
 * radix_tree_deref_retry() returns true.  If so, call this function
 * and continue the iteration.
 */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result))
void **Model0_radix_tree_iter_retry(struct Model0_radix_tree_iter *Model0_iter)
{
 Model0_iter->Model0_next_index = Model0_iter->Model0_index;
 Model0_iter->Model0_tags = 0;
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) unsigned long
Model0___radix_tree_iter_add(struct Model0_radix_tree_iter *Model0_iter, unsigned long Model0_slots)
{
 return Model0_iter->Model0_index + (Model0_slots << Model0_iter_shift(Model0_iter));
}

/**
 * radix_tree_iter_next - resume iterating when the chunk may be invalid
 * @iter:	iterator state
 *
 * If the iterator needs to release then reacquire a lock, the chunk may
 * have been invalidated by an insertion or deletion.  Call this function
 * to continue the iteration from the next index.
 */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result))
void **Model0_radix_tree_iter_next(struct Model0_radix_tree_iter *Model0_iter)
{
 Model0_iter->Model0_next_index = Model0___radix_tree_iter_add(Model0_iter, 1);
 Model0_iter->Model0_tags = 0;
 return ((void *)0);
}

/**
 * radix_tree_chunk_size - get current chunk size
 *
 * @iter:	pointer to radix tree iterator
 * Returns:	current chunk size
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) long
Model0_radix_tree_chunk_size(struct Model0_radix_tree_iter *Model0_iter)
{
 return (Model0_iter->Model0_next_index - Model0_iter->Model0_index) >> Model0_iter_shift(Model0_iter);
}

static inline __attribute__((no_instrument_function)) struct Model0_radix_tree_node *Model0_entry_to_node(void *Model0_ptr)
{
 return (void *)((unsigned long)Model0_ptr & ~1UL);
}

/**
 * radix_tree_next_slot - find next slot in chunk
 *
 * @slot:	pointer to current slot
 * @iter:	pointer to interator state
 * @flags:	RADIX_TREE_ITER_*, should be constant
 * Returns:	pointer to next slot, or NULL if there no more left
 *
 * This function updates @iter->index in the case of a successful lookup.
 * For tagged lookup it also eats @iter->tags.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void **
Model0_radix_tree_next_slot(void **Model0_slot, struct Model0_radix_tree_iter *Model0_iter, unsigned Model0_flags)
{
 if (Model0_flags & 0x0100) {
  void *Model0_canon = Model0_slot;

  Model0_iter->Model0_tags >>= 1;
  if (__builtin_expect(!!(!Model0_iter->Model0_tags), 0))
   return ((void *)0);
  while (0 &&
     Model0_radix_tree_is_internal_node(Model0_slot[1])) {
   if (Model0_entry_to_node(Model0_slot[1]) == Model0_canon) {
    Model0_iter->Model0_tags >>= 1;
    Model0_iter->Model0_index = Model0___radix_tree_iter_add(Model0_iter, 1);
    Model0_slot++;
    continue;
   }
   Model0_iter->Model0_next_index = Model0___radix_tree_iter_add(Model0_iter, 1);
   return ((void *)0);
  }
  if (__builtin_expect(!!(Model0_iter->Model0_tags & 1ul), 1)) {
   Model0_iter->Model0_index = Model0___radix_tree_iter_add(Model0_iter, 1);
   return Model0_slot + 1;
  }
  if (!(Model0_flags & 0x0200)) {
   unsigned Model0_offset = Model0___ffs(Model0_iter->Model0_tags);

   Model0_iter->Model0_tags >>= Model0_offset;
   Model0_iter->Model0_index = Model0___radix_tree_iter_add(Model0_iter, Model0_offset + 1);
   return Model0_slot + Model0_offset + 1;
  }
 } else {
  long Model0_count = Model0_radix_tree_chunk_size(Model0_iter);
  void *Model0_canon = Model0_slot;

  while (--Model0_count > 0) {
   Model0_slot++;
   Model0_iter->Model0_index = Model0___radix_tree_iter_add(Model0_iter, 1);

   if (0 &&
       Model0_radix_tree_is_internal_node(*Model0_slot)) {
    if (Model0_entry_to_node(*Model0_slot) == Model0_canon)
     continue;
    Model0_iter->Model0_next_index = Model0_iter->Model0_index;
    break;
   }

   if (__builtin_expect(!!(*Model0_slot), 1))
    return Model0_slot;
   if (Model0_flags & 0x0200) {
    /* forbid switching to the next chunk */
    Model0_iter->Model0_next_index = 0;
    break;
   }
  }
 }
 return ((void *)0);
}

/**
 * radix_tree_for_each_slot - iterate over non-empty slots
 *
 * @slot:	the void** variable for pointer to slot
 * @root:	the struct radix_tree_root pointer
 * @iter:	the struct radix_tree_iter pointer
 * @start:	iteration starting index
 *
 * @slot points to radix tree slot, @iter->index contains its index.
 */





/**
 * radix_tree_for_each_contig - iterate over contiguous slots
 *
 * @slot:	the void** variable for pointer to slot
 * @root:	the struct radix_tree_root pointer
 * @iter:	the struct radix_tree_iter pointer
 * @start:	iteration starting index
 *
 * @slot points to radix tree slot, @iter->index contains its index.
 */







/**
 * radix_tree_for_each_tagged - iterate over tagged slots
 *
 * @slot:	the void** variable for pointer to slot
 * @root:	the struct radix_tree_root pointer
 * @iter:	the struct radix_tree_iter pointer
 * @start:	iteration starting index
 * @tag:	tag index
 *
 * @slot points to radix tree slot, @iter->index contains its index.
 */







/*
 * Copyright (c) 2008 Intel Corporation
 * Author: Matthew Wilcox <willy@linux.intel.com>
 *
 * Distributed under the terms of the GNU GPL, version 2
 *
 * Please see kernel/semaphore.c for documentation of these functions
 */






/* Please don't access any members of this structure directly */
struct Model0_semaphore {
 Model0_raw_spinlock_t Model0_lock;
 unsigned int Model0_count;
 struct Model0_list_head Model0_wait_list;
};
static inline __attribute__((no_instrument_function)) void Model0_sema_init(struct Model0_semaphore *Model0_sem, int Model0_val)
{
 static struct Model0_lock_class_key Model0___key;
 *Model0_sem = (struct Model0_semaphore) { .Model0_lock = (Model0_raw_spinlock_t) { .Model0_raw_lock = { { (0) } }, }, .Model0_count = Model0_val, .Model0_wait_list = { &((*Model0_sem).Model0_wait_list), &((*Model0_sem).Model0_wait_list) }, };
 do { (void)("semaphore->lock"); (void)(&Model0___key); } while (0);
}

extern void Model0_down(struct Model0_semaphore *Model0_sem);
extern int __attribute__((warn_unused_result)) Model0_down_interruptible(struct Model0_semaphore *Model0_sem);
extern int __attribute__((warn_unused_result)) Model0_down_killable(struct Model0_semaphore *Model0_sem);
extern int __attribute__((warn_unused_result)) Model0_down_trylock(struct Model0_semaphore *Model0_sem);
extern int __attribute__((warn_unused_result)) Model0_down_timeout(struct Model0_semaphore *Model0_sem, long Model0_jiffies);
extern void Model0_up(struct Model0_semaphore *Model0_sem);
/*
 * FS_IOC_FIEMAP ioctl infrastructure.
 *
 * Some portions copyright (C) 2007 Cluster File Systems, Inc
 *
 * Authors: Mark Fasheh <mfasheh@suse.com>
 *          Kalpak Shah <kalpak.shah@sun.com>
 *          Andreas Dilger <adilger@sun.com>
 */






struct Model0_fiemap_extent {
 __u64 Model0_fe_logical; /* logical offset in bytes for the start of
			    * the extent from the beginning of the file */
 __u64 Model0_fe_physical; /* physical offset in bytes for the start
			    * of the extent from the beginning of the disk */
 __u64 Model0_fe_length; /* length in bytes for this extent */
 __u64 Model0_fe_reserved64[2];
 __u32 Model0_fe_flags; /* FIEMAP_EXTENT_* flags for this extent */
 __u32 Model0_fe_reserved[3];
};

struct Model0_fiemap {
 __u64 Model0_fm_start; /* logical offset (inclusive) at
				 * which to start mapping (in) */
 __u64 Model0_fm_length; /* logical length of mapping which
				 * userspace wants (in) */
 __u32 Model0_fm_flags; /* FIEMAP_FLAG_* flags for request (in/out) */
 __u32 Model0_fm_mapped_extents;/* number of extents that were mapped (out) */
 __u32 Model0_fm_extent_count; /* size of fm_extents array (in) */
 __u32 Model0_fm_reserved;
 struct Model0_fiemap_extent Model0_fm_extents[0]; /* array of mapped extents (out) */
};





/*
 * MIGRATE_ASYNC means never block
 * MIGRATE_SYNC_LIGHT in the current implementation means to allow blocking
 *	on most operations but not ->writepage as the potential stall time
 *	is too significant
 * MIGRATE_SYNC will block when migrating pages
 */
enum Model0_migrate_mode {
 Model0_MIGRATE_ASYNC,
 Model0_MIGRATE_SYNC_LIGHT,
 Model0_MIGRATE_SYNC,
};



/*
 * Block data types and constants.  Directly include this file only to
 * break include dependency loop.
 */





/*
 * bvec iterator
 *
 * Copyright (C) 2001 Ming Lei <ming.lei@canonical.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 *
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public Licens
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-
 */






/*
 * was unsigned short, but we might as well be ready for > 64kB I/O pages
 */
struct Model0_bio_vec {
 struct Model0_page *Model0_bv_page;
 unsigned int Model0_bv_len;
 unsigned int Model0_bv_offset;
};

struct Model0_bvec_iter {
 Model0_sector_t Model0_bi_sector; /* device address in 512 byte
						   sectors */
 unsigned int Model0_bi_size; /* residual I/O count */

 unsigned int Model0_bi_idx; /* current index into bvl_vec */

 unsigned int Model0_bi_bvec_done; /* number of bytes completed in
						   current bvec */
};

/*
 * various member access, note that bio_data should of course not be used
 * on highmem page vectors
 */
static inline __attribute__((no_instrument_function)) void Model0_bvec_iter_advance(const struct Model0_bio_vec *Model0_bv,
         struct Model0_bvec_iter *Model0_iter,
         unsigned Model0_bytes)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(Model0_bytes > Model0_iter->Model0_bi_size); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_fmt("./include/linux/bvec.h", 74, "Attempted to advance past end of bvec iter\n"); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });


 while (Model0_bytes) {
  unsigned Model0_iter_len = ({ typeof((*Model0_iter).Model0_bi_size) Model0__min1 = ((*Model0_iter).Model0_bi_size); typeof((&((Model0_bv))[((*Model0_iter)).Model0_bi_idx])->Model0_bv_len - (*Model0_iter).Model0_bi_bvec_done) Model0__min2 = ((&((Model0_bv))[((*Model0_iter)).Model0_bi_idx])->Model0_bv_len - (*Model0_iter).Model0_bi_bvec_done); (void) (&Model0__min1 == &Model0__min2); Model0__min1 < Model0__min2 ? Model0__min1 : Model0__min2; });
  unsigned Model0_len = ({ typeof(Model0_bytes) Model0__min1 = (Model0_bytes); typeof(Model0_iter_len) Model0__min2 = (Model0_iter_len); (void) (&Model0__min1 == &Model0__min2); Model0__min1 < Model0__min2 ? Model0__min1 : Model0__min2; });

  Model0_bytes -= Model0_len;
  Model0_iter->Model0_bi_size -= Model0_len;
  Model0_iter->Model0_bi_bvec_done += Model0_len;

  if (Model0_iter->Model0_bi_bvec_done == (&(Model0_bv)[(*Model0_iter).Model0_bi_idx])->Model0_bv_len) {
   Model0_iter->Model0_bi_bvec_done = 0;
   Model0_iter->Model0_bi_idx++;
  }
 }
}

struct Model0_bio_set;
struct Model0_bio;
struct Model0_bio_integrity_payload;
struct Model0_page;
struct Model0_block_device;
struct Model0_io_context;
struct Model0_cgroup_subsys_state;
typedef void (Model0_bio_end_io_t) (struct Model0_bio *);
typedef void (Model0_bio_destructor_t) (struct Model0_bio *);


/*
 * main unit of I/O for the block layer and lower layers (ie drivers and
 * stacking drivers)
 */
struct Model0_bio {
 struct Model0_bio *Model0_bi_next; /* request queue link */
 struct Model0_block_device *Model0_bi_bdev;
 int Model0_bi_error;
 unsigned int Model0_bi_opf; /* bottom bits req flags,
						 * top bits REQ_OP. Use
						 * accessors.
						 */
 unsigned short Model0_bi_flags; /* status, command, etc */
 unsigned short Model0_bi_ioprio;

 struct Model0_bvec_iter Model0_bi_iter;

 /* Number of segments in this BIO after
	 * physical address coalescing is performed.
	 */
 unsigned int Model0_bi_phys_segments;

 /*
	 * To keep track of the max segment size, we account for the
	 * sizes of the first and last mergeable segments in this bio.
	 */
 unsigned int Model0_bi_seg_front_size;
 unsigned int Model0_bi_seg_back_size;

 Model0_atomic_t Model0___bi_remaining;

 Model0_bio_end_io_t *Model0_bi_end_io;

 void *Model0_bi_private;
 union {



 };

 unsigned short Model0_bi_vcnt; /* how many bio_vec's */

 /*
	 * Everything starting with bi_max_vecs will be preserved by bio_reset()
	 */

 unsigned short Model0_bi_max_vecs; /* max bvl_vecs we can hold */

 Model0_atomic_t Model0___bi_cnt; /* pin count */

 struct Model0_bio_vec *Model0_bi_io_vec; /* the actual vec list */

 struct Model0_bio_set *Model0_bi_pool;

 /*
	 * We can inline a number of vecs at the end of the bio, to avoid
	 * double allocations for a small number of bio_vecs. This member
	 * MUST obviously be kept at the very end of the bio.
	 */
 struct Model0_bio_vec Model0_bi_inline_vecs[0];
};
/*
 * bio flags
 */
/*
 * Flags starting here get preserved by bio_reset() - this includes
 * BVEC_POOL_IDX()
 */


/*
 * We support 6 different bvec pools, the last one is magic in that it
 * is backed by a mempool.
 */



/*
 * Top 4 bits of bio flags indicate the pool the bvecs came from.  We add
 * 1 to the actual index so that 0 indicates that there are no bvecs to be
 * freed.
 */






/*
 * Request flags.  For use in the cmd_flags field of struct request, and in
 * bi_opf of struct bio.  Note that some flags are only valid in either one.
 */
enum Model0_rq_flag_bits {
 /* common flags */
 Model0___REQ_FAILFAST_DEV, /* no driver retries of device errors */
 Model0___REQ_FAILFAST_TRANSPORT, /* no driver retries of transport errors */
 Model0___REQ_FAILFAST_DRIVER, /* no driver retries of driver errors */

 Model0___REQ_SYNC, /* request is sync (sync write or read) */
 Model0___REQ_META, /* metadata io request */
 Model0___REQ_PRIO, /* boost priority in cfq */

 Model0___REQ_NOIDLE, /* don't anticipate more IO after this one */
 Model0___REQ_INTEGRITY, /* I/O includes block integrity payload */
 Model0___REQ_FUA, /* forced unit access */
 Model0___REQ_PREFLUSH, /* request for cache flush */

 /* bio only flags */
 Model0___REQ_RAHEAD, /* read ahead, can fail anytime */
 Model0___REQ_THROTTLED, /* This bio has already been subjected to
				 * throttling rules. Don't do it again. */

 /* request only flags */
 Model0___REQ_SORTED, /* elevator knows about this request */
 Model0___REQ_SOFTBARRIER, /* may not be passed by ioscheduler */
 Model0___REQ_NOMERGE, /* don't touch this for merging */
 Model0___REQ_STARTED, /* drive already may have started this one */
 Model0___REQ_DONTPREP, /* don't call prep for this one */
 Model0___REQ_QUEUED, /* uses queueing */
 Model0___REQ_ELVPRIV, /* elevator private data attached */
 Model0___REQ_FAILED, /* set if the request failed */
 Model0___REQ_QUIET, /* don't worry about errors */
 Model0___REQ_PREEMPT, /* set for "ide_preempt" requests and also
				   for requests for which the SCSI "quiesce"
				   state must be ignored. */
 Model0___REQ_ALLOCED, /* request came from our alloc pool */
 Model0___REQ_COPY_USER, /* contains copies of user pages */
 Model0___REQ_FLUSH_SEQ, /* request for flush sequence */
 Model0___REQ_IO_STAT, /* account I/O stat */
 Model0___REQ_MIXED_MERGE, /* merge of different types, fail separately */
 Model0___REQ_PM, /* runtime pm request */
 Model0___REQ_HASHED, /* on IO scheduler merge hash */
 Model0___REQ_MQ_INFLIGHT, /* track inflight for MQ */
 Model0___REQ_NR_BITS, /* stops here */
};
/* This mask is used for both bio and request merge checking */
enum Model0_req_op {
 Model0_REQ_OP_READ,
 Model0_REQ_OP_WRITE,
 Model0_REQ_OP_DISCARD, /* request to discard sectors */
 Model0_REQ_OP_SECURE_ERASE, /* request to securely erase sectors */
 Model0_REQ_OP_WRITE_SAME, /* write same block many times */
 Model0_REQ_OP_FLUSH, /* request for cache flush */
};



typedef unsigned int Model0_blk_qc_t;



static inline __attribute__((no_instrument_function)) bool Model0_blk_qc_t_valid(Model0_blk_qc_t Model0_cookie)
{
 return Model0_cookie != -1U;
}

static inline __attribute__((no_instrument_function)) Model0_blk_qc_t Model0_blk_tag_to_qc_t(unsigned int Model0_tag, unsigned int Model0_queue_num)
{
 return Model0_tag | (Model0_queue_num << 16);
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_blk_qc_t_to_queue_num(Model0_blk_qc_t Model0_cookie)
{
 return Model0_cookie >> 16;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_blk_qc_t_to_tag(Model0_blk_qc_t Model0_cookie)
{
 return Model0_cookie & ((1u << 16) - 1);
}





/*
 * Poor man's closures; I wish we could've done them sanely polymorphic,
 * but...
 */

struct Model0_delayed_call {
 void (*Model0_fn)(void *);
 void *Model0_arg;
};



/* I really wish we had closures with sane typechecking... */
static inline __attribute__((no_instrument_function)) void Model0_set_delayed_call(struct Model0_delayed_call *Model0_call,
  void (*Model0_fn)(void *), void *Model0_arg)
{
 Model0_call->Model0_fn = Model0_fn;
 Model0_call->Model0_arg = Model0_arg;
}

static inline __attribute__((no_instrument_function)) void Model0_do_delayed_call(struct Model0_delayed_call *Model0_call)
{
 if (Model0_call->Model0_fn)
  Model0_call->Model0_fn(Model0_call->Model0_arg);
}

static inline __attribute__((no_instrument_function)) void Model0_clear_delayed_call(struct Model0_delayed_call *Model0_call)
{
 Model0_call->Model0_fn = ((void *)0);
}





/*
 * This file has definitions for some important file table structures
 * and constants and structures used by various generic file system
 * ioctl's.  Please do not make any changes in this file before
 * sending patches for review to linux-fsdevel@vger.kernel.org and
 * linux-api@vger.kernel.org.
 */





/*
 * It's silly to have NR_OPEN bigger than NR_FILE, but you can change
 * the file limit at runtime and only root can increase the per-process
 * nr_file rlimit, so it's safe to set up a ridiculously high absolute
 * upper limit on files-per-process.
 *
 * Some programs (notably those using select()) may have to be 
 * recompiled to take full advantage of the new limits..  
 */

/* Fixed constants first: */
struct Model0_file_clone_range {
 Model0___s64 Model0_src_fd;
 __u64 Model0_src_offset;
 __u64 Model0_src_length;
 __u64 Model0_dest_offset;
};

struct Model0_fstrim_range {
 __u64 Model0_start;
 __u64 Model0_len;
 __u64 Model0_minlen;
};

/* extent-same (dedupe) ioctls; these MUST match the btrfs ioctl definitions */



/* from struct btrfs_ioctl_file_extent_same_info */
struct Model0_file_dedupe_range_info {
 Model0___s64 Model0_dest_fd; /* in - destination file */
 __u64 Model0_dest_offset; /* in - start of extent in destination */
 __u64 Model0_bytes_deduped; /* out - total # of bytes we were able
				 * to dedupe from this file. */
 /* status of this dedupe operation:
	 * < 0 for error
	 * == FILE_DEDUPE_RANGE_SAME if dedupe succeeds
	 * == FILE_DEDUPE_RANGE_DIFFERS if data differs
	 */
 Model0___s32 Model0_status; /* out - see above description */
 __u32 Model0_reserved; /* must be zero */
};

/* from struct btrfs_ioctl_file_extent_same_args */
struct Model0_file_dedupe_range {
 __u64 Model0_src_offset; /* in - start of extent in source */
 __u64 Model0_src_length; /* in - length of extent */
 Model0___u16 Model0_dest_count; /* in - total elements in info array */
 Model0___u16 Model0_reserved1; /* must be zero */
 __u32 Model0_reserved2; /* must be zero */
 struct Model0_file_dedupe_range_info Model0_info[0];
};

/* And dynamically-tunable limits and defaults: */
struct Model0_files_stat_struct {
 unsigned long Model0_nr_files; /* read only */
 unsigned long Model0_nr_free_files; /* read only */
 unsigned long Model0_max_files; /* tunable */
};

struct Model0_inodes_stat_t {
 long Model0_nr_inodes;
 long Model0_nr_unused;
 long Model0_dummy[5]; /* padding for sysctl ABI compatibility */
};





/*
 * These are the fs-independent mount-flags: up to 32 flags are supported
 */
/* These sb flags are internal to the kernel */





/*
 * Superblock flags that can be altered by MS_REMOUNT
 */



/*
 * Old magic mount flag and mask
 */



/*
 * Structure for FS_IOC_FSGETXATTR[A] and FS_IOC_FSSETXATTR.
 */
struct Model0_fsxattr {
 __u32 Model0_fsx_xflags; /* xflags field value (get/set) */
 __u32 Model0_fsx_extsize; /* extsize field value (get/set)*/
 __u32 Model0_fsx_nextents; /* nextents field value (get)	*/
 __u32 Model0_fsx_projid; /* project identifier (get/set) */
 unsigned char Model0_fsx_pad[12];
};

/*
 * Flags for the fsx_xflags field
 */
/* the read-only stuff doesn't really belong here, but any other place is
   probably as bad and I don't want to create yet another include file. */
/* A jump here: 108-111 have been used for various private purposes. */
/*
 * File system encryption support
 */
/* Policy provided via an ioctl on the topmost directory */


struct Model0_fscrypt_policy {
 __u8 Model0_version;
 __u8 Model0_contents_encryption_mode;
 __u8 Model0_filenames_encryption_mode;
 __u8 Model0_flags;
 __u8 Model0_master_key_descriptor[8];
} __attribute__((packed));





/*
 * Inode flags (FS_IOC_GETFLAGS / FS_IOC_SETFLAGS)
 *
 * Note: for historical reasons, these flags were originally used and
 * defined for use by ext2/ext3, and then other file systems started
 * using these flags so they wouldn't need to write their own version
 * of chattr/lsattr (which was shipped as part of e2fsprogs).  You
 * should think twice before trying to use these flags in new
 * contexts, or trying to assign these flags, since they are used both
 * as the UAPI and the on-disk encoding for ext2/3/4.  Also, we are
 * almost out of 32-bit flags.  :-)
 *
 * We have recently hoisted FS_IOC_FSGETXATTR / FS_IOC_FSSETXATTR from
 * XFS to the generic FS level interface.  This uses a structure that
 * has padding and hence has more room to grow, so it may be more
 * appropriate for many new use cases.
 *
 * Please do not change these flags or interfaces before checking with
 * linux-fsdevel@vger.kernel.org and linux-api@vger.kernel.org.
 */
/* Reserved for compression usage... */



/* End compression flags --- maybe not all used */
/* flags for preadv2/pwritev2: */

struct Model0_backing_dev_info;
struct Model0_bdi_writeback;
struct Model0_export_operations;
struct Model0_hd_geometry;
struct Model0_iovec;
struct Model0_kiocb;
struct Model0_kobject;
struct Model0_pipe_inode_info;
struct Model0_poll_table_struct;
struct Model0_kstatfs;
struct Model0_vm_area_struct;
struct Model0_vfsmount;
struct Model0_cred;
struct Model0_swap_info_struct;
struct Model0_seq_file;
struct Model0_workqueue_struct;
struct Model0_iov_iter;
struct Model0_fscrypt_info;
struct Model0_fscrypt_operations;

extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_inode_init(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_inode_init_early(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_files_init(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_files_maxfiles_init(void);

extern struct Model0_files_stat_struct Model0_files_stat;
extern unsigned long Model0_get_max_files(void);
extern int Model0_sysctl_nr_open;
extern struct Model0_inodes_stat_t Model0_inodes_stat;
extern int Model0_leases_enable, Model0_lease_break_time;
extern int Model0_sysctl_protected_symlinks;
extern int Model0_sysctl_protected_hardlinks;

struct Model0_buffer_head;
typedef int (Model0_get_block_t)(struct Model0_inode *Model0_inode, Model0_sector_t Model0_iblock,
   struct Model0_buffer_head *Model0_bh_result, int Model0_create);
typedef int (Model0_dio_iodone_t)(struct Model0_kiocb *Model0_iocb, Model0_loff_t Model0_offset,
   Model0_ssize_t Model0_bytes, void *Model0_private);
/* called from RCU mode, don't block */


/*
 * flags in file.f_mode.  Note that FMODE_READ and FMODE_WRITE must correspond
 * to O_WRONLY and O_RDWR via the strange trick in __dentry_open()
 */

/* file is open for reading */

/* file is open for writing */

/* file is seekable */

/* file can be accessed using pread */

/* file can be accessed using pwrite */

/* File is opened for execution with sys_execve / sys_uselib */

/* File is opened with O_NDELAY (only set for block devices) */

/* File is opened with O_EXCL (only set for block devices) */

/* File is opened using open(.., 3, ..) and is writeable only for ioctls
   (specialy hack for floppy.c) */

/* 32bit hashes as llseek() offset (for directories) */

/* 64bit hashes as llseek() offset (for directories) */


/*
 * Don't update ctime and mtime.
 *
 * Currently a special hack for the XFS open_by_handle ioctl, but we'll
 * hopefully graduate it to a proper O_CMTIME flag supported by open(2) soon.
 */


/* Expect random access pattern */


/* File is huge (eg. /dev/kmem): treat loff_t as unsigned */


/* File is opened with O_PATH; almost nothing can be done with it */


/* File needs atomic accesses to f_pos */

/* Write access to underlying fs */

/* Has read method(s) */

/* Has write method(s) */


/* File was opened by fanotify and shouldn't generate fanotify events */


/*
 * Flag for rw_copy_check_uvector and compat_rw_copy_check_uvector
 * that indicates that they should check the contents of the iovec are
 * valid, but not check the memory that the iovec elements
 * points too.
 */


/*
 * The below are the various read and write flags that we support. Some of
 * them include behavioral modifiers that send information down to the
 * block layer and IO scheduler. They should be used along with a req_op.
 * Terminology:
 *
 *	The block layer uses device plugging to defer IO a little bit, in
 *	the hope that we will see more IO very shortly. This increases
 *	coalescing of adjacent IO and thus reduces the number of IOs we
 *	have to send to the device. It also allows for better queuing,
 *	if the IO isn't mergeable. If the caller is going to be waiting
 *	for the IO, then he must ensure that the device is unplugged so
 *	that the IO is dispatched to the driver.
 *
 *	All IO is handled async in Linux. This is fine for background
 *	writes, but for reads or writes that someone waits for completion
 *	on, we want to notify the block layer and IO scheduler so that they
 *	know about it. That allows them to make better scheduling
 *	decisions. So when the below references 'sync' and 'async', it
 *	is referencing this priority hint.
 *
 * With that in mind, the available types are:
 *
 * READ			A normal read operation. Device will be plugged.
 * READ_SYNC		A synchronous read. Device is not plugged, caller can
 *			immediately wait on this read without caring about
 *			unplugging.
 * WRITE		A normal async write. Device will be plugged.
 * WRITE_SYNC		Synchronous write. Identical to WRITE, but passes down
 *			the hint that someone will be waiting on this IO
 *			shortly. The write equivalent of READ_SYNC.
 * WRITE_ODIRECT	Special case write for O_DIRECT only.
 * WRITE_FLUSH		Like WRITE_SYNC but with preceding cache flush.
 * WRITE_FUA		Like WRITE_SYNC but data is guaranteed to be on
 *			non-volatile media on completion.
 * WRITE_FLUSH_FUA	Combination of WRITE_FLUSH and FUA. The IO is preceded
 *			by a cache flush and data is guaranteed to be on
 *			non-volatile media on completion.
 *
 */
/*
 * Attribute flags.  These should be or-ed together to figure out what
 * has been changed!
 */
/*
 * Whiteout is represented by a char device.  The following constants define the
 * mode and device number to use.
 */



/*
 * This is the Inode Attributes structure, used for notify_change().  It
 * uses the above definitions as flags, to know which values have changed.
 * Also, in this manner, a Filesystem can look at only the values it cares
 * about.  Basically, these are the attributes that the VFS layer can
 * request to change from the FS layer.
 *
 * Derek Atkins <warlord@MIT.EDU> 94-10-20
 */
struct Model0_iattr {
 unsigned int Model0_ia_valid;
 Model0_umode_t Model0_ia_mode;
 Model0_kuid_t Model0_ia_uid;
 Model0_kgid_t Model0_ia_gid;
 Model0_loff_t Model0_ia_size;
 struct Model0_timespec Model0_ia_atime;
 struct Model0_timespec Model0_ia_mtime;
 struct Model0_timespec Model0_ia_ctime;

 /*
	 * Not an attribute, but an auxiliary info for filesystems wanting to
	 * implement an ftruncate() like method.  NOTE: filesystem should
	 * check for (ia_valid & ATTR_FILE), and not for (ia_file != NULL).
	 */
 struct Model0_file *Model0_ia_file;
};

/*
 * Includes for diskquotas.
 */

/*
 * Copyright (c) 1982, 1986 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Robert Elz at The University of Melbourne.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */
/*
 * Copyright (c) 1995-2001,2004 Silicon Graphics, Inc.  All Rights Reserved.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesset General Public License
 * along with this program; if not, write to the Free Software Foundation,
 * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
 */





/*
 * Disk quota - quotactl(2) commands for the XFS Quota Manager (XQM).
 */
/*
 * fs_disk_quota structure:
 *
 * This contains the current quota information regarding a user/proj/group.
 * It is 64-bit aligned, and all the blk units are in BBs (Basic Blocks) of
 * 512 bytes.
 */

typedef struct Model0_fs_disk_quota {
 Model0___s8 Model0_d_version; /* version of this structure */
 Model0___s8 Model0_d_flags; /* FS_{USER,PROJ,GROUP}_QUOTA */
 Model0___u16 Model0_d_fieldmask; /* field specifier */
 __u32 Model0_d_id; /* user, project, or group ID */
 __u64 Model0_d_blk_hardlimit;/* absolute limit on disk blks */
 __u64 Model0_d_blk_softlimit;/* preferred limit on disk blks */
 __u64 Model0_d_ino_hardlimit;/* maximum # allocated inodes */
 __u64 Model0_d_ino_softlimit;/* preferred inode limit */
 __u64 Model0_d_bcount; /* # disk blocks owned by the user */
 __u64 Model0_d_icount; /* # inodes owned by the user */
 Model0___s32 Model0_d_itimer; /* zero if within inode limits */
     /* if not, we refuse service */
 Model0___s32 Model0_d_btimer; /* similar to above; for disk blocks */
 Model0___u16 Model0_d_iwarns; /* # warnings issued wrt num inodes */
 Model0___u16 Model0_d_bwarns; /* # warnings issued wrt disk blocks */
 Model0___s32 Model0_d_padding2; /* padding2 - for future use */
 __u64 Model0_d_rtb_hardlimit;/* absolute limit on realtime blks */
 __u64 Model0_d_rtb_softlimit;/* preferred limit on RT disk blks */
 __u64 Model0_d_rtbcount; /* # realtime blocks owned */
 Model0___s32 Model0_d_rtbtimer; /* similar to above; for RT disk blks */
 Model0___u16 Model0_d_rtbwarns; /* # warnings issued wrt RT disk blks */
 Model0___s16 Model0_d_padding3; /* padding3 - for future use */
 char Model0_d_padding4[8]; /* yet more padding */
} Model0_fs_disk_quota_t;

/*
 * These fields are sent to Q_XSETQLIM to specify fields that need to change.
 */
/*
 * These timers can only be set in super user's dquot. For others, timers are
 * automatically started and stopped. Superusers timer values set the limits
 * for the rest.  In case these values are zero, the DQ_{F,B}TIMELIMIT values
 * defined below are used. 
 * These values also apply only to the d_fieldmask field for Q_XSETQLIM.
 */





/*
 * Warning counts are set in both super user's dquot and others. For others,
 * warnings are set/cleared by the administrators (or automatically by going
 * below the soft limit).  Superusers warning values set the warning limits
 * for the rest.  In case these values are zero, the DQ_{F,B}WARNLIMIT values
 * defined below are used. 
 * These values also apply only to the d_fieldmask field for Q_XSETQLIM.
 */





/*
 * Accounting values.  These can only be set for filesystem with
 * non-transactional quotas that require quotacheck(8) in userspace.
 */





/*
 * Various flags related to quotactl(2).
 */
/*
 * fs_quota_stat is the struct returned in Q_XGETQSTAT for a given file system.
 * Provides a centralized way to get meta information about the quota subsystem.
 * eg. space taken up for user and group quotas, number of dquots currently
 * incore.
 */


/*
 * Some basic information about 'quota files'.
 */
typedef struct Model0_fs_qfilestat {
 __u64 Model0_qfs_ino; /* inode number */
 __u64 Model0_qfs_nblks; /* number of BBs 512-byte-blks */
 __u32 Model0_qfs_nextents; /* number of extents */
} Model0_fs_qfilestat_t;

typedef struct Model0_fs_quota_stat {
 Model0___s8 Model0_qs_version; /* version number for future changes */
 Model0___u16 Model0_qs_flags; /* FS_QUOTA_{U,P,G}DQ_{ACCT,ENFD} */
 Model0___s8 Model0_qs_pad; /* unused */
 Model0_fs_qfilestat_t Model0_qs_uquota; /* user quota storage information */
 Model0_fs_qfilestat_t Model0_qs_gquota; /* group quota storage information */
 __u32 Model0_qs_incoredqs; /* number of dquots incore */
 Model0___s32 Model0_qs_btimelimit; /* limit for blks timer */
 Model0___s32 Model0_qs_itimelimit; /* limit for inodes timer */
 Model0___s32 Model0_qs_rtbtimelimit;/* limit for rt blks timer */
 Model0___u16 Model0_qs_bwarnlimit; /* limit for num warnings */
 Model0___u16 Model0_qs_iwarnlimit; /* limit for num warnings */
} Model0_fs_quota_stat_t;

/*
 * fs_quota_statv is used by Q_XGETQSTATV for a given file system. It provides
 * a centralized way to get meta information about the quota subsystem. eg.
 * space taken up for user, group, and project quotas, number of dquots
 * currently incore.
 *
 * This version has proper versioning support with appropriate padding for
 * future expansions, and ability to expand for future without creating any
 * backward compatibility issues.
 *
 * Q_XGETQSTATV uses the passed in value of the requested version via
 * fs_quota_statv.qs_version to determine the return data layout of
 * fs_quota_statv.  The kernel will fill the data fields relevant to that
 * version.
 *
 * If kernel does not support user space caller specified version, EINVAL will
 * be returned. User space caller can then reduce the version number and retry
 * the same command.
 */

/*
 * Some basic information about 'quota files' for Q_XGETQSTATV command
 */
struct Model0_fs_qfilestatv {
 __u64 Model0_qfs_ino; /* inode number */
 __u64 Model0_qfs_nblks; /* number of BBs 512-byte-blks */
 __u32 Model0_qfs_nextents; /* number of extents */
 __u32 Model0_qfs_pad; /* pad for 8-byte alignment */
};

struct Model0_fs_quota_statv {
 Model0___s8 Model0_qs_version; /* version for future changes */
 __u8 Model0_qs_pad1; /* pad for 16bit alignment */
 Model0___u16 Model0_qs_flags; /* FS_QUOTA_.* flags */
 __u32 Model0_qs_incoredqs; /* number of dquots incore */
 struct Model0_fs_qfilestatv Model0_qs_uquota; /* user quota information */
 struct Model0_fs_qfilestatv Model0_qs_gquota; /* group quota information */
 struct Model0_fs_qfilestatv Model0_qs_pquota; /* project quota information */
 Model0___s32 Model0_qs_btimelimit; /* limit for blks timer */
 Model0___s32 Model0_qs_itimelimit; /* limit for inodes timer */
 Model0___s32 Model0_qs_rtbtimelimit;/* limit for rt blks timer */
 Model0___u16 Model0_qs_bwarnlimit; /* limit for num warnings */
 Model0___u16 Model0_qs_iwarnlimit; /* limit for num warnings */
 __u64 Model0_qs_pad2[8]; /* for future proofing */
};
/*
 *	File with in-memory structures of old quota format
 */




/* Numbers of blocks needed for updates */
/*
 *  Definitions for vfsv0 quota format
 */





/*
 *	Definitions of structures and functions for quota formats using trie
 */






/* Numbers of blocks needed for updates - we count with the smallest
 * possible block size (1024) */





struct Model0_dquot;
struct Model0_kqid;

/* Operations */
struct Model0_qtree_fmt_operations {
 void (*Model0_mem2disk_dqblk)(void *Model0_disk, struct Model0_dquot *Model0_dquot); /* Convert given entry from in memory format to disk one */
 void (*Model0_disk2mem_dqblk)(struct Model0_dquot *Model0_dquot, void *Model0_disk); /* Convert given entry from disk format to in memory one */
 int (*Model0_is_id)(void *Model0_disk, struct Model0_dquot *Model0_dquot); /* Is this structure for given id? */
};

/* Inmemory copy of version specific information */
struct Model0_qtree_mem_dqinfo {
 struct Model0_super_block *Model0_dqi_sb; /* Sb quota is on */
 int Model0_dqi_type; /* Quota type */
 unsigned int Model0_dqi_blocks; /* # of blocks in quota file */
 unsigned int Model0_dqi_free_blk; /* First block in list of free blocks */
 unsigned int Model0_dqi_free_entry; /* First block with free entry */
 unsigned int Model0_dqi_blocksize_bits; /* Block size of quota file */
 unsigned int Model0_dqi_entry_size; /* Size of quota entry in quota file */
 unsigned int Model0_dqi_usable_bs; /* Space usable in block for quota data */
 unsigned int Model0_dqi_qtree_depth; /* Precomputed depth of quota tree */
 const struct Model0_qtree_fmt_operations *Model0_dqi_ops; /* Operations for entry manipulation */
};

int Model0_qtree_write_dquot(struct Model0_qtree_mem_dqinfo *Model0_info, struct Model0_dquot *Model0_dquot);
int Model0_qtree_read_dquot(struct Model0_qtree_mem_dqinfo *Model0_info, struct Model0_dquot *Model0_dquot);
int Model0_qtree_delete_dquot(struct Model0_qtree_mem_dqinfo *Model0_info, struct Model0_dquot *Model0_dquot);
int Model0_qtree_release_dquot(struct Model0_qtree_mem_dqinfo *Model0_info, struct Model0_dquot *Model0_dquot);
int Model0_qtree_entry_unused(struct Model0_qtree_mem_dqinfo *Model0_info, char *Model0_disk);
static inline __attribute__((no_instrument_function)) int Model0_qtree_depth(struct Model0_qtree_mem_dqinfo *Model0_info)
{
 unsigned int Model0_epb = Model0_info->Model0_dqi_usable_bs >> 2;
 unsigned long long Model0_entries = Model0_epb;
 int Model0_i;

 for (Model0_i = 1; Model0_entries < (1ULL << 32); Model0_i++)
  Model0_entries *= Model0_epb;
 return Model0_i;
}
int Model0_qtree_get_next_id(struct Model0_qtree_mem_dqinfo *Model0_info, struct Model0_kqid *Model0_qid);

/* Numbers of blocks needed for updates */






/*
 * A set of types for the internal kernel types representing project ids.
 *
 * The types defined in this header allow distinguishing which project ids in
 * the kernel are values used by userspace and which project id values are
 * the internal kernel values.  With the addition of user namespaces the values
 * can be different.  Using the type system makes it possible for the compiler
 * to detect when we overlook these differences.
 *
 */


struct Model0_user_namespace;
extern struct Model0_user_namespace Model0_init_user_ns;

typedef Model0___kernel_uid32_t Model0_projid_t;

typedef struct {
 Model0_projid_t Model0_val;
} Model0_kprojid_t;

static inline __attribute__((no_instrument_function)) Model0_projid_t Model0___kprojid_val(Model0_kprojid_t Model0_projid)
{
 return Model0_projid.Model0_val;
}






static inline __attribute__((no_instrument_function)) bool Model0_projid_eq(Model0_kprojid_t Model0_left, Model0_kprojid_t Model0_right)
{
 return Model0___kprojid_val(Model0_left) == Model0___kprojid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_projid_lt(Model0_kprojid_t Model0_left, Model0_kprojid_t Model0_right)
{
 return Model0___kprojid_val(Model0_left) < Model0___kprojid_val(Model0_right);
}

static inline __attribute__((no_instrument_function)) bool Model0_projid_valid(Model0_kprojid_t Model0_projid)
{
 return !Model0_projid_eq(Model0_projid, (Model0_kprojid_t){ -1 });
}
static inline __attribute__((no_instrument_function)) Model0_kprojid_t Model0_make_kprojid(struct Model0_user_namespace *Model0_from, Model0_projid_t Model0_projid)
{
 return (Model0_kprojid_t){ Model0_projid };
}

static inline __attribute__((no_instrument_function)) Model0_projid_t Model0_from_kprojid(struct Model0_user_namespace *Model0_to, Model0_kprojid_t Model0_kprojid)
{
 return Model0___kprojid_val(Model0_kprojid);
}

static inline __attribute__((no_instrument_function)) Model0_projid_t Model0_from_kprojid_munged(struct Model0_user_namespace *Model0_to, Model0_kprojid_t Model0_kprojid)
{
 Model0_projid_t Model0_projid = Model0_from_kprojid(Model0_to, Model0_kprojid);
 if (Model0_projid == (Model0_projid_t)-1)
  Model0_projid = 65534;
 return Model0_projid;
}

static inline __attribute__((no_instrument_function)) bool Model0_kprojid_has_mapping(struct Model0_user_namespace *Model0_ns, Model0_kprojid_t Model0_projid)
{
 return true;
}
/*
 * Copyright (c) 1982, 1986 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Robert Elz at The University of Melbourne.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */
/*
 * Definitions for the default names of the quotas files.
 */







/*
 * Command definitions for the 'quotactl' system call.
 * The commands are broken into a main command defined below
 * and a subcommand that is used to convey the type of
 * quota that is being manipulated (see above).
 */
/* Quota format type IDs */





/* Size of block in which space limits are passed through the quota
 * interface */



/*
 * Quota structure used for communication with userspace via quotactl
 * Following flags are used to specify which fields are valid
 */
enum {
 Model0_QIF_BLIMITS_B = 0,
 Model0_QIF_SPACE_B,
 Model0_QIF_ILIMITS_B,
 Model0_QIF_INODES_B,
 Model0_QIF_BTIME_B,
 Model0_QIF_ITIME_B,
};
struct Model0_if_dqblk {
 __u64 Model0_dqb_bhardlimit;
 __u64 Model0_dqb_bsoftlimit;
 __u64 Model0_dqb_curspace;
 __u64 Model0_dqb_ihardlimit;
 __u64 Model0_dqb_isoftlimit;
 __u64 Model0_dqb_curinodes;
 __u64 Model0_dqb_btime;
 __u64 Model0_dqb_itime;
 __u32 Model0_dqb_valid;
};

struct Model0_if_nextdqblk {
 __u64 Model0_dqb_bhardlimit;
 __u64 Model0_dqb_bsoftlimit;
 __u64 Model0_dqb_curspace;
 __u64 Model0_dqb_ihardlimit;
 __u64 Model0_dqb_isoftlimit;
 __u64 Model0_dqb_curinodes;
 __u64 Model0_dqb_btime;
 __u64 Model0_dqb_itime;
 __u32 Model0_dqb_valid;
 __u32 Model0_dqb_id;
};

/*
 * Structure used for setting quota information about file via quotactl
 * Following flags are used to specify which fields are valid
 */





enum {
 Model0_DQF_ROOT_SQUASH_B = 0,
 Model0_DQF_SYS_FILE_B = 16,
 /* Kernel internal flags invisible to userspace */
 Model0_DQF_PRIVATE
};

/* Root squash enabled (for v1 quota format) */

/* Quota stored in a system file */


struct Model0_if_dqinfo {
 __u64 Model0_dqi_bgrace;
 __u64 Model0_dqi_igrace;
 __u32 Model0_dqi_flags; /* DFQ_* */
 __u32 Model0_dqi_valid;
};

/*
 * Definitions for quota netlink interface
 */
enum {
 Model0_QUOTA_NL_C_UNSPEC,
 Model0_QUOTA_NL_C_WARNING,
 Model0___QUOTA_NL_C_MAX,
};


enum {
 Model0_QUOTA_NL_A_UNSPEC,
 Model0_QUOTA_NL_A_QTYPE,
 Model0_QUOTA_NL_A_EXCESS_ID,
 Model0_QUOTA_NL_A_WARNING,
 Model0_QUOTA_NL_A_DEV_MAJOR,
 Model0_QUOTA_NL_A_DEV_MINOR,
 Model0_QUOTA_NL_A_CAUSED_ID,
 Model0_QUOTA_NL_A_PAD,
 Model0___QUOTA_NL_A_MAX,
};




enum Model0_quota_type {
 Model0_USRQUOTA = 0, /* element used for user quotas */
 Model0_GRPQUOTA = 1, /* element used for group quotas */
 Model0_PRJQUOTA = 2, /* element used for project quotas */
};

/* Masks for quota types when used as a bitmask */




typedef Model0___kernel_uid32_t Model0_qid_t; /* Type in which we store ids in memory */
typedef long long Model0_qsize_t; /* Type in which we store sizes */

struct Model0_kqid { /* Type in which we store the quota identifier */
 union {
  Model0_kuid_t Model0_uid;
  Model0_kgid_t Model0_gid;
  Model0_kprojid_t Model0_projid;
 };
 enum Model0_quota_type Model0_type; /* USRQUOTA (uid) or GRPQUOTA (gid) or PRJQUOTA (projid) */
};

extern bool Model0_qid_eq(struct Model0_kqid Model0_left, struct Model0_kqid Model0_right);
extern bool Model0_qid_lt(struct Model0_kqid Model0_left, struct Model0_kqid Model0_right);
extern Model0_qid_t Model0_from_kqid(struct Model0_user_namespace *Model0_to, struct Model0_kqid Model0_qid);
extern Model0_qid_t Model0_from_kqid_munged(struct Model0_user_namespace *Model0_to, struct Model0_kqid Model0_qid);
extern bool Model0_qid_valid(struct Model0_kqid Model0_qid);

/**
 *	make_kqid - Map a user-namespace, type, qid tuple into a kqid.
 *	@from: User namespace that the qid is in
 *	@type: The type of quota
 *	@qid: Quota identifier
 *
 *	Maps a user-namespace, type qid tuple into a kernel internal
 *	kqid, and returns that kqid.
 *
 *	When there is no mapping defined for the user-namespace, type,
 *	qid tuple an invalid kqid is returned.  Callers are expected to
 *	test for and handle handle invalid kqids being returned.
 *	Invalid kqids may be tested for using qid_valid().
 */
static inline __attribute__((no_instrument_function)) struct Model0_kqid Model0_make_kqid(struct Model0_user_namespace *Model0_from,
        enum Model0_quota_type Model0_type, Model0_qid_t Model0_qid)
{
 struct Model0_kqid Model0_kqid;

 Model0_kqid.Model0_type = Model0_type;
 switch (Model0_type) {
 case Model0_USRQUOTA:
  Model0_kqid.Model0_uid = Model0_make_kuid(Model0_from, Model0_qid);
  break;
 case Model0_GRPQUOTA:
  Model0_kqid.Model0_gid = Model0_make_kgid(Model0_from, Model0_qid);
  break;
 case Model0_PRJQUOTA:
  Model0_kqid.Model0_projid = Model0_make_kprojid(Model0_from, Model0_qid);
  break;
 default:
  do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/quota.h"), "i" (114), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0);
 }
 return Model0_kqid;
}

/**
 *	make_kqid_invalid - Explicitly make an invalid kqid
 *	@type: The type of quota identifier
 *
 *	Returns an invalid kqid with the specified type.
 */
static inline __attribute__((no_instrument_function)) struct Model0_kqid Model0_make_kqid_invalid(enum Model0_quota_type Model0_type)
{
 struct Model0_kqid Model0_kqid;

 Model0_kqid.Model0_type = Model0_type;
 switch (Model0_type) {
 case Model0_USRQUOTA:
  Model0_kqid.Model0_uid = (Model0_kuid_t){ -1 };
  break;
 case Model0_GRPQUOTA:
  Model0_kqid.Model0_gid = (Model0_kgid_t){ -1 };
  break;
 case Model0_PRJQUOTA:
  Model0_kqid.Model0_projid = (Model0_kprojid_t){ -1 };
  break;
 default:
  do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/quota.h"), "i" (141), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0);
 }
 return Model0_kqid;
}

/**
 *	make_kqid_uid - Make a kqid from a kuid
 *	@uid: The kuid to make the quota identifier from
 */
static inline __attribute__((no_instrument_function)) struct Model0_kqid Model0_make_kqid_uid(Model0_kuid_t Model0_uid)
{
 struct Model0_kqid Model0_kqid;
 Model0_kqid.Model0_type = Model0_USRQUOTA;
 Model0_kqid.Model0_uid = Model0_uid;
 return Model0_kqid;
}

/**
 *	make_kqid_gid - Make a kqid from a kgid
 *	@gid: The kgid to make the quota identifier from
 */
static inline __attribute__((no_instrument_function)) struct Model0_kqid Model0_make_kqid_gid(Model0_kgid_t Model0_gid)
{
 struct Model0_kqid Model0_kqid;
 Model0_kqid.Model0_type = Model0_GRPQUOTA;
 Model0_kqid.Model0_gid = Model0_gid;
 return Model0_kqid;
}

/**
 *	make_kqid_projid - Make a kqid from a projid
 *	@projid: The kprojid to make the quota identifier from
 */
static inline __attribute__((no_instrument_function)) struct Model0_kqid Model0_make_kqid_projid(Model0_kprojid_t Model0_projid)
{
 struct Model0_kqid Model0_kqid;
 Model0_kqid.Model0_type = Model0_PRJQUOTA;
 Model0_kqid.Model0_projid = Model0_projid;
 return Model0_kqid;
}

/**
 *	qid_has_mapping - Report if a qid maps into a user namespace.
 *	@ns:  The user namespace to see if a value maps into.
 *	@qid: The kernel internal quota identifier to test.
 */
static inline __attribute__((no_instrument_function)) bool Model0_qid_has_mapping(struct Model0_user_namespace *Model0_ns, struct Model0_kqid Model0_qid)
{
 return Model0_from_kqid(Model0_ns, Model0_qid) != (Model0_qid_t) -1;
}


extern Model0_spinlock_t Model0_dq_data_lock;

/* Maximal numbers of writes for quota operation (insert/delete/update)
 * (over VFS all formats) */





/*
 * Data for one user/group kept in memory
 */
struct Model0_mem_dqblk {
 Model0_qsize_t Model0_dqb_bhardlimit; /* absolute limit on disk blks alloc */
 Model0_qsize_t Model0_dqb_bsoftlimit; /* preferred limit on disk blks */
 Model0_qsize_t Model0_dqb_curspace; /* current used space */
 Model0_qsize_t Model0_dqb_rsvspace; /* current reserved space for delalloc*/
 Model0_qsize_t Model0_dqb_ihardlimit; /* absolute limit on allocated inodes */
 Model0_qsize_t Model0_dqb_isoftlimit; /* preferred inode limit */
 Model0_qsize_t Model0_dqb_curinodes; /* current # allocated inodes */
 Model0_time64_t Model0_dqb_btime; /* time limit for excessive disk use */
 Model0_time64_t Model0_dqb_itime; /* time limit for excessive inode use */
};

/*
 * Data for one quotafile kept in memory
 */
struct Model0_quota_format_type;

struct Model0_mem_dqinfo {
 struct Model0_quota_format_type *Model0_dqi_format;
 int Model0_dqi_fmt_id; /* Id of the dqi_format - used when turning
				 * quotas on after remount RW */
 struct Model0_list_head Model0_dqi_dirty_list; /* List of dirty dquots */
 unsigned long Model0_dqi_flags;
 unsigned int Model0_dqi_bgrace;
 unsigned int Model0_dqi_igrace;
 Model0_qsize_t Model0_dqi_max_spc_limit;
 Model0_qsize_t Model0_dqi_max_ino_limit;
 void *Model0_dqi_priv;
};

struct Model0_super_block;

/* Mask for flags passed to userspace */

/* Mask for flags modifiable from userspace */


enum {
 Model0_DQF_INFO_DIRTY_B = Model0_DQF_PRIVATE,
};


extern void Model0_mark_info_dirty(struct Model0_super_block *Model0_sb, int Model0_type);
static inline __attribute__((no_instrument_function)) int Model0_info_dirty(struct Model0_mem_dqinfo *Model0_info)
{
 return (__builtin_constant_p((Model0_DQF_INFO_DIRTY_B)) ? Model0_constant_test_bit((Model0_DQF_INFO_DIRTY_B), (&Model0_info->Model0_dqi_flags)) : Model0_variable_test_bit((Model0_DQF_INFO_DIRTY_B), (&Model0_info->Model0_dqi_flags)));
}

enum {
 Model0_DQST_LOOKUPS,
 Model0_DQST_DROPS,
 Model0_DQST_READS,
 Model0_DQST_WRITES,
 Model0_DQST_CACHE_HITS,
 Model0_DQST_ALLOC_DQUOTS,
 Model0_DQST_FREE_DQUOTS,
 Model0_DQST_SYNCS,
 Model0__DQST_DQSTAT_LAST
};

struct Model0_dqstats {
 int Model0_stat[Model0__DQST_DQSTAT_LAST];
 struct Model0_percpu_counter Model0_counter[Model0__DQST_DQSTAT_LAST];
};

extern struct Model0_dqstats *Model0_dqstats_pcpu;
extern struct Model0_dqstats Model0_dqstats;

static inline __attribute__((no_instrument_function)) void Model0_dqstats_inc(unsigned int Model0_type)
{
 Model0_percpu_counter_inc(&Model0_dqstats.Model0_counter[Model0_type]);
}

static inline __attribute__((no_instrument_function)) void Model0_dqstats_dec(unsigned int Model0_type)
{
 Model0_percpu_counter_dec(&Model0_dqstats.Model0_counter[Model0_type]);
}
struct Model0_dquot {
 struct Model0_hlist_node Model0_dq_hash; /* Hash list in memory */
 struct Model0_list_head Model0_dq_inuse; /* List of all quotas */
 struct Model0_list_head Model0_dq_free; /* Free list element */
 struct Model0_list_head Model0_dq_dirty; /* List of dirty dquots */
 struct Model0_mutex Model0_dq_lock; /* dquot IO lock */
 Model0_atomic_t Model0_dq_count; /* Use count */
 Model0_wait_queue_head_t Model0_dq_wait_unused; /* Wait queue for dquot to become unused */
 struct Model0_super_block *Model0_dq_sb; /* superblock this applies to */
 struct Model0_kqid Model0_dq_id; /* ID this applies to (uid, gid, projid) */
 Model0_loff_t Model0_dq_off; /* Offset of dquot on disk */
 unsigned long Model0_dq_flags; /* See DQ_* */
 struct Model0_mem_dqblk Model0_dq_dqb; /* Diskquota usage */
};

/* Operations which must be implemented by each quota format */
struct Model0_quota_format_ops {
 int (*Model0_check_quota_file)(struct Model0_super_block *Model0_sb, int Model0_type); /* Detect whether file is in our format */
 int (*Model0_read_file_info)(struct Model0_super_block *Model0_sb, int Model0_type); /* Read main info about file - called on quotaon() */
 int (*Model0_write_file_info)(struct Model0_super_block *Model0_sb, int Model0_type); /* Write main info about file */
 int (*Model0_free_file_info)(struct Model0_super_block *Model0_sb, int Model0_type); /* Called on quotaoff() */
 int (*Model0_read_dqblk)(struct Model0_dquot *Model0_dquot); /* Read structure for one user */
 int (*Model0_commit_dqblk)(struct Model0_dquot *Model0_dquot); /* Write structure for one user */
 int (*Model0_release_dqblk)(struct Model0_dquot *Model0_dquot); /* Called when last reference to dquot is being dropped */
 int (*Model0_get_next_id)(struct Model0_super_block *Model0_sb, struct Model0_kqid *Model0_qid); /* Get next ID with existing structure in the quota file */
};

/* Operations working with dquots */
struct Model0_dquot_operations {
 int (*Model0_write_dquot) (struct Model0_dquot *); /* Ordinary dquot write */
 struct Model0_dquot *(*Model0_alloc_dquot)(struct Model0_super_block *, int); /* Allocate memory for new dquot */
 void (*Model0_destroy_dquot)(struct Model0_dquot *); /* Free memory for dquot */
 int (*Model0_acquire_dquot) (struct Model0_dquot *); /* Quota is going to be created on disk */
 int (*Model0_release_dquot) (struct Model0_dquot *); /* Quota is going to be deleted from disk */
 int (*Model0_mark_dirty) (struct Model0_dquot *); /* Dquot is marked dirty */
 int (*Model0_write_info) (struct Model0_super_block *, int); /* Write of quota "superblock" */
 /* get reserved quota for delayed alloc, value returned is managed by
	 * quota code only */
 Model0_qsize_t *(*Model0_get_reserved_space) (struct Model0_inode *);
 int (*Model0_get_projid) (struct Model0_inode *, Model0_kprojid_t *);/* Get project ID */
 /* Get next ID with active quota structure */
 int (*Model0_get_next_id) (struct Model0_super_block *Model0_sb, struct Model0_kqid *Model0_qid);
};

struct Model0_path;

/* Structure for communicating via ->get_dqblk() & ->set_dqblk() */
struct Model0_qc_dqblk {
 int Model0_d_fieldmask; /* mask of fields to change in ->set_dqblk() */
 Model0_u64 Model0_d_spc_hardlimit; /* absolute limit on used space */
 Model0_u64 Model0_d_spc_softlimit; /* preferred limit on used space */
 Model0_u64 Model0_d_ino_hardlimit; /* maximum # allocated inodes */
 Model0_u64 Model0_d_ino_softlimit; /* preferred inode limit */
 Model0_u64 Model0_d_space; /* Space owned by the user */
 Model0_u64 Model0_d_ino_count; /* # inodes owned by the user */
 Model0_s64 Model0_d_ino_timer; /* zero if within inode limits */
    /* if not, we refuse service */
 Model0_s64 Model0_d_spc_timer; /* similar to above; for space */
 int Model0_d_ino_warns; /* # warnings issued wrt num inodes */
 int Model0_d_spc_warns; /* # warnings issued wrt used space */
 Model0_u64 Model0_d_rt_spc_hardlimit; /* absolute limit on realtime space */
 Model0_u64 Model0_d_rt_spc_softlimit; /* preferred limit on RT space */
 Model0_u64 Model0_d_rt_space; /* realtime space owned */
 Model0_s64 Model0_d_rt_spc_timer; /* similar to above; for RT space */
 int Model0_d_rt_spc_warns; /* # warnings issued wrt RT space */
};

/*
 * Field specifiers for ->set_dqblk() in struct qc_dqblk and also for
 * ->set_info() in struct qc_info
 */
/* Structures for communicating via ->get_state */
struct Model0_qc_type_state {
 unsigned int Model0_flags; /* Flags QCI_* */
 unsigned int Model0_spc_timelimit; /* Time after which space softlimit is
					 * enforced */
 unsigned int Model0_ino_timelimit; /* Ditto for inode softlimit */
 unsigned int Model0_rt_spc_timelimit; /* Ditto for real-time space */
 unsigned int Model0_spc_warnlimit; /* Limit for number of space warnings */
 unsigned int Model0_ino_warnlimit; /* Ditto for inodes */
 unsigned int Model0_rt_spc_warnlimit; /* Ditto for real-time space */
 unsigned long long Model0_ino; /* Inode number of quota file */
 Model0_blkcnt_t Model0_blocks; /* Number of 512-byte blocks in the file */
 Model0_blkcnt_t Model0_nextents; /* Number of extents in the file */
};

struct Model0_qc_state {
 unsigned int Model0_s_incoredqs; /* Number of dquots in core */
 /*
	 * Per quota type information. The array should really have
	 * max(MAXQUOTAS, XQM_MAXQUOTAS) entries. BUILD_BUG_ON in
	 * quota_getinfo() makes sure XQM_MAXQUOTAS is large enough.  Once VFS
	 * supports project quotas, this can be changed to MAXQUOTAS
	 */
 struct Model0_qc_type_state Model0_s_state[3];
};

/* Structure for communicating via ->set_info */
struct Model0_qc_info {
 int Model0_i_fieldmask; /* mask of fields to change in ->set_info() */
 unsigned int Model0_i_flags; /* Flags QCI_* */
 unsigned int Model0_i_spc_timelimit; /* Time after which space softlimit is
					 * enforced */
 unsigned int Model0_i_ino_timelimit; /* Ditto for inode softlimit */
 unsigned int Model0_i_rt_spc_timelimit;/* Ditto for real-time space */
 unsigned int Model0_i_spc_warnlimit; /* Limit for number of space warnings */
 unsigned int Model0_i_ino_warnlimit; /* Limit for number of inode warnings */
 unsigned int Model0_i_rt_spc_warnlimit; /* Ditto for real-time space */
};

/* Operations handling requests from userspace */
struct Model0_quotactl_ops {
 int (*Model0_quota_on)(struct Model0_super_block *, int, int, struct Model0_path *);
 int (*Model0_quota_off)(struct Model0_super_block *, int);
 int (*Model0_quota_enable)(struct Model0_super_block *, unsigned int);
 int (*Model0_quota_disable)(struct Model0_super_block *, unsigned int);
 int (*Model0_quota_sync)(struct Model0_super_block *, int);
 int (*Model0_set_info)(struct Model0_super_block *, int, struct Model0_qc_info *);
 int (*Model0_get_dqblk)(struct Model0_super_block *, struct Model0_kqid, struct Model0_qc_dqblk *);
 int (*Model0_get_nextdqblk)(struct Model0_super_block *, struct Model0_kqid *,
        struct Model0_qc_dqblk *);
 int (*Model0_set_dqblk)(struct Model0_super_block *, struct Model0_kqid, struct Model0_qc_dqblk *);
 int (*Model0_get_state)(struct Model0_super_block *, struct Model0_qc_state *);
 int (*Model0_rm_xquota)(struct Model0_super_block *, unsigned int);
};

struct Model0_quota_format_type {
 int Model0_qf_fmt_id; /* Quota format id */
 const struct Model0_quota_format_ops *Model0_qf_ops; /* Operations of format */
 struct Model0_module *Model0_qf_owner; /* Module implementing quota format */
 struct Model0_quota_format_type *Model0_qf_next;
};

/**
 * Quota state flags - they actually come in two flavors - for users and groups.
 *
 * Actual typed flags layout:
 *				USRQUOTA	GRPQUOTA
 *  DQUOT_USAGE_ENABLED		0x0001		0x0002
 *  DQUOT_LIMITS_ENABLED	0x0004		0x0008
 *  DQUOT_SUSPENDED		0x0010		0x0020
 *
 * Following bits are used for non-typed flags:
 *  DQUOT_QUOTA_SYS_FILE	0x0040
 *  DQUOT_NEGATIVE_USAGE	0x0080
 */
enum {
 Model0__DQUOT_USAGE_ENABLED = 0, /* Track disk usage for users */
 Model0__DQUOT_LIMITS_ENABLED, /* Enforce quota limits for users */
 Model0__DQUOT_SUSPENDED, /* User diskquotas are off, but
						 * we have necessary info in
						 * memory to turn them on */
 Model0__DQUOT_STATE_FLAGS
};





/* Other quota flags */


      /* Quota file is a special
						 * system file and user cannot
						 * touch it. Filesystem is
						 * responsible for setting
						 * S_NOQUOTA, S_NOATIME flags
						 */

            /* Allow negative quota usage */
static inline __attribute__((no_instrument_function)) unsigned int Model0_dquot_state_flag(unsigned int Model0_flags, int Model0_type)
{
 return Model0_flags << Model0_type;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_dquot_generic_flag(unsigned int Model0_flags, int Model0_type)
{
 return (Model0_flags >> Model0_type) & ((1 << Model0__DQUOT_USAGE_ENABLED * 3) | (1 << Model0__DQUOT_LIMITS_ENABLED * 3) | (1 << Model0__DQUOT_SUSPENDED * 3));
}

/* Bitmap of quota types where flag is set in flags */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned Model0_dquot_state_types(unsigned Model0_flags, unsigned Model0_flag)
{
 do { bool Model0___cond = !(!((Model0_flag) == 0 || (((Model0_flag) & ((Model0_flag) - 1)) != 0))); extern void Model0___compiletime_assert_505(void) ; if (Model0___cond) Model0___compiletime_assert_505(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 return (Model0_flags / Model0_flag) & ((1 << 3) - 1);
}


extern void Model0_quota_send_warning(struct Model0_kqid Model0_qid, Model0_dev_t Model0_dev,
          const char Model0_warntype);
struct Model0_quota_info {
 unsigned int Model0_flags; /* Flags for diskquotas on this device */
 struct Model0_mutex Model0_dqio_mutex; /* lock device while I/O in progress */
 struct Model0_mutex Model0_dqonoff_mutex; /* Serialize quotaon & quotaoff */
 struct Model0_inode *Model0_files[3]; /* inodes of quotafiles */
 struct Model0_mem_dqinfo Model0_info[3]; /* Information for each quota type */
 const struct Model0_quota_format_ops *Model0_ops[3]; /* Operations for each type */
};

int Model0_register_quota_format(struct Model0_quota_format_type *Model0_fmt);
void Model0_unregister_quota_format(struct Model0_quota_format_type *Model0_fmt);

struct Model0_quota_module_name {
 int Model0_qm_fmt_id;
 char *Model0_qm_mod_name;
};

/*
 * Maximum number of layers of fs stack.  Needs to be limited to
 * prevent kernel stack overflow
 */


/** 
 * enum positive_aop_returns - aop return codes with specific semantics
 *
 * @AOP_WRITEPAGE_ACTIVATE: Informs the caller that page writeback has
 * 			    completed, that the page is still locked, and
 * 			    should be considered active.  The VM uses this hint
 * 			    to return the page to the active list -- it won't
 * 			    be a candidate for writeback again in the near
 * 			    future.  Other callers must be careful to unlock
 * 			    the page if they get this return.  Returned by
 * 			    writepage(); 
 *
 * @AOP_TRUNCATED_PAGE: The AOP method that was handed a locked page has
 *  			unlocked it and the page might have been truncated.
 *  			The caller should back up to acquiring a new page and
 *  			trying again.  The aop will be taking reasonable
 *  			precautions not to livelock.  If the caller held a page
 *  			reference, it should drop it before retrying.  Returned
 *  			by readpage().
 *
 * address_space_operation functions return these large constants to indicate
 * special semantics to the caller.  These are much larger than the bytes in a
 * page to allow for functions that return the number of bytes operated on in a
 * given page.
 */

enum Model0_positive_aop_returns {
 Model0_AOP_WRITEPAGE_ACTIVATE = 0x80000,
 Model0_AOP_TRUNCATED_PAGE = 0x80001,
};







/*
 * oh the beauties of C type declarations.
 */
struct Model0_page;
struct Model0_address_space;
struct Model0_writeback_control;
struct Model0_kiocb {
 struct Model0_file *Model0_ki_filp;
 Model0_loff_t Model0_ki_pos;
 void (*Model0_ki_complete)(struct Model0_kiocb *Model0_iocb, long Model0_ret, long Model0_ret2);
 void *Model0_private;
 int Model0_ki_flags;
};

static inline __attribute__((no_instrument_function)) bool Model0_is_sync_kiocb(struct Model0_kiocb *Model0_kiocb)
{
 return Model0_kiocb->Model0_ki_complete == ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_iocb_flags(struct Model0_file *Model0_file);

static inline __attribute__((no_instrument_function)) void Model0_init_sync_kiocb(struct Model0_kiocb *Model0_kiocb, struct Model0_file *Model0_filp)
{
 *Model0_kiocb = (struct Model0_kiocb) {
  .Model0_ki_filp = Model0_filp,
  .Model0_ki_flags = Model0_iocb_flags(Model0_filp),
 };
}

/*
 * "descriptor" for what we're up to with a read.
 * This allows us to use the same read code yet
 * have multiple different users of the data that
 * we read from a file.
 *
 * The simplest case just copies the data to user
 * mode.
 */
typedef struct {
 Model0_size_t Model0_written;
 Model0_size_t Model0_count;
 union {
  char *Model0_buf;
  void *Model0_data;
 } Model0_arg;
 int error;
} Model0_read_descriptor_t;

typedef int (*Model0_read_actor_t)(Model0_read_descriptor_t *, struct Model0_page *,
  unsigned long, unsigned long);

struct Model0_address_space_operations {
 int (*Model0_writepage)(struct Model0_page *Model0_page, struct Model0_writeback_control *Model0_wbc);
 int (*Model0_readpage)(struct Model0_file *, struct Model0_page *);

 /* Write back some dirty pages from this mapping. */
 int (*Model0_writepages)(struct Model0_address_space *, struct Model0_writeback_control *);

 /* Set a page dirty.  Return true if this dirtied it */
 int (*Model0_set_page_dirty)(struct Model0_page *Model0_page);

 int (*Model0_readpages)(struct Model0_file *Model0_filp, struct Model0_address_space *Model0_mapping,
   struct Model0_list_head *Model0_pages, unsigned Model0_nr_pages);

 int (*Model0_write_begin)(struct Model0_file *, struct Model0_address_space *Model0_mapping,
    Model0_loff_t Model0_pos, unsigned Model0_len, unsigned Model0_flags,
    struct Model0_page **Model0_pagep, void **Model0_fsdata);
 int (*Model0_write_end)(struct Model0_file *, struct Model0_address_space *Model0_mapping,
    Model0_loff_t Model0_pos, unsigned Model0_len, unsigned Model0_copied,
    struct Model0_page *Model0_page, void *Model0_fsdata);

 /* Unfortunately this kludge is needed for FIBMAP. Don't use it */
 Model0_sector_t (*Model0_bmap)(struct Model0_address_space *, Model0_sector_t);
 void (*Model0_invalidatepage) (struct Model0_page *, unsigned int, unsigned int);
 int (*Model0_releasepage) (struct Model0_page *, Model0_gfp_t);
 void (*Model0_freepage)(struct Model0_page *);
 Model0_ssize_t (*Model0_direct_IO)(struct Model0_kiocb *, struct Model0_iov_iter *Model0_iter);
 /*
	 * migrate the contents of a page to the specified target. If
	 * migrate_mode is MIGRATE_ASYNC, it must not block.
	 */
 int (*Model0_migratepage) (struct Model0_address_space *,
   struct Model0_page *, struct Model0_page *, enum Model0_migrate_mode);
 bool (*Model0_isolate_page)(struct Model0_page *, Model0_isolate_mode_t);
 void (*Model0_putback_page)(struct Model0_page *);
 int (*Model0_launder_page) (struct Model0_page *);
 int (*Model0_is_partially_uptodate) (struct Model0_page *, unsigned long,
     unsigned long);
 void (*Model0_is_dirty_writeback) (struct Model0_page *, bool *, bool *);
 int (*Model0_error_remove_page)(struct Model0_address_space *, struct Model0_page *);

 /* swapfile support */
 int (*Model0_swap_activate)(struct Model0_swap_info_struct *Model0_sis, struct Model0_file *Model0_file,
    Model0_sector_t *Model0_span);
 void (*Model0_swap_deactivate)(struct Model0_file *Model0_file);
};

extern const struct Model0_address_space_operations Model0_empty_aops;

/*
 * pagecache_write_begin/pagecache_write_end must be used by general code
 * to write into the pagecache.
 */
int Model0_pagecache_write_begin(struct Model0_file *, struct Model0_address_space *Model0_mapping,
    Model0_loff_t Model0_pos, unsigned Model0_len, unsigned Model0_flags,
    struct Model0_page **Model0_pagep, void **Model0_fsdata);

int Model0_pagecache_write_end(struct Model0_file *, struct Model0_address_space *Model0_mapping,
    Model0_loff_t Model0_pos, unsigned Model0_len, unsigned Model0_copied,
    struct Model0_page *Model0_page, void *Model0_fsdata);

struct Model0_address_space {
 struct Model0_inode *Model0_host; /* owner: inode, block_device */
 struct Model0_radix_tree_root Model0_page_tree; /* radix tree of all pages */
 Model0_spinlock_t Model0_tree_lock; /* and lock protecting it */
 Model0_atomic_t Model0_i_mmap_writable;/* count VM_SHARED mappings */
 struct Model0_rb_root Model0_i_mmap; /* tree of private and shared mappings */
 struct Model0_rw_semaphore Model0_i_mmap_rwsem; /* protect tree, count, list */
 /* Protected by tree_lock together with the radix tree */
 unsigned long Model0_nrpages; /* number of total pages */
 /* number of shadow or DAX exceptional entries */
 unsigned long Model0_nrexceptional;
 unsigned long Model0_writeback_index;/* writeback starts here */
 const struct Model0_address_space_operations *Model0_a_ops; /* methods */
 unsigned long Model0_flags; /* error bits/gfp mask */
 Model0_spinlock_t Model0_private_lock; /* for use by the address_space */
 struct Model0_list_head Model0_private_list; /* ditto */
 void *Model0_private_data; /* ditto */
} __attribute__((aligned(sizeof(long))));
 /*
	 * On most architectures that alignment is already the case; but
	 * must be enforced here for CRIS, to let the least significant bit
	 * of struct page's "mapping" pointer be used for PAGE_MAPPING_ANON.
	 */
struct Model0_request_queue;

struct Model0_block_device {
 Model0_dev_t Model0_bd_dev; /* not a kdev_t - it's a search key */
 int Model0_bd_openers;
 struct Model0_inode * Model0_bd_inode; /* will die */
 struct Model0_super_block * Model0_bd_super;
 struct Model0_mutex Model0_bd_mutex; /* open/close mutex */
 void * Model0_bd_claiming;
 void * Model0_bd_holder;
 int Model0_bd_holders;
 bool Model0_bd_write_holder;

 struct Model0_list_head Model0_bd_holder_disks;

 struct Model0_block_device * Model0_bd_contains;
 unsigned Model0_bd_block_size;
 struct Model0_hd_struct * Model0_bd_part;
 /* number of times partitions within this device have been opened. */
 unsigned Model0_bd_part_count;
 int Model0_bd_invalidated;
 struct Model0_gendisk * Model0_bd_disk;
 struct Model0_request_queue * Model0_bd_queue;
 struct Model0_list_head Model0_bd_list;
 /*
	 * Private data.  You must have bd_claim'ed the block_device
	 * to use this.  NOTE:  bd_claim allows an owner to claim
	 * the same device multiple times, the owner must take special
	 * care to not mess up bd_private for that case.
	 */
 unsigned long Model0_bd_private;

 /* The counter of freeze processes */
 int Model0_bd_fsfreeze_count;
 /* Mutex for freeze */
 struct Model0_mutex Model0_bd_fsfreeze_mutex;
};

/*
 * Radix-tree tags, for tagging dirty and writeback pages within the pagecache
 * radix trees
 */




int Model0_mapping_tagged(struct Model0_address_space *Model0_mapping, int Model0_tag);

static inline __attribute__((no_instrument_function)) void Model0_i_mmap_lock_write(struct Model0_address_space *Model0_mapping)
{
 Model0_down_write(&Model0_mapping->Model0_i_mmap_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model0_i_mmap_unlock_write(struct Model0_address_space *Model0_mapping)
{
 Model0_up_write(&Model0_mapping->Model0_i_mmap_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model0_i_mmap_lock_read(struct Model0_address_space *Model0_mapping)
{
 Model0_down_read(&Model0_mapping->Model0_i_mmap_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model0_i_mmap_unlock_read(struct Model0_address_space *Model0_mapping)
{
 Model0_up_read(&Model0_mapping->Model0_i_mmap_rwsem);
}

/*
 * Might pages of this file be mapped into userspace?
 */
static inline __attribute__((no_instrument_function)) int Model0_mapping_mapped(struct Model0_address_space *Model0_mapping)
{
 return !(({ union { typeof((&Model0_mapping->Model0_i_mmap)->Model0_rb_node) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((&Model0_mapping->Model0_i_mmap)->Model0_rb_node), Model0___u.Model0___c, sizeof((&Model0_mapping->Model0_i_mmap)->Model0_rb_node)); else Model0___read_once_size_nocheck(&((&Model0_mapping->Model0_i_mmap)->Model0_rb_node), Model0___u.Model0___c, sizeof((&Model0_mapping->Model0_i_mmap)->Model0_rb_node)); Model0___u.Model0___val; }) == ((void *)0));
}

/*
 * Might pages of this file have been modified in userspace?
 * Note that i_mmap_writable counts all VM_SHARED vmas: do_mmap_pgoff
 * marks vma as VM_SHARED if it is shared, and the file was opened for
 * writing i.e. vma may be mprotected writable even if now readonly.
 *
 * If i_mmap_writable is negative, no new writable mappings are allowed. You
 * can only deny writable mappings, if none exists right now.
 */
static inline __attribute__((no_instrument_function)) int Model0_mapping_writably_mapped(struct Model0_address_space *Model0_mapping)
{
 return Model0_atomic_read(&Model0_mapping->Model0_i_mmap_writable) > 0;
}

static inline __attribute__((no_instrument_function)) int Model0_mapping_map_writable(struct Model0_address_space *Model0_mapping)
{
 return Model0_atomic_inc_unless_negative(&Model0_mapping->Model0_i_mmap_writable) ?
  0 : -1;
}

static inline __attribute__((no_instrument_function)) void Model0_mapping_unmap_writable(struct Model0_address_space *Model0_mapping)
{
 Model0_atomic_dec(&Model0_mapping->Model0_i_mmap_writable);
}

static inline __attribute__((no_instrument_function)) int Model0_mapping_deny_writable(struct Model0_address_space *Model0_mapping)
{
 return Model0_atomic_dec_unless_positive(&Model0_mapping->Model0_i_mmap_writable) ?
  0 : -16;
}

static inline __attribute__((no_instrument_function)) void Model0_mapping_allow_writable(struct Model0_address_space *Model0_mapping)
{
 Model0_atomic_inc(&Model0_mapping->Model0_i_mmap_writable);
}

/*
 * Use sequence counter to get consistent i_size on 32-bit processors.
 */
struct Model0_posix_acl;



static inline __attribute__((no_instrument_function)) struct Model0_posix_acl *
Model0_uncached_acl_sentinel(struct Model0_task_struct *Model0_task)
{
 return (void *)Model0_task + 1;
}

static inline __attribute__((no_instrument_function)) bool
Model0_is_uncached_acl(struct Model0_posix_acl *Model0_acl)
{
 return (long)Model0_acl & 1;
}





/*
 * Keep mostly read-only and often accessed (especially for
 * the RCU path lookup and 'stat' data) fields at the beginning
 * of the 'struct inode'
 */
struct Model0_inode {
 Model0_umode_t Model0_i_mode;
 unsigned short Model0_i_opflags;
 Model0_kuid_t Model0_i_uid;
 Model0_kgid_t Model0_i_gid;
 unsigned int Model0_i_flags;


 struct Model0_posix_acl *Model0_i_acl;
 struct Model0_posix_acl *Model0_i_default_acl;


 const struct Model0_inode_operations *Model0_i_op;
 struct Model0_super_block *Model0_i_sb;
 struct Model0_address_space *Model0_i_mapping;


 void *Model0_i_security;


 /* Stat data, not accessed from path walking */
 unsigned long Model0_i_ino;
 /*
	 * Filesystems may only read i_nlink directly.  They shall use the
	 * following functions for modification:
	 *
	 *    (set|clear|inc|drop)_nlink
	 *    inode_(inc|dec)_link_count
	 */
 union {
  const unsigned int Model0_i_nlink;
  unsigned int Model0___i_nlink;
 };
 Model0_dev_t Model0_i_rdev;
 Model0_loff_t Model0_i_size;
 struct Model0_timespec Model0_i_atime;
 struct Model0_timespec Model0_i_mtime;
 struct Model0_timespec Model0_i_ctime;
 Model0_spinlock_t Model0_i_lock; /* i_blocks, i_bytes, maybe i_size */
 unsigned short Model0_i_bytes;
 unsigned int Model0_i_blkbits;
 Model0_blkcnt_t Model0_i_blocks;





 /* Misc */
 unsigned long Model0_i_state;
 struct Model0_rw_semaphore Model0_i_rwsem;

 unsigned long Model0_dirtied_when; /* jiffies of first dirtying */
 unsigned long Model0_dirtied_time_when;

 struct Model0_hlist_node Model0_i_hash;
 struct Model0_list_head Model0_i_io_list; /* backing dev IO list */
 struct Model0_list_head Model0_i_lru; /* inode LRU list */
 struct Model0_list_head Model0_i_sb_list;
 struct Model0_list_head Model0_i_wb_list; /* backing dev writeback list */
 union {
  struct Model0_hlist_head Model0_i_dentry;
  struct Model0_callback_head Model0_i_rcu;
 };
 Model0_u64 Model0_i_version;
 Model0_atomic_t Model0_i_count;
 Model0_atomic_t Model0_i_dio_count;
 Model0_atomic_t Model0_i_writecount;



 const struct Model0_file_operations *Model0_i_fop; /* former ->i_op->default_file_ops */
 struct Model0_file_lock_context *Model0_i_flctx;
 struct Model0_address_space Model0_i_data;
 struct Model0_list_head Model0_i_devices;
 union {
  struct Model0_pipe_inode_info *Model0_i_pipe;
  struct Model0_block_device *Model0_i_bdev;
  struct Model0_cdev *Model0_i_cdev;
  char *Model0_i_link;
  unsigned Model0_i_dir_seq;
 };

 __u32 Model0_i_generation;


 __u32 Model0_i_fsnotify_mask; /* all events this inode cares about */
 struct Model0_hlist_head Model0_i_fsnotify_marks;






 void *Model0_i_private; /* fs or device private pointer */
};

static inline __attribute__((no_instrument_function)) int Model0_inode_unhashed(struct Model0_inode *Model0_inode)
{
 return Model0_hlist_unhashed(&Model0_inode->Model0_i_hash);
}

/*
 * inode->i_mutex nesting subclasses for the lock validator:
 *
 * 0: the object of the current VFS operation
 * 1: parent
 * 2: child/target
 * 3: xattr
 * 4: second non-directory
 * 5: second parent (when locking independent directories in rename)
 *
 * I_MUTEX_NONDIR2 is for certain operations (such as rename) which lock two
 * non-directories at once.
 *
 * The locking order between these classes is
 * parent[2] -> child -> grandchild -> normal -> xattr -> second non-directory
 */
enum Model0_inode_i_mutex_lock_class
{
 Model0_I_MUTEX_NORMAL,
 Model0_I_MUTEX_PARENT,
 Model0_I_MUTEX_CHILD,
 Model0_I_MUTEX_XATTR,
 Model0_I_MUTEX_NONDIR2,
 Model0_I_MUTEX_PARENT2,
};

static inline __attribute__((no_instrument_function)) void Model0_inode_lock(struct Model0_inode *Model0_inode)
{
 Model0_down_write(&Model0_inode->Model0_i_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model0_inode_unlock(struct Model0_inode *Model0_inode)
{
 Model0_up_write(&Model0_inode->Model0_i_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model0_inode_lock_shared(struct Model0_inode *Model0_inode)
{
 Model0_down_read(&Model0_inode->Model0_i_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model0_inode_unlock_shared(struct Model0_inode *Model0_inode)
{
 Model0_up_read(&Model0_inode->Model0_i_rwsem);
}

static inline __attribute__((no_instrument_function)) int Model0_inode_trylock(struct Model0_inode *Model0_inode)
{
 return Model0_down_write_trylock(&Model0_inode->Model0_i_rwsem);
}

static inline __attribute__((no_instrument_function)) int Model0_inode_trylock_shared(struct Model0_inode *Model0_inode)
{
 return Model0_down_read_trylock(&Model0_inode->Model0_i_rwsem);
}

static inline __attribute__((no_instrument_function)) int Model0_inode_is_locked(struct Model0_inode *Model0_inode)
{
 return Model0_rwsem_is_locked(&Model0_inode->Model0_i_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model0_inode_lock_nested(struct Model0_inode *Model0_inode, unsigned Model0_subclass)
{
 Model0_down_write(&Model0_inode->Model0_i_rwsem);
}

void Model0_lock_two_nondirectories(struct Model0_inode *, struct Model0_inode*);
void Model0_unlock_two_nondirectories(struct Model0_inode *, struct Model0_inode*);

/*
 * NOTE: in a 32bit arch with a preemptable kernel and
 * an UP compile the i_size_read/write must be atomic
 * with respect to the local cpu (unlike with preempt disabled),
 * but they don't need to be atomic with respect to other cpus like in
 * true SMP (so they need either to either locally disable irq around
 * the read or for example on x86 they can be still implemented as a
 * cmpxchg8b without the need of the lock prefix). For SMP compiles
 * and 64bit archs it makes no difference if preempt is enabled or not.
 */
static inline __attribute__((no_instrument_function)) Model0_loff_t Model0_i_size_read(const struct Model0_inode *Model0_inode)
{
 return Model0_inode->Model0_i_size;

}

/*
 * NOTE: unlike i_size_read(), i_size_write() does need locking around it
 * (normally i_mutex), otherwise on 32bit/SMP an update of i_size_seqcount
 * can be lost, resulting in subsequent i_size_read() calls spinning forever.
 */
static inline __attribute__((no_instrument_function)) void Model0_i_size_write(struct Model0_inode *Model0_inode, Model0_loff_t Model0_i_size)
{
 Model0_inode->Model0_i_size = Model0_i_size;

}

static inline __attribute__((no_instrument_function)) unsigned Model0_iminor(const struct Model0_inode *Model0_inode)
{
 return ((unsigned int) ((Model0_inode->Model0_i_rdev) & ((1U << 20) - 1)));
}

static inline __attribute__((no_instrument_function)) unsigned Model0_imajor(const struct Model0_inode *Model0_inode)
{
 return ((unsigned int) ((Model0_inode->Model0_i_rdev) >> 20));
}

extern struct Model0_block_device *Model0_I_BDEV(struct Model0_inode *Model0_inode);

struct Model0_fown_struct {
 Model0_rwlock_t Model0_lock; /* protects pid, uid, euid fields */
 struct Model0_pid *Model0_pid; /* pid or -pgrp where SIGIO should be sent */
 enum Model0_pid_type Model0_pid_type; /* Kind of process group SIGIO should be sent to */
 Model0_kuid_t Model0_uid, Model0_euid; /* uid/euid of process setting the owner */
 int Model0_signum; /* posix.1b rt signal to be delivered on IO */
};

/*
 * Track a single file's readahead state
 */
struct Model0_file_ra_state {
 unsigned long Model0_start; /* where readahead started */
 unsigned int Model0_size; /* # of readahead pages */
 unsigned int Model0_async_size; /* do asynchronous readahead when
					   there are only # of pages ahead */

 unsigned int Model0_ra_pages; /* Maximum readahead window */
 unsigned int Model0_mmap_miss; /* Cache miss stat for mmap accesses */
 Model0_loff_t Model0_prev_pos; /* Cache last read() position */
};

/*
 * Check if @index falls in the readahead windows.
 */
static inline __attribute__((no_instrument_function)) int Model0_ra_has_index(struct Model0_file_ra_state *Model0_ra, unsigned long Model0_index)
{
 return (Model0_index >= Model0_ra->Model0_start &&
  Model0_index < Model0_ra->Model0_start + Model0_ra->Model0_size);
}

struct Model0_file {
 union {
  struct Model0_llist_node Model0_fu_llist;
  struct Model0_callback_head Model0_fu_rcuhead;
 } Model0_f_u;
 struct Model0_path Model0_f_path;
 struct Model0_inode *Model0_f_inode; /* cached value */
 const struct Model0_file_operations *Model0_f_op;

 /*
	 * Protects f_ep_links, f_flags.
	 * Must not be taken from IRQ context.
	 */
 Model0_spinlock_t Model0_f_lock;
 Model0_atomic_long_t Model0_f_count;
 unsigned int Model0_f_flags;
 Model0_fmode_t Model0_f_mode;
 struct Model0_mutex Model0_f_pos_lock;
 Model0_loff_t Model0_f_pos;
 struct Model0_fown_struct Model0_f_owner;
 const struct Model0_cred *Model0_f_cred;
 struct Model0_file_ra_state Model0_f_ra;

 Model0_u64 Model0_f_version;

 void *Model0_f_security;

 /* needed for tty driver, and maybe others */
 void *Model0_private_data;


 /* Used by fs/eventpoll.c to link all the hooks to this file */
 struct Model0_list_head Model0_f_ep_links;
 struct Model0_list_head Model0_f_tfile_llink;

 struct Model0_address_space *Model0_f_mapping;
} __attribute__((aligned(4))); /* lest something weird decides that 2 is OK */

struct Model0_file_handle {
 __u32 Model0_handle_bytes;
 int Model0_handle_type;
 /* file identifier */
 unsigned char Model0_f_handle[0];
};

static inline __attribute__((no_instrument_function)) struct Model0_file *Model0_get_file(struct Model0_file *Model0_f)
{
 Model0_atomic_long_inc(&Model0_f->Model0_f_count);
 return Model0_f;
}






/* Page cache limit. The filesystems should put that into their s_maxbytes 
   limits, otherwise bad things can happen in VM. */
/*
 * Special return value from posix_lock_file() and vfs_lock_file() for
 * asynchronous locking.
 */


/* legacy typedef, should eventually be removed */
typedef void *Model0_fl_owner_t;

struct Model0_file_lock;

struct Model0_file_lock_operations {
 void (*Model0_fl_copy_lock)(struct Model0_file_lock *, struct Model0_file_lock *);
 void (*Model0_fl_release_private)(struct Model0_file_lock *);
};

struct Model0_lock_manager_operations {
 int (*Model0_lm_compare_owner)(struct Model0_file_lock *, struct Model0_file_lock *);
 unsigned long (*Model0_lm_owner_key)(struct Model0_file_lock *);
 Model0_fl_owner_t (*Model0_lm_get_owner)(Model0_fl_owner_t);
 void (*Model0_lm_put_owner)(Model0_fl_owner_t);
 void (*Model0_lm_notify)(struct Model0_file_lock *); /* unblock callback */
 int (*Model0_lm_grant)(struct Model0_file_lock *, int);
 bool (*Model0_lm_break)(struct Model0_file_lock *);
 int (*Model0_lm_change)(struct Model0_file_lock *, int, struct Model0_list_head *);
 void (*Model0_lm_setup)(struct Model0_file_lock *, void **);
};

struct Model0_lock_manager {
 struct Model0_list_head Model0_list;
 /*
	 * NFSv4 and up also want opens blocked during the grace period;
	 * NLM doesn't care:
	 */
 bool Model0_block_opens;
};

struct Model0_net;
void Model0_locks_start_grace(struct Model0_net *, struct Model0_lock_manager *);
void Model0_locks_end_grace(struct Model0_lock_manager *);
int Model0_locks_in_grace(struct Model0_net *);
int Model0_opens_in_grace(struct Model0_net *);

/* that will die - we need it for nfs_lock_info */




struct Model0_nlm_lockowner;

/*
 * NFS lock info
 */
struct Model0_nfs_lock_info {
 Model0_u32 Model0_state;
 struct Model0_nlm_lockowner *Model0_owner;
 struct Model0_list_head Model0_list;
};

struct Model0_nfs4_lock_state;
struct Model0_nfs4_lock_info {
 struct Model0_nfs4_lock_state *Model0_owner;
};

/*
 * struct file_lock represents a generic "file lock". It's used to represent
 * POSIX byte range locks, BSD (flock) locks, and leases. It's important to
 * note that the same struct is used to represent both a request for a lock and
 * the lock itself, but the same object is never used for both.
 *
 * FIXME: should we create a separate "struct lock_request" to help distinguish
 * these two uses?
 *
 * The varous i_flctx lists are ordered by:
 *
 * 1) lock owner
 * 2) lock range start
 * 3) lock range end
 *
 * Obviously, the last two criteria only matter for POSIX locks.
 */
struct Model0_file_lock {
 struct Model0_file_lock *Model0_fl_next; /* singly linked list for this inode  */
 struct Model0_list_head Model0_fl_list; /* link into file_lock_context */
 struct Model0_hlist_node Model0_fl_link; /* node in global lists */
 struct Model0_list_head Model0_fl_block; /* circular list of blocked processes */
 Model0_fl_owner_t Model0_fl_owner;
 unsigned int Model0_fl_flags;
 unsigned char Model0_fl_type;
 unsigned int Model0_fl_pid;
 int Model0_fl_link_cpu; /* what cpu's list is this on? */
 struct Model0_pid *Model0_fl_nspid;
 Model0_wait_queue_head_t Model0_fl_wait;
 struct Model0_file *Model0_fl_file;
 Model0_loff_t Model0_fl_start;
 Model0_loff_t Model0_fl_end;

 struct Model0_fasync_struct * Model0_fl_fasync; /* for lease break notifications */
 /* for lease breaks: */
 unsigned long Model0_fl_break_time;
 unsigned long Model0_fl_downgrade_time;

 const struct Model0_file_lock_operations *Model0_fl_ops; /* Callbacks for filesystems */
 const struct Model0_lock_manager_operations *Model0_fl_lmops; /* Callbacks for lockmanagers */
 union {
  struct Model0_nfs_lock_info Model0_nfs_fl;
  struct Model0_nfs4_lock_info Model0_nfs4_fl;
  struct {
   struct Model0_list_head Model0_link; /* link in AFS vnode's pending_locks list */
   int Model0_state; /* state of grant or error if -ve */
  } Model0_afs;
 } Model0_fl_u;
};

struct Model0_file_lock_context {
 Model0_spinlock_t Model0_flc_lock;
 struct Model0_list_head Model0_flc_flock;
 struct Model0_list_head Model0_flc_posix;
 struct Model0_list_head Model0_flc_lease;
};

/* The following constant reflects the upper bound of the file/locking space */


















/*
 * FMODE_EXEC is 0x20
 * FMODE_NONOTIFY is 0x4000000
 * These cannot be used by userspace O_* until internal and external open
 * flags are split.
 * -Eric Paris
 */

/*
 * When introducing new O_* bits, please check its uniqueness in fcntl_init().
 */
/*
 * Before Linux 2.6.33 only O_DSYNC semantics were implemented, but using
 * the O_SYNC flag.  We continue to use the existing numerical value
 * for O_DSYNC semantics now, but using the correct symbolic name for it.
 * This new value is used to request true Posix O_SYNC semantics.  It is
 * defined in this strange way to make sure applications compiled against
 * new headers get at least O_DSYNC semantics on older kernels.
 *
 * This has the nice side-effect that we can simply test for O_DSYNC
 * wherever we do not care if O_DSYNC or O_SYNC is used.
 *
 * Note: __O_SYNC must never be used directly.
 */
/* a horrid kludge trying to make sure that this will fail on old kernels */
/*
 * Open File Description Locks
 *
 * Usually record locks held by a process are released on *any* close and are
 * not inherited across a fork().
 *
 * These cmd values will set locks that conflict with process-associated
 * record  locks, but are "owned" by the open file description, not the
 * process. This means that they are inherited across fork() like BSD (flock)
 * locks, and they are only released automatically when the last reference to
 * the the open file against which they were acquired is put.
 */
struct Model0_f_owner_ex {
 int Model0_type;
 Model0___kernel_pid_t Model0_pid;
};

/* for F_[GET|SET]FL */


/* for posix fcntl() and lockf() */






/* for old implementation of bsd flock () */





/* operations for bsd flock(), also used by the kernel implementation */
struct Model0_flock {
 short Model0_l_type;
 short Model0_l_whence;
 Model0___kernel_off_t Model0_l_start;
 Model0___kernel_off_t Model0_l_len;
 Model0___kernel_pid_t Model0_l_pid;

};







struct Model0_flock64 {
 short Model0_l_type;
 short Model0_l_whence;
 Model0___kernel_loff_t Model0_l_start;
 Model0___kernel_loff_t Model0_l_len;
 Model0___kernel_pid_t Model0_l_pid;

};




/*
 * Cancel a blocking posix lock; internal use only until we expose an
 * asynchronous lock api to userspace:
 */


/* Create a file descriptor with FD_CLOEXEC set. */


/*
 * Request nofications on a directory.
 * See below for events that may be notified.
 */


/*
 * Set and get of pipe page size array
 */



/*
 * Set/Get seals
 */



/*
 * Types of seals
 */




/* (1U << 31) is reserved for signed error codes */

/*
 * Types of directory notifications that may be requested.
 */

extern void Model0_send_sigio(struct Model0_fown_struct *Model0_fown, int Model0_fd, int Model0_band);


extern int Model0_fcntl_getlk(struct Model0_file *, unsigned int, struct Model0_flock *);
extern int Model0_fcntl_setlk(unsigned int, struct Model0_file *, unsigned int,
   struct Model0_flock *);







extern int Model0_fcntl_setlease(unsigned int Model0_fd, struct Model0_file *Model0_filp, long Model0_arg);
extern int Model0_fcntl_getlease(struct Model0_file *Model0_filp);

/* fs/locks.c */
void Model0_locks_free_lock_context(struct Model0_inode *Model0_inode);
void Model0_locks_free_lock(struct Model0_file_lock *Model0_fl);
extern void Model0_locks_init_lock(struct Model0_file_lock *);
extern struct Model0_file_lock * Model0_locks_alloc_lock(void);
extern void Model0_locks_copy_lock(struct Model0_file_lock *, struct Model0_file_lock *);
extern void Model0_locks_copy_conflock(struct Model0_file_lock *, struct Model0_file_lock *);
extern void Model0_locks_remove_posix(struct Model0_file *, Model0_fl_owner_t);
extern void Model0_locks_remove_file(struct Model0_file *);
extern void Model0_locks_release_private(struct Model0_file_lock *);
extern void Model0_posix_test_lock(struct Model0_file *, struct Model0_file_lock *);
extern int Model0_posix_lock_file(struct Model0_file *, struct Model0_file_lock *, struct Model0_file_lock *);
extern int Model0_posix_unblock_lock(struct Model0_file_lock *);
extern int Model0_vfs_test_lock(struct Model0_file *, struct Model0_file_lock *);
extern int Model0_vfs_lock_file(struct Model0_file *, unsigned int, struct Model0_file_lock *, struct Model0_file_lock *);
extern int Model0_vfs_cancel_lock(struct Model0_file *Model0_filp, struct Model0_file_lock *Model0_fl);
extern int Model0_locks_lock_inode_wait(struct Model0_inode *Model0_inode, struct Model0_file_lock *Model0_fl);
extern int Model0___break_lease(struct Model0_inode *Model0_inode, unsigned int Model0_flags, unsigned int Model0_type);
extern void Model0_lease_get_mtime(struct Model0_inode *, struct Model0_timespec *Model0_time);
extern int Model0_generic_setlease(struct Model0_file *, long, struct Model0_file_lock **, void **Model0_priv);
extern int Model0_vfs_setlease(struct Model0_file *, long, struct Model0_file_lock **, void **);
extern int Model0_lease_modify(struct Model0_file_lock *, int, struct Model0_list_head *);
struct Model0_files_struct;
extern void Model0_show_fd_locks(struct Model0_seq_file *Model0_f,
    struct Model0_file *Model0_filp, struct Model0_files_struct *Model0_files);
static inline __attribute__((no_instrument_function)) struct Model0_inode *Model0_file_inode(const struct Model0_file *Model0_f)
{
 return Model0_f->Model0_f_inode;
}

static inline __attribute__((no_instrument_function)) struct Model0_dentry *Model0_file_dentry(const struct Model0_file *Model0_file)
{
 return Model0_d_real(Model0_file->Model0_f_path.Model0_dentry, Model0_file_inode(Model0_file), 0);
}

static inline __attribute__((no_instrument_function)) int Model0_locks_lock_file_wait(struct Model0_file *Model0_filp, struct Model0_file_lock *Model0_fl)
{
 return Model0_locks_lock_inode_wait(Model0_file_inode(Model0_filp), Model0_fl);
}

struct Model0_fasync_struct {
 Model0_spinlock_t Model0_fa_lock;
 int Model0_magic;
 int Model0_fa_fd;
 struct Model0_fasync_struct *Model0_fa_next; /* singly linked list */
 struct Model0_file *Model0_fa_file;
 struct Model0_callback_head Model0_fa_rcu;
};



/* SMP safe fasync helpers: */
extern int Model0_fasync_helper(int, struct Model0_file *, int, struct Model0_fasync_struct **);
extern struct Model0_fasync_struct *Model0_fasync_insert_entry(int, struct Model0_file *, struct Model0_fasync_struct **, struct Model0_fasync_struct *);
extern int Model0_fasync_remove_entry(struct Model0_file *, struct Model0_fasync_struct **);
extern struct Model0_fasync_struct *Model0_fasync_alloc(void);
extern void Model0_fasync_free(struct Model0_fasync_struct *);

/* can be called from interrupts */
extern void Model0_kill_fasync(struct Model0_fasync_struct **, int, int);

extern void Model0___f_setown(struct Model0_file *Model0_filp, struct Model0_pid *, enum Model0_pid_type, int Model0_force);
extern void Model0_f_setown(struct Model0_file *Model0_filp, unsigned long Model0_arg, int Model0_force);
extern void Model0_f_delown(struct Model0_file *Model0_filp);
extern Model0_pid_t Model0_f_getown(struct Model0_file *Model0_filp);
extern int Model0_send_sigurg(struct Model0_fown_struct *Model0_fown);

struct Model0_mm_struct;

/*
 *	Umount options
 */







/* sb->s_iflags */




/* sb->s_iflags to limit user namespace mounts */


/* Possible states of 'frozen' field */
enum {
 Model0_SB_UNFROZEN = 0, /* FS is unfrozen */
 Model0_SB_FREEZE_WRITE = 1, /* Writes, dir ops, ioctls frozen */
 Model0_SB_FREEZE_PAGEFAULT = 2, /* Page faults stopped as well */
 Model0_SB_FREEZE_FS = 3, /* For internal FS use (e.g. to stop
					 * internal threads if needed) */
 Model0_SB_FREEZE_COMPLETE = 4, /* ->freeze_fs finished successfully */
};



struct Model0_sb_writers {
 int Model0_frozen; /* Is sb frozen? */
 Model0_wait_queue_head_t Model0_wait_unfrozen; /* for get_super_thawed() */
 struct Model0_percpu_rw_semaphore Model0_rw_sem[(Model0_SB_FREEZE_COMPLETE - 1)];
};

struct Model0_super_block {
 struct Model0_list_head Model0_s_list; /* Keep this first */
 Model0_dev_t Model0_s_dev; /* search index; _not_ kdev_t */
 unsigned char Model0_s_blocksize_bits;
 unsigned long Model0_s_blocksize;
 Model0_loff_t Model0_s_maxbytes; /* Max file size */
 struct Model0_file_system_type *Model0_s_type;
 const struct Model0_super_operations *Model0_s_op;
 const struct Model0_dquot_operations *Model0_dq_op;
 const struct Model0_quotactl_ops *Model0_s_qcop;
 const struct Model0_export_operations *Model0_s_export_op;
 unsigned long Model0_s_flags;
 unsigned long Model0_s_iflags; /* internal SB_I_* flags */
 unsigned long Model0_s_magic;
 struct Model0_dentry *Model0_s_root;
 struct Model0_rw_semaphore Model0_s_umount;
 int Model0_s_count;
 Model0_atomic_t Model0_s_active;

 void *Model0_s_security;

 const struct Model0_xattr_handler **Model0_s_xattr;

 const struct Model0_fscrypt_operations *Model0_s_cop;

 struct Model0_hlist_bl_head Model0_s_anon; /* anonymous dentries for (nfs) exporting */
 struct Model0_list_head Model0_s_mounts; /* list of mounts; _not_ for fs use */
 struct Model0_block_device *Model0_s_bdev;
 struct Model0_backing_dev_info *Model0_s_bdi;
 struct Model0_mtd_info *Model0_s_mtd;
 struct Model0_hlist_node Model0_s_instances;
 unsigned int Model0_s_quota_types; /* Bitmask of supported quota types */
 struct Model0_quota_info Model0_s_dquot; /* Diskquota specific options */

 struct Model0_sb_writers Model0_s_writers;

 char Model0_s_id[32]; /* Informational name */
 Model0_u8 Model0_s_uuid[16]; /* UUID */

 void *Model0_s_fs_info; /* Filesystem private info */
 unsigned int Model0_s_max_links;
 Model0_fmode_t Model0_s_mode;

 /* Granularity of c/m/atime in ns.
	   Cannot be worse than a second */
 Model0_u32 Model0_s_time_gran;

 /*
	 * The next field is for VFS *only*. No filesystems have any business
	 * even looking at it. You had been warned.
	 */
 struct Model0_mutex Model0_s_vfs_rename_mutex; /* Kludge */

 /*
	 * Filesystem subtype.  If non-empty the filesystem type field
	 * in /proc/mounts will be "type.subtype"
	 */
 char *Model0_s_subtype;

 /*
	 * Saved mount options for lazy filesystems using
	 * generic_show_options()
	 */
 char *Model0_s_options;
 const struct Model0_dentry_operations *Model0_s_d_op; /* default d_op for dentries */

 /*
	 * Saved pool identifier for cleancache (-1 means none)
	 */
 int Model0_cleancache_poolid;

 struct Model0_shrinker Model0_s_shrink; /* per-sb shrinker handle */

 /* Number of inodes with nlink == 0 but still referenced */
 Model0_atomic_long_t Model0_s_remove_count;

 /* Being remounted read-only */
 int Model0_s_readonly_remount;

 /* AIO completions deferred from interrupt context */
 struct Model0_workqueue_struct *Model0_s_dio_done_wq;
 struct Model0_hlist_head Model0_s_pins;

 /*
	 * Owning user namespace and default context in which to
	 * interpret filesystem uids, gids, quotas, device nodes,
	 * xattrs and security labels.
	 */
 struct Model0_user_namespace *Model0_s_user_ns;

 /*
	 * Keep the lru lists last in the structure so they always sit on their
	 * own individual cachelines.
	 */
 struct Model0_list_lru Model0_s_dentry_lru __attribute__((__aligned__((1 << (6)))));
 struct Model0_list_lru Model0_s_inode_lru __attribute__((__aligned__((1 << (6)))));
 struct Model0_callback_head Model0_rcu;
 struct Model0_work_struct Model0_destroy_work;

 struct Model0_mutex Model0_s_sync_lock; /* sync serialisation lock */

 /*
	 * Indicates how deep in a filesystem stack this SB is
	 */
 int Model0_s_stack_depth;

 /* s_inode_list_lock protects s_inodes */
 Model0_spinlock_t Model0_s_inode_list_lock __attribute__((__aligned__((1 << (6)))));
 struct Model0_list_head Model0_s_inodes; /* all inodes */

 Model0_spinlock_t Model0_s_inode_wblist_lock;
 struct Model0_list_head Model0_s_inodes_wb; /* writeback inodes */
};

/* Helper functions so that in most cases filesystems will
 * not need to deal directly with kuid_t and kgid_t and can
 * instead deal with the raw numeric values that are stored
 * in the filesystem.
 */
static inline __attribute__((no_instrument_function)) Model0_uid_t Model0_i_uid_read(const struct Model0_inode *Model0_inode)
{
 return Model0_from_kuid(Model0_inode->Model0_i_sb->Model0_s_user_ns, Model0_inode->Model0_i_uid);
}

static inline __attribute__((no_instrument_function)) Model0_gid_t Model0_i_gid_read(const struct Model0_inode *Model0_inode)
{
 return Model0_from_kgid(Model0_inode->Model0_i_sb->Model0_s_user_ns, Model0_inode->Model0_i_gid);
}

static inline __attribute__((no_instrument_function)) void Model0_i_uid_write(struct Model0_inode *Model0_inode, Model0_uid_t Model0_uid)
{
 Model0_inode->Model0_i_uid = Model0_make_kuid(Model0_inode->Model0_i_sb->Model0_s_user_ns, Model0_uid);
}

static inline __attribute__((no_instrument_function)) void Model0_i_gid_write(struct Model0_inode *Model0_inode, Model0_gid_t Model0_gid)
{
 Model0_inode->Model0_i_gid = Model0_make_kgid(Model0_inode->Model0_i_sb->Model0_s_user_ns, Model0_gid);
}

extern struct Model0_timespec Model0_current_fs_time(struct Model0_super_block *Model0_sb);

/*
 * Snapshotting support.
 */

void Model0___sb_end_write(struct Model0_super_block *Model0_sb, int Model0_level);
int Model0___sb_start_write(struct Model0_super_block *Model0_sb, int Model0_level, bool Model0_wait);






/**
 * sb_end_write - drop write access to a superblock
 * @sb: the super we wrote to
 *
 * Decrement number of writers to the filesystem. Wake up possible waiters
 * wanting to freeze the filesystem.
 */
static inline __attribute__((no_instrument_function)) void Model0_sb_end_write(struct Model0_super_block *Model0_sb)
{
 Model0___sb_end_write(Model0_sb, Model0_SB_FREEZE_WRITE);
}

/**
 * sb_end_pagefault - drop write access to a superblock from a page fault
 * @sb: the super we wrote to
 *
 * Decrement number of processes handling write page fault to the filesystem.
 * Wake up possible waiters wanting to freeze the filesystem.
 */
static inline __attribute__((no_instrument_function)) void Model0_sb_end_pagefault(struct Model0_super_block *Model0_sb)
{
 Model0___sb_end_write(Model0_sb, Model0_SB_FREEZE_PAGEFAULT);
}

/**
 * sb_end_intwrite - drop write access to a superblock for internal fs purposes
 * @sb: the super we wrote to
 *
 * Decrement fs-internal number of writers to the filesystem.  Wake up possible
 * waiters wanting to freeze the filesystem.
 */
static inline __attribute__((no_instrument_function)) void Model0_sb_end_intwrite(struct Model0_super_block *Model0_sb)
{
 Model0___sb_end_write(Model0_sb, Model0_SB_FREEZE_FS);
}

/**
 * sb_start_write - get write access to a superblock
 * @sb: the super we write to
 *
 * When a process wants to write data or metadata to a file system (i.e. dirty
 * a page or an inode), it should embed the operation in a sb_start_write() -
 * sb_end_write() pair to get exclusion against file system freezing. This
 * function increments number of writers preventing freezing. If the file
 * system is already frozen, the function waits until the file system is
 * thawed.
 *
 * Since freeze protection behaves as a lock, users have to preserve
 * ordering of freeze protection and other filesystem locks. Generally,
 * freeze protection should be the outermost lock. In particular, we have:
 *
 * sb_start_write
 *   -> i_mutex			(write path, truncate, directory ops, ...)
 *   -> s_umount		(freeze_super, thaw_super)
 */
static inline __attribute__((no_instrument_function)) void Model0_sb_start_write(struct Model0_super_block *Model0_sb)
{
 Model0___sb_start_write(Model0_sb, Model0_SB_FREEZE_WRITE, true);
}

static inline __attribute__((no_instrument_function)) int Model0_sb_start_write_trylock(struct Model0_super_block *Model0_sb)
{
 return Model0___sb_start_write(Model0_sb, Model0_SB_FREEZE_WRITE, false);
}

/**
 * sb_start_pagefault - get write access to a superblock from a page fault
 * @sb: the super we write to
 *
 * When a process starts handling write page fault, it should embed the
 * operation into sb_start_pagefault() - sb_end_pagefault() pair to get
 * exclusion against file system freezing. This is needed since the page fault
 * is going to dirty a page. This function increments number of running page
 * faults preventing freezing. If the file system is already frozen, the
 * function waits until the file system is thawed.
 *
 * Since page fault freeze protection behaves as a lock, users have to preserve
 * ordering of freeze protection and other filesystem locks. It is advised to
 * put sb_start_pagefault() close to mmap_sem in lock ordering. Page fault
 * handling code implies lock dependency:
 *
 * mmap_sem
 *   -> sb_start_pagefault
 */
static inline __attribute__((no_instrument_function)) void Model0_sb_start_pagefault(struct Model0_super_block *Model0_sb)
{
 Model0___sb_start_write(Model0_sb, Model0_SB_FREEZE_PAGEFAULT, true);
}

/*
 * sb_start_intwrite - get write access to a superblock for internal fs purposes
 * @sb: the super we write to
 *
 * This is the third level of protection against filesystem freezing. It is
 * free for use by a filesystem. The only requirement is that it must rank
 * below sb_start_pagefault.
 *
 * For example filesystem can call sb_start_intwrite() when starting a
 * transaction which somewhat eases handling of freezing for internal sources
 * of filesystem changes (internal fs threads, discarding preallocation on file
 * close, etc.).
 */
static inline __attribute__((no_instrument_function)) void Model0_sb_start_intwrite(struct Model0_super_block *Model0_sb)
{
 Model0___sb_start_write(Model0_sb, Model0_SB_FREEZE_FS, true);
}


extern bool Model0_inode_owner_or_capable(const struct Model0_inode *Model0_inode);

/*
 * VFS helper functions..
 */
extern int Model0_vfs_create(struct Model0_inode *, struct Model0_dentry *, Model0_umode_t, bool);
extern int Model0_vfs_mkdir(struct Model0_inode *, struct Model0_dentry *, Model0_umode_t);
extern int Model0_vfs_mknod(struct Model0_inode *, struct Model0_dentry *, Model0_umode_t, Model0_dev_t);
extern int Model0_vfs_symlink(struct Model0_inode *, struct Model0_dentry *, const char *);
extern int Model0_vfs_link(struct Model0_dentry *, struct Model0_inode *, struct Model0_dentry *, struct Model0_inode **);
extern int Model0_vfs_rmdir(struct Model0_inode *, struct Model0_dentry *);
extern int Model0_vfs_unlink(struct Model0_inode *, struct Model0_dentry *, struct Model0_inode **);
extern int Model0_vfs_rename(struct Model0_inode *, struct Model0_dentry *, struct Model0_inode *, struct Model0_dentry *, struct Model0_inode **, unsigned int);
extern int Model0_vfs_whiteout(struct Model0_inode *, struct Model0_dentry *);

/*
 * VFS file helper functions.
 */
extern void Model0_inode_init_owner(struct Model0_inode *Model0_inode, const struct Model0_inode *Model0_dir,
   Model0_umode_t Model0_mode);
extern bool Model0_may_open_dev(const struct Model0_path *Model0_path);
/*
 * VFS FS_IOC_FIEMAP helper definitions.
 */
struct Model0_fiemap_extent_info {
 unsigned int Model0_fi_flags; /* Flags as passed from user */
 unsigned int Model0_fi_extents_mapped; /* Number of mapped extents */
 unsigned int Model0_fi_extents_max; /* Size of fiemap_extent array */
 struct Model0_fiemap_extent *Model0_fi_extents_start; /* Start of
							fiemap_extent array */
};
int Model0_fiemap_fill_next_extent(struct Model0_fiemap_extent_info *Model0_info, Model0_u64 Model0_logical,
       Model0_u64 Model0_phys, Model0_u64 Model0_len, Model0_u32 Model0_flags);
int Model0_fiemap_check_flags(struct Model0_fiemap_extent_info *Model0_fieinfo, Model0_u32 Model0_fs_flags);

/*
 * File types
 *
 * NOTE! These match bits 12..15 of stat.st_mode
 * (ie "(i_mode >> 12) & 15").
 */
/*
 * This is the "filldir" function type, used by readdir() to let
 * the kernel specify what kind of dirent layout it wants to have.
 * This allows the kernel to read directories into kernel space or
 * to have different dirent layouts depending on the binary type.
 */
struct Model0_dir_context;
typedef int (*Model0_filldir_t)(struct Model0_dir_context *, const char *, int, Model0_loff_t, Model0_u64,
    unsigned);

struct Model0_dir_context {
 const Model0_filldir_t Model0_actor;
 Model0_loff_t Model0_pos;
};

struct Model0_block_device_operations;

/* These macros are for out of kernel modules to test that
 * the kernel supports the unlocked_ioctl and compat_ioctl
 * fields in struct file_operations. */



/*
 * These flags let !MMU mmap() govern direct device mapping vs immediate
 * copying more easily for MAP_PRIVATE, especially for ROM filesystems.
 *
 * NOMMU_MAP_COPY:	Copy can be mapped (MAP_PRIVATE)
 * NOMMU_MAP_DIRECT:	Can be mapped directly (MAP_SHARED)
 * NOMMU_MAP_READ:	Can be mapped for reading
 * NOMMU_MAP_WRITE:	Can be mapped for writing
 * NOMMU_MAP_EXEC:	Can be mapped for execution
 */
struct Model0_iov_iter;

struct Model0_file_operations {
 struct Model0_module *Model0_owner;
 Model0_loff_t (*Model0_llseek) (struct Model0_file *, Model0_loff_t, int);
 Model0_ssize_t (*Model0_read) (struct Model0_file *, char *, Model0_size_t, Model0_loff_t *);
 Model0_ssize_t (*Model0_write) (struct Model0_file *, const char *, Model0_size_t, Model0_loff_t *);
 Model0_ssize_t (*Model0_read_iter) (struct Model0_kiocb *, struct Model0_iov_iter *);
 Model0_ssize_t (*Model0_write_iter) (struct Model0_kiocb *, struct Model0_iov_iter *);
 int (*Model0_iterate) (struct Model0_file *, struct Model0_dir_context *);
 int (*Model0_iterate_shared) (struct Model0_file *, struct Model0_dir_context *);
 unsigned int (*Model0_poll) (struct Model0_file *, struct Model0_poll_table_struct *);
 long (*Model0_unlocked_ioctl) (struct Model0_file *, unsigned int, unsigned long);
 long (*Model0_compat_ioctl) (struct Model0_file *, unsigned int, unsigned long);
 int (*Model0_mmap) (struct Model0_file *, struct Model0_vm_area_struct *);
 int (*Model0_open) (struct Model0_inode *, struct Model0_file *);
 int (*Model0_flush) (struct Model0_file *, Model0_fl_owner_t Model0_id);
 int (*Model0_release) (struct Model0_inode *, struct Model0_file *);
 int (*Model0_fsync) (struct Model0_file *, Model0_loff_t, Model0_loff_t, int Model0_datasync);
 int (*Model0_aio_fsync) (struct Model0_kiocb *, int Model0_datasync);
 int (*Model0_fasync) (int, struct Model0_file *, int);
 int (*Model0_lock) (struct Model0_file *, int, struct Model0_file_lock *);
 Model0_ssize_t (*Model0_sendpage) (struct Model0_file *, struct Model0_page *, int, Model0_size_t, Model0_loff_t *, int);
 unsigned long (*Model0_get_unmapped_area)(struct Model0_file *, unsigned long, unsigned long, unsigned long, unsigned long);
 int (*Model0_check_flags)(int);
 int (*Model0_flock) (struct Model0_file *, int, struct Model0_file_lock *);
 Model0_ssize_t (*Model0_splice_write)(struct Model0_pipe_inode_info *, struct Model0_file *, Model0_loff_t *, Model0_size_t, unsigned int);
 Model0_ssize_t (*Model0_splice_read)(struct Model0_file *, Model0_loff_t *, struct Model0_pipe_inode_info *, Model0_size_t, unsigned int);
 int (*Model0_setlease)(struct Model0_file *, long, struct Model0_file_lock **, void **);
 long (*Model0_fallocate)(struct Model0_file *Model0_file, int Model0_mode, Model0_loff_t Model0_offset,
     Model0_loff_t Model0_len);
 void (*Model0_show_fdinfo)(struct Model0_seq_file *Model0_m, struct Model0_file *Model0_f);



 Model0_ssize_t (*Model0_copy_file_range)(struct Model0_file *, Model0_loff_t, struct Model0_file *,
   Model0_loff_t, Model0_size_t, unsigned int);
 int (*Model0_clone_file_range)(struct Model0_file *, Model0_loff_t, struct Model0_file *, Model0_loff_t,
   Model0_u64);
 Model0_ssize_t (*Model0_dedupe_file_range)(struct Model0_file *, Model0_u64, Model0_u64, struct Model0_file *,
   Model0_u64);
};

struct Model0_inode_operations {
 struct Model0_dentry * (*Model0_lookup) (struct Model0_inode *,struct Model0_dentry *, unsigned int);
 const char * (*Model0_get_link) (struct Model0_dentry *, struct Model0_inode *, struct Model0_delayed_call *);
 int (*Model0_permission) (struct Model0_inode *, int);
 struct Model0_posix_acl * (*Model0_get_acl)(struct Model0_inode *, int);

 int (*Model0_readlink) (struct Model0_dentry *, char *,int);

 int (*Model0_create) (struct Model0_inode *,struct Model0_dentry *, Model0_umode_t, bool);
 int (*Model0_link) (struct Model0_dentry *,struct Model0_inode *,struct Model0_dentry *);
 int (*Model0_unlink) (struct Model0_inode *,struct Model0_dentry *);
 int (*Model0_symlink) (struct Model0_inode *,struct Model0_dentry *,const char *);
 int (*Model0_mkdir) (struct Model0_inode *,struct Model0_dentry *,Model0_umode_t);
 int (*Model0_rmdir) (struct Model0_inode *,struct Model0_dentry *);
 int (*Model0_mknod) (struct Model0_inode *,struct Model0_dentry *,Model0_umode_t,Model0_dev_t);
 int (*Model0_rename) (struct Model0_inode *, struct Model0_dentry *,
   struct Model0_inode *, struct Model0_dentry *);
 int (*Model0_rename2) (struct Model0_inode *, struct Model0_dentry *,
   struct Model0_inode *, struct Model0_dentry *, unsigned int);
 int (*Model0_setattr) (struct Model0_dentry *, struct Model0_iattr *);
 int (*Model0_getattr) (struct Model0_vfsmount *Model0_mnt, struct Model0_dentry *, struct Model0_kstat *);
 int (*Model0_setxattr) (struct Model0_dentry *, struct Model0_inode *,
    const char *, const void *, Model0_size_t, int);
 Model0_ssize_t (*Model0_getxattr) (struct Model0_dentry *, struct Model0_inode *,
        const char *, void *, Model0_size_t);
 Model0_ssize_t (*Model0_listxattr) (struct Model0_dentry *, char *, Model0_size_t);
 int (*Model0_removexattr) (struct Model0_dentry *, const char *);
 int (*Model0_fiemap)(struct Model0_inode *, struct Model0_fiemap_extent_info *, Model0_u64 Model0_start,
        Model0_u64 Model0_len);
 int (*Model0_update_time)(struct Model0_inode *, struct Model0_timespec *, int);
 int (*Model0_atomic_open)(struct Model0_inode *, struct Model0_dentry *,
      struct Model0_file *, unsigned Model0_open_flag,
      Model0_umode_t Model0_create_mode, int *Model0_opened);
 int (*Model0_tmpfile) (struct Model0_inode *, struct Model0_dentry *, Model0_umode_t);
 int (*Model0_set_acl)(struct Model0_inode *, struct Model0_posix_acl *, int);
} __attribute__((__aligned__((1 << (6)))));

Model0_ssize_t Model0_rw_copy_check_uvector(int Model0_type, const struct Model0_iovec * Model0_uvector,
         unsigned long Model0_nr_segs, unsigned long Model0_fast_segs,
         struct Model0_iovec *Model0_fast_pointer,
         struct Model0_iovec **Model0_ret_pointer);

extern Model0_ssize_t Model0___vfs_read(struct Model0_file *, char *, Model0_size_t, Model0_loff_t *);
extern Model0_ssize_t Model0___vfs_write(struct Model0_file *, const char *, Model0_size_t, Model0_loff_t *);
extern Model0_ssize_t Model0_vfs_read(struct Model0_file *, char *, Model0_size_t, Model0_loff_t *);
extern Model0_ssize_t Model0_vfs_write(struct Model0_file *, const char *, Model0_size_t, Model0_loff_t *);
extern Model0_ssize_t Model0_vfs_readv(struct Model0_file *, const struct Model0_iovec *,
  unsigned long, Model0_loff_t *, int);
extern Model0_ssize_t Model0_vfs_writev(struct Model0_file *, const struct Model0_iovec *,
  unsigned long, Model0_loff_t *, int);
extern Model0_ssize_t Model0_vfs_copy_file_range(struct Model0_file *, Model0_loff_t , struct Model0_file *,
       Model0_loff_t, Model0_size_t, unsigned int);
extern int Model0_vfs_clone_file_range(struct Model0_file *Model0_file_in, Model0_loff_t Model0_pos_in,
  struct Model0_file *Model0_file_out, Model0_loff_t Model0_pos_out, Model0_u64 Model0_len);
extern int Model0_vfs_dedupe_file_range(struct Model0_file *Model0_file,
     struct Model0_file_dedupe_range *Model0_same);

struct Model0_super_operations {
    struct Model0_inode *(*Model0_alloc_inode)(struct Model0_super_block *Model0_sb);
 void (*Model0_destroy_inode)(struct Model0_inode *);

    void (*Model0_dirty_inode) (struct Model0_inode *, int Model0_flags);
 int (*Model0_write_inode) (struct Model0_inode *, struct Model0_writeback_control *Model0_wbc);
 int (*Model0_drop_inode) (struct Model0_inode *);
 void (*Model0_evict_inode) (struct Model0_inode *);
 void (*Model0_put_super) (struct Model0_super_block *);
 int (*Model0_sync_fs)(struct Model0_super_block *Model0_sb, int Model0_wait);
 int (*Model0_freeze_super) (struct Model0_super_block *);
 int (*Model0_freeze_fs) (struct Model0_super_block *);
 int (*Model0_thaw_super) (struct Model0_super_block *);
 int (*Model0_unfreeze_fs) (struct Model0_super_block *);
 int (*Model0_statfs) (struct Model0_dentry *, struct Model0_kstatfs *);
 int (*Model0_remount_fs) (struct Model0_super_block *, int *, char *);
 void (*Model0_umount_begin) (struct Model0_super_block *);

 int (*Model0_show_options)(struct Model0_seq_file *, struct Model0_dentry *);
 int (*Model0_show_devname)(struct Model0_seq_file *, struct Model0_dentry *);
 int (*Model0_show_path)(struct Model0_seq_file *, struct Model0_dentry *);
 int (*Model0_show_stats)(struct Model0_seq_file *, struct Model0_dentry *);

 Model0_ssize_t (*Model0_quota_read)(struct Model0_super_block *, int, char *, Model0_size_t, Model0_loff_t);
 Model0_ssize_t (*Model0_quota_write)(struct Model0_super_block *, int, const char *, Model0_size_t, Model0_loff_t);
 struct Model0_dquot **(*Model0_get_dquots)(struct Model0_inode *);

 int (*Model0_bdev_try_to_free_page)(struct Model0_super_block*, struct Model0_page*, Model0_gfp_t);
 long (*Model0_nr_cached_objects)(struct Model0_super_block *,
      struct Model0_shrink_control *);
 long (*Model0_free_cached_objects)(struct Model0_super_block *,
        struct Model0_shrink_control *);
};

/*
 * Inode flags - they have no relation to superblock flags now
 */
/*
 * Note that nosuid etc flags are inode-specific: setting some file-system
 * flags just means all the inodes inherit those flags by default. It might be
 * possible to override it selectively if you really wanted to with some
 * ioctl() that is not currently implemented.
 *
 * Exception: MS_RDONLY is always applied to the entire file system.
 *
 * Unfortunately, it is possible to change a filesystems flags with it mounted
 * with files in use.  This means that all of the inodes will not have their
 * i_flags updated.  Hence, i_flags no longer inherit the superblock mount
 * flags, so these have to be checked separately. -- rmk@arm.uk.linux.org
 */
static inline __attribute__((no_instrument_function)) bool Model0_HAS_UNMAPPED_ID(struct Model0_inode *Model0_inode)
{
 return !Model0_uid_valid(Model0_inode->Model0_i_uid) || !Model0_gid_valid(Model0_inode->Model0_i_gid);
}

/*
 * Inode state bits.  Protected by inode->i_lock
 *
 * Three bits determine the dirty state of the inode, I_DIRTY_SYNC,
 * I_DIRTY_DATASYNC and I_DIRTY_PAGES.
 *
 * Four bits define the lifetime of an inode.  Initially, inodes are I_NEW,
 * until that flag is cleared.  I_WILL_FREE, I_FREEING and I_CLEAR are set at
 * various stages of removing an inode.
 *
 * Two bits are used for locking and completion notification, I_NEW and I_SYNC.
 *
 * I_DIRTY_SYNC		Inode is dirty, but doesn't have to be written on
 *			fdatasync().  i_atime is the usual cause.
 * I_DIRTY_DATASYNC	Data-related inode changes pending. We keep track of
 *			these changes separately from I_DIRTY_SYNC so that we
 *			don't have to write inode on fdatasync() when only
 *			mtime has changed in it.
 * I_DIRTY_PAGES	Inode has dirty pages.  Inode itself may be clean.
 * I_NEW		Serves as both a mutex and completion notification.
 *			New inodes set I_NEW.  If two processes both create
 *			the same inode, one of them will release its inode and
 *			wait for I_NEW to be released before returning.
 *			Inodes in I_WILL_FREE, I_FREEING or I_CLEAR state can
 *			also cause waiting on I_NEW, without I_NEW actually
 *			being set.  find_inode() uses this to prevent returning
 *			nearly-dead inodes.
 * I_WILL_FREE		Must be set when calling write_inode_now() if i_count
 *			is zero.  I_FREEING must be set when I_WILL_FREE is
 *			cleared.
 * I_FREEING		Set when inode is about to be freed but still has dirty
 *			pages or buffers attached or the inode itself is still
 *			dirty.
 * I_CLEAR		Added by clear_inode().  In this state the inode is
 *			clean and can be destroyed.  Inode keeps I_FREEING.
 *
 *			Inodes that are I_WILL_FREE, I_FREEING or I_CLEAR are
 *			prohibited for many purposes.  iget() must wait for
 *			the inode to be completely released, then create it
 *			anew.  Other functions will just ignore such inodes,
 *			if appropriate.  I_NEW is used for waiting.
 *
 * I_SYNC		Writeback of inode is running. The bit is set during
 *			data writeback, and cleared with a wakeup on the bit
 *			address once it is done. The bit is also used to pin
 *			the inode in memory for flusher thread.
 *
 * I_REFERENCED		Marks the inode as recently references on the LRU list.
 *
 * I_DIO_WAKEUP		Never set.  Only used as a key for wait_on_bit().
 *
 * I_WB_SWITCH		Cgroup bdi_writeback switching in progress.  Used to
 *			synchronize competing switching instances and to tell
 *			wb stat updates to grab mapping->tree_lock.  See
 *			inode_switch_wb_work_fn() for details.
 *
 * Q: What is the difference between I_WILL_FREE and I_FREEING?
 */
extern void Model0___mark_inode_dirty(struct Model0_inode *, int);
static inline __attribute__((no_instrument_function)) void Model0_mark_inode_dirty(struct Model0_inode *Model0_inode)
{
 Model0___mark_inode_dirty(Model0_inode, ((1 << 0) | (1 << 1) | (1 << 2)));
}

static inline __attribute__((no_instrument_function)) void Model0_mark_inode_dirty_sync(struct Model0_inode *Model0_inode)
{
 Model0___mark_inode_dirty(Model0_inode, (1 << 0));
}

extern void Model0_inc_nlink(struct Model0_inode *Model0_inode);
extern void Model0_drop_nlink(struct Model0_inode *Model0_inode);
extern void Model0_clear_nlink(struct Model0_inode *Model0_inode);
extern void Model0_set_nlink(struct Model0_inode *Model0_inode, unsigned int Model0_nlink);

static inline __attribute__((no_instrument_function)) void Model0_inode_inc_link_count(struct Model0_inode *Model0_inode)
{
 Model0_inc_nlink(Model0_inode);
 Model0_mark_inode_dirty(Model0_inode);
}

static inline __attribute__((no_instrument_function)) void Model0_inode_dec_link_count(struct Model0_inode *Model0_inode)
{
 Model0_drop_nlink(Model0_inode);
 Model0_mark_inode_dirty(Model0_inode);
}

/**
 * inode_inc_iversion - increments i_version
 * @inode: inode that need to be updated
 *
 * Every time the inode is modified, the i_version field will be incremented.
 * The filesystem has to be mounted with i_version flag
 */

static inline __attribute__((no_instrument_function)) void Model0_inode_inc_iversion(struct Model0_inode *Model0_inode)
{
       Model0_spin_lock(&Model0_inode->Model0_i_lock);
       Model0_inode->Model0_i_version++;
       Model0_spin_unlock(&Model0_inode->Model0_i_lock);
}

enum Model0_file_time_flags {
 Model0_S_ATIME = 1,
 Model0_S_MTIME = 2,
 Model0_S_CTIME = 4,
 Model0_S_VERSION = 8,
};

extern bool Model0_atime_needs_update(const struct Model0_path *, struct Model0_inode *);
extern void Model0_touch_atime(const struct Model0_path *);
static inline __attribute__((no_instrument_function)) void Model0_file_accessed(struct Model0_file *Model0_file)
{
 if (!(Model0_file->Model0_f_flags & 01000000))
  Model0_touch_atime(&Model0_file->Model0_f_path);
}

int Model0_sync_inode(struct Model0_inode *Model0_inode, struct Model0_writeback_control *Model0_wbc);
int Model0_sync_inode_metadata(struct Model0_inode *Model0_inode, int Model0_wait);

struct Model0_file_system_type {
 const char *Model0_name;
 int Model0_fs_flags;





 struct Model0_dentry *(*Model0_mount) (struct Model0_file_system_type *, int,
         const char *, void *);
 void (*Model0_kill_sb) (struct Model0_super_block *);
 struct Model0_module *Model0_owner;
 struct Model0_file_system_type * Model0_next;
 struct Model0_hlist_head Model0_fs_supers;

 struct Model0_lock_class_key Model0_s_lock_key;
 struct Model0_lock_class_key Model0_s_umount_key;
 struct Model0_lock_class_key Model0_s_vfs_rename_key;
 struct Model0_lock_class_key Model0_s_writers_key[(Model0_SB_FREEZE_COMPLETE - 1)];

 struct Model0_lock_class_key Model0_i_lock_key;
 struct Model0_lock_class_key Model0_i_mutex_key;
 struct Model0_lock_class_key Model0_i_mutex_dir_key;
};



extern struct Model0_dentry *Model0_mount_ns(struct Model0_file_system_type *Model0_fs_type,
 int Model0_flags, void *Model0_data, void *Model0_ns, struct Model0_user_namespace *Model0_user_ns,
 int (*Model0_fill_super)(struct Model0_super_block *, void *, int));
extern struct Model0_dentry *Model0_mount_bdev(struct Model0_file_system_type *Model0_fs_type,
 int Model0_flags, const char *Model0_dev_name, void *Model0_data,
 int (*Model0_fill_super)(struct Model0_super_block *, void *, int));
extern struct Model0_dentry *Model0_mount_single(struct Model0_file_system_type *Model0_fs_type,
 int Model0_flags, void *Model0_data,
 int (*Model0_fill_super)(struct Model0_super_block *, void *, int));
extern struct Model0_dentry *Model0_mount_nodev(struct Model0_file_system_type *Model0_fs_type,
 int Model0_flags, void *Model0_data,
 int (*Model0_fill_super)(struct Model0_super_block *, void *, int));
extern struct Model0_dentry *Model0_mount_subtree(struct Model0_vfsmount *Model0_mnt, const char *Model0_path);
void Model0_generic_shutdown_super(struct Model0_super_block *Model0_sb);
void Model0_kill_block_super(struct Model0_super_block *Model0_sb);
void Model0_kill_anon_super(struct Model0_super_block *Model0_sb);
void Model0_kill_litter_super(struct Model0_super_block *Model0_sb);
void Model0_deactivate_super(struct Model0_super_block *Model0_sb);
void Model0_deactivate_locked_super(struct Model0_super_block *Model0_sb);
int Model0_set_anon_super(struct Model0_super_block *Model0_s, void *Model0_data);
int Model0_get_anon_bdev(Model0_dev_t *);
void Model0_free_anon_bdev(Model0_dev_t);
struct Model0_super_block *Model0_sget_userns(struct Model0_file_system_type *Model0_type,
   int (*Model0_test)(struct Model0_super_block *,void *),
   int (*Model0_set)(struct Model0_super_block *,void *),
   int Model0_flags, struct Model0_user_namespace *Model0_user_ns,
   void *Model0_data);
struct Model0_super_block *Model0_sget(struct Model0_file_system_type *Model0_type,
   int (*Model0_test)(struct Model0_super_block *,void *),
   int (*Model0_set)(struct Model0_super_block *,void *),
   int Model0_flags, void *Model0_data);
extern struct Model0_dentry *Model0_mount_pseudo(struct Model0_file_system_type *, char *,
 const struct Model0_super_operations *Model0_ops,
 const struct Model0_dentry_operations *Model0_dops,
 unsigned long);

/* Alas, no aliases. Too much hassle with bringing module.h everywhere */




/*
 * This one is to be used *ONLY* from ->open() instances.
 * fops must be non-NULL, pinned down *and* module dependencies
 * should be sufficient to pin the caller down as well.
 */







extern int Model0_register_filesystem(struct Model0_file_system_type *);
extern int Model0_unregister_filesystem(struct Model0_file_system_type *);
extern struct Model0_vfsmount *Model0_kern_mount_data(struct Model0_file_system_type *, void *Model0_data);

extern void Model0_kern_unmount(struct Model0_vfsmount *Model0_mnt);
extern int Model0_may_umount_tree(struct Model0_vfsmount *);
extern int Model0_may_umount(struct Model0_vfsmount *);
extern long Model0_do_mount(const char *, const char *,
       const char *, unsigned long, void *);
extern struct Model0_vfsmount *Model0_collect_mounts(struct Model0_path *);
extern void Model0_drop_collected_mounts(struct Model0_vfsmount *);
extern int Model0_iterate_mounts(int (*)(struct Model0_vfsmount *, void *), void *,
     struct Model0_vfsmount *);
extern int Model0_vfs_statfs(struct Model0_path *, struct Model0_kstatfs *);
extern int Model0_user_statfs(const char *, struct Model0_kstatfs *);
extern int Model0_fd_statfs(int, struct Model0_kstatfs *);
extern int Model0_vfs_ustat(Model0_dev_t, struct Model0_kstatfs *);
extern int Model0_freeze_super(struct Model0_super_block *Model0_super);
extern int Model0_thaw_super(struct Model0_super_block *Model0_super);
extern bool Model0_our_mnt(struct Model0_vfsmount *Model0_mnt);

extern int Model0_current_umask(void);

extern void Model0_ihold(struct Model0_inode * Model0_inode);
extern void Model0_iput(struct Model0_inode *);
extern int Model0_generic_update_time(struct Model0_inode *, struct Model0_timespec *, int);

/* /sys/fs */
extern struct Model0_kobject *Model0_fs_kobj;




extern int Model0_locks_mandatory_locked(struct Model0_file *);
extern int Model0_locks_mandatory_area(struct Model0_inode *, struct Model0_file *, Model0_loff_t, Model0_loff_t, unsigned char);

/*
 * Candidates for mandatory locking have the setgid bit set
 * but no group execute bit -  an otherwise meaningless combination.
 */

static inline __attribute__((no_instrument_function)) int Model0___mandatory_lock(struct Model0_inode *Model0_ino)
{
 return (Model0_ino->Model0_i_mode & (0002000 | 00010)) == 0002000;
}

/*
 * ... and these candidates should be on MS_MANDLOCK mounted fs,
 * otherwise these will be advisory locks
 */

static inline __attribute__((no_instrument_function)) int Model0_mandatory_lock(struct Model0_inode *Model0_ino)
{
 return ((Model0_ino)->Model0_i_sb->Model0_s_flags & (64)) && Model0___mandatory_lock(Model0_ino);
}

static inline __attribute__((no_instrument_function)) int Model0_locks_verify_locked(struct Model0_file *Model0_file)
{
 if (Model0_mandatory_lock(Model0_file_inode(Model0_file)))
  return Model0_locks_mandatory_locked(Model0_file);
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_locks_verify_truncate(struct Model0_inode *Model0_inode,
        struct Model0_file *Model0_f,
        Model0_loff_t Model0_size)
{
 if (!Model0_inode->Model0_i_flctx || !Model0_mandatory_lock(Model0_inode))
  return 0;

 if (Model0_size < Model0_inode->Model0_i_size) {
  return Model0_locks_mandatory_area(Model0_inode, Model0_f, Model0_size, Model0_inode->Model0_i_size - 1,
    1);
 } else {
  return Model0_locks_mandatory_area(Model0_inode, Model0_f, Model0_inode->Model0_i_size, Model0_size - 1,
    1);
 }
}
static inline __attribute__((no_instrument_function)) int Model0_break_lease(struct Model0_inode *Model0_inode, unsigned int Model0_mode)
{
 /*
	 * Since this check is lockless, we must ensure that any refcounts
	 * taken are done before checking i_flctx->flc_lease. Otherwise, we
	 * could end up racing with tasks trying to set a new lease on this
	 * file.
	 */
 asm volatile("mfence":::"memory");
 if (Model0_inode->Model0_i_flctx && !Model0_list_empty_careful(&Model0_inode->Model0_i_flctx->Model0_flc_lease))
  return Model0___break_lease(Model0_inode, Model0_mode, 32);
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_break_deleg(struct Model0_inode *Model0_inode, unsigned int Model0_mode)
{
 /*
	 * Since this check is lockless, we must ensure that any refcounts
	 * taken are done before checking i_flctx->flc_lease. Otherwise, we
	 * could end up racing with tasks trying to set a new lease on this
	 * file.
	 */
 asm volatile("mfence":::"memory");
 if (Model0_inode->Model0_i_flctx && !Model0_list_empty_careful(&Model0_inode->Model0_i_flctx->Model0_flc_lease))
  return Model0___break_lease(Model0_inode, Model0_mode, 4);
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_try_break_deleg(struct Model0_inode *Model0_inode, struct Model0_inode **Model0_delegated_inode)
{
 int Model0_ret;

 Model0_ret = Model0_break_deleg(Model0_inode, 00000001|00004000);
 if (Model0_ret == -11 && Model0_delegated_inode) {
  *Model0_delegated_inode = Model0_inode;
  Model0_ihold(Model0_inode);
 }
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) int Model0_break_deleg_wait(struct Model0_inode **Model0_delegated_inode)
{
 int Model0_ret;

 Model0_ret = Model0_break_deleg(*Model0_delegated_inode, 00000001);
 Model0_iput(*Model0_delegated_inode);
 *Model0_delegated_inode = ((void *)0);
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) int Model0_break_layout(struct Model0_inode *Model0_inode, bool Model0_wait)
{
 asm volatile("mfence":::"memory");
 if (Model0_inode->Model0_i_flctx && !Model0_list_empty_careful(&Model0_inode->Model0_i_flctx->Model0_flc_lease))
  return Model0___break_lease(Model0_inode,
    Model0_wait ? 00000001 : 00000001 | 00004000,
    2048);
 return 0;
}
/* fs/open.c */
struct Model0_audit_names;
struct Model0_filename {
 const char *Model0_name; /* pointer to actual string */
 const char *Model0_uptr; /* original userland pointer */
 struct Model0_audit_names *Model0_aname;
 int Model0_refcnt;
 const char Model0_iname[];
};

extern long Model0_vfs_truncate(const struct Model0_path *, Model0_loff_t);
extern int Model0_do_truncate(struct Model0_dentry *, Model0_loff_t Model0_start, unsigned int Model0_time_attrs,
         struct Model0_file *Model0_filp);
extern int Model0_vfs_fallocate(struct Model0_file *Model0_file, int Model0_mode, Model0_loff_t Model0_offset,
   Model0_loff_t Model0_len);
extern long Model0_do_sys_open(int Model0_dfd, const char *Model0_filename, int Model0_flags,
   Model0_umode_t Model0_mode);
extern struct Model0_file *Model0_file_open_name(struct Model0_filename *, int, Model0_umode_t);
extern struct Model0_file *Model0_filp_open(const char *, int, Model0_umode_t);
extern struct Model0_file *Model0_file_open_root(struct Model0_dentry *, struct Model0_vfsmount *,
       const char *, int, Model0_umode_t);
extern struct Model0_file * Model0_dentry_open(const struct Model0_path *, int, const struct Model0_cred *);
extern int Model0_filp_close(struct Model0_file *, Model0_fl_owner_t Model0_id);

extern struct Model0_filename *Model0_getname_flags(const char *, int, int *);
extern struct Model0_filename *Model0_getname(const char *);
extern struct Model0_filename *Model0_getname_kernel(const char *);
extern void Model0_putname(struct Model0_filename *Model0_name);

enum {
 Model0_FILE_CREATED = 1,
 Model0_FILE_OPENED = 2
};
extern int Model0_finish_open(struct Model0_file *Model0_file, struct Model0_dentry *Model0_dentry,
   int (*Model0_open)(struct Model0_inode *, struct Model0_file *),
   int *Model0_opened);
extern int Model0_finish_no_open(struct Model0_file *Model0_file, struct Model0_dentry *Model0_dentry);

/* fs/ioctl.c */

extern int Model0_ioctl_preallocate(struct Model0_file *Model0_filp, void *Model0_argp);

/* fs/dcache.c */
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_vfs_caches_init_early(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_vfs_caches_init(void);

extern struct Model0_kmem_cache *Model0_names_cachep;





extern int Model0_register_blkdev(unsigned int, const char *);
extern void Model0_unregister_blkdev(unsigned int, const char *);
extern struct Model0_block_device *Model0_bdget(Model0_dev_t);
extern struct Model0_block_device *Model0_bdgrab(struct Model0_block_device *Model0_bdev);
extern void Model0_bd_set_size(struct Model0_block_device *, Model0_loff_t Model0_size);
extern void Model0_bd_forget(struct Model0_inode *Model0_inode);
extern void Model0_bdput(struct Model0_block_device *);
extern void Model0_invalidate_bdev(struct Model0_block_device *);
extern void Model0_iterate_bdevs(void (*)(struct Model0_block_device *, void *), void *);
extern int Model0_sync_blockdev(struct Model0_block_device *Model0_bdev);
extern void Model0_kill_bdev(struct Model0_block_device *);
extern struct Model0_super_block *Model0_freeze_bdev(struct Model0_block_device *);
extern void Model0_emergency_thaw_all(void);
extern int Model0_thaw_bdev(struct Model0_block_device *Model0_bdev, struct Model0_super_block *Model0_sb);
extern int Model0_fsync_bdev(struct Model0_block_device *);

extern struct Model0_super_block *Model0_blockdev_superblock;

static inline __attribute__((no_instrument_function)) bool Model0_sb_is_blkdev_sb(struct Model0_super_block *Model0_sb)
{
 return Model0_sb == Model0_blockdev_superblock;
}
extern int Model0_sync_filesystem(struct Model0_super_block *);
extern const struct Model0_file_operations Model0_def_blk_fops;
extern const struct Model0_file_operations Model0_def_chr_fops;

extern int Model0_ioctl_by_bdev(struct Model0_block_device *, unsigned, unsigned long);
extern int Model0_blkdev_ioctl(struct Model0_block_device *, Model0_fmode_t, unsigned, unsigned long);
extern long Model0_compat_blkdev_ioctl(struct Model0_file *, unsigned, unsigned long);
extern int Model0_blkdev_get(struct Model0_block_device *Model0_bdev, Model0_fmode_t Model0_mode, void *Model0_holder);
extern struct Model0_block_device *Model0_blkdev_get_by_path(const char *Model0_path, Model0_fmode_t Model0_mode,
            void *Model0_holder);
extern struct Model0_block_device *Model0_blkdev_get_by_dev(Model0_dev_t Model0_dev, Model0_fmode_t Model0_mode,
           void *Model0_holder);
extern void Model0_blkdev_put(struct Model0_block_device *Model0_bdev, Model0_fmode_t Model0_mode);
extern int Model0___blkdev_reread_part(struct Model0_block_device *Model0_bdev);
extern int Model0_blkdev_reread_part(struct Model0_block_device *Model0_bdev);


extern int Model0_bd_link_disk_holder(struct Model0_block_device *Model0_bdev, struct Model0_gendisk *Model0_disk);
extern void Model0_bd_unlink_disk_holder(struct Model0_block_device *Model0_bdev,
      struct Model0_gendisk *Model0_disk);
/* fs/char_dev.c */

/* Marks the bottom of the first segment of free char majors */

extern int Model0_alloc_chrdev_region(Model0_dev_t *, unsigned, unsigned, const char *);
extern int Model0_register_chrdev_region(Model0_dev_t, unsigned, const char *);
extern int Model0___register_chrdev(unsigned int Model0_major, unsigned int Model0_baseminor,
        unsigned int Model0_count, const char *Model0_name,
        const struct Model0_file_operations *Model0_fops);
extern void Model0___unregister_chrdev(unsigned int Model0_major, unsigned int Model0_baseminor,
    unsigned int Model0_count, const char *Model0_name);
extern void Model0_unregister_chrdev_region(Model0_dev_t, unsigned);
extern void Model0_chrdev_show(struct Model0_seq_file *,Model0_off_t);

static inline __attribute__((no_instrument_function)) int Model0_register_chrdev(unsigned int Model0_major, const char *Model0_name,
      const struct Model0_file_operations *Model0_fops)
{
 return Model0___register_chrdev(Model0_major, 0, 256, Model0_name, Model0_fops);
}

static inline __attribute__((no_instrument_function)) void Model0_unregister_chrdev(unsigned int Model0_major, const char *Model0_name)
{
 Model0___unregister_chrdev(Model0_major, 0, 256, Model0_name);
}

/* fs/block_dev.c */





extern const char *Model0___bdevname(Model0_dev_t, char *Model0_buffer);
extern const char *Model0_bdevname(struct Model0_block_device *Model0_bdev, char *Model0_buffer);
extern struct Model0_block_device *Model0_lookup_bdev(const char *);
extern void Model0_blkdev_show(struct Model0_seq_file *,Model0_off_t);





extern void Model0_init_special_inode(struct Model0_inode *, Model0_umode_t, Model0_dev_t);

/* Invalid inode operations -- fs/bad_inode.c */
extern void Model0_make_bad_inode(struct Model0_inode *);
extern bool Model0_is_bad_inode(struct Model0_inode *);


static inline __attribute__((no_instrument_function)) bool Model0_op_is_write(unsigned int Model0_op)
{
 return Model0_op == Model0_REQ_OP_READ ? false : true;
}

/*
 * return data direction, READ or WRITE
 */
static inline __attribute__((no_instrument_function)) int Model0_bio_data_dir(struct Model0_bio *Model0_bio)
{
 return Model0_op_is_write(((Model0_bio)->Model0_bi_opf >> (8 * sizeof(unsigned int) - 3))) ? Model0_REQ_OP_WRITE : Model0_REQ_OP_READ;
}

extern void Model0_check_disk_size_change(struct Model0_gendisk *Model0_disk,
       struct Model0_block_device *Model0_bdev);
extern int Model0_revalidate_disk(struct Model0_gendisk *);
extern int Model0_check_disk_change(struct Model0_block_device *);
extern int Model0___invalidate_device(struct Model0_block_device *, bool);
extern int Model0_invalidate_partition(struct Model0_gendisk *, int);

unsigned long Model0_invalidate_mapping_pages(struct Model0_address_space *Model0_mapping,
     unsigned long Model0_start, unsigned long Model0_end);

static inline __attribute__((no_instrument_function)) void Model0_invalidate_remote_inode(struct Model0_inode *Model0_inode)
{
 if ((((Model0_inode->Model0_i_mode) & 00170000) == 0100000) || (((Model0_inode->Model0_i_mode) & 00170000) == 0040000) ||
     (((Model0_inode->Model0_i_mode) & 00170000) == 0120000))
  Model0_invalidate_mapping_pages(Model0_inode->Model0_i_mapping, 0, -1);
}
extern int Model0_invalidate_inode_pages2(struct Model0_address_space *Model0_mapping);
extern int Model0_invalidate_inode_pages2_range(struct Model0_address_space *Model0_mapping,
      unsigned long Model0_start, unsigned long Model0_end);
extern int Model0_write_inode_now(struct Model0_inode *, int);
extern int Model0_filemap_fdatawrite(struct Model0_address_space *);
extern int Model0_filemap_flush(struct Model0_address_space *);
extern int Model0_filemap_fdatawait(struct Model0_address_space *);
extern void Model0_filemap_fdatawait_keep_errors(struct Model0_address_space *);
extern int Model0_filemap_fdatawait_range(struct Model0_address_space *, Model0_loff_t Model0_lstart,
       Model0_loff_t Model0_lend);
extern int Model0_filemap_write_and_wait(struct Model0_address_space *Model0_mapping);
extern int Model0_filemap_write_and_wait_range(struct Model0_address_space *Model0_mapping,
            Model0_loff_t Model0_lstart, Model0_loff_t Model0_lend);
extern int Model0___filemap_fdatawrite_range(struct Model0_address_space *Model0_mapping,
    Model0_loff_t Model0_start, Model0_loff_t Model0_end, int Model0_sync_mode);
extern int Model0_filemap_fdatawrite_range(struct Model0_address_space *Model0_mapping,
    Model0_loff_t Model0_start, Model0_loff_t Model0_end);
extern int Model0_filemap_check_errors(struct Model0_address_space *Model0_mapping);

extern int Model0_vfs_fsync_range(struct Model0_file *Model0_file, Model0_loff_t Model0_start, Model0_loff_t Model0_end,
      int Model0_datasync);
extern int Model0_vfs_fsync(struct Model0_file *Model0_file, int Model0_datasync);

/*
 * Sync the bytes written if this was a synchronous write.  Expect ki_pos
 * to already be updated for the write, and will return either the amount
 * of bytes passed in, or an error if syncing the file failed.
 */
static inline __attribute__((no_instrument_function)) Model0_ssize_t Model0_generic_write_sync(struct Model0_kiocb *Model0_iocb, Model0_ssize_t Model0_count)
{
 if (Model0_iocb->Model0_ki_flags & (1 << 4)) {
  int Model0_ret = Model0_vfs_fsync_range(Model0_iocb->Model0_ki_filp,
    Model0_iocb->Model0_ki_pos - Model0_count, Model0_iocb->Model0_ki_pos - 1,
    (Model0_iocb->Model0_ki_flags & (1 << 5)) ? 0 : 1);
  if (Model0_ret)
   return Model0_ret;
 }

 return Model0_count;
}

extern void Model0_emergency_sync(void);
extern void Model0_emergency_remount(void);

extern Model0_sector_t Model0_bmap(struct Model0_inode *, Model0_sector_t);

extern int Model0_notify_change(struct Model0_dentry *, struct Model0_iattr *, struct Model0_inode **);
extern int Model0_inode_permission(struct Model0_inode *, int);
extern int Model0___inode_permission(struct Model0_inode *, int);
extern int Model0_generic_permission(struct Model0_inode *, int);
extern int Model0___check_sticky(struct Model0_inode *Model0_dir, struct Model0_inode *Model0_inode);

static inline __attribute__((no_instrument_function)) bool Model0_execute_ok(struct Model0_inode *Model0_inode)
{
 return (Model0_inode->Model0_i_mode & (00100|00010|00001)) || (((Model0_inode->Model0_i_mode) & 00170000) == 0040000);
}

static inline __attribute__((no_instrument_function)) void Model0_file_start_write(struct Model0_file *Model0_file)
{
 if (!(((Model0_file_inode(Model0_file)->Model0_i_mode) & 00170000) == 0100000))
  return;
 Model0___sb_start_write(Model0_file_inode(Model0_file)->Model0_i_sb, Model0_SB_FREEZE_WRITE, true);
}

static inline __attribute__((no_instrument_function)) bool Model0_file_start_write_trylock(struct Model0_file *Model0_file)
{
 if (!(((Model0_file_inode(Model0_file)->Model0_i_mode) & 00170000) == 0100000))
  return true;
 return Model0___sb_start_write(Model0_file_inode(Model0_file)->Model0_i_sb, Model0_SB_FREEZE_WRITE, false);
}

static inline __attribute__((no_instrument_function)) void Model0_file_end_write(struct Model0_file *Model0_file)
{
 if (!(((Model0_file_inode(Model0_file)->Model0_i_mode) & 00170000) == 0100000))
  return;
 Model0___sb_end_write(Model0_file_inode(Model0_file)->Model0_i_sb, Model0_SB_FREEZE_WRITE);
}

/*
 * get_write_access() gets write permission for a file.
 * put_write_access() releases this write permission.
 * This is used for regular files.
 * We cannot support write (and maybe mmap read-write shared) accesses and
 * MAP_DENYWRITE mmappings simultaneously. The i_writecount field of an inode
 * can have the following values:
 * 0: no writers, no VM_DENYWRITE mappings
 * < 0: (-i_writecount) vm_area_structs with VM_DENYWRITE set exist
 * > 0: (i_writecount) users are writing to the file.
 *
 * Normally we operate on that counter with atomic_{inc,dec} and it's safe
 * except for the cases where we don't hold i_writecount yet. Then we need to
 * use {get,deny}_write_access() - these functions check the sign and refuse
 * to do the change if sign is wrong.
 */
static inline __attribute__((no_instrument_function)) int Model0_get_write_access(struct Model0_inode *Model0_inode)
{
 return Model0_atomic_inc_unless_negative(&Model0_inode->Model0_i_writecount) ? 0 : -26;
}
static inline __attribute__((no_instrument_function)) int Model0_deny_write_access(struct Model0_file *Model0_file)
{
 struct Model0_inode *Model0_inode = Model0_file_inode(Model0_file);
 return Model0_atomic_dec_unless_positive(&Model0_inode->Model0_i_writecount) ? 0 : -26;
}
static inline __attribute__((no_instrument_function)) void Model0_put_write_access(struct Model0_inode * Model0_inode)
{
 Model0_atomic_dec(&Model0_inode->Model0_i_writecount);
}
static inline __attribute__((no_instrument_function)) void Model0_allow_write_access(struct Model0_file *Model0_file)
{
 if (Model0_file)
  Model0_atomic_inc(&Model0_file_inode(Model0_file)->Model0_i_writecount);
}
static inline __attribute__((no_instrument_function)) bool Model0_inode_is_open_for_write(const struct Model0_inode *Model0_inode)
{
 return Model0_atomic_read(&Model0_inode->Model0_i_writecount) > 0;
}
static inline __attribute__((no_instrument_function)) void Model0_i_readcount_dec(struct Model0_inode *Model0_inode)
{
 return;
}
static inline __attribute__((no_instrument_function)) void Model0_i_readcount_inc(struct Model0_inode *Model0_inode)
{
 return;
}

extern int Model0_do_pipe_flags(int *, int);
enum Model0_kernel_read_file_id {
 Model0_READING_UNKNOWN, Model0_READING_FIRMWARE, Model0_READING_FIRMWARE_PREALLOC_BUFFER, Model0_READING_MODULE, Model0_READING_KEXEC_IMAGE, Model0_READING_KEXEC_INITRAMFS, Model0_READING_POLICY, Model0_READING_MAX_ID,
};

static const char * const Model0_kernel_read_file_str[] = {
 "unknown", "firmware", "firmware", "kernel-module", "kexec-image", "kexec-initramfs", "security-policy", "",
};

static inline __attribute__((no_instrument_function)) const char *Model0_kernel_read_file_id_str(enum Model0_kernel_read_file_id Model0_id)
{
 if (Model0_id < 0 || Model0_id >= Model0_READING_MAX_ID)
  return Model0_kernel_read_file_str[Model0_READING_UNKNOWN];

 return Model0_kernel_read_file_str[Model0_id];
}

extern int Model0_kernel_read(struct Model0_file *, Model0_loff_t, char *, unsigned long);
extern int Model0_kernel_read_file(struct Model0_file *, void **, Model0_loff_t *, Model0_loff_t,
       enum Model0_kernel_read_file_id);
extern int Model0_kernel_read_file_from_path(char *, void **, Model0_loff_t *, Model0_loff_t,
          enum Model0_kernel_read_file_id);
extern int Model0_kernel_read_file_from_fd(int, void **, Model0_loff_t *, Model0_loff_t,
        enum Model0_kernel_read_file_id);
extern Model0_ssize_t Model0_kernel_write(struct Model0_file *, const char *, Model0_size_t, Model0_loff_t);
extern Model0_ssize_t Model0___kernel_write(struct Model0_file *, const char *, Model0_size_t, Model0_loff_t *);
extern struct Model0_file * Model0_open_exec(const char *);

/* fs/dcache.c -- generic fs support functions */
extern bool Model0_is_subdir(struct Model0_dentry *, struct Model0_dentry *);
extern bool Model0_path_is_under(struct Model0_path *, struct Model0_path *);

extern char *Model0_file_path(struct Model0_file *, char *, int);



/* needed for stackable file system support */
extern Model0_loff_t Model0_default_llseek(struct Model0_file *Model0_file, Model0_loff_t Model0_offset, int Model0_whence);

extern Model0_loff_t Model0_vfs_llseek(struct Model0_file *Model0_file, Model0_loff_t Model0_offset, int Model0_whence);

extern int Model0_inode_init_always(struct Model0_super_block *, struct Model0_inode *);
extern void Model0_inode_init_once(struct Model0_inode *);
extern void Model0_address_space_init_once(struct Model0_address_space *Model0_mapping);
extern struct Model0_inode * Model0_igrab(struct Model0_inode *);
extern Model0_ino_t Model0_iunique(struct Model0_super_block *, Model0_ino_t);
extern int Model0_inode_needs_sync(struct Model0_inode *Model0_inode);
extern int Model0_generic_delete_inode(struct Model0_inode *Model0_inode);
static inline __attribute__((no_instrument_function)) int Model0_generic_drop_inode(struct Model0_inode *Model0_inode)
{
 return !Model0_inode->Model0_i_nlink || Model0_inode_unhashed(Model0_inode);
}

extern struct Model0_inode *Model0_ilookup5_nowait(struct Model0_super_block *Model0_sb,
  unsigned long Model0_hashval, int (*Model0_test)(struct Model0_inode *, void *),
  void *Model0_data);
extern struct Model0_inode *Model0_ilookup5(struct Model0_super_block *Model0_sb, unsigned long Model0_hashval,
  int (*Model0_test)(struct Model0_inode *, void *), void *Model0_data);
extern struct Model0_inode *Model0_ilookup(struct Model0_super_block *Model0_sb, unsigned long Model0_ino);

extern struct Model0_inode * Model0_iget5_locked(struct Model0_super_block *, unsigned long, int (*Model0_test)(struct Model0_inode *, void *), int (*Model0_set)(struct Model0_inode *, void *), void *);
extern struct Model0_inode * Model0_iget_locked(struct Model0_super_block *, unsigned long);
extern struct Model0_inode *Model0_find_inode_nowait(struct Model0_super_block *,
           unsigned long,
           int (*Model0_match)(struct Model0_inode *,
          unsigned long, void *),
           void *Model0_data);
extern int Model0_insert_inode_locked4(struct Model0_inode *, unsigned long, int (*Model0_test)(struct Model0_inode *, void *), void *);
extern int Model0_insert_inode_locked(struct Model0_inode *);



static inline __attribute__((no_instrument_function)) void Model0_lockdep_annotate_inode_mutex_key(struct Model0_inode *Model0_inode) { };

extern void Model0_unlock_new_inode(struct Model0_inode *);
extern unsigned int Model0_get_next_ino(void);

extern void Model0___iget(struct Model0_inode * Model0_inode);
extern void Model0_iget_failed(struct Model0_inode *);
extern void Model0_clear_inode(struct Model0_inode *);
extern void Model0___destroy_inode(struct Model0_inode *);
extern struct Model0_inode *Model0_new_inode_pseudo(struct Model0_super_block *Model0_sb);
extern struct Model0_inode *Model0_new_inode(struct Model0_super_block *Model0_sb);
extern void Model0_free_inode_nonrcu(struct Model0_inode *Model0_inode);
extern int Model0_should_remove_suid(struct Model0_dentry *);
extern int Model0_file_remove_privs(struct Model0_file *);

extern void Model0___insert_inode_hash(struct Model0_inode *, unsigned long Model0_hashval);
static inline __attribute__((no_instrument_function)) void Model0_insert_inode_hash(struct Model0_inode *Model0_inode)
{
 Model0___insert_inode_hash(Model0_inode, Model0_inode->Model0_i_ino);
}

extern void Model0___remove_inode_hash(struct Model0_inode *);
static inline __attribute__((no_instrument_function)) void Model0_remove_inode_hash(struct Model0_inode *Model0_inode)
{
 if (!Model0_inode_unhashed(Model0_inode) && !Model0_hlist_fake(&Model0_inode->Model0_i_hash))
  Model0___remove_inode_hash(Model0_inode);
}

extern void Model0_inode_sb_list_add(struct Model0_inode *Model0_inode);


extern Model0_blk_qc_t Model0_submit_bio(struct Model0_bio *);
extern int Model0_bdev_read_only(struct Model0_block_device *);

extern int Model0_set_blocksize(struct Model0_block_device *, int);
extern int Model0_sb_set_blocksize(struct Model0_super_block *, int);
extern int Model0_sb_min_blocksize(struct Model0_super_block *, int);

extern int Model0_generic_file_mmap(struct Model0_file *, struct Model0_vm_area_struct *);
extern int Model0_generic_file_readonly_mmap(struct Model0_file *, struct Model0_vm_area_struct *);
extern Model0_ssize_t Model0_generic_write_checks(struct Model0_kiocb *, struct Model0_iov_iter *);
extern Model0_ssize_t Model0_generic_file_read_iter(struct Model0_kiocb *, struct Model0_iov_iter *);
extern Model0_ssize_t Model0___generic_file_write_iter(struct Model0_kiocb *, struct Model0_iov_iter *);
extern Model0_ssize_t Model0_generic_file_write_iter(struct Model0_kiocb *, struct Model0_iov_iter *);
extern Model0_ssize_t Model0_generic_file_direct_write(struct Model0_kiocb *, struct Model0_iov_iter *);
extern Model0_ssize_t Model0_generic_perform_write(struct Model0_file *, struct Model0_iov_iter *, Model0_loff_t);

Model0_ssize_t Model0_vfs_iter_read(struct Model0_file *Model0_file, struct Model0_iov_iter *Model0_iter, Model0_loff_t *Model0_ppos);
Model0_ssize_t Model0_vfs_iter_write(struct Model0_file *Model0_file, struct Model0_iov_iter *Model0_iter, Model0_loff_t *Model0_ppos);

/* fs/block_dev.c */
extern Model0_ssize_t Model0_blkdev_read_iter(struct Model0_kiocb *Model0_iocb, struct Model0_iov_iter *Model0_to);
extern Model0_ssize_t Model0_blkdev_write_iter(struct Model0_kiocb *Model0_iocb, struct Model0_iov_iter *Model0_from);
extern int Model0_blkdev_fsync(struct Model0_file *Model0_filp, Model0_loff_t Model0_start, Model0_loff_t Model0_end,
   int Model0_datasync);
extern void Model0_block_sync_page(struct Model0_page *Model0_page);

/* fs/splice.c */
extern Model0_ssize_t Model0_generic_file_splice_read(struct Model0_file *, Model0_loff_t *,
  struct Model0_pipe_inode_info *, Model0_size_t, unsigned int);
extern Model0_ssize_t Model0_default_file_splice_read(struct Model0_file *, Model0_loff_t *,
  struct Model0_pipe_inode_info *, Model0_size_t, unsigned int);
extern Model0_ssize_t Model0_iter_file_splice_write(struct Model0_pipe_inode_info *,
  struct Model0_file *, Model0_loff_t *, Model0_size_t, unsigned int);
extern Model0_ssize_t Model0_generic_splice_sendpage(struct Model0_pipe_inode_info *Model0_pipe,
  struct Model0_file *Model0_out, Model0_loff_t *, Model0_size_t Model0_len, unsigned int Model0_flags);
extern long Model0_do_splice_direct(struct Model0_file *Model0_in, Model0_loff_t *Model0_ppos, struct Model0_file *Model0_out,
  Model0_loff_t *Model0_opos, Model0_size_t Model0_len, unsigned int Model0_flags);


extern void
Model0_file_ra_state_init(struct Model0_file_ra_state *Model0_ra, struct Model0_address_space *Model0_mapping);
extern Model0_loff_t Model0_noop_llseek(struct Model0_file *Model0_file, Model0_loff_t Model0_offset, int Model0_whence);
extern Model0_loff_t Model0_no_llseek(struct Model0_file *Model0_file, Model0_loff_t Model0_offset, int Model0_whence);
extern Model0_loff_t Model0_vfs_setpos(struct Model0_file *Model0_file, Model0_loff_t Model0_offset, Model0_loff_t Model0_maxsize);
extern Model0_loff_t Model0_generic_file_llseek(struct Model0_file *Model0_file, Model0_loff_t Model0_offset, int Model0_whence);
extern Model0_loff_t Model0_generic_file_llseek_size(struct Model0_file *Model0_file, Model0_loff_t Model0_offset,
  int Model0_whence, Model0_loff_t Model0_maxsize, Model0_loff_t Model0_eof);
extern Model0_loff_t Model0_fixed_size_llseek(struct Model0_file *Model0_file, Model0_loff_t Model0_offset,
  int Model0_whence, Model0_loff_t Model0_size);
extern Model0_loff_t Model0_no_seek_end_llseek_size(struct Model0_file *, Model0_loff_t, int, Model0_loff_t);
extern Model0_loff_t Model0_no_seek_end_llseek(struct Model0_file *, Model0_loff_t, int);
extern int Model0_generic_file_open(struct Model0_inode * Model0_inode, struct Model0_file * Model0_filp);
extern int Model0_nonseekable_open(struct Model0_inode * Model0_inode, struct Model0_file * Model0_filp);


typedef void (Model0_dio_submit_t)(struct Model0_bio *Model0_bio, struct Model0_inode *Model0_inode,
       Model0_loff_t Model0_file_offset);

enum {
 /* need locking between buffered and direct access */
 Model0_DIO_LOCKING = 0x01,

 /* filesystem does not support filling holes */
 Model0_DIO_SKIP_HOLES = 0x02,

 /* filesystem can handle aio writes beyond i_size */
 Model0_DIO_ASYNC_EXTEND = 0x04,

 /* inode/fs/bdev does not need truncate protection */
 Model0_DIO_SKIP_DIO_COUNT = 0x08,
};

void Model0_dio_end_io(struct Model0_bio *Model0_bio, int error);

Model0_ssize_t Model0___blockdev_direct_IO(struct Model0_kiocb *Model0_iocb, struct Model0_inode *Model0_inode,
        struct Model0_block_device *Model0_bdev, struct Model0_iov_iter *Model0_iter,
        Model0_get_block_t Model0_get_block,
        Model0_dio_iodone_t Model0_end_io, Model0_dio_submit_t Model0_submit_io,
        int Model0_flags);

static inline __attribute__((no_instrument_function)) Model0_ssize_t Model0_blockdev_direct_IO(struct Model0_kiocb *Model0_iocb,
      struct Model0_inode *Model0_inode,
      struct Model0_iov_iter *Model0_iter,
      Model0_get_block_t Model0_get_block)
{
 return Model0___blockdev_direct_IO(Model0_iocb, Model0_inode, Model0_inode->Model0_i_sb->Model0_s_bdev, Model0_iter,
   Model0_get_block, ((void *)0), ((void *)0), Model0_DIO_LOCKING | Model0_DIO_SKIP_HOLES);
}


void Model0_inode_dio_wait(struct Model0_inode *Model0_inode);

/*
 * inode_dio_begin - signal start of a direct I/O requests
 * @inode: inode the direct I/O happens on
 *
 * This is called once we've finished processing a direct I/O request,
 * and is used to wake up callers waiting for direct I/O to be quiesced.
 */
static inline __attribute__((no_instrument_function)) void Model0_inode_dio_begin(struct Model0_inode *Model0_inode)
{
 Model0_atomic_inc(&Model0_inode->Model0_i_dio_count);
}

/*
 * inode_dio_end - signal finish of a direct I/O requests
 * @inode: inode the direct I/O happens on
 *
 * This is called once we've finished processing a direct I/O request,
 * and is used to wake up callers waiting for direct I/O to be quiesced.
 */
static inline __attribute__((no_instrument_function)) void Model0_inode_dio_end(struct Model0_inode *Model0_inode)
{
 if (Model0_atomic_dec_and_test(&Model0_inode->Model0_i_dio_count))
  Model0_wake_up_bit(&Model0_inode->Model0_i_state, 9);
}

extern void Model0_inode_set_flags(struct Model0_inode *Model0_inode, unsigned int Model0_flags,
       unsigned int Model0_mask);

extern const struct Model0_file_operations Model0_generic_ro_fops;



extern int Model0_readlink_copy(char *, int, const char *);
extern int Model0_page_readlink(struct Model0_dentry *, char *, int);
extern const char *Model0_page_get_link(struct Model0_dentry *, struct Model0_inode *,
     struct Model0_delayed_call *);
extern void Model0_page_put_link(void *);
extern int Model0___page_symlink(struct Model0_inode *Model0_inode, const char *Model0_symname, int Model0_len,
  int Model0_nofs);
extern int Model0_page_symlink(struct Model0_inode *Model0_inode, const char *Model0_symname, int Model0_len);
extern const struct Model0_inode_operations Model0_page_symlink_inode_operations;
extern void Model0_kfree_link(void *);
extern int Model0_generic_readlink(struct Model0_dentry *, char *, int);
extern void Model0_generic_fillattr(struct Model0_inode *, struct Model0_kstat *);
int Model0_vfs_getattr_nosec(struct Model0_path *Model0_path, struct Model0_kstat *Model0_stat);
extern int Model0_vfs_getattr(struct Model0_path *, struct Model0_kstat *);
void Model0___inode_add_bytes(struct Model0_inode *Model0_inode, Model0_loff_t Model0_bytes);
void Model0_inode_add_bytes(struct Model0_inode *Model0_inode, Model0_loff_t Model0_bytes);
void Model0___inode_sub_bytes(struct Model0_inode *Model0_inode, Model0_loff_t Model0_bytes);
void Model0_inode_sub_bytes(struct Model0_inode *Model0_inode, Model0_loff_t Model0_bytes);
Model0_loff_t Model0_inode_get_bytes(struct Model0_inode *Model0_inode);
void Model0_inode_set_bytes(struct Model0_inode *Model0_inode, Model0_loff_t Model0_bytes);
const char *Model0_simple_get_link(struct Model0_dentry *, struct Model0_inode *,
       struct Model0_delayed_call *);
extern const struct Model0_inode_operations Model0_simple_symlink_inode_operations;

extern int Model0_iterate_dir(struct Model0_file *, struct Model0_dir_context *);

extern int Model0_vfs_stat(const char *, struct Model0_kstat *);
extern int Model0_vfs_lstat(const char *, struct Model0_kstat *);
extern int Model0_vfs_fstat(unsigned int, struct Model0_kstat *);
extern int Model0_vfs_fstatat(int , const char *, struct Model0_kstat *, int);

extern int Model0___generic_block_fiemap(struct Model0_inode *Model0_inode,
      struct Model0_fiemap_extent_info *Model0_fieinfo,
      Model0_loff_t Model0_start, Model0_loff_t Model0_len,
      Model0_get_block_t *Model0_get_block);
extern int Model0_generic_block_fiemap(struct Model0_inode *Model0_inode,
    struct Model0_fiemap_extent_info *Model0_fieinfo, Model0_u64 Model0_start,
    Model0_u64 Model0_len, Model0_get_block_t *Model0_get_block);

extern void Model0_get_filesystem(struct Model0_file_system_type *Model0_fs);
extern void Model0_put_filesystem(struct Model0_file_system_type *Model0_fs);
extern struct Model0_file_system_type *Model0_get_fs_type(const char *Model0_name);
extern struct Model0_super_block *Model0_get_super(struct Model0_block_device *);
extern struct Model0_super_block *Model0_get_super_thawed(struct Model0_block_device *);
extern struct Model0_super_block *Model0_get_active_super(struct Model0_block_device *Model0_bdev);
extern void Model0_drop_super(struct Model0_super_block *Model0_sb);
extern void Model0_iterate_supers(void (*)(struct Model0_super_block *, void *), void *);
extern void Model0_iterate_supers_type(struct Model0_file_system_type *,
           void (*)(struct Model0_super_block *, void *), void *);

extern int Model0_dcache_dir_open(struct Model0_inode *, struct Model0_file *);
extern int Model0_dcache_dir_close(struct Model0_inode *, struct Model0_file *);
extern Model0_loff_t Model0_dcache_dir_lseek(struct Model0_file *, Model0_loff_t, int);
extern int Model0_dcache_readdir(struct Model0_file *, struct Model0_dir_context *);
extern int Model0_simple_setattr(struct Model0_dentry *, struct Model0_iattr *);
extern int Model0_simple_getattr(struct Model0_vfsmount *, struct Model0_dentry *, struct Model0_kstat *);
extern int Model0_simple_statfs(struct Model0_dentry *, struct Model0_kstatfs *);
extern int Model0_simple_open(struct Model0_inode *Model0_inode, struct Model0_file *Model0_file);
extern int Model0_simple_link(struct Model0_dentry *, struct Model0_inode *, struct Model0_dentry *);
extern int Model0_simple_unlink(struct Model0_inode *, struct Model0_dentry *);
extern int Model0_simple_rmdir(struct Model0_inode *, struct Model0_dentry *);
extern int Model0_simple_rename(struct Model0_inode *, struct Model0_dentry *, struct Model0_inode *, struct Model0_dentry *);
extern int Model0_noop_fsync(struct Model0_file *, Model0_loff_t, Model0_loff_t, int);
extern int Model0_simple_empty(struct Model0_dentry *);
extern int Model0_simple_readpage(struct Model0_file *Model0_file, struct Model0_page *Model0_page);
extern int Model0_simple_write_begin(struct Model0_file *Model0_file, struct Model0_address_space *Model0_mapping,
   Model0_loff_t Model0_pos, unsigned Model0_len, unsigned Model0_flags,
   struct Model0_page **Model0_pagep, void **Model0_fsdata);
extern int Model0_simple_write_end(struct Model0_file *Model0_file, struct Model0_address_space *Model0_mapping,
   Model0_loff_t Model0_pos, unsigned Model0_len, unsigned Model0_copied,
   struct Model0_page *Model0_page, void *Model0_fsdata);
extern int Model0_always_delete_dentry(const struct Model0_dentry *);
extern struct Model0_inode *Model0_alloc_anon_inode(struct Model0_super_block *);
extern int Model0_simple_nosetlease(struct Model0_file *, long, struct Model0_file_lock **, void **);
extern const struct Model0_dentry_operations Model0_simple_dentry_operations;

extern struct Model0_dentry *Model0_simple_lookup(struct Model0_inode *, struct Model0_dentry *, unsigned int Model0_flags);
extern Model0_ssize_t Model0_generic_read_dir(struct Model0_file *, char *, Model0_size_t, Model0_loff_t *);
extern const struct Model0_file_operations Model0_simple_dir_operations;
extern const struct Model0_inode_operations Model0_simple_dir_inode_operations;
extern void Model0_make_empty_dir_inode(struct Model0_inode *Model0_inode);
extern bool Model0_is_empty_dir_inode(struct Model0_inode *Model0_inode);
struct Model0_tree_descr { char *Model0_name; const struct Model0_file_operations *Model0_ops; int Model0_mode; };
struct Model0_dentry *Model0_d_alloc_name(struct Model0_dentry *, const char *);
extern int Model0_simple_fill_super(struct Model0_super_block *, unsigned long, struct Model0_tree_descr *);
extern int Model0_simple_pin_fs(struct Model0_file_system_type *, struct Model0_vfsmount **Model0_mount, int *Model0_count);
extern void Model0_simple_release_fs(struct Model0_vfsmount **Model0_mount, int *Model0_count);

extern Model0_ssize_t Model0_simple_read_from_buffer(void *Model0_to, Model0_size_t Model0_count,
   Model0_loff_t *Model0_ppos, const void *Model0_from, Model0_size_t Model0_available);
extern Model0_ssize_t Model0_simple_write_to_buffer(void *Model0_to, Model0_size_t Model0_available, Model0_loff_t *Model0_ppos,
  const void *Model0_from, Model0_size_t Model0_count);

extern int Model0___generic_file_fsync(struct Model0_file *, Model0_loff_t, Model0_loff_t, int);
extern int Model0_generic_file_fsync(struct Model0_file *, Model0_loff_t, Model0_loff_t, int);

extern int Model0_generic_check_addressable(unsigned, Model0_u64);


extern int Model0_buffer_migrate_page(struct Model0_address_space *,
    struct Model0_page *, struct Model0_page *,
    enum Model0_migrate_mode);




extern int Model0_inode_change_ok(const struct Model0_inode *, struct Model0_iattr *);
extern int Model0_inode_newsize_ok(const struct Model0_inode *, Model0_loff_t Model0_offset);
extern void Model0_setattr_copy(struct Model0_inode *Model0_inode, const struct Model0_iattr *Model0_attr);

extern int Model0_file_update_time(struct Model0_file *Model0_file);

extern int Model0_generic_show_options(struct Model0_seq_file *Model0_m, struct Model0_dentry *Model0_root);
extern void Model0_save_mount_options(struct Model0_super_block *Model0_sb, char *Model0_options);
extern void Model0_replace_mount_options(struct Model0_super_block *Model0_sb, char *Model0_options);

static inline __attribute__((no_instrument_function)) bool Model0_io_is_direct(struct Model0_file *Model0_filp)
{
 return (Model0_filp->Model0_f_flags & 00040000) || ((Model0_filp->Model0_f_mapping->Model0_host)->Model0_i_flags & 0);
}

static inline __attribute__((no_instrument_function)) int Model0_iocb_flags(struct Model0_file *Model0_file)
{
 int Model0_res = 0;
 if (Model0_file->Model0_f_flags & 00002000)
  Model0_res |= (1 << 1);
 if (Model0_io_is_direct(Model0_file))
  Model0_res |= (1 << 2);
 if ((Model0_file->Model0_f_flags & 00010000) || (((Model0_file->Model0_f_mapping->Model0_host)->Model0_i_sb->Model0_s_flags & (16)) || ((Model0_file->Model0_f_mapping->Model0_host)->Model0_i_flags & 1)))
  Model0_res |= (1 << 4);
 if (Model0_file->Model0_f_flags & 04000000)
  Model0_res |= (1 << 5);
 return Model0_res;
}

static inline __attribute__((no_instrument_function)) Model0_ino_t Model0_parent_ino(struct Model0_dentry *Model0_dentry)
{
 Model0_ino_t Model0_res;

 /*
	 * Don't strictly need d_lock here? If the parent ino could change
	 * then surely we'd have a deeper race in the caller?
	 */
 Model0_spin_lock(&Model0_dentry->Model0_d_lockref.Model0_lock);
 Model0_res = Model0_dentry->Model0_d_parent->Model0_d_inode->Model0_i_ino;
 Model0_spin_unlock(&Model0_dentry->Model0_d_lockref.Model0_lock);
 return Model0_res;
}

/* Transaction based IO helpers */

/*
 * An argresp is stored in an allocated page and holds the
 * size of the argument or response, along with its content
 */
struct Model0_simple_transaction_argresp {
 Model0_ssize_t Model0_size;
 char Model0_data[0];
};



char *Model0_simple_transaction_get(struct Model0_file *Model0_file, const char *Model0_buf,
    Model0_size_t Model0_size);
Model0_ssize_t Model0_simple_transaction_read(struct Model0_file *Model0_file, char *Model0_buf,
    Model0_size_t Model0_size, Model0_loff_t *Model0_pos);
int Model0_simple_transaction_release(struct Model0_inode *Model0_inode, struct Model0_file *Model0_file);

void Model0_simple_transaction_set(struct Model0_file *Model0_file, Model0_size_t Model0_n);

/*
 * simple attribute files
 *
 * These attributes behave similar to those in sysfs:
 *
 * Writing to an attribute immediately sets a value, an open file can be
 * written to multiple times.
 *
 * Reading from an attribute creates a buffer from the value that might get
 * read with multiple read calls. When the attribute has been read
 * completely, no further read calls are possible until the file is opened
 * again.
 *
 * All attributes contain a text representation of a numeric value
 * that are accessed with the get() and set() functions.
 */
static inline __attribute__((no_instrument_function)) __attribute__((format(printf, 1, 2)))
void Model0___simple_attr_check_format(const char *Model0_fmt, ...)
{
 /* don't do anything, just let the compiler check the arguments; */
}

int Model0_simple_attr_open(struct Model0_inode *Model0_inode, struct Model0_file *Model0_file,
       int (*Model0_get)(void *, Model0_u64 *), int (*Model0_set)(void *, Model0_u64),
       const char *Model0_fmt);
int Model0_simple_attr_release(struct Model0_inode *Model0_inode, struct Model0_file *Model0_file);
Model0_ssize_t Model0_simple_attr_read(struct Model0_file *Model0_file, char *Model0_buf,
    Model0_size_t Model0_len, Model0_loff_t *Model0_ppos);
Model0_ssize_t Model0_simple_attr_write(struct Model0_file *Model0_file, const char *Model0_buf,
     Model0_size_t Model0_len, Model0_loff_t *Model0_ppos);

struct Model0_ctl_table;
int Model0_proc_nr_files(struct Model0_ctl_table *Model0_table, int Model0_write,
    void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);
int Model0_proc_nr_dentry(struct Model0_ctl_table *Model0_table, int Model0_write,
    void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);
int Model0_proc_nr_inodes(struct Model0_ctl_table *Model0_table, int Model0_write,
     void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);
int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_get_filesystem_list(char *Model0_buf);
static inline __attribute__((no_instrument_function)) bool Model0_is_sxid(Model0_umode_t Model0_mode)
{
 return (Model0_mode & 0004000) || ((Model0_mode & 0002000) && (Model0_mode & 00010));
}

static inline __attribute__((no_instrument_function)) int Model0_check_sticky(struct Model0_inode *Model0_dir, struct Model0_inode *Model0_inode)
{
 if (!(Model0_dir->Model0_i_mode & 0001000))
  return 0;

 return Model0___check_sticky(Model0_dir, Model0_inode);
}

static inline __attribute__((no_instrument_function)) void Model0_inode_has_no_xattr(struct Model0_inode *Model0_inode)
{
 if (!Model0_is_sxid(Model0_inode->Model0_i_mode) && (Model0_inode->Model0_i_sb->Model0_s_flags & (1<<28)))
  Model0_inode->Model0_i_flags |= 4096;
}

static inline __attribute__((no_instrument_function)) bool Model0_is_root_inode(struct Model0_inode *Model0_inode)
{
 return Model0_inode == Model0_inode->Model0_i_sb->Model0_s_root->Model0_d_inode;
}

static inline __attribute__((no_instrument_function)) bool Model0_dir_emit(struct Model0_dir_context *Model0_ctx,
       const char *Model0_name, int Model0_namelen,
       Model0_u64 Model0_ino, unsigned Model0_type)
{
 return Model0_ctx->Model0_actor(Model0_ctx, Model0_name, Model0_namelen, Model0_ctx->Model0_pos, Model0_ino, Model0_type) == 0;
}
static inline __attribute__((no_instrument_function)) bool Model0_dir_emit_dot(struct Model0_file *Model0_file, struct Model0_dir_context *Model0_ctx)
{
 return Model0_ctx->Model0_actor(Model0_ctx, ".", 1, Model0_ctx->Model0_pos,
     Model0_file->Model0_f_path.Model0_dentry->Model0_d_inode->Model0_i_ino, 4) == 0;
}
static inline __attribute__((no_instrument_function)) bool Model0_dir_emit_dotdot(struct Model0_file *Model0_file, struct Model0_dir_context *Model0_ctx)
{
 return Model0_ctx->Model0_actor(Model0_ctx, "..", 2, Model0_ctx->Model0_pos,
     Model0_parent_ino(Model0_file->Model0_f_path.Model0_dentry), 4) == 0;
}
static inline __attribute__((no_instrument_function)) bool Model0_dir_emit_dots(struct Model0_file *Model0_file, struct Model0_dir_context *Model0_ctx)
{
 if (Model0_ctx->Model0_pos == 0) {
  if (!Model0_dir_emit_dot(Model0_file, Model0_ctx))
   return false;
  Model0_ctx->Model0_pos = 1;
 }
 if (Model0_ctx->Model0_pos == 1) {
  if (!Model0_dir_emit_dotdot(Model0_file, Model0_ctx))
   return false;
  Model0_ctx->Model0_pos = 2;
 }
 return true;
}
static inline __attribute__((no_instrument_function)) bool Model0_dir_relax(struct Model0_inode *Model0_inode)
{
 Model0_inode_unlock(Model0_inode);
 Model0_inode_lock(Model0_inode);
 return !((Model0_inode)->Model0_i_flags & 16);
}

static inline __attribute__((no_instrument_function)) bool Model0_dir_relax_shared(struct Model0_inode *Model0_inode)
{
 Model0_inode_unlock_shared(Model0_inode);
 Model0_inode_lock_shared(Model0_inode);
 return !((Model0_inode)->Model0_i_flags & 16);
}

extern bool Model0_path_noexec(const struct Model0_path *Model0_path);
extern void Model0_inode_nohighmem(struct Model0_inode *Model0_inode);
/* include/linux/aio_abi.h
 *
 * Copyright 2000,2001,2002 Red Hat.
 *
 * Written by Benjamin LaHaise <bcrl@kvack.org>
 *
 * Distribute under the terms of the GPLv2 (see ../../COPYING) or under 
 * the following terms.
 *
 * Permission to use, copy, modify, and distribute this software and its
 * documentation is hereby granted, provided that the above copyright
 * notice appears in all copies.  This software is provided without any
 * warranty, express or implied.  Red Hat makes no representations about
 * the suitability of this software for any purpose.
 *
 * IN NO EVENT SHALL RED HAT BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT,
 * SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF
 * THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF RED HAT HAS BEEN ADVISED
 * OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * RED HAT DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS ON AN "AS IS" BASIS, AND
 * RED HAT HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES,
 * ENHANCEMENTS, OR MODIFICATIONS.
 */






typedef Model0___kernel_ulong_t Model0_aio_context_t;

enum {
 Model0_IOCB_CMD_PREAD = 0,
 Model0_IOCB_CMD_PWRITE = 1,
 Model0_IOCB_CMD_FSYNC = 2,
 Model0_IOCB_CMD_FDSYNC = 3,
 /* These two are experimental.
	 * IOCB_CMD_PREADX = 4,
	 * IOCB_CMD_POLL = 5,
	 */
 Model0_IOCB_CMD_NOOP = 6,
 Model0_IOCB_CMD_PREADV = 7,
 Model0_IOCB_CMD_PWRITEV = 8,
};

/*
 * Valid flags for the "aio_flags" member of the "struct iocb".
 *
 * IOCB_FLAG_RESFD - Set if the "aio_resfd" member of the "struct iocb"
 *                   is valid.
 */


/* read() from /dev/aio returns these structures. */
struct Model0_io_event {
 __u64 Model0_data; /* the data field from the iocb */
 __u64 Model0_obj; /* what iocb this event came from */
 Model0___s64 Model0_res; /* result code for this event */
 Model0___s64 Model0_res2; /* secondary result */
};
/*
 * we always use a 64bit off_t when communicating
 * with userland.  its up to libraries to do the
 * proper padding and aio_error abstraction
 */

struct Model0_iocb {
 /* these are internal to the kernel/libc. */
 __u64 Model0_aio_data; /* data to be returned in event's data */
 __u32 Model0_aio_key, Model0_aio_reserved1;
    /* the kernel sets aio_key to the req # */

 /* common fields */
 Model0___u16 Model0_aio_lio_opcode; /* see IOCB_CMD_ above */
 Model0___s16 Model0_aio_reqprio;
 __u32 Model0_aio_fildes;

 __u64 Model0_aio_buf;
 __u64 Model0_aio_nbytes;
 Model0___s64 Model0_aio_offset;

 /* extra parameters */
 __u64 Model0_aio_reserved2; /* TODO: use this for a (struct sigevent *) */

 /* flags for the "struct iocb" */
 __u32 Model0_aio_flags;

 /*
	 * if the IOCB_FLAG_RESFD flag of "aio_flags" is set, this is an
	 * eventfd to signal AIO readiness to
	 */
 __u32 Model0_aio_resfd;
}; /* 64 bytes */





/*
 * Architecture specific compatibility types
 */







/* IA32 compatible user structures for ptrace.
 * These should be used for 32bit coredumps too. */

struct Model0_user_i387_ia32_struct {
 Model0_u32 Model0_cwd;
 Model0_u32 Model0_swd;
 Model0_u32 Model0_twd;
 Model0_u32 Model0_fip;
 Model0_u32 Model0_fcs;
 Model0_u32 Model0_foo;
 Model0_u32 Model0_fos;
 Model0_u32 Model0_st_space[20]; /* 8*10 bytes for each FP-reg = 80 bytes */
};

/* FSAVE frame with extensions */
struct Model0_user32_fxsr_struct {
 unsigned short Model0_cwd;
 unsigned short Model0_swd;
 unsigned short Model0_twd; /* not compatible to 64bit twd */
 unsigned short Model0_fop;
 int Model0_fip;
 int Model0_fcs;
 int Model0_foo;
 int Model0_fos;
 int Model0_mxcsr;
 int Model0_reserved;
 int Model0_st_space[32]; /* 8*16 bytes for each FP-reg = 128 bytes */
 int Model0_xmm_space[32]; /* 8*16 bytes for each XMM-reg = 128 bytes */
 int Model0_padding[56];
};

struct Model0_user_regs_struct32 {
 __u32 Model0_ebx, Model0_ecx, Model0_edx, Model0_esi, Model0_edi, Model0_ebp, Model0_eax;
 unsigned short Model0_ds, Model0___ds, Model0_es, Model0___es;
 unsigned short Model0_fs, Model0___fs, Model0_gs, Model0___gs;
 __u32 Model0_orig_eax, Model0_eip;
 unsigned short Model0_cs, Model0___cs;
 __u32 Model0_eflags, Model0_esp;
 unsigned short Model0_ss, Model0___ss;
};

struct Model0_user32 {
  struct Model0_user_regs_struct32 Model0_regs; /* Where the registers are actually stored */
  int Model0_u_fpvalid; /* True if math co-processor being used. */
    /* for this mess. Not yet used. */
  struct Model0_user_i387_ia32_struct Model0_i387; /* Math Co-processor registers. */
/* The rest of this junk is to help gdb figure out what goes where */
  __u32 Model0_u_tsize; /* Text segment size (pages). */
  __u32 Model0_u_dsize; /* Data segment size (pages). */
  __u32 Model0_u_ssize; /* Stack segment size (pages). */
  __u32 Model0_start_code; /* Starting virtual address of text. */
  __u32 Model0_start_stack; /* Starting virtual address of stack area.
				   This is actually the bottom of the stack,
				   the top of the stack is always found in the
				   esp register.  */
  __u32 Model0_signal; /* Signal that caused the core dump. */
  int Model0_reserved; /* No __u32er used */
  __u32 Model0_u_ar0; /* Used by gdb to help find the values for */
    /* the registers. */
  __u32 Model0_u_fpstate; /* Math Co-processor pointer. */
  __u32 Model0_magic; /* To uniquely identify a core file */
  char Model0_u_comm[32]; /* User command that was responsible */
  int Model0_u_debugreg[8];
};





typedef Model0_u32 Model0_compat_size_t;
typedef Model0_s32 Model0_compat_ssize_t;
typedef Model0_s32 Model0_compat_time_t;
typedef Model0_s32 Model0_compat_clock_t;
typedef Model0_s32 Model0_compat_pid_t;
typedef Model0_u16 Model0___compat_uid_t;
typedef Model0_u16 Model0___compat_gid_t;
typedef Model0_u32 Model0___compat_uid32_t;
typedef Model0_u32 Model0___compat_gid32_t;
typedef Model0_u16 Model0_compat_mode_t;
typedef Model0_u32 Model0_compat_ino_t;
typedef Model0_u16 Model0_compat_dev_t;
typedef Model0_s32 Model0_compat_off_t;
typedef Model0_s64 Model0_compat_loff_t;
typedef Model0_u16 Model0_compat_nlink_t;
typedef Model0_u16 Model0_compat_ipc_pid_t;
typedef Model0_s32 Model0_compat_daddr_t;
typedef Model0_u32 Model0_compat_caddr_t;
typedef Model0___kernel_fsid_t Model0_compat_fsid_t;
typedef Model0_s32 Model0_compat_timer_t;
typedef Model0_s32 Model0_compat_key_t;

typedef Model0_s32 Model0_compat_int_t;
typedef Model0_s32 Model0_compat_long_t;
typedef Model0_s64 __attribute__((aligned(4))) Model0_compat_s64;
typedef Model0_u32 Model0_compat_uint_t;
typedef Model0_u32 Model0_compat_ulong_t;
typedef Model0_u32 Model0_compat_u32;
typedef Model0_u64 __attribute__((aligned(4))) Model0_compat_u64;
typedef Model0_u32 Model0_compat_uptr_t;

struct Model0_compat_timespec {
 Model0_compat_time_t Model0_tv_sec;
 Model0_s32 Model0_tv_nsec;
};

struct Model0_compat_timeval {
 Model0_compat_time_t Model0_tv_sec;
 Model0_s32 Model0_tv_usec;
};

struct Model0_compat_stat {
 Model0_compat_dev_t Model0_st_dev;
 Model0_u16 Model0___pad1;
 Model0_compat_ino_t Model0_st_ino;
 Model0_compat_mode_t Model0_st_mode;
 Model0_compat_nlink_t Model0_st_nlink;
 Model0___compat_uid_t Model0_st_uid;
 Model0___compat_gid_t Model0_st_gid;
 Model0_compat_dev_t Model0_st_rdev;
 Model0_u16 Model0___pad2;
 Model0_u32 Model0_st_size;
 Model0_u32 Model0_st_blksize;
 Model0_u32 Model0_st_blocks;
 Model0_u32 Model0_st_atime;
 Model0_u32 Model0_st_atime_nsec;
 Model0_u32 Model0_st_mtime;
 Model0_u32 Model0_st_mtime_nsec;
 Model0_u32 Model0_st_ctime;
 Model0_u32 Model0_st_ctime_nsec;
 Model0_u32 Model0___unused4;
 Model0_u32 Model0___unused5;
};

struct Model0_compat_flock {
 short Model0_l_type;
 short Model0_l_whence;
 Model0_compat_off_t Model0_l_start;
 Model0_compat_off_t Model0_l_len;
 Model0_compat_pid_t Model0_l_pid;
};





/*
 * IA32 uses 4 byte alignment for 64 bit quantities,
 * so we need to pack this structure.
 */
struct Model0_compat_flock64 {
 short Model0_l_type;
 short Model0_l_whence;
 Model0_compat_loff_t Model0_l_start;
 Model0_compat_loff_t Model0_l_len;
 Model0_compat_pid_t Model0_l_pid;
} __attribute__((packed));

struct Model0_compat_statfs {
 int Model0_f_type;
 int Model0_f_bsize;
 int Model0_f_blocks;
 int Model0_f_bfree;
 int Model0_f_bavail;
 int Model0_f_files;
 int Model0_f_ffree;
 Model0_compat_fsid_t Model0_f_fsid;
 int Model0_f_namelen; /* SunOS ignores this field. */
 int Model0_f_frsize;
 int Model0_f_flags;
 int Model0_f_spare[4];
};




typedef Model0_u32 Model0_compat_old_sigset_t; /* at least 32 bits */




typedef Model0_u32 Model0_compat_sigset_word;

typedef union Model0_compat_sigval {
 Model0_compat_int_t Model0_sival_int;
 Model0_compat_uptr_t Model0_sival_ptr;
} Model0_compat_sigval_t;

typedef struct Model0_compat_siginfo {
 int Model0_si_signo;
 int Model0_si_errno;
 int Model0_si_code;

 union {
  int Model0__pad[128/sizeof(int) - 3];

  /* kill() */
  struct {
   unsigned int Model0__pid; /* sender's pid */
   unsigned int Model0__uid; /* sender's uid */
  } Model0__kill;

  /* POSIX.1b timers */
  struct {
   Model0_compat_timer_t Model0__tid; /* timer id */
   int Model0__overrun; /* overrun count */
   Model0_compat_sigval_t Model0__sigval; /* same as below */
   int Model0__sys_private; /* not to be passed to user */
   int Model0__overrun_incr; /* amount to add to overrun */
  } Model0__timer;

  /* POSIX.1b signals */
  struct {
   unsigned int Model0__pid; /* sender's pid */
   unsigned int Model0__uid; /* sender's uid */
   Model0_compat_sigval_t Model0__sigval;
  } Model0__rt;

  /* SIGCHLD */
  struct {
   unsigned int Model0__pid; /* which child */
   unsigned int Model0__uid; /* sender's uid */
   int Model0__status; /* exit code */
   Model0_compat_clock_t Model0__utime;
   Model0_compat_clock_t Model0__stime;
  } Model0__sigchld;

  /* SIGCHLD (x32 version) */
  struct {
   unsigned int Model0__pid; /* which child */
   unsigned int Model0__uid; /* sender's uid */
   int Model0__status; /* exit code */
   Model0_compat_s64 Model0__utime;
   Model0_compat_s64 Model0__stime;
  } Model0__sigchld_x32;

  /* SIGILL, SIGFPE, SIGSEGV, SIGBUS */
  struct {
   unsigned int Model0__addr; /* faulting insn/memory ref. */
   short int Model0__addr_lsb; /* Valid LSB of the reported address. */
   union {
    /* used when si_code=SEGV_BNDERR */
    struct {
     Model0_compat_uptr_t Model0__lower;
     Model0_compat_uptr_t Model0__upper;
    } Model0__addr_bnd;
    /* used when si_code=SEGV_PKUERR */
    Model0_compat_u32 Model0__pkey;
   };
  } Model0__sigfault;

  /* SIGPOLL */
  struct {
   int Model0__band; /* POLL_IN, POLL_OUT, POLL_MSG */
   int Model0__fd;
  } Model0__sigpoll;

  struct {
   unsigned int Model0__call_addr; /* calling insn */
   int Model0__syscall; /* triggering system call number */
   unsigned int Model0__arch; /* AUDIT_ARCH_* of syscall */
  } Model0__sigsys;
 } Model0__sifields;
} Model0_compat_siginfo_t;




struct Model0_compat_ipc64_perm {
 Model0_compat_key_t Model0_key;
 Model0___compat_uid32_t Model0_uid;
 Model0___compat_gid32_t Model0_gid;
 Model0___compat_uid32_t Model0_cuid;
 Model0___compat_gid32_t Model0_cgid;
 unsigned short Model0_mode;
 unsigned short Model0___pad1;
 unsigned short Model0_seq;
 unsigned short Model0___pad2;
 Model0_compat_ulong_t Model0_unused1;
 Model0_compat_ulong_t Model0_unused2;
};

struct Model0_compat_semid64_ds {
 struct Model0_compat_ipc64_perm Model0_sem_perm;
 Model0_compat_time_t Model0_sem_otime;
 Model0_compat_ulong_t Model0___unused1;
 Model0_compat_time_t Model0_sem_ctime;
 Model0_compat_ulong_t Model0___unused2;
 Model0_compat_ulong_t Model0_sem_nsems;
 Model0_compat_ulong_t Model0___unused3;
 Model0_compat_ulong_t Model0___unused4;
};

struct Model0_compat_msqid64_ds {
 struct Model0_compat_ipc64_perm Model0_msg_perm;
 Model0_compat_time_t Model0_msg_stime;
 Model0_compat_ulong_t Model0___unused1;
 Model0_compat_time_t Model0_msg_rtime;
 Model0_compat_ulong_t Model0___unused2;
 Model0_compat_time_t Model0_msg_ctime;
 Model0_compat_ulong_t Model0___unused3;
 Model0_compat_ulong_t Model0_msg_cbytes;
 Model0_compat_ulong_t Model0_msg_qnum;
 Model0_compat_ulong_t Model0_msg_qbytes;
 Model0_compat_pid_t Model0_msg_lspid;
 Model0_compat_pid_t Model0_msg_lrpid;
 Model0_compat_ulong_t Model0___unused4;
 Model0_compat_ulong_t Model0___unused5;
};

struct Model0_compat_shmid64_ds {
 struct Model0_compat_ipc64_perm Model0_shm_perm;
 Model0_compat_size_t Model0_shm_segsz;
 Model0_compat_time_t Model0_shm_atime;
 Model0_compat_ulong_t Model0___unused1;
 Model0_compat_time_t Model0_shm_dtime;
 Model0_compat_ulong_t Model0___unused2;
 Model0_compat_time_t Model0_shm_ctime;
 Model0_compat_ulong_t Model0___unused3;
 Model0_compat_pid_t Model0_shm_cpid;
 Model0_compat_pid_t Model0_shm_lpid;
 Model0_compat_ulong_t Model0_shm_nattch;
 Model0_compat_ulong_t Model0___unused4;
 Model0_compat_ulong_t Model0___unused5;
};

/*
 * The type of struct elf_prstatus.pr_reg in compatible core dumps.
 */
typedef struct Model0_user_regs_struct32 Model0_compat_elf_gregset_t;


/*
 * A pointer passed in from user mode. This should not
 * be used for syscall parameters, just declare them
 * as pointers because the syscall entry code will have
 * appropriately converted them already.
 */

static inline __attribute__((no_instrument_function)) void *Model0_compat_ptr(Model0_compat_uptr_t Model0_uptr)
{
 return (void *)(unsigned long)Model0_uptr;
}

static inline __attribute__((no_instrument_function)) Model0_compat_uptr_t Model0_ptr_to_compat(void *Model0_uptr)
{
 return (Model0_u32)(unsigned long)Model0_uptr;
}

static inline __attribute__((no_instrument_function)) void *Model0_arch_compat_alloc_user_space(long Model0_len)
{
 Model0_compat_uptr_t Model0_sp;

 if (Model0_test_ti_thread_flag(Model0_current_thread_info(), 17)) {
  Model0_sp = ((struct Model0_pt_regs *)(Model0_get_current())->thread.Model0_sp0 - 1)->Model0_sp;
 } else {
  /* -128 for the x32 ABI redzone */
  Model0_sp = ((struct Model0_pt_regs *)(Model0_get_current())->thread.Model0_sp0 - 1)->Model0_sp - 128;
 }

 return (void *)((Model0_sp - Model0_len) & ~((__typeof__(Model0_sp - Model0_len))((16)-1)));
}

static inline __attribute__((no_instrument_function)) bool Model0_in_x32_syscall(void)
{




 return false;
}

static inline __attribute__((no_instrument_function)) bool Model0_in_compat_syscall(void)
{
 return Model0_in_ia32_syscall() || Model0_in_x32_syscall();
}
typedef struct Model0_compat_sigaltstack {
 Model0_compat_uptr_t Model0_ss_sp;
 int Model0_ss_flags;
 Model0_compat_size_t Model0_ss_size;
} Model0_compat_stack_t;





typedef Model0___compat_uid32_t Model0_compat_uid_t;
typedef Model0___compat_gid32_t Model0_compat_gid_t;

typedef Model0_compat_ulong_t Model0_compat_aio_context_t;

struct Model0_compat_sel_arg_struct;
struct Model0_rusage;

struct Model0_compat_itimerspec {
 struct Model0_compat_timespec Model0_it_interval;
 struct Model0_compat_timespec Model0_it_value;
};

struct Model0_compat_utimbuf {
 Model0_compat_time_t Model0_actime;
 Model0_compat_time_t Model0_modtime;
};

struct Model0_compat_itimerval {
 struct Model0_compat_timeval Model0_it_interval;
 struct Model0_compat_timeval Model0_it_value;
};

struct Model0_compat_tms {
 Model0_compat_clock_t Model0_tms_utime;
 Model0_compat_clock_t Model0_tms_stime;
 Model0_compat_clock_t Model0_tms_cutime;
 Model0_compat_clock_t Model0_tms_cstime;
};

struct Model0_compat_timex {
 Model0_compat_uint_t Model0_modes;
 Model0_compat_long_t Model0_offset;
 Model0_compat_long_t Model0_freq;
 Model0_compat_long_t Model0_maxerror;
 Model0_compat_long_t Model0_esterror;
 Model0_compat_int_t Model0_status;
 Model0_compat_long_t Model0_constant;
 Model0_compat_long_t Model0_precision;
 Model0_compat_long_t Model0_tolerance;
 struct Model0_compat_timeval Model0_time;
 Model0_compat_long_t Model0_tick;
 Model0_compat_long_t Model0_ppsfreq;
 Model0_compat_long_t Model0_jitter;
 Model0_compat_int_t Model0_shift;
 Model0_compat_long_t Model0_stabil;
 Model0_compat_long_t Model0_jitcnt;
 Model0_compat_long_t Model0_calcnt;
 Model0_compat_long_t Model0_errcnt;
 Model0_compat_long_t Model0_stbcnt;
 Model0_compat_int_t Model0_tai;

 Model0_compat_int_t:32; Model0_compat_int_t:32; Model0_compat_int_t:32; Model0_compat_int_t:32;
 Model0_compat_int_t:32; Model0_compat_int_t:32; Model0_compat_int_t:32; Model0_compat_int_t:32;
 Model0_compat_int_t:32; Model0_compat_int_t:32; Model0_compat_int_t:32;
};



typedef struct {
 Model0_compat_sigset_word Model0_sig[(64 / 32)];
} Model0_compat_sigset_t;

struct Model0_compat_sigaction {

 Model0_compat_uptr_t Model0_sa_handler;
 Model0_compat_ulong_t Model0_sa_flags;





 Model0_compat_uptr_t Model0_sa_restorer;

 Model0_compat_sigset_t Model0_sa_mask __attribute__((packed));
};

/*
 * These functions operate on 32- or 64-bit specs depending on
 * COMPAT_USE_64BIT_TIME, hence the void user pointer arguments.
 */
extern int Model0_compat_get_timespec(struct Model0_timespec *, const void *);
extern int Model0_compat_put_timespec(const struct Model0_timespec *, void *);
extern int Model0_compat_get_timeval(struct Model0_timeval *, const void *);
extern int Model0_compat_put_timeval(const struct Model0_timeval *, void *);

/*
 * This function convert a timespec if necessary and returns a *user
 * space* pointer.  If no conversion is necessary, it returns the
 * initial pointer.  NULL is a legitimate argument and will always
 * output NULL.
 */
extern int Model0_compat_convert_timespec(struct Model0_timespec **,
       const void *);

struct Model0_compat_iovec {
 Model0_compat_uptr_t Model0_iov_base;
 Model0_compat_size_t Model0_iov_len;
};

struct Model0_compat_rlimit {
 Model0_compat_ulong_t Model0_rlim_cur;
 Model0_compat_ulong_t Model0_rlim_max;
};

struct Model0_compat_rusage {
 struct Model0_compat_timeval Model0_ru_utime;
 struct Model0_compat_timeval Model0_ru_stime;
 Model0_compat_long_t Model0_ru_maxrss;
 Model0_compat_long_t Model0_ru_ixrss;
 Model0_compat_long_t Model0_ru_idrss;
 Model0_compat_long_t Model0_ru_isrss;
 Model0_compat_long_t Model0_ru_minflt;
 Model0_compat_long_t Model0_ru_majflt;
 Model0_compat_long_t Model0_ru_nswap;
 Model0_compat_long_t Model0_ru_inblock;
 Model0_compat_long_t Model0_ru_oublock;
 Model0_compat_long_t Model0_ru_msgsnd;
 Model0_compat_long_t Model0_ru_msgrcv;
 Model0_compat_long_t Model0_ru_nsignals;
 Model0_compat_long_t Model0_ru_nvcsw;
 Model0_compat_long_t Model0_ru_nivcsw;
};

extern int Model0_put_compat_rusage(const struct Model0_rusage *,
        struct Model0_compat_rusage *);

struct Model0_compat_siginfo;

extern long Model0_compat_sys_waitid(int, Model0_compat_pid_t,
  struct Model0_compat_siginfo *, int,
  struct Model0_compat_rusage *);

struct Model0_compat_dirent {
 Model0_u32 Model0_d_ino;
 Model0_compat_off_t Model0_d_off;
 Model0_u16 Model0_d_reclen;
 char Model0_d_name[256];
};

struct Model0_compat_ustat {
 Model0_compat_daddr_t Model0_f_tfree;
 Model0_compat_ino_t Model0_f_tinode;
 char Model0_f_fname[6];
 char Model0_f_fpack[6];
};



typedef struct Model0_compat_sigevent {
 Model0_compat_sigval_t Model0_sigev_value;
 Model0_compat_int_t Model0_sigev_signo;
 Model0_compat_int_t Model0_sigev_notify;
 union {
  Model0_compat_int_t Model0__pad[((64/sizeof(int)) - 3)];
  Model0_compat_int_t Model0__tid;

  struct {
   Model0_compat_uptr_t Model0__function;
   Model0_compat_uptr_t Model0__attribute;
  } Model0__sigev_thread;
 } Model0__sigev_un;
} Model0_compat_sigevent_t;

struct Model0_compat_ifmap {
 Model0_compat_ulong_t Model0_mem_start;
 Model0_compat_ulong_t Model0_mem_end;
 unsigned short Model0_base_addr;
 unsigned char Model0_irq;
 unsigned char Model0_dma;
 unsigned char Model0_port;
};

struct Model0_compat_if_settings {
 unsigned int Model0_type; /* Type of physical device or protocol */
 unsigned int Model0_size; /* Size of the data allocated by the caller */
 Model0_compat_uptr_t Model0_ifs_ifsu; /* union of pointers */
};

struct Model0_compat_ifreq {
 union {
  char Model0_ifrn_name[16]; /* if name, e.g. "en0" */
 } Model0_ifr_ifrn;
 union {
  struct Model0_sockaddr Model0_ifru_addr;
  struct Model0_sockaddr Model0_ifru_dstaddr;
  struct Model0_sockaddr Model0_ifru_broadaddr;
  struct Model0_sockaddr Model0_ifru_netmask;
  struct Model0_sockaddr Model0_ifru_hwaddr;
  short Model0_ifru_flags;
  Model0_compat_int_t Model0_ifru_ivalue;
  Model0_compat_int_t Model0_ifru_mtu;
  struct Model0_compat_ifmap Model0_ifru_map;
  char Model0_ifru_slave[16]; /* Just fits the size */
  char Model0_ifru_newname[16];
  Model0_compat_caddr_t Model0_ifru_data;
  struct Model0_compat_if_settings Model0_ifru_settings;
 } Model0_ifr_ifru;
};

struct Model0_compat_ifconf {
 Model0_compat_int_t Model0_ifc_len; /* size of buffer */
 Model0_compat_caddr_t Model0_ifcbuf;
};

struct Model0_compat_robust_list {
 Model0_compat_uptr_t Model0_next;
};

struct Model0_compat_robust_list_head {
 struct Model0_compat_robust_list Model0_list;
 Model0_compat_long_t Model0_futex_offset;
 Model0_compat_uptr_t Model0_list_op_pending;
};


struct Model0_compat_old_sigaction {
 Model0_compat_uptr_t Model0_sa_handler;
 Model0_compat_old_sigset_t Model0_sa_mask;
 Model0_compat_ulong_t Model0_sa_flags;
 Model0_compat_uptr_t Model0_sa_restorer;
};


struct Model0_compat_statfs;
struct Model0_compat_statfs64;
struct Model0_compat_old_linux_dirent;
struct Model0_compat_linux_dirent;
struct Model0_linux_dirent64;
struct Model0_compat_msghdr;
struct Model0_compat_mmsghdr;
struct Model0_compat_sysinfo;
struct Model0_compat_sysctl_args;
struct Model0_compat_kexec_segment;
struct Model0_compat_mq_attr;
struct Model0_compat_msgbuf;

extern void Model0_compat_exit_robust_list(struct Model0_task_struct *Model0_curr);

           long
Model0_compat_sys_set_robust_list(struct Model0_compat_robust_list_head *Model0_head,
      Model0_compat_size_t Model0_len);
           long
Model0_compat_sys_get_robust_list(int Model0_pid, Model0_compat_uptr_t *Model0_head_ptr,
      Model0_compat_size_t *Model0_len_ptr);

           long Model0_compat_sys_ipc(Model0_u32, int, int, Model0_u32, Model0_compat_uptr_t, Model0_u32);
           long Model0_compat_sys_shmat(int Model0_shmid, Model0_compat_uptr_t Model0_shmaddr, int Model0_shmflg);
           long Model0_compat_sys_semctl(int Model0_semid, int Model0_semnum, int Model0_cmd, int Model0_arg);
           long Model0_compat_sys_msgsnd(int Model0_msqid, Model0_compat_uptr_t Model0_msgp,
  Model0_compat_ssize_t Model0_msgsz, int Model0_msgflg);
           long Model0_compat_sys_msgrcv(int Model0_msqid, Model0_compat_uptr_t Model0_msgp,
  Model0_compat_ssize_t Model0_msgsz, Model0_compat_long_t Model0_msgtyp, int Model0_msgflg);
long Model0_compat_sys_msgctl(int Model0_first, int Model0_second, void *Model0_uptr);
long Model0_compat_sys_shmctl(int Model0_first, int Model0_second, void *Model0_uptr);
long Model0_compat_sys_semtimedop(int Model0_semid, struct Model0_sembuf *Model0_tsems,
  unsigned Model0_nsems, const struct Model0_compat_timespec *Model0_timeout);
           long Model0_compat_sys_keyctl(Model0_u32 Model0_option,
         Model0_u32 Model0_arg2, Model0_u32 Model0_arg3, Model0_u32 Model0_arg4, Model0_u32 Model0_arg5);
           long Model0_compat_sys_ustat(unsigned Model0_dev, struct Model0_compat_ustat *Model0_u32);

           Model0_ssize_t Model0_compat_sys_readv(Model0_compat_ulong_t Model0_fd,
  const struct Model0_compat_iovec *Model0_vec, Model0_compat_ulong_t Model0_vlen);
           Model0_ssize_t Model0_compat_sys_writev(Model0_compat_ulong_t Model0_fd,
  const struct Model0_compat_iovec *Model0_vec, Model0_compat_ulong_t Model0_vlen);
           Model0_ssize_t Model0_compat_sys_preadv(Model0_compat_ulong_t Model0_fd,
  const struct Model0_compat_iovec *Model0_vec,
  Model0_compat_ulong_t Model0_vlen, Model0_u32 Model0_pos_low, Model0_u32 Model0_pos_high);
           Model0_ssize_t Model0_compat_sys_pwritev(Model0_compat_ulong_t Model0_fd,
  const struct Model0_compat_iovec *Model0_vec,
  Model0_compat_ulong_t Model0_vlen, Model0_u32 Model0_pos_low, Model0_u32 Model0_pos_high);
           Model0_ssize_t Model0_compat_sys_preadv2(Model0_compat_ulong_t Model0_fd,
  const struct Model0_compat_iovec *Model0_vec,
  Model0_compat_ulong_t Model0_vlen, Model0_u32 Model0_pos_low, Model0_u32 Model0_pos_high, int Model0_flags);
           Model0_ssize_t Model0_compat_sys_pwritev2(Model0_compat_ulong_t Model0_fd,
  const struct Model0_compat_iovec *Model0_vec,
  Model0_compat_ulong_t Model0_vlen, Model0_u32 Model0_pos_low, Model0_u32 Model0_pos_high, int Model0_flags);


           long Model0_compat_sys_preadv64(unsigned long Model0_fd,
  const struct Model0_compat_iovec *Model0_vec,
  unsigned long Model0_vlen, Model0_loff_t Model0_pos);



           long Model0_compat_sys_pwritev64(unsigned long Model0_fd,
  const struct Model0_compat_iovec *Model0_vec,
  unsigned long Model0_vlen, Model0_loff_t Model0_pos);


           long Model0_compat_sys_lseek(unsigned int, Model0_compat_off_t, unsigned int);

           long Model0_compat_sys_execve(const char *Model0_filename, const Model0_compat_uptr_t *Model0_argv,
       const Model0_compat_uptr_t *Model0_envp);
           long Model0_compat_sys_execveat(int Model0_dfd, const char *Model0_filename,
       const Model0_compat_uptr_t *Model0_argv,
       const Model0_compat_uptr_t *Model0_envp, int Model0_flags);

           long Model0_compat_sys_select(int Model0_n, Model0_compat_ulong_t *Model0_inp,
  Model0_compat_ulong_t *Model0_outp, Model0_compat_ulong_t *Model0_exp,
  struct Model0_compat_timeval *Model0_tvp);

           long Model0_compat_sys_old_select(struct Model0_compat_sel_arg_struct *Model0_arg);

           long Model0_compat_sys_wait4(Model0_compat_pid_t Model0_pid,
     Model0_compat_uint_t *Model0_stat_addr, int Model0_options,
     struct Model0_compat_rusage *Model0_ru);






long Model0_compat_get_bitmap(unsigned long *Model0_mask, const Model0_compat_ulong_t *Model0_umask,
         unsigned long Model0_bitmap_size);
long Model0_compat_put_bitmap(Model0_compat_ulong_t *Model0_umask, unsigned long *Model0_mask,
         unsigned long Model0_bitmap_size);
int Model0_copy_siginfo_from_user32(Model0_siginfo_t *Model0_to, struct Model0_compat_siginfo *Model0_from);
int Model0_copy_siginfo_to_user32(struct Model0_compat_siginfo *Model0_to, const Model0_siginfo_t *Model0_from);
int Model0_get_compat_sigevent(struct Model0_sigevent *Model0_event,
  const struct Model0_compat_sigevent *Model0_u_event);
long Model0_compat_sys_rt_tgsigqueueinfo(Model0_compat_pid_t Model0_tgid, Model0_compat_pid_t Model0_pid, int Model0_sig,
      struct Model0_compat_siginfo *Model0_uinfo);

           long Model0_compat_sys_sigaction(int Model0_sig,
                                   const struct Model0_compat_old_sigaction *Model0_act,
                                   struct Model0_compat_old_sigaction *Model0_oact);


static inline __attribute__((no_instrument_function)) int Model0_compat_timeval_compare(struct Model0_compat_timeval *Model0_lhs,
     struct Model0_compat_timeval *Model0_rhs)
{
 if (Model0_lhs->Model0_tv_sec < Model0_rhs->Model0_tv_sec)
  return -1;
 if (Model0_lhs->Model0_tv_sec > Model0_rhs->Model0_tv_sec)
  return 1;
 return Model0_lhs->Model0_tv_usec - Model0_rhs->Model0_tv_usec;
}

static inline __attribute__((no_instrument_function)) int Model0_compat_timespec_compare(struct Model0_compat_timespec *Model0_lhs,
     struct Model0_compat_timespec *Model0_rhs)
{
 if (Model0_lhs->Model0_tv_sec < Model0_rhs->Model0_tv_sec)
  return -1;
 if (Model0_lhs->Model0_tv_sec > Model0_rhs->Model0_tv_sec)
  return 1;
 return Model0_lhs->Model0_tv_nsec - Model0_rhs->Model0_tv_nsec;
}

extern int Model0_get_compat_itimerspec(struct Model0_itimerspec *Model0_dst,
     const struct Model0_compat_itimerspec *Model0_src);
extern int Model0_put_compat_itimerspec(struct Model0_compat_itimerspec *Model0_dst,
     const struct Model0_itimerspec *Model0_src);

           long Model0_compat_sys_gettimeofday(struct Model0_compat_timeval *Model0_tv,
  struct Model0_timezone *Model0_tz);
           long Model0_compat_sys_settimeofday(struct Model0_compat_timeval *Model0_tv,
  struct Model0_timezone *Model0_tz);

           long Model0_compat_sys_adjtimex(struct Model0_compat_timex *Model0_utp);

extern __attribute__((format(printf, 1, 2))) int Model0_compat_printk(const char *Model0_fmt, ...);
extern void Model0_sigset_from_compat(Model0_sigset_t *Model0_set, const Model0_compat_sigset_t *Model0_compat);
extern void Model0_sigset_to_compat(Model0_compat_sigset_t *Model0_compat, const Model0_sigset_t *Model0_set);

           long Model0_compat_sys_migrate_pages(Model0_compat_pid_t Model0_pid,
  Model0_compat_ulong_t Model0_maxnode, const Model0_compat_ulong_t *Model0_old_nodes,
  const Model0_compat_ulong_t *Model0_new_nodes);

extern int Model0_compat_ptrace_request(struct Model0_task_struct *Model0_child,
     Model0_compat_long_t Model0_request,
     Model0_compat_ulong_t Model0_addr, Model0_compat_ulong_t Model0_data);

extern long Model0_compat_arch_ptrace(struct Model0_task_struct *Model0_child, Model0_compat_long_t Model0_request,
          Model0_compat_ulong_t Model0_addr, Model0_compat_ulong_t Model0_data);
           long Model0_compat_sys_ptrace(Model0_compat_long_t Model0_request, Model0_compat_long_t Model0_pid,
      Model0_compat_long_t Model0_addr, Model0_compat_long_t Model0_data);

           long Model0_compat_sys_lookup_dcookie(Model0_u32, Model0_u32, char *, Model0_compat_size_t);
/*
 * epoll (fs/eventpoll.c) compat bits follow ...
 */
struct Model0_epoll_event; /* fortunately, this one is fixed-layout */
           long Model0_compat_sys_epoll_pwait(int Model0_epfd,
   struct Model0_epoll_event *Model0_events,
   int Model0_maxevents, int Model0_timeout,
   const Model0_compat_sigset_t *Model0_sigmask,
   Model0_compat_size_t Model0_sigsetsize);

           long Model0_compat_sys_utime(const char *Model0_filename,
     struct Model0_compat_utimbuf *Model0_t);
           long Model0_compat_sys_utimensat(unsigned int Model0_dfd,
         const char *Model0_filename,
         struct Model0_compat_timespec *Model0_t,
         int Model0_flags);

           long Model0_compat_sys_time(Model0_compat_time_t *Model0_tloc);
           long Model0_compat_sys_stime(Model0_compat_time_t *Model0_tptr);
           long Model0_compat_sys_signalfd(int Model0_ufd,
        const Model0_compat_sigset_t *Model0_sigmask,
        Model0_compat_size_t Model0_sigsetsize);
           long Model0_compat_sys_timerfd_settime(int Model0_ufd, int Model0_flags,
       const struct Model0_compat_itimerspec *Model0_utmr,
       struct Model0_compat_itimerspec *Model0_otmr);
           long Model0_compat_sys_timerfd_gettime(int Model0_ufd,
       struct Model0_compat_itimerspec *Model0_otmr);

           long Model0_compat_sys_move_pages(Model0_pid_t Model0_pid, Model0_compat_ulong_t Model0_nr_pages,
          __u32 *Model0_pages,
          const int *Model0_nodes,
          int *Model0_status,
          int Model0_flags);
           long Model0_compat_sys_futimesat(unsigned int Model0_dfd,
         const char *Model0_filename,
         struct Model0_compat_timeval *Model0_t);
           long Model0_compat_sys_utimes(const char *Model0_filename,
      struct Model0_compat_timeval *Model0_t);
           long Model0_compat_sys_newstat(const char *Model0_filename,
       struct Model0_compat_stat *Model0_statbuf);
           long Model0_compat_sys_newlstat(const char *Model0_filename,
        struct Model0_compat_stat *Model0_statbuf);
           long Model0_compat_sys_newfstatat(unsigned int Model0_dfd,
          const char *Model0_filename,
          struct Model0_compat_stat *Model0_statbuf,
          int Model0_flag);
           long Model0_compat_sys_newfstat(unsigned int Model0_fd,
        struct Model0_compat_stat *Model0_statbuf);
           long Model0_compat_sys_statfs(const char *Model0_pathname,
      struct Model0_compat_statfs *Model0_buf);
           long Model0_compat_sys_fstatfs(unsigned int Model0_fd,
       struct Model0_compat_statfs *Model0_buf);
           long Model0_compat_sys_statfs64(const char *Model0_pathname,
        Model0_compat_size_t Model0_sz,
        struct Model0_compat_statfs64 *Model0_buf);
           long Model0_compat_sys_fstatfs64(unsigned int Model0_fd, Model0_compat_size_t Model0_sz,
         struct Model0_compat_statfs64 *Model0_buf);
           long Model0_compat_sys_fcntl64(unsigned int Model0_fd, unsigned int Model0_cmd,
       Model0_compat_ulong_t Model0_arg);
           long Model0_compat_sys_fcntl(unsigned int Model0_fd, unsigned int Model0_cmd,
     Model0_compat_ulong_t Model0_arg);
           long Model0_compat_sys_io_setup(unsigned Model0_nr_reqs, Model0_u32 *Model0_ctx32p);
           long Model0_compat_sys_io_getevents(Model0_compat_aio_context_t Model0_ctx_id,
     Model0_compat_long_t Model0_min_nr,
     Model0_compat_long_t Model0_nr,
     struct Model0_io_event *Model0_events,
     struct Model0_compat_timespec *Model0_timeout);
           long Model0_compat_sys_io_submit(Model0_compat_aio_context_t Model0_ctx_id, int Model0_nr,
         Model0_u32 *Model0_iocb);
           long Model0_compat_sys_mount(const char *Model0_dev_name,
     const char *Model0_dir_name,
     const char *Model0_type, Model0_compat_ulong_t Model0_flags,
     const void *Model0_data);
           long Model0_compat_sys_old_readdir(unsigned int Model0_fd,
           struct Model0_compat_old_linux_dirent *,
           unsigned int Model0_count);
           long Model0_compat_sys_getdents(unsigned int Model0_fd,
        struct Model0_compat_linux_dirent *Model0_dirent,
        unsigned int Model0_count);

           long Model0_compat_sys_getdents64(unsigned int Model0_fd,
          struct Model0_linux_dirent64 *Model0_dirent,
          unsigned int Model0_count);

           long Model0_compat_sys_vmsplice(int Model0_fd, const struct Model0_compat_iovec *,
        unsigned int Model0_nr_segs, unsigned int Model0_flags);
           long Model0_compat_sys_open(const char *Model0_filename, int Model0_flags,
    Model0_umode_t Model0_mode);
           long Model0_compat_sys_openat(int Model0_dfd, const char *Model0_filename,
      int Model0_flags, Model0_umode_t Model0_mode);
           long Model0_compat_sys_open_by_handle_at(int Model0_mountdirfd,
          struct Model0_file_handle *Model0_handle,
          int Model0_flags);
           long Model0_compat_sys_truncate(const char *, Model0_compat_off_t);
           long Model0_compat_sys_ftruncate(unsigned int, Model0_compat_ulong_t);
           long Model0_compat_sys_pselect6(int Model0_n, Model0_compat_ulong_t *Model0_inp,
        Model0_compat_ulong_t *Model0_outp,
        Model0_compat_ulong_t *Model0_exp,
        struct Model0_compat_timespec *Model0_tsp,
        void *Model0_sig);
           long Model0_compat_sys_ppoll(struct Model0_pollfd *Model0_ufds,
     unsigned int Model0_nfds,
     struct Model0_compat_timespec *Model0_tsp,
     const Model0_compat_sigset_t *Model0_sigmask,
     Model0_compat_size_t Model0_sigsetsize);
           long Model0_compat_sys_signalfd4(int Model0_ufd,
         const Model0_compat_sigset_t *Model0_sigmask,
         Model0_compat_size_t Model0_sigsetsize, int Model0_flags);
           long Model0_compat_sys_get_mempolicy(int *Model0_policy,
      Model0_compat_ulong_t *Model0_nmask,
      Model0_compat_ulong_t Model0_maxnode,
      Model0_compat_ulong_t Model0_addr,
      Model0_compat_ulong_t Model0_flags);
           long Model0_compat_sys_set_mempolicy(int Model0_mode, Model0_compat_ulong_t *Model0_nmask,
      Model0_compat_ulong_t Model0_maxnode);
           long Model0_compat_sys_mbind(Model0_compat_ulong_t Model0_start, Model0_compat_ulong_t Model0_len,
     Model0_compat_ulong_t Model0_mode,
     Model0_compat_ulong_t *Model0_nmask,
     Model0_compat_ulong_t Model0_maxnode, Model0_compat_ulong_t Model0_flags);

           long Model0_compat_sys_setsockopt(int Model0_fd, int Model0_level, int Model0_optname,
          char *Model0_optval, unsigned int Model0_optlen);
           long Model0_compat_sys_sendmsg(int Model0_fd, struct Model0_compat_msghdr *Model0_msg,
       unsigned Model0_flags);
           long Model0_compat_sys_sendmmsg(int Model0_fd, struct Model0_compat_mmsghdr *Model0_mmsg,
        unsigned Model0_vlen, unsigned int Model0_flags);
           long Model0_compat_sys_recvmsg(int Model0_fd, struct Model0_compat_msghdr *Model0_msg,
       unsigned int Model0_flags);
           long Model0_compat_sys_recv(int Model0_fd, void *Model0_buf, Model0_compat_size_t Model0_len,
    unsigned Model0_flags);
           long Model0_compat_sys_recvfrom(int Model0_fd, void *Model0_buf, Model0_compat_size_t Model0_len,
       unsigned Model0_flags, struct Model0_sockaddr *Model0_addr,
       int *Model0_addrlen);
           long Model0_compat_sys_recvmmsg(int Model0_fd, struct Model0_compat_mmsghdr *Model0_mmsg,
        unsigned Model0_vlen, unsigned int Model0_flags,
        struct Model0_compat_timespec *Model0_timeout);
           long Model0_compat_sys_nanosleep(struct Model0_compat_timespec *Model0_rqtp,
         struct Model0_compat_timespec *Model0_rmtp);
           long Model0_compat_sys_getitimer(int Model0_which,
         struct Model0_compat_itimerval *Model0_it);
           long Model0_compat_sys_setitimer(int Model0_which,
         struct Model0_compat_itimerval *Model0_in,
         struct Model0_compat_itimerval *Model0_out);
           long Model0_compat_sys_times(struct Model0_compat_tms *Model0_tbuf);
           long Model0_compat_sys_setrlimit(unsigned int Model0_resource,
         struct Model0_compat_rlimit *Model0_rlim);
           long Model0_compat_sys_getrlimit(unsigned int Model0_resource,
         struct Model0_compat_rlimit *Model0_rlim);
           long Model0_compat_sys_getrusage(int Model0_who, struct Model0_compat_rusage *Model0_ru);
           long Model0_compat_sys_sched_setaffinity(Model0_compat_pid_t Model0_pid,
         unsigned int Model0_len,
         Model0_compat_ulong_t *Model0_user_mask_ptr);
           long Model0_compat_sys_sched_getaffinity(Model0_compat_pid_t Model0_pid,
         unsigned int Model0_len,
         Model0_compat_ulong_t *Model0_user_mask_ptr);
           long Model0_compat_sys_timer_create(Model0_clockid_t Model0_which_clock,
   struct Model0_compat_sigevent *Model0_timer_event_spec,
   Model0_timer_t *Model0_created_timer_id);
           long Model0_compat_sys_timer_settime(Model0_timer_t Model0_timer_id, int Model0_flags,
      struct Model0_compat_itimerspec *Model0_new,
      struct Model0_compat_itimerspec *old);
           long Model0_compat_sys_timer_gettime(Model0_timer_t Model0_timer_id,
     struct Model0_compat_itimerspec *Model0_setting);
           long Model0_compat_sys_clock_settime(Model0_clockid_t Model0_which_clock,
      struct Model0_compat_timespec *Model0_tp);
           long Model0_compat_sys_clock_gettime(Model0_clockid_t Model0_which_clock,
      struct Model0_compat_timespec *Model0_tp);
           long Model0_compat_sys_clock_adjtime(Model0_clockid_t Model0_which_clock,
      struct Model0_compat_timex *Model0_tp);
           long Model0_compat_sys_clock_getres(Model0_clockid_t Model0_which_clock,
     struct Model0_compat_timespec *Model0_tp);
           long Model0_compat_sys_clock_nanosleep(Model0_clockid_t Model0_which_clock, int Model0_flags,
        struct Model0_compat_timespec *Model0_rqtp,
        struct Model0_compat_timespec *Model0_rmtp);
           long Model0_compat_sys_rt_sigtimedwait(Model0_compat_sigset_t *Model0_uthese,
  struct Model0_compat_siginfo *Model0_uinfo,
  struct Model0_compat_timespec *Model0_uts, Model0_compat_size_t Model0_sigsetsize);
           long Model0_compat_sys_rt_sigsuspend(Model0_compat_sigset_t *Model0_unewset,
      Model0_compat_size_t Model0_sigsetsize);
           long Model0_compat_sys_rt_sigprocmask(int Model0_how, Model0_compat_sigset_t *Model0_set,
       Model0_compat_sigset_t *Model0_oset,
       Model0_compat_size_t Model0_sigsetsize);
           long Model0_compat_sys_rt_sigpending(Model0_compat_sigset_t *Model0_uset,
      Model0_compat_size_t Model0_sigsetsize);

           long Model0_compat_sys_rt_sigaction(int,
     const struct Model0_compat_sigaction *,
     struct Model0_compat_sigaction *,
     Model0_compat_size_t);

           long Model0_compat_sys_rt_sigqueueinfo(Model0_compat_pid_t Model0_pid, int Model0_sig,
    struct Model0_compat_siginfo *Model0_uinfo);
           long Model0_compat_sys_sysinfo(struct Model0_compat_sysinfo *Model0_info);
           long Model0_compat_sys_ioctl(unsigned int Model0_fd, unsigned int Model0_cmd,
     Model0_compat_ulong_t Model0_arg);
           long Model0_compat_sys_futex(Model0_u32 *Model0_uaddr, int Model0_op, Model0_u32 Model0_val,
  struct Model0_compat_timespec *Model0_utime, Model0_u32 *Model0_uaddr2,
  Model0_u32 Model0_val3);
           long Model0_compat_sys_getsockopt(int Model0_fd, int Model0_level, int Model0_optname,
          char *Model0_optval, int *Model0_optlen);
           long Model0_compat_sys_kexec_load(Model0_compat_ulong_t Model0_entry,
          Model0_compat_ulong_t Model0_nr_segments,
          struct Model0_compat_kexec_segment *,
          Model0_compat_ulong_t Model0_flags);
           long Model0_compat_sys_mq_getsetattr(Model0_mqd_t Model0_mqdes,
   const struct Model0_compat_mq_attr *Model0_u_mqstat,
   struct Model0_compat_mq_attr *Model0_u_omqstat);
           long Model0_compat_sys_mq_notify(Model0_mqd_t Model0_mqdes,
   const struct Model0_compat_sigevent *Model0_u_notification);
           long Model0_compat_sys_mq_open(const char *Model0_u_name,
   int Model0_oflag, Model0_compat_mode_t Model0_mode,
   struct Model0_compat_mq_attr *Model0_u_attr);
           long Model0_compat_sys_mq_timedsend(Model0_mqd_t Model0_mqdes,
   const char *Model0_u_msg_ptr,
   Model0_compat_size_t Model0_msg_len, unsigned int Model0_msg_prio,
   const struct Model0_compat_timespec *Model0_u_abs_timeout);
           Model0_ssize_t Model0_compat_sys_mq_timedreceive(Model0_mqd_t Model0_mqdes,
   char *Model0_u_msg_ptr,
   Model0_compat_size_t Model0_msg_len, unsigned int *Model0_u_msg_prio,
   const struct Model0_compat_timespec *Model0_u_abs_timeout);
           long Model0_compat_sys_socketcall(int Model0_call, Model0_u32 *Model0_args);
           long Model0_compat_sys_sysctl(struct Model0_compat_sysctl_args *Model0_args);

extern Model0_ssize_t Model0_compat_rw_copy_check_uvector(int Model0_type,
  const struct Model0_compat_iovec *Model0_uvector,
  unsigned long Model0_nr_segs,
  unsigned long Model0_fast_segs, struct Model0_iovec *Model0_fast_pointer,
  struct Model0_iovec **Model0_ret_pointer);

extern void *Model0_compat_alloc_user_space(unsigned long Model0_len);

           Model0_ssize_t Model0_compat_sys_process_vm_readv(Model0_compat_pid_t Model0_pid,
  const struct Model0_compat_iovec *Model0_lvec,
  Model0_compat_ulong_t Model0_liovcnt, const struct Model0_compat_iovec *Model0_rvec,
  Model0_compat_ulong_t Model0_riovcnt, Model0_compat_ulong_t Model0_flags);
           Model0_ssize_t Model0_compat_sys_process_vm_writev(Model0_compat_pid_t Model0_pid,
  const struct Model0_compat_iovec *Model0_lvec,
  Model0_compat_ulong_t Model0_liovcnt, const struct Model0_compat_iovec *Model0_rvec,
  Model0_compat_ulong_t Model0_riovcnt, Model0_compat_ulong_t Model0_flags);

           long Model0_compat_sys_sendfile(int Model0_out_fd, int Model0_in_fd,
        Model0_compat_off_t *Model0_offset, Model0_compat_size_t Model0_count);
           long Model0_compat_sys_sendfile64(int Model0_out_fd, int Model0_in_fd,
        Model0_compat_loff_t *Model0_offset, Model0_compat_size_t Model0_count);
           long Model0_compat_sys_sigaltstack(const Model0_compat_stack_t *Model0_uss_ptr,
           Model0_compat_stack_t *Model0_uoss_ptr);


           long Model0_compat_sys_sigpending(Model0_compat_old_sigset_t *Model0_set);



           long Model0_compat_sys_sigprocmask(int Model0_how, Model0_compat_old_sigset_t *Model0_nset,
           Model0_compat_old_sigset_t *Model0_oset);


int Model0_compat_restore_altstack(const Model0_compat_stack_t *Model0_uss);
int Model0___compat_save_altstack(Model0_compat_stack_t *, unsigned long);
           long Model0_compat_sys_sched_rr_get_interval(Model0_compat_pid_t Model0_pid,
       struct Model0_compat_timespec *Model0_interval);

           long Model0_compat_sys_fanotify_mark(int, unsigned int, __u32, __u32,
         int, const char *);

/*
 * For most but not all architectures, "am I in a compat syscall?" and
 * "am I a compat task?" are the same question.  For architectures on which
 * they aren't the same question, arch code can override in_compat_syscall.
 */
/*
 * ethtool.h: Defines for Linux ethtool.
 *
 * Copyright (C) 1998 David S. Miller (davem@redhat.com)
 * Copyright 2001 Jeff Garzik <jgarzik@pobox.com>
 * Portions Copyright 2001 Sun Microsystems (thockin@sun.com)
 * Portions Copyright 2002 Intel (eli.kupermann@intel.com,
 *                                christopher.leech@intel.com,
 *                                scott.feldman@intel.com)
 * Portions Copyright (C) Sun Microsystems 2008
 */







/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the Ethernet IEEE 802.3 interface.
 *
 * Version:	@(#)if_ether.h	1.0.1a	02/08/94
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Donald Becker, <becker@super.org>
 *		Alan Cox, <alan@lxorguk.ukuu.org.uk>
 *		Steve Whitehouse, <gw7rrm@eeshack3.swan.ac.uk>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */




/*
 *	Definitions for the 'struct sk_buff' memory handlers.
 *
 *	Authors:
 *		Alan Cox, <gw4pts@gw4pts.ampr.org>
 *		Florian La Roche, <rzsfl@rz.uni-sb.de>
 *
 *	This program is free software; you can redistribute it and/or
 *	modify it under the terms of the GNU General Public License
 *	as published by the Free Software Foundation; either version
 *	2 of the License, or (at your option) any later version.
 */






static inline __attribute__((no_instrument_function)) void
Model0_kmemcheck_alloc_shadow(struct Model0_page *Model0_page, int Model0_order, Model0_gfp_t Model0_flags, int Model0_node)
{
}

static inline __attribute__((no_instrument_function)) void
Model0_kmemcheck_free_shadow(struct Model0_page *Model0_page, int Model0_order)
{
}

static inline __attribute__((no_instrument_function)) void
Model0_kmemcheck_slab_alloc(struct Model0_kmem_cache *Model0_s, Model0_gfp_t Model0_gfpflags, void *Model0_object,
       Model0_size_t Model0_size)
{
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_slab_free(struct Model0_kmem_cache *Model0_s, void *Model0_object,
           Model0_size_t Model0_size)
{
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_pagealloc_alloc(struct Model0_page *Model0_p,
 unsigned int Model0_order, Model0_gfp_t Model0_gfpflags)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_kmemcheck_page_is_tracked(struct Model0_page *Model0_p)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_mark_unallocated(void *Model0_address, unsigned int Model0_n)
{
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_mark_uninitialized(void *Model0_address, unsigned int Model0_n)
{
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_mark_initialized(void *Model0_address, unsigned int Model0_n)
{
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_mark_freed(void *Model0_address, unsigned int Model0_n)
{
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_mark_unallocated_pages(struct Model0_page *Model0_p,
          unsigned int Model0_n)
{
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_mark_uninitialized_pages(struct Model0_page *Model0_p,
            unsigned int Model0_n)
{
}

static inline __attribute__((no_instrument_function)) void Model0_kmemcheck_mark_initialized_pages(struct Model0_page *Model0_p,
          unsigned int Model0_n)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_kmemcheck_is_obj_initialized(unsigned long Model0_addr, Model0_size_t Model0_size)
{
 return true;
}
/*
 * NET		An implementation of the SOCKET network access protocol.
 *		This is the master header file for the Linux NET layer,
 *		or, in plain English: the networking handling part of the
 *		kernel.
 *
 * Version:	@(#)net.h	1.0.3	05/25/93
 *
 * Authors:	Orest Zborowski, <obz@Kodak.COM>
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 * include/linux/random.h
 *
 * Include file for the random number generator.
 */











bool Model0___do_once_start(bool *Model0_done, unsigned long *Model0_flags);
void Model0___do_once_done(bool *Model0_done, struct Model0_static_key *Model0_once_key,
      unsigned long *Model0_flags);

/* Call a function exactly once. The idea of DO_ONCE() is to perform
 * a function call such as initialization of random seeds, etc, only
 * once, where DO_ONCE() can live in the fast-path. After @func has
 * been called with the passed arguments, the static key will patch
 * out the condition into a nop. DO_ONCE() guarantees type safety of
 * arguments!
 *
 * Not that the following is not equivalent ...
 *
 *   DO_ONCE(func, arg);
 *   DO_ONCE(func, arg);
 *
 * ... to this version:
 *
 *   void foo(void)
 *   {
 *     DO_ONCE(func, arg);
 *   }
 *
 *   foo();
 *   foo();
 *
 * In case the one-time invocation could be triggered from multiple
 * places, then a common helper function must be defined, so that only
 * a single static key will be placed there!
 */

/*
 * include/linux/random.h
 *
 * Include file for the random number generator.
 */










/*
 * There isn't anything here anymore, but the file must not be empty or patch
 * will delete it.
 */


extern int Model0_nr_irqs;
extern struct Model0_irq_desc *Model0_irq_to_desc(unsigned int Model0_irq);
unsigned int Model0_irq_get_next_irq(unsigned int Model0_offset);

/* ioctl()'s for the random number generator */

/* Get the entropy count. */


/* Add to (or subtract from) the entropy count.  (Superuser only.) */


/* Get the contents of the entropy pool.  (Superuser only.) */


/* 
 * Write bytes into the entropy pool and add to the entropy count.
 * (Superuser only.)
 */


/* Clear entropy count to 0.  (Superuser only.) */


/* Clear the entropy pool and associated counters.  (Superuser only.) */


struct Model0_rand_pool_info {
 int Model0_entropy_count;
 int Model0_buf_size;
 __u32 Model0_buf[0];
};

/*
 * Flags for getrandom(2)
 *
 * GRND_NONBLOCK	Don't block and return EAGAIN instead
 * GRND_RANDOM		Use the /dev/random pool instead of /dev/urandom
 */

struct Model0_random_ready_callback {
 struct Model0_list_head Model0_list;
 void (*func)(struct Model0_random_ready_callback *Model0_rdy);
 struct Model0_module *Model0_owner;
};

extern void Model0_add_device_randomness(const void *, unsigned int);
extern void Model0_add_input_randomness(unsigned int Model0_type, unsigned int Model0_code,
     unsigned int Model0_value);
extern void Model0_add_interrupt_randomness(int Model0_irq, int Model0_irq_flags);

extern void Model0_get_random_bytes(void *Model0_buf, int Model0_nbytes);
extern int Model0_add_random_ready_callback(struct Model0_random_ready_callback *Model0_rdy);
extern void Model0_del_random_ready_callback(struct Model0_random_ready_callback *Model0_rdy);
extern void Model0_get_random_bytes_arch(void *Model0_buf, int Model0_nbytes);
extern int Model0_random_int_secret_init(void);


extern const struct Model0_file_operations Model0_random_fops, Model0_urandom_fops;


unsigned int Model0_get_random_int(void);
unsigned long Model0_get_random_long(void);
unsigned long Model0_randomize_range(unsigned long Model0_start, unsigned long Model0_end, unsigned long Model0_len);

Model0_u32 Model0_prandom_u32(void);
void Model0_prandom_bytes(void *Model0_buf, Model0_size_t Model0_nbytes);
void Model0_prandom_seed(Model0_u32 Model0_seed);
void Model0_prandom_reseed_late(void);

struct Model0_rnd_state {
 __u32 Model0_s1, Model0_s2, Model0_s3, Model0_s4;
};

Model0_u32 Model0_prandom_u32_state(struct Model0_rnd_state *Model0_state);
void Model0_prandom_bytes_state(struct Model0_rnd_state *Model0_state, void *Model0_buf, Model0_size_t Model0_nbytes);
void Model0_prandom_seed_full_state(struct Model0_rnd_state *Model0_pcpu_state);




/**
 * prandom_u32_max - returns a pseudo-random number in interval [0, ep_ro)
 * @ep_ro: right open interval endpoint
 *
 * Returns a pseudo-random number that is in interval [0, ep_ro). Note
 * that the result depends on PRNG being well distributed in [0, ~0U]
 * u32 space. Here we use maximally equidistributed combined Tausworthe
 * generator, that is, prandom_u32(). This is useful when requesting a
 * random index of an array containing ep_ro elements, for example.
 *
 * Returns: pseudo-random number in interval [0, ep_ro)
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_prandom_u32_max(Model0_u32 Model0_ep_ro)
{
 return (Model0_u32)(((Model0_u64) Model0_prandom_u32() * Model0_ep_ro) >> 32);
}

/*
 * Handle minimum values for seeds
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0___seed(Model0_u32 Model0_x, Model0_u32 Model0_m)
{
 return (Model0_x < Model0_m) ? Model0_x + Model0_m : Model0_x;
}

/**
 * prandom_seed_state - set seed for prandom_u32_state().
 * @state: pointer to state structure to receive the seed.
 * @seed: arbitrary 64-bit value to use as a seed.
 */
static inline __attribute__((no_instrument_function)) void Model0_prandom_seed_state(struct Model0_rnd_state *Model0_state, Model0_u64 Model0_seed)
{
 Model0_u32 Model0_i = (Model0_seed >> 32) ^ (Model0_seed << 10) ^ Model0_seed;

 Model0_state->Model0_s1 = Model0___seed(Model0_i, 2U);
 Model0_state->Model0_s2 = Model0___seed(Model0_i, 8U);
 Model0_state->Model0_s3 = Model0___seed(Model0_i, 16U);
 Model0_state->Model0_s4 = Model0___seed(Model0_i, 128U);
}



/*
 * This file is part of the Linux kernel.
 *
 * Copyright (c) 2011-2014, Intel Corporation
 * Authors: Fenghua Yu <fenghua.yu@intel.com>,
 *          H. Peter Anvin <hpa@linux.intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms and conditions of the GNU General Public License,
 * version 2, as published by the Free Software Foundation.
 *
 * This program is distributed in the hope it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
 * more details.
 *
 * You should have received a copy of the GNU General Public License along with
 * this program; if not, write to the Free Software Foundation, Inc.,
 * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
 *
 */
/* Unconditional execution of RDRAND and RDSEED */

static inline __attribute__((no_instrument_function)) bool Model0_rdrand_long(unsigned long *Model0_v)
{
 bool Model0_ok;
 unsigned int Model0_retry = 10;
 do {
  asm volatile(".byte 0x48,0x0f,0xc7,0xf0" "\n\t"
        "\n\tset" "c" " %[_cc_" "c" "]\n"
        : [_cc_c] "=qm" (Model0_ok), "=a" (*Model0_v));
  if (Model0_ok)
   return true;
 } while (--Model0_retry);
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model0_rdrand_int(unsigned int *Model0_v)
{
 bool Model0_ok;
 unsigned int Model0_retry = 10;
 do {
  asm volatile(".byte 0x0f,0xc7,0xf0" "\n\t"
        "\n\tset" "c" " %[_cc_" "c" "]\n"
        : [_cc_c] "=qm" (Model0_ok), "=a" (*Model0_v));
  if (Model0_ok)
   return true;
 } while (--Model0_retry);
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model0_rdseed_long(unsigned long *Model0_v)
{
 bool Model0_ok;
 asm volatile(".byte 0x48,0x0f,0xc7,0xf8" "\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model0_ok), "=a" (*Model0_v));
 return Model0_ok;
}

static inline __attribute__((no_instrument_function)) bool Model0_rdseed_int(unsigned int *Model0_v)
{
 bool Model0_ok;
 asm volatile(".byte 0x0f,0xc7,0xf8" "\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model0_ok), "=a" (*Model0_v));
 return Model0_ok;
}

/* Conditional execution based on CPU type */



/*
 * These are the generic interfaces; they must not be declared if the
 * stubs in <linux/random.h> are to be invoked,
 * i.e. CONFIG_ARCH_RANDOM is not defined.
 */


static inline __attribute__((no_instrument_function)) bool Model0_arch_get_random_long(unsigned long *Model0_v)
{
 return (__builtin_constant_p(( 4*32+30)) && ( (((( 4*32+30))>>5)==(0) && (1UL<<((( 4*32+30))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 4*32+30))>>5)==(1) && (1UL<<((( 4*32+30))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 4*32+30))>>5)==(2) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(3) && (1UL<<((( 4*32+30))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 4*32+30))>>5)==(4) && (1UL<<((( 4*32+30))&31) & (0) )) || (((( 4*32+30))>>5)==(5) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(6) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(7) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(8) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(9) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(10) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(11) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(12) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(13) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(14) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(15) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(16) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(17) && (1UL<<((( 4*32+30))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 4*32+30))) ? Model0_constant_test_bit((( 4*32+30)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))) : Model0_variable_test_bit((( 4*32+30)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))))) ? Model0_rdrand_long(Model0_v) : false;
}

static inline __attribute__((no_instrument_function)) bool Model0_arch_get_random_int(unsigned int *Model0_v)
{
 return (__builtin_constant_p(( 4*32+30)) && ( (((( 4*32+30))>>5)==(0) && (1UL<<((( 4*32+30))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 4*32+30))>>5)==(1) && (1UL<<((( 4*32+30))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 4*32+30))>>5)==(2) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(3) && (1UL<<((( 4*32+30))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 4*32+30))>>5)==(4) && (1UL<<((( 4*32+30))&31) & (0) )) || (((( 4*32+30))>>5)==(5) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(6) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(7) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(8) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(9) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(10) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(11) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(12) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(13) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(14) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(15) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(16) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(17) && (1UL<<((( 4*32+30))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 4*32+30))) ? Model0_constant_test_bit((( 4*32+30)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))) : Model0_variable_test_bit((( 4*32+30)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))))) ? Model0_rdrand_int(Model0_v) : false;
}

static inline __attribute__((no_instrument_function)) bool Model0_arch_get_random_seed_long(unsigned long *Model0_v)
{
 return (__builtin_constant_p(( 9*32+18)) && ( (((( 9*32+18))>>5)==(0) && (1UL<<((( 9*32+18))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 9*32+18))>>5)==(1) && (1UL<<((( 9*32+18))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 9*32+18))>>5)==(2) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(3) && (1UL<<((( 9*32+18))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 9*32+18))>>5)==(4) && (1UL<<((( 9*32+18))&31) & (0) )) || (((( 9*32+18))>>5)==(5) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(6) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(7) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(8) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(9) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(10) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(11) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(12) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(13) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(14) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(15) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(16) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(17) && (1UL<<((( 9*32+18))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 9*32+18))) ? Model0_constant_test_bit((( 9*32+18)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))) : Model0_variable_test_bit((( 9*32+18)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))))) ? Model0_rdseed_long(Model0_v) : false;
}

static inline __attribute__((no_instrument_function)) bool Model0_arch_get_random_seed_int(unsigned int *Model0_v)
{
 return (__builtin_constant_p(( 9*32+18)) && ( (((( 9*32+18))>>5)==(0) && (1UL<<((( 9*32+18))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 9*32+18))>>5)==(1) && (1UL<<((( 9*32+18))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 9*32+18))>>5)==(2) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(3) && (1UL<<((( 9*32+18))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 9*32+18))>>5)==(4) && (1UL<<((( 9*32+18))&31) & (0) )) || (((( 9*32+18))>>5)==(5) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(6) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(7) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(8) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(9) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(10) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(11) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(12) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(13) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(14) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(15) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(16) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(17) && (1UL<<((( 9*32+18))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 9*32+18))) ? Model0_constant_test_bit((( 9*32+18)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))) : Model0_variable_test_bit((( 9*32+18)), ((unsigned long *)((&Model0_boot_cpu_data)->Model0_x86_capability))))) ? Model0_rdseed_int(Model0_v) : false;
}

extern void Model0_x86_init_rdrand(struct Model0_cpuinfo_x86 *Model0_c);
/* Pseudo random number generator from numerical recipes. */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_next_pseudo_random32(Model0_u32 Model0_seed)
{
 return Model0_seed * 1664525 + 1013904223;
}






/*
 * NET		An implementation of the SOCKET network access protocol.
 *		This is the master header file for the Linux NET layer,
 *		or, in plain English: the networking handling part of the
 *		kernel.
 *
 * Version:	@(#)net.h	1.0.3	05/25/93
 *
 * Authors:	Orest Zborowski, <obz@Kodak.COM>
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





typedef enum {
 Model0_SS_FREE = 0, /* not allocated		*/
 Model0_SS_UNCONNECTED, /* unconnected to any socket	*/
 Model0_SS_CONNECTING, /* in process of connecting	*/
 Model0_SS_CONNECTED, /* connected to socket		*/
 Model0_SS_DISCONNECTING /* in process of disconnecting	*/
} Model0_socket_state;

struct Model0_poll_table_struct;
struct Model0_pipe_inode_info;
struct Model0_inode;
struct Model0_file;
struct Model0_net;

/* Historically, SOCKWQ_ASYNC_NOSPACE & SOCKWQ_ASYNC_WAITDATA were located
 * in sock->flags, but moved into sk->sk_wq->flags to be RCU protected.
 * Eventually all flags will be in sk->sk_wq_flags.
 */







/**
 * enum sock_type - Socket types
 * @SOCK_STREAM: stream (connection) socket
 * @SOCK_DGRAM: datagram (conn.less) socket
 * @SOCK_RAW: raw socket
 * @SOCK_RDM: reliably-delivered message
 * @SOCK_SEQPACKET: sequential packet socket
 * @SOCK_DCCP: Datagram Congestion Control Protocol socket
 * @SOCK_PACKET: linux specific way of getting packets at the dev level.
 *		  For writing rarp and other similar things on the user level.
 *
 * When adding some new socket type please
 * grep ARCH_HAS_SOCKET_TYPE include/asm-* /socket.h, at least MIPS
 * overrides this enum for binary compat reasons.
 */
enum Model0_sock_type {
 Model0_SOCK_STREAM = 1,
 Model0_SOCK_DGRAM = 2,
 Model0_SOCK_RAW = 3,
 Model0_SOCK_RDM = 4,
 Model0_SOCK_SEQPACKET = 5,
 Model0_SOCK_DCCP = 6,
 Model0_SOCK_PACKET = 10,
};


/* Mask which covers at least up to SOCK_MASK-1.  The
 * remaining bits are used as flags. */


/* Flags for socket, socketpair, accept4 */







enum Model0_sock_shutdown_cmd {
 Model0_SHUT_RD,
 Model0_SHUT_WR,
 Model0_SHUT_RDWR,
};

struct Model0_socket_wq {
 /* Note: wait MUST be first field of socket_wq */
 Model0_wait_queue_head_t Model0_wait;
 struct Model0_fasync_struct *Model0_fasync_list;
 unsigned long Model0_flags; /* %SOCKWQ_ASYNC_NOSPACE, etc */
 struct Model0_callback_head Model0_rcu;
} __attribute__((__aligned__((1 << (6)))));

/**
 *  struct socket - general BSD socket
 *  @state: socket state (%SS_CONNECTED, etc)
 *  @type: socket type (%SOCK_STREAM, etc)
 *  @flags: socket flags (%SOCK_NOSPACE, etc)
 *  @ops: protocol specific socket operations
 *  @file: File back pointer for gc
 *  @sk: internal networking protocol agnostic socket representation
 *  @wq: wait queue for several uses
 */
struct Model0_socket {
 Model0_socket_state Model0_state;

                               ;
 short Model0_type;
                             ;

 unsigned long Model0_flags;

 struct Model0_socket_wq *Model0_wq;

 struct Model0_file *Model0_file;
 struct Model0_sock *Model0_sk;
 const struct Model0_proto_ops *Model0_ops;
};

struct Model0_vm_area_struct;
struct Model0_page;
struct Model0_sockaddr;
struct Model0_msghdr;
struct Model0_module;

struct Model0_proto_ops {
 int Model0_family;
 struct Model0_module *Model0_owner;
 int (*Model0_release) (struct Model0_socket *Model0_sock);
 int (*Model0_bind) (struct Model0_socket *Model0_sock,
          struct Model0_sockaddr *Model0_myaddr,
          int Model0_sockaddr_len);
 int (*Model0_connect) (struct Model0_socket *Model0_sock,
          struct Model0_sockaddr *Model0_vaddr,
          int Model0_sockaddr_len, int Model0_flags);
 int (*Model0_socketpair)(struct Model0_socket *Model0_sock1,
          struct Model0_socket *Model0_sock2);
 int (*Model0_accept) (struct Model0_socket *Model0_sock,
          struct Model0_socket *Model0_newsock, int Model0_flags);
 int (*Model0_getname) (struct Model0_socket *Model0_sock,
          struct Model0_sockaddr *Model0_addr,
          int *Model0_sockaddr_len, int Model0_peer);
 unsigned int (*Model0_poll) (struct Model0_file *Model0_file, struct Model0_socket *Model0_sock,
          struct Model0_poll_table_struct *Model0_wait);
 int (*Model0_ioctl) (struct Model0_socket *Model0_sock, unsigned int Model0_cmd,
          unsigned long Model0_arg);

 int (*Model0_compat_ioctl) (struct Model0_socket *Model0_sock, unsigned int Model0_cmd,
          unsigned long Model0_arg);

 int (*Model0_listen) (struct Model0_socket *Model0_sock, int Model0_len);
 int (*Model0_shutdown) (struct Model0_socket *Model0_sock, int Model0_flags);
 int (*Model0_setsockopt)(struct Model0_socket *Model0_sock, int Model0_level,
          int Model0_optname, char *Model0_optval, unsigned int Model0_optlen);
 int (*Model0_getsockopt)(struct Model0_socket *Model0_sock, int Model0_level,
          int Model0_optname, char *Model0_optval, int *Model0_optlen);

 int (*Model0_compat_setsockopt)(struct Model0_socket *Model0_sock, int Model0_level,
          int Model0_optname, char *Model0_optval, unsigned int Model0_optlen);
 int (*Model0_compat_getsockopt)(struct Model0_socket *Model0_sock, int Model0_level,
          int Model0_optname, char *Model0_optval, int *Model0_optlen);

 int (*Model0_sendmsg) (struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_m,
          Model0_size_t Model0_total_len);
 /* Notes for implementing recvmsg:
	 * ===============================
	 * msg->msg_namelen should get updated by the recvmsg handlers
	 * iff msg_name != NULL. It is by default 0 to prevent
	 * returning uninitialized memory to user space.  The recvfrom
	 * handlers can assume that msg.msg_name is either NULL or has
	 * a minimum size of sizeof(struct sockaddr_storage).
	 */
 int (*Model0_recvmsg) (struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_m,
          Model0_size_t Model0_total_len, int Model0_flags);
 int (*Model0_mmap) (struct Model0_file *Model0_file, struct Model0_socket *Model0_sock,
          struct Model0_vm_area_struct * Model0_vma);
 Model0_ssize_t (*Model0_sendpage) (struct Model0_socket *Model0_sock, struct Model0_page *Model0_page,
          int Model0_offset, Model0_size_t Model0_size, int Model0_flags);
 Model0_ssize_t (*Model0_splice_read)(struct Model0_socket *Model0_sock, Model0_loff_t *Model0_ppos,
           struct Model0_pipe_inode_info *Model0_pipe, Model0_size_t Model0_len, unsigned int Model0_flags);
 int (*Model0_set_peek_off)(struct Model0_sock *Model0_sk, int Model0_val);
 int (*Model0_peek_len)(struct Model0_socket *Model0_sock);
};




struct Model0_net_proto_family {
 int Model0_family;
 int (*Model0_create)(struct Model0_net *Model0_net, struct Model0_socket *Model0_sock,
      int Model0_protocol, int Model0_kern);
 struct Model0_module *Model0_owner;
};

struct Model0_iovec;
struct Model0_kvec;

enum {
 Model0_SOCK_WAKE_IO,
 Model0_SOCK_WAKE_WAITD,
 Model0_SOCK_WAKE_SPACE,
 Model0_SOCK_WAKE_URG,
};

int Model0_sock_wake_async(struct Model0_socket_wq *Model0_sk_wq, int Model0_how, int Model0_band);
int Model0_sock_register(const struct Model0_net_proto_family *Model0_fam);
void Model0_sock_unregister(int Model0_family);
int Model0___sock_create(struct Model0_net *Model0_net, int Model0_family, int Model0_type, int Model0_proto,
    struct Model0_socket **Model0_res, int Model0_kern);
int Model0_sock_create(int Model0_family, int Model0_type, int Model0_proto, struct Model0_socket **Model0_res);
int Model0_sock_create_kern(struct Model0_net *Model0_net, int Model0_family, int Model0_type, int Model0_proto, struct Model0_socket **Model0_res);
int Model0_sock_create_lite(int Model0_family, int Model0_type, int Model0_proto, struct Model0_socket **Model0_res);
struct Model0_socket *Model0_sock_alloc(void);
void Model0_sock_release(struct Model0_socket *Model0_sock);
int Model0_sock_sendmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg);
int Model0_sock_recvmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, int Model0_flags);
struct Model0_file *Model0_sock_alloc_file(struct Model0_socket *Model0_sock, int Model0_flags, const char *Model0_dname);
struct Model0_socket *Model0_sockfd_lookup(int Model0_fd, int *err);
struct Model0_socket *Model0_sock_from_file(struct Model0_file *Model0_file, int *err);

int Model0_net_ratelimit(void);
int Model0_kernel_sendmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, struct Model0_kvec *Model0_vec,
     Model0_size_t Model0_num, Model0_size_t Model0_len);
int Model0_kernel_recvmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, struct Model0_kvec *Model0_vec,
     Model0_size_t Model0_num, Model0_size_t Model0_len, int Model0_flags);

int Model0_kernel_bind(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_addr, int Model0_addrlen);
int Model0_kernel_listen(struct Model0_socket *Model0_sock, int Model0_backlog);
int Model0_kernel_accept(struct Model0_socket *Model0_sock, struct Model0_socket **Model0_newsock, int Model0_flags);
int Model0_kernel_connect(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_addr, int Model0_addrlen,
     int Model0_flags);
int Model0_kernel_getsockname(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_addr,
         int *Model0_addrlen);
int Model0_kernel_getpeername(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_addr,
         int *Model0_addrlen);
int Model0_kernel_getsockopt(struct Model0_socket *Model0_sock, int Model0_level, int Model0_optname, char *Model0_optval,
        int *Model0_optlen);
int Model0_kernel_setsockopt(struct Model0_socket *Model0_sock, int Model0_level, int Model0_optname, char *Model0_optval,
        unsigned int Model0_optlen);
int Model0_kernel_sendpage(struct Model0_socket *Model0_sock, struct Model0_page *Model0_page, int Model0_offset,
      Model0_size_t Model0_size, int Model0_flags);
int Model0_kernel_sock_ioctl(struct Model0_socket *Model0_sock, int Model0_cmd, unsigned long Model0_arg);
int Model0_kernel_sock_shutdown(struct Model0_socket *Model0_sock, enum Model0_sock_shutdown_cmd Model0_how);
struct Model0_module;

struct Model0_ts_config;




/**
 * struct ts_state - search state
 * @offset: offset for next match
 * @cb: control buffer, for persistent variables of get_next_block()
 */
struct Model0_ts_state
{
 unsigned int Model0_offset;
 char Model0_cb[40];
};

/**
 * struct ts_ops - search module operations
 * @name: name of search algorithm
 * @init: initialization function to prepare a search
 * @find: find the next occurrence of the pattern
 * @destroy: destroy algorithm specific parts of a search configuration
 * @get_pattern: return head of pattern
 * @get_pattern_len: return length of pattern
 * @owner: module reference to algorithm
 */
struct Model0_ts_ops
{
 const char *Model0_name;
 struct Model0_ts_config * (*Model0_init)(const void *, unsigned int, Model0_gfp_t, int);
 unsigned int (*Model0_find)(struct Model0_ts_config *,
     struct Model0_ts_state *);
 void (*Model0_destroy)(struct Model0_ts_config *);
 void * (*Model0_get_pattern)(struct Model0_ts_config *);
 unsigned int (*Model0_get_pattern_len)(struct Model0_ts_config *);
 struct Model0_module *Model0_owner;
 struct Model0_list_head Model0_list;
};

/**
 * struct ts_config - search configuration
 * @ops: operations of chosen algorithm
 * @flags: flags
 * @get_next_block: callback to fetch the next block to search in
 * @finish: callback to finalize a search
 */
struct Model0_ts_config
{
 struct Model0_ts_ops *Model0_ops;
 int Model0_flags;

 /**
	 * get_next_block - fetch next block of data
	 * @consumed: number of bytes consumed by the caller
	 * @dst: destination buffer
	 * @conf: search configuration
	 * @state: search state
	 *
	 * Called repeatedly until 0 is returned. Must assign the
	 * head of the next block of data to &*dst and return the length
	 * of the block or 0 if at the end. consumed == 0 indicates
	 * a new search. May store/read persistent values in state->cb.
	 */
 unsigned int (*Model0_get_next_block)(unsigned int Model0_consumed,
        const Model0_u8 **Model0_dst,
        struct Model0_ts_config *Model0_conf,
        struct Model0_ts_state *Model0_state);

 /**
	 * finish - finalize/clean a series of get_next_block() calls
	 * @conf: search configuration
	 * @state: search state
	 *
	 * Called after the last use of get_next_block(), may be used
	 * to cleanup any leftovers.
	 */
 void (*Model0_finish)(struct Model0_ts_config *Model0_conf,
       struct Model0_ts_state *Model0_state);
};

/**
 * textsearch_next - continue searching for a pattern
 * @conf: search configuration
 * @state: search state
 *
 * Continues a search looking for more occurrences of the pattern.
 * textsearch_find() must be called to find the first occurrence
 * in order to reset the state.
 *
 * Returns the position of the next occurrence of the pattern or
 * UINT_MAX if not match was found.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_textsearch_next(struct Model0_ts_config *Model0_conf,
        struct Model0_ts_state *Model0_state)
{
 unsigned int Model0_ret = Model0_conf->Model0_ops->Model0_find(Model0_conf, Model0_state);

 if (Model0_conf->Model0_finish)
  Model0_conf->Model0_finish(Model0_conf, Model0_state);

 return Model0_ret;
}

/**
 * textsearch_find - start searching for a pattern
 * @conf: search configuration
 * @state: search state
 *
 * Returns the position of first occurrence of the pattern or
 * UINT_MAX if no match was found.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_textsearch_find(struct Model0_ts_config *Model0_conf,
        struct Model0_ts_state *Model0_state)
{
 Model0_state->Model0_offset = 0;
 return Model0_textsearch_next(Model0_conf, Model0_state);
}

/**
 * textsearch_get_pattern - return head of the pattern
 * @conf: search configuration
 */
static inline __attribute__((no_instrument_function)) void *Model0_textsearch_get_pattern(struct Model0_ts_config *Model0_conf)
{
 return Model0_conf->Model0_ops->Model0_get_pattern(Model0_conf);
}

/**
 * textsearch_get_pattern_len - return length of the pattern
 * @conf: search configuration
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_textsearch_get_pattern_len(struct Model0_ts_config *Model0_conf)
{
 return Model0_conf->Model0_ops->Model0_get_pattern_len(Model0_conf);
}

extern int Model0_textsearch_register(struct Model0_ts_ops *);
extern int Model0_textsearch_unregister(struct Model0_ts_ops *);
extern struct Model0_ts_config *Model0_textsearch_prepare(const char *, const void *,
         unsigned int, Model0_gfp_t, int);
extern void Model0_textsearch_destroy(struct Model0_ts_config *Model0_conf);
extern unsigned int Model0_textsearch_find_continuous(struct Model0_ts_config *,
            struct Model0_ts_state *,
            const void *, unsigned int);





static inline __attribute__((no_instrument_function)) struct Model0_ts_config *Model0_alloc_ts_config(Model0_size_t Model0_payload,
      Model0_gfp_t Model0_gfp_mask)
{
 struct Model0_ts_config *Model0_conf;

 Model0_conf = Model0_kzalloc((((sizeof(*Model0_conf)) + 8 -1) & ~(8 -1)) + Model0_payload, Model0_gfp_mask);
 if (Model0_conf == ((void *)0))
  return Model0_ERR_PTR(-12);

 return Model0_conf;
}

static inline __attribute__((no_instrument_function)) void *Model0_ts_config_priv(struct Model0_ts_config *Model0_conf)
{
 return ((Model0_u8 *) Model0_conf + (((sizeof(struct Model0_ts_config)) + 8 -1) & ~(8 -1)));
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Checksumming functions for IP, TCP, UDP and so on
 *
 * Authors:	Jorge Cwik, <jorge@laser.satlink.net>
 *		Arnt Gulbrandsen, <agulbra@nvg.unit.no>
 *		Borrows very liberally from tcp.c and ip.c, see those
 *		files for more names.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */










/*
 * User space memory access functions
 */










static inline __attribute__((no_instrument_function)) void Model0_kasan_check_read(const void *Model0_p, unsigned int Model0_size) { }
static inline __attribute__((no_instrument_function)) void Model0_kasan_check_write(const void *Model0_p, unsigned int Model0_size) { }




/*
 * Supervisor Mode Access Prevention support
 *
 * Copyright (C) 2012 Intel Corporation
 * Author: H. Peter Anvin <hpa@linux.intel.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; version 2
 * of the License.
 */
/* "Raw" instruction opcodes */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_clac(void)
{
 /* Note: a barrier is implicit in alternative() */
 asm volatile ("661:\n\t" "" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 9*32+20)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0x0f,0x01,0xca" "\n" "665""1" ":\n\t" ".popsection" : : : "memory");
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_stac(void)
{
 /* Note: a barrier is implicit in alternative() */
 asm volatile ("661:\n\t" "" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 9*32+20)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0x0f,0x01,0xcb" "\n" "665""1" ":\n\t" ".popsection" : : : "memory");
}

/* These macros can be used in asm() statements */




/*
 * The fs value determines whether argument validity checking should be
 * performed or not.  If get_fs() == USER_DS, checking is performed, with
 * get_fs() == KERNEL_DS, checking is bypassed.
 *
 * For historical reasons, these macros are grossly misnamed.
 */
/*
 * Test whether a block of memory is a valid user space address.
 * Returns 0 if the range is valid, nonzero otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0___chk_range_not_ok(unsigned long Model0_addr, unsigned long Model0_size, unsigned long Model0_limit)
{
 /*
	 * If we have used "sizeof()" for the size,
	 * we know it won't overflow the limit (but
	 * it might overflow the 'addr', so it's
	 * important to subtract the size from the
	 * limit, not add it to the address).
	 */
 if (__builtin_constant_p(Model0_size))
  return __builtin_expect(!!(Model0_addr > Model0_limit - Model0_size), 0);

 /* Arbitrary sizes? Be careful about overflow */
 Model0_addr += Model0_size;
 if (__builtin_expect(!!(Model0_addr < Model0_size), 0))
  return true;
 return __builtin_expect(!!(Model0_addr > Model0_limit), 0);
}







/**
 * access_ok: - Checks if a user space pointer is valid
 * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that
 *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe
 *        to write to a block, it is always safe to read from it.
 * @addr: User space pointer to start of block to check
 * @size: Size of block to check
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * Checks if a pointer to a block of memory in user space is valid.
 *
 * Returns true (nonzero) if the memory block may be valid, false (zero)
 * if it is definitely invalid.
 *
 * Note that, depending on architecture, this function probably just
 * checks that the pointer is in the user space range - after calling
 * this function, memory access functions may still return -EFAULT.
 */



/*
 * The exception table consists of triples of addresses relative to the
 * exception table entry itself. The first address is of an instruction
 * that is allowed to fault, the second is the target at which the program
 * should continue. The third is a handler function to deal with the fault
 * caused by the instruction in the first field.
 *
 * All the routines below use bits of fixup code that are out of line
 * with the main instruction path.  This means when everything is well,
 * we don't even have to jump over them.  Further, they do not intrude
 * on our cache or tlb entries.
 */

struct Model0_exception_table_entry {
 int Model0_insn, Model0_fixup, Model0_handler;
};
extern int Model0_fixup_exception(struct Model0_pt_regs *Model0_regs, int Model0_trapnr);
extern bool Model0_ex_has_fault_handler(unsigned long Model0_ip);
extern void Model0_early_fixup_exception(struct Model0_pt_regs *Model0_regs, int Model0_trapnr);

/*
 * These are the main single-value transfer routines.  They automatically
 * use the right size if we just have the right pointer type.
 *
 * This gets kind of ugly. We want to return _two_ values in "get_user()"
 * and yet we don't want to do any pointers, because that is too much
 * of a performance impact. Thus we have a few rather ugly macros here,
 * and hide all the ugliness from the user.
 *
 * The "__xxx" versions of the user access functions are versions that
 * do not verify the address space, that must have been done previously
 * with a separate "access_ok()" call (this is used when we do multiple
 * accesses to the same area of user memory).
 */

extern int Model0___get_user_1(void);
extern int Model0___get_user_2(void);
extern int Model0___get_user_4(void);
extern int Model0___get_user_8(void);
extern int Model0___get_user_bad(void);




/*
 * This is a type: either unsigned long, if the argument fits into
 * that type, or otherwise unsigned long long.
 */



/**
 * get_user: - Get a simple variable from user space.
 * @x:   Variable to store result.
 * @ptr: Source address, in user space.
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * This macro copies a single simple variable from user space to kernel
 * space.  It supports simple types like char and int, but not larger
 * data types like structures or arrays.
 *
 * @ptr must have pointer-to-simple-variable type, and the result of
 * dereferencing @ptr must be assignable to @x without a cast.
 *
 * Returns zero on success, or -EFAULT on error.
 * On error, the variable @x is set to zero.
 */
/*
 * Careful: we have to cast the result to the type of the pointer
 * for sign reasons.
 *
 * The use of _ASM_DX as the register specifier is a bit of a
 * simplification, as gcc only cares about it as the starting point
 * and not size: for a 64-bit value it will use %ecx:%edx on 32 bits
 * (%ecx being the next register in gcc's x86 register sequence), and
 * %rdx on 64 bits.
 *
 * Clang/LLVM cares about the size of the register, but still wants
 * the base register for something that ends up being a pair.
 */
extern void Model0___put_user_bad(void);

/*
 * Strange magic calling convention: pointer in %ecx,
 * value in %eax(:%edx), return value in %eax. clobbers %rbx
 */
extern void Model0___put_user_1(void);
extern void Model0___put_user_2(void);
extern void Model0___put_user_4(void);
extern void Model0___put_user_8(void);

/**
 * put_user: - Write a simple value into user space.
 * @x:   Value to copy to user space.
 * @ptr: Destination address, in user space.
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * This macro copies a single simple value from kernel space to user
 * space.  It supports simple types like char and int, but not larger
 * data types like structures or arrays.
 *
 * @ptr must have pointer-to-simple-variable type, and @x must be assignable
 * to the result of dereferencing @ptr.
 *
 * Returns zero on success, or -EFAULT on error.
 */
/*
 * This doesn't do __uaccess_begin/end - the exception handling
 * around it must do that.
 */
/*
 * This doesn't do __uaccess_begin/end - the exception handling
 * around it must do that.
 */
/* FIXME: this hack is definitely wrong -AK */
struct Model0___large_struct { unsigned long Model0_buf[100]; };


/*
 * Tell gcc we read from memory instead of writing: this is because
 * we do not write to any memory gcc knows about, so there are no
 * aliasing issues.
 */
/*
 * uaccess_try and catch
 */
/**
 * __get_user: - Get a simple variable from user space, with less checking.
 * @x:   Variable to store result.
 * @ptr: Source address, in user space.
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * This macro copies a single simple variable from user space to kernel
 * space.  It supports simple types like char and int, but not larger
 * data types like structures or arrays.
 *
 * @ptr must have pointer-to-simple-variable type, and the result of
 * dereferencing @ptr must be assignable to @x without a cast.
 *
 * Caller must check the pointer with access_ok() before calling this
 * function.
 *
 * Returns zero on success, or -EFAULT on error.
 * On error, the variable @x is set to zero.
 */




/**
 * __put_user: - Write a simple value into user space, with less checking.
 * @x:   Value to copy to user space.
 * @ptr: Destination address, in user space.
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * This macro copies a single simple value from kernel space to user
 * space.  It supports simple types like char and int, but not larger
 * data types like structures or arrays.
 *
 * @ptr must have pointer-to-simple-variable type, and @x must be assignable
 * to the result of dereferencing @ptr.
 *
 * Caller must check the pointer with access_ok() before calling this
 * function.
 *
 * Returns zero on success, or -EFAULT on error.
 */







/*
 * {get|put}_user_try and catch
 *
 * get_user_try {
 *	get_user_ex(...);
 * } get_user_catch(err)
 */
extern unsigned long
Model0_copy_from_user_nmi(void *Model0_to, const void *Model0_from, unsigned long Model0_n);
extern __attribute__((warn_unused_result)) long
Model0_strncpy_from_user(char *Model0_dst, const char *Model0_src, long Model0_count);

extern __attribute__((warn_unused_result)) long Model0_strlen_user(const char *Model0_str);
extern __attribute__((warn_unused_result)) long Model0_strnlen_user(const char *Model0_str, long Model0_n);

unsigned long __attribute__((warn_unused_result)) Model0_clear_user(void *Model0_mem, unsigned long Model0_len);
unsigned long __attribute__((warn_unused_result)) Model0___clear_user(void *Model0_mem, unsigned long Model0_len);

extern void Model0___cmpxchg_wrong_size(void)
                                                     ;
/*
 * movsl can be slow when source and dest are not both 8-byte aligned
 */



/*
 * User space memory access functions
 */
/*
 * Copy To/From Userspace
 */

/* Handles exceptions in both to and from, but doesn't do access_ok */
__attribute__((warn_unused_result)) unsigned long
Model0_copy_user_enhanced_fast_string(void *Model0_to, const void *Model0_from, unsigned Model0_len);
__attribute__((warn_unused_result)) unsigned long
Model0_copy_user_generic_string(void *Model0_to, const void *Model0_from, unsigned Model0_len);
__attribute__((warn_unused_result)) unsigned long
Model0_copy_user_generic_unrolled(void *Model0_to, const void *Model0_from, unsigned Model0_len);

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result)) unsigned long
Model0_copy_user_generic(void *Model0_to, const void *Model0_from, unsigned Model0_len)
{
 unsigned Model0_ret;

 /*
	 * If CPU has ERMS feature, use copy_user_enhanced_fast_string.
	 * Otherwise, if CPU has rep_good feature, use copy_user_generic_string.
	 * Otherwise, use copy_user_generic_unrolled.
	 */
 asm volatile ("661:\n\t" "call %P[old]" "\n662:\n" ".skip -((" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")) > 0) * " "(" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")), 0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 3*32+16)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" " .long 661b - .\n" " .long " "664""2""f - .\n" " .word " "( 9*32+ 9)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""2""f-""664""2""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "call %P[new1]" "\n" "665""1" ":\n\t" "664""2"":\n\t" "call %P[new2]" "\n" "665""2" ":\n\t" ".popsection" : "=a" (Model0_ret), "=D" (Model0_to), "=S" (Model0_from), "=d" (Model0_len) : [old] "i" (Model0_copy_user_generic_unrolled), [new1] "i" (Model0_copy_user_generic_string), [new2] "i" (Model0_copy_user_enhanced_fast_string), "1" (Model0_to), "2" (Model0_from), "3" (Model0_len) : "memory", "rcx", "r8", "r9", "r10", "r11");
 return Model0_ret;
}

__attribute__((warn_unused_result)) unsigned long
Model0_copy_in_user(void *Model0_to, const void *Model0_from, unsigned Model0_len);

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model0___copy_from_user_nocheck(void *Model0_dst, const void *Model0_src, unsigned Model0_size)
{
 int Model0_ret = 0;

 Model0_check_object_size(Model0_dst, Model0_size, false);
 if (!__builtin_constant_p(Model0_size))
  return Model0_copy_user_generic(Model0_dst, ( void *)Model0_src, Model0_size);
 switch (Model0_size) {
 case 1:
  Model0_stac();
  asm volatile("\n" "1:	mov""b"" %2,%""b""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""b"" %""b""1,%""b""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=q"(*(Model0_u8 *)Model0_dst) : "m" ((*(struct Model0___large_struct *)((Model0_u8 *)Model0_src))), "i" (1), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 case 2:
  Model0_stac();
  asm volatile("\n" "1:	mov""w"" %2,%""w""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""w"" %""w""1,%""w""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(*(Model0_u16 *)Model0_dst) : "m" ((*(struct Model0___large_struct *)((Model0_u16 *)Model0_src))), "i" (2), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 case 4:
  Model0_stac();
  asm volatile("\n" "1:	mov""l"" %2,%""k""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""l"" %""k""1,%""k""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(*(Model0_u32 *)Model0_dst) : "m" ((*(struct Model0___large_struct *)((Model0_u32 *)Model0_src))), "i" (4), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 case 8:
  Model0_stac();
  asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(*(Model0_u64 *)Model0_dst) : "m" ((*(struct Model0___large_struct *)((Model0_u64 *)Model0_src))), "i" (8), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 case 10:
  Model0_stac();
  asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(*(Model0_u64 *)Model0_dst) : "m" ((*(struct Model0___large_struct *)((Model0_u64 *)Model0_src))), "i" (10), "0" (Model0_ret));

  if (__builtin_expect(!!(!Model0_ret), 1))
   asm volatile("\n" "1:	mov""w"" %2,%""w""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""w"" %""w""1,%""w""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(*(Model0_u16 *)(8 + (char *)Model0_dst)) : "m" ((*(struct Model0___large_struct *)((Model0_u16 *)(8 + (char *)Model0_src)))), "i" (2), "0" (Model0_ret));


  Model0_clac();
  return Model0_ret;
 case 16:
  Model0_stac();
  asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(*(Model0_u64 *)Model0_dst) : "m" ((*(struct Model0___large_struct *)((Model0_u64 *)Model0_src))), "i" (16), "0" (Model0_ret));

  if (__builtin_expect(!!(!Model0_ret), 1))
   asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(*(Model0_u64 *)(8 + (char *)Model0_dst)) : "m" ((*(struct Model0___large_struct *)((Model0_u64 *)(8 + (char *)Model0_src)))), "i" (8), "0" (Model0_ret));


  Model0_clac();
  return Model0_ret;
 default:
  return Model0_copy_user_generic(Model0_dst, ( void *)Model0_src, Model0_size);
 }
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model0___copy_from_user(void *Model0_dst, const void *Model0_src, unsigned Model0_size)
{
 Model0_might_fault();
 Model0_kasan_check_write(Model0_dst, Model0_size);
 return Model0___copy_from_user_nocheck(Model0_dst, Model0_src, Model0_size);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model0___copy_to_user_nocheck(void *Model0_dst, const void *Model0_src, unsigned Model0_size)
{
 int Model0_ret = 0;

 Model0_check_object_size(Model0_src, Model0_size, true);
 if (!__builtin_constant_p(Model0_size))
  return Model0_copy_user_generic(( void *)Model0_dst, Model0_src, Model0_size);
 switch (Model0_size) {
 case 1:
  Model0_stac();
  asm volatile("\n" "1:	mov""b"" %""b""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "iq"(*(Model0_u8 *)Model0_src), "m" ((*(struct Model0___large_struct *)((Model0_u8 *)Model0_dst))), "i" (1), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 case 2:
  Model0_stac();
  asm volatile("\n" "1:	mov""w"" %""w""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "ir"(*(Model0_u16 *)Model0_src), "m" ((*(struct Model0___large_struct *)((Model0_u16 *)Model0_dst))), "i" (2), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 case 4:
  Model0_stac();
  asm volatile("\n" "1:	mov""l"" %""k""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "ir"(*(Model0_u32 *)Model0_src), "m" ((*(struct Model0___large_struct *)((Model0_u32 *)Model0_dst))), "i" (4), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 case 8:
  Model0_stac();
  asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "er"(*(Model0_u64 *)Model0_src), "m" ((*(struct Model0___large_struct *)((Model0_u64 *)Model0_dst))), "i" (8), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 case 10:
  Model0_stac();
  asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "er"(*(Model0_u64 *)Model0_src), "m" ((*(struct Model0___large_struct *)((Model0_u64 *)Model0_dst))), "i" (10), "0" (Model0_ret));

  if (__builtin_expect(!!(!Model0_ret), 1)) {
   asm("":::"memory");
   asm volatile("\n" "1:	mov""w"" %""w""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "ir"(4[(Model0_u16 *)Model0_src]), "m" ((*(struct Model0___large_struct *)(4 + (Model0_u16 *)Model0_dst))), "i" (2), "0" (Model0_ret));

  }
  Model0_clac();
  return Model0_ret;
 case 16:
  Model0_stac();
  asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "er"(*(Model0_u64 *)Model0_src), "m" ((*(struct Model0___large_struct *)((Model0_u64 *)Model0_dst))), "i" (16), "0" (Model0_ret));

  if (__builtin_expect(!!(!Model0_ret), 1)) {
   asm("":::"memory");
   asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "er"(1[(Model0_u64 *)Model0_src]), "m" ((*(struct Model0___large_struct *)(1 + (Model0_u64 *)Model0_dst))), "i" (8), "0" (Model0_ret));

  }
  Model0_clac();
  return Model0_ret;
 default:
  return Model0_copy_user_generic(( void *)Model0_dst, Model0_src, Model0_size);
 }
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model0___copy_to_user(void *Model0_dst, const void *Model0_src, unsigned Model0_size)
{
 Model0_might_fault();
 Model0_kasan_check_read(Model0_src, Model0_size);
 return Model0___copy_to_user_nocheck(Model0_dst, Model0_src, Model0_size);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model0___copy_in_user(void *Model0_dst, const void *Model0_src, unsigned Model0_size)
{
 int Model0_ret = 0;

 Model0_might_fault();
 if (!__builtin_constant_p(Model0_size))
  return Model0_copy_user_generic(( void *)Model0_dst,
      ( void *)Model0_src, Model0_size);
 switch (Model0_size) {
 case 1: {
  Model0_u8 Model0_tmp;
  Model0_stac();
  asm volatile("\n" "1:	mov""b"" %2,%""b""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""b"" %""b""1,%""b""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=q"(Model0_tmp) : "m" ((*(struct Model0___large_struct *)((Model0_u8 *)Model0_src))), "i" (1), "0" (Model0_ret));

  if (__builtin_expect(!!(!Model0_ret), 1))
   asm volatile("\n" "1:	mov""b"" %""b""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "iq"(Model0_tmp), "m" ((*(struct Model0___large_struct *)((Model0_u8 *)Model0_dst))), "i" (1), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 }
 case 2: {
  Model0_u16 Model0_tmp;
  Model0_stac();
  asm volatile("\n" "1:	mov""w"" %2,%""w""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""w"" %""w""1,%""w""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(Model0_tmp) : "m" ((*(struct Model0___large_struct *)((Model0_u16 *)Model0_src))), "i" (2), "0" (Model0_ret));

  if (__builtin_expect(!!(!Model0_ret), 1))
   asm volatile("\n" "1:	mov""w"" %""w""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "ir"(Model0_tmp), "m" ((*(struct Model0___large_struct *)((Model0_u16 *)Model0_dst))), "i" (2), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 }

 case 4: {
  Model0_u32 Model0_tmp;
  Model0_stac();
  asm volatile("\n" "1:	mov""l"" %2,%""k""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""l"" %""k""1,%""k""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(Model0_tmp) : "m" ((*(struct Model0___large_struct *)((Model0_u32 *)Model0_src))), "i" (4), "0" (Model0_ret));

  if (__builtin_expect(!!(!Model0_ret), 1))
   asm volatile("\n" "1:	mov""l"" %""k""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "ir"(Model0_tmp), "m" ((*(struct Model0___large_struct *)((Model0_u32 *)Model0_dst))), "i" (4), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 }
 case 8: {
  Model0_u64 Model0_tmp;
  Model0_stac();
  asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model0_ret), "=r"(Model0_tmp) : "m" ((*(struct Model0___large_struct *)((Model0_u64 *)Model0_src))), "i" (8), "0" (Model0_ret));

  if (__builtin_expect(!!(!Model0_ret), 1))
   asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model0_ret) : "er"(Model0_tmp), "m" ((*(struct Model0___large_struct *)((Model0_u64 *)Model0_dst))), "i" (8), "0" (Model0_ret));

  Model0_clac();
  return Model0_ret;
 }
 default:
  return Model0_copy_user_generic(( void *)Model0_dst,
      ( void *)Model0_src, Model0_size);
 }
}

static __attribute__((warn_unused_result)) inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int
Model0___copy_from_user_inatomic(void *Model0_dst, const void *Model0_src, unsigned Model0_size)
{
 Model0_kasan_check_write(Model0_dst, Model0_size);
 return Model0___copy_from_user_nocheck(Model0_dst, Model0_src, Model0_size);
}

static __attribute__((warn_unused_result)) inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int
Model0___copy_to_user_inatomic(void *Model0_dst, const void *Model0_src, unsigned Model0_size)
{
 Model0_kasan_check_read(Model0_src, Model0_size);
 return Model0___copy_to_user_nocheck(Model0_dst, Model0_src, Model0_size);
}

extern long Model0___copy_user_nocache(void *Model0_dst, const void *Model0_src,
    unsigned Model0_size, int Model0_zerorest);

static inline __attribute__((no_instrument_function)) int
Model0___copy_from_user_nocache(void *Model0_dst, const void *Model0_src, unsigned Model0_size)
{
 Model0_might_fault();
 Model0_kasan_check_write(Model0_dst, Model0_size);
 return Model0___copy_user_nocache(Model0_dst, Model0_src, Model0_size, 1);
}

static inline __attribute__((no_instrument_function)) int
Model0___copy_from_user_inatomic_nocache(void *Model0_dst, const void *Model0_src,
      unsigned Model0_size)
{
 Model0_kasan_check_write(Model0_dst, Model0_size);
 return Model0___copy_user_nocache(Model0_dst, Model0_src, Model0_size, 0);
}

unsigned long
Model0_copy_user_handle_tail(char *Model0_to, char *Model0_from, unsigned Model0_len);


unsigned long __attribute__((warn_unused_result)) Model0__copy_from_user(void *Model0_to, const void *Model0_from,
        unsigned Model0_n);
unsigned long __attribute__((warn_unused_result)) Model0__copy_to_user(void *Model0_to, const void *Model0_from,
      unsigned Model0_n);

extern void
Model0___bad_copy_user(void);

static inline __attribute__((no_instrument_function)) void Model0_copy_user_overflow(int Model0_size, unsigned long Model0_count)
{
 ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_fmt("./arch/x86/include/asm/uaccess.h", 709, "Buffer overflow detected (%d < %lu)!\n", Model0_size, Model0_count); __builtin_expect(!!(Model0___ret_warn_on), 0); });
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long __attribute__((warn_unused_result))
Model0_copy_from_user(void *Model0_to, const void *Model0_from, unsigned long Model0_n)
{
 int Model0_sz = __builtin_object_size(Model0_to, 0);

 Model0_might_fault();

 Model0_kasan_check_write(Model0_to, Model0_n);

 if (__builtin_expect(!!(Model0_sz < 0 || Model0_sz >= Model0_n), 1)) {
  Model0_check_object_size(Model0_to, Model0_n, false);
  Model0_n = Model0__copy_from_user(Model0_to, Model0_from, Model0_n);
 } else if (!__builtin_constant_p(Model0_n))
  Model0_copy_user_overflow(Model0_sz, Model0_n);
 else
  Model0___bad_copy_user();

 return Model0_n;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long __attribute__((warn_unused_result))
Model0_copy_to_user(void *Model0_to, const void *Model0_from, unsigned long Model0_n)
{
 int Model0_sz = __builtin_object_size(Model0_from, 0);

 Model0_kasan_check_read(Model0_from, Model0_n);

 Model0_might_fault();

 if (__builtin_expect(!!(Model0_sz < 0 || Model0_sz >= Model0_n), 1)) {
  Model0_check_object_size(Model0_from, Model0_n, true);
  Model0_n = Model0__copy_to_user(Model0_to, Model0_from, Model0_n);
 } else if (!__builtin_constant_p(Model0_n))
  Model0_copy_user_overflow(Model0_sz, Model0_n);
 else
  Model0___bad_copy_user();

 return Model0_n;
}

/*
 * We rely on the nested NMI work to allow atomic faults from the NMI path; the
 * nested NMI paths are careful to preserve CR2.
 *
 * Caller must use pagefault_enable/disable, or run in interrupt context,
 * and also do a uaccess_ok() check
 */


/*
 * The "unsafe" user accesses aren't really "unsafe", but the naming
 * is a big fat warning: you have to not only do the access_ok()
 * checking before using them, but you have to surround them with the
 * user_access_begin/end() pair.
 */






/*
 * Checksums for x86-64
 * Copyright 2002 by Andi Kleen, SuSE Labs
 * with some code from asm-x86/checksum.h
 */





/**
 * csum_fold - Fold and invert a 32bit checksum.
 * sum: 32bit unfolded sum
 *
 * Fold a 32bit running checksum to 16bit and invert it. This is usually
 * the last step before putting a checksum into a packet.
 * Make sure not to mix with 64bit checksums.
 */
static inline __attribute__((no_instrument_function)) Model0___sum16 Model0_csum_fold(Model0___wsum Model0_sum)
{
#if CY_ABSTRACT1
    //We assume no csum is used
    return 0;
#else
 asm("  addl %1,%0\n"
     "  adcl $0xffff,%0"
     : "=r" (Model0_sum)
     : "r" (( Model0_u32)Model0_sum << 16),
       "0" (( Model0_u32)Model0_sum & 0xffff0000));
 return ( Model0___sum16)(~( Model0_u32)Model0_sum >> 16);
#endif
}

/*
 *	This is a version of ip_compute_csum() optimized for IP headers,
 *	which always checksum on 4 octet boundaries.
 *
 *	By Jorge Cwik <jorge@laser.satlink.net>, adapted for linux by
 *	Arnt Gulbrandsen.
 */

/**
 * ip_fast_csum - Compute the IPv4 header checksum efficiently.
 * iph: ipv4 header
 * ihl: length of header / 4
 */
static inline __attribute__((no_instrument_function)) Model0___sum16 Model0_ip_fast_csum(const void *Model0_iph, unsigned int Model0_ihl)
{
#if CY_ABSTRACT1
    //Not sure if it will be called, but we assume no csum is used
    return 0;
#else
 unsigned int Model0_sum;

 asm("  movl (%1), %0\n"
     "  subl $4, %2\n"
     "  jbe 2f\n"
     "  addl 4(%1), %0\n"
     "  adcl 8(%1), %0\n"
     "  adcl 12(%1), %0\n"
     "1: adcl 16(%1), %0\n"
     "  lea 4(%1), %1\n"
     "  decl %2\n"
     "  jne	1b\n"
     "  adcl $0, %0\n"
     "  movl %0, %2\n"
     "  shrl $16, %0\n"
     "  addw %w2, %w0\n"
     "  adcl $0, %0\n"
     "  notl %0\n"
     "2:"
 /* Since the input registers which are loaded with iph and ihl
	   are modified, we must also specify them as outputs, or gcc
	   will assume they contain their original values. */
     : "=r" (Model0_sum), "=r" (Model0_iph), "=r" (Model0_ihl)
     : "1" (Model0_iph), "2" (Model0_ihl)
     : "memory");
 return ( Model0___sum16)Model0_sum;
#endif
}

/**
 * csum_tcpup_nofold - Compute an IPv4 pseudo header checksum.
 * @saddr: source address
 * @daddr: destination address
 * @len: length of packet
 * @proto: ip protocol of packet
 * @sum: initial sum to be added in (32bit unfolded)
 *
 * Returns the pseudo header checksum the input data. Result is
 * 32bit unfolded.
 */
static inline __attribute__((no_instrument_function)) Model0___wsum
Model0_csum_tcpudp_nofold(Model0___be32 Model0_saddr, Model0___be32 Model0_daddr, __u32 Model0_len,
     __u8 Model0_proto, Model0___wsum Model0_sum)
{
#if CY_ABSTRACT1
    //We assume no csum is used
    return 0;
#else
 asm("  addl %1, %0\n"
     "  adcl %2, %0\n"
     "  adcl %3, %0\n"
     "  adcl $0, %0\n"
     : "=r" (Model0_sum)
     : "g" (Model0_daddr), "g" (Model0_saddr),
       "g" ((Model0_len + Model0_proto)<<8), "0" (Model0_sum));
 return Model0_sum;
#endif
}


/**
 * csum_tcpup_magic - Compute an IPv4 pseudo header checksum.
 * @saddr: source address
 * @daddr: destination address
 * @len: length of packet
 * @proto: ip protocol of packet
 * @sum: initial sum to be added in (32bit unfolded)
 *
 * Returns the 16bit pseudo header checksum the input data already
 * complemented and ready to be filled in.
 */
static inline __attribute__((no_instrument_function)) Model0___sum16 Model0_csum_tcpudp_magic(Model0___be32 Model0_saddr, Model0___be32 Model0_daddr,
     __u32 Model0_len, __u8 Model0_proto,
     Model0___wsum Model0_sum)
{
 return Model0_csum_fold(Model0_csum_tcpudp_nofold(Model0_saddr, Model0_daddr, Model0_len, Model0_proto, Model0_sum));
}

/**
 * csum_partial - Compute an internet checksum.
 * @buff: buffer to be checksummed
 * @len: length of buffer.
 * @sum: initial sum to be added in (32bit unfolded)
 *
 * Returns the 32bit unfolded internet checksum of the buffer.
 * Before filling it in it needs to be csum_fold()'ed.
 * buff should be aligned to a 64bit boundary if possible.
 */
extern Model0___wsum Model0_csum_partial(const void *Model0_buff, int Model0_len, Model0___wsum Model0_sum);





/* Do not call this directly. Use the wrappers below */
extern Model0___wsum Model0_csum_partial_copy_generic(const void *Model0_src, const void *Model0_dst,
     int Model0_len, Model0___wsum Model0_sum,
     int *Model0_src_err_ptr, int *Model0_dst_err_ptr);


extern Model0___wsum Model0_csum_partial_copy_from_user(const void *Model0_src, void *Model0_dst,
       int Model0_len, Model0___wsum Model0_isum, int *Model0_errp);
extern Model0___wsum Model0_csum_partial_copy_to_user(const void *Model0_src, void *Model0_dst,
     int Model0_len, Model0___wsum Model0_isum, int *Model0_errp);
extern Model0___wsum Model0_csum_partial_copy_nocheck(const void *Model0_src, void *Model0_dst,
     int Model0_len, Model0___wsum Model0_sum);

/* Old names. To be removed. */



/**
 * ip_compute_csum - Compute an 16bit IP checksum.
 * @buff: buffer address.
 * @len: length of buffer.
 *
 * Returns the 16bit folded/inverted checksum of the passed buffer.
 * Ready to fill in.
 */
extern Model0___sum16 Model0_ip_compute_csum(const void *Model0_buff, int Model0_len);

/**
 * csum_ipv6_magic - Compute checksum of an IPv6 pseudo header.
 * @saddr: source address
 * @daddr: destination address
 * @len: length of packet
 * @proto: protocol of packet
 * @sum: initial sum (32bit unfolded) to be added in
 *
 * Computes an IPv6 pseudo header checksum. This sum is added the checksum
 * into UDP/TCP packets and contains some link layer information.
 * Returns the unfolded 32bit checksum.
 */

struct Model0_in6_addr;


extern Model0___sum16
Model0_csum_ipv6_magic(const struct Model0_in6_addr *Model0_saddr, const struct Model0_in6_addr *Model0_daddr,
  __u32 Model0_len, __u8 Model0_proto, Model0___wsum Model0_sum);

static inline __attribute__((no_instrument_function)) unsigned Model0_add32_with_carry(unsigned Model0_a, unsigned Model0_b)
{
 asm("addl %2,%0\n\t"
     "adcl $0,%0"
     : "=r" (Model0_a)
     : "0" (Model0_a), "rm" (Model0_b));
 return Model0_a;
}


static inline __attribute__((no_instrument_function)) Model0___wsum Model0_csum_add(Model0___wsum Model0_csum, Model0___wsum Model0_addend)
{
 return ( Model0___wsum)Model0_add32_with_carry(( unsigned)Model0_csum,
      ( unsigned)Model0_addend);
}
static inline __attribute__((no_instrument_function)) Model0___wsum Model0_csum_sub(Model0___wsum Model0_csum, Model0___wsum Model0_addend)
{
 return Model0_csum_add(Model0_csum, ~Model0_addend);
}

static inline __attribute__((no_instrument_function)) Model0___sum16 Model0_csum16_add(Model0___sum16 Model0_csum, Model0___be16 Model0_addend)
{
 Model0_u16 Model0_res = ( Model0_u16)Model0_csum;

 Model0_res += ( Model0_u16)Model0_addend;
 return ( Model0___sum16)(Model0_res + (Model0_res < ( Model0_u16)Model0_addend));
}

static inline __attribute__((no_instrument_function)) Model0___sum16 Model0_csum16_sub(Model0___sum16 Model0_csum, Model0___be16 Model0_addend)
{
 return Model0_csum16_add(Model0_csum, ~Model0_addend);
}

static inline __attribute__((no_instrument_function)) Model0___wsum
Model0_csum_block_add(Model0___wsum Model0_csum, Model0___wsum Model0_csum2, int Model0_offset)
{
 Model0_u32 Model0_sum = ( Model0_u32)Model0_csum2;

 /* rotate sum to align it with a 16b boundary */
 if (Model0_offset & 1)
  Model0_sum = Model0_ror32(Model0_sum, 8);

 return Model0_csum_add(Model0_csum, ( Model0___wsum)Model0_sum);
}

static inline __attribute__((no_instrument_function)) Model0___wsum
Model0_csum_block_add_ext(Model0___wsum Model0_csum, Model0___wsum Model0_csum2, int Model0_offset, int Model0_len)
{
 return Model0_csum_block_add(Model0_csum, Model0_csum2, Model0_offset);
}

static inline __attribute__((no_instrument_function)) Model0___wsum
Model0_csum_block_sub(Model0___wsum Model0_csum, Model0___wsum Model0_csum2, int Model0_offset)
{
 return Model0_csum_block_add(Model0_csum, ~Model0_csum2, Model0_offset);
}

static inline __attribute__((no_instrument_function)) Model0___wsum Model0_csum_unfold(Model0___sum16 Model0_n)
{
 return ( Model0___wsum)Model0_n;
}

static inline __attribute__((no_instrument_function)) Model0___wsum Model0_csum_partial_ext(const void *Model0_buff, int Model0_len, Model0___wsum Model0_sum)
{
 return Model0_csum_partial(Model0_buff, Model0_len, Model0_sum);
}



static inline __attribute__((no_instrument_function)) void Model0_csum_replace_by_diff(Model0___sum16 *Model0_sum, Model0___wsum Model0_diff)
{
 *Model0_sum = Model0_csum_fold(Model0_csum_add(Model0_diff, ~Model0_csum_unfold(*Model0_sum)));
}

static inline __attribute__((no_instrument_function)) void Model0_csum_replace4(Model0___sum16 *Model0_sum, Model0___be32 Model0_from, Model0___be32 Model0_to)
{
 Model0___wsum Model0_tmp = Model0_csum_sub(~Model0_csum_unfold(*Model0_sum), ( Model0___wsum)Model0_from);

 *Model0_sum = Model0_csum_fold(Model0_csum_add(Model0_tmp, ( Model0___wsum)Model0_to));
}

/* Implements RFC 1624 (Incremental Internet Checksum)
 * 3. Discussion states :
 *     HC' = ~(~HC + ~m + m')
 *  m : old value of a 16bit field
 *  m' : new value of a 16bit field
 */
static inline __attribute__((no_instrument_function)) void Model0_csum_replace2(Model0___sum16 *Model0_sum, Model0___be16 old, Model0___be16 Model0_new)
{
 *Model0_sum = ~Model0_csum16_add(Model0_csum16_sub(~(*Model0_sum), old), Model0_new);
}

struct Model0_sk_buff;
void Model0_inet_proto_csum_replace4(Model0___sum16 *Model0_sum, struct Model0_sk_buff *Model0_skb,
         Model0___be32 Model0_from, Model0___be32 Model0_to, bool Model0_pseudohdr);
void Model0_inet_proto_csum_replace16(Model0___sum16 *Model0_sum, struct Model0_sk_buff *Model0_skb,
          const Model0___be32 *Model0_from, const Model0___be32 *Model0_to,
          bool Model0_pseudohdr);
void Model0_inet_proto_csum_replace_by_diff(Model0___sum16 *Model0_sum, struct Model0_sk_buff *Model0_skb,
         Model0___wsum Model0_diff, bool Model0_pseudohdr);

static inline __attribute__((no_instrument_function)) void Model0_inet_proto_csum_replace2(Model0___sum16 *Model0_sum, struct Model0_sk_buff *Model0_skb,
         Model0___be16 Model0_from, Model0___be16 Model0_to,
         bool Model0_pseudohdr)
{
 Model0_inet_proto_csum_replace4(Model0_sum, Model0_skb, ( Model0___be32)Model0_from,
     ( Model0___be32)Model0_to, Model0_pseudohdr);
}

static inline __attribute__((no_instrument_function)) Model0___wsum Model0_remcsum_adjust(void *Model0_ptr, Model0___wsum Model0_csum,
        int Model0_start, int Model0_offset)
{
 Model0___sum16 *Model0_psum = (Model0___sum16 *)(Model0_ptr + Model0_offset);
 Model0___wsum Model0_delta;

 /* Subtract out checksum up to start */
 Model0_csum = Model0_csum_sub(Model0_csum, Model0_csum_partial(Model0_ptr, Model0_start, 0));

 /* Set derived checksum in packet */
 Model0_delta = Model0_csum_sub(( Model0___wsum)Model0_csum_fold(Model0_csum),
    ( Model0___wsum)*Model0_psum);
 *Model0_psum = Model0_csum_fold(Model0_csum);

 return Model0_delta;
}

static inline __attribute__((no_instrument_function)) void Model0_remcsum_unadjust(Model0___sum16 *Model0_psum, Model0___wsum Model0_delta)
{
 *Model0_psum = Model0_csum_fold(Model0_csum_sub(Model0_delta, *Model0_psum));
}





/*
 * include/linux/sizes.h
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */



/*
 * Copyright (C) 2008 Advanced Micro Devices, Inc.
 *
 * Author: Joerg Roedel <joerg.roedel@amd.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 as published
 * by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
 */






struct Model0_device;
struct Model0_scatterlist;
struct Model0_bus_type;
static inline __attribute__((no_instrument_function)) void Model0_dma_debug_add_bus(struct Model0_bus_type *Model0_bus)
{
}

static inline __attribute__((no_instrument_function)) void Model0_dma_debug_init(Model0_u32 Model0_num_entries)
{
}

static inline __attribute__((no_instrument_function)) int Model0_dma_debug_resize_entries(Model0_u32 Model0_num_entries)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_map_page(struct Model0_device *Model0_dev, struct Model0_page *Model0_page,
          Model0_size_t Model0_offset, Model0_size_t Model0_size,
          int Model0_direction, Model0_dma_addr_t Model0_dma_addr,
          bool Model0_map_single)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_mapping_error(struct Model0_device *Model0_dev,
       Model0_dma_addr_t Model0_dma_addr)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_unmap_page(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_addr,
     Model0_size_t Model0_size, int Model0_direction,
     bool Model0_map_single)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_map_sg(struct Model0_device *Model0_dev, struct Model0_scatterlist *Model0_sg,
        int Model0_nents, int Model0_mapped_ents, int Model0_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_unmap_sg(struct Model0_device *Model0_dev,
          struct Model0_scatterlist *Model0_sglist,
          int Model0_nelems, int Model0_dir)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_alloc_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
         Model0_dma_addr_t Model0_dma_addr, void *Model0_virt)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_free_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
        void *Model0_virt, Model0_dma_addr_t Model0_addr)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_sync_single_for_cpu(struct Model0_device *Model0_dev,
       Model0_dma_addr_t Model0_dma_handle,
       Model0_size_t Model0_size, int Model0_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_sync_single_for_device(struct Model0_device *Model0_dev,
          Model0_dma_addr_t Model0_dma_handle,
          Model0_size_t Model0_size, int Model0_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_sync_single_range_for_cpu(struct Model0_device *Model0_dev,
             Model0_dma_addr_t Model0_dma_handle,
             unsigned long Model0_offset,
             Model0_size_t Model0_size,
             int Model0_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_sync_single_range_for_device(struct Model0_device *Model0_dev,
         Model0_dma_addr_t Model0_dma_handle,
         unsigned long Model0_offset,
         Model0_size_t Model0_size,
         int Model0_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_sync_sg_for_cpu(struct Model0_device *Model0_dev,
          struct Model0_scatterlist *Model0_sg,
          int Model0_nelems, int Model0_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_sync_sg_for_device(struct Model0_device *Model0_dev,
      struct Model0_scatterlist *Model0_sg,
      int Model0_nelems, int Model0_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_dump_mappings(struct Model0_device *Model0_dev)
{
}

static inline __attribute__((no_instrument_function)) void Model0_debug_dma_assert_idle(struct Model0_page *Model0_page)
{
}


/*
 * These definitions mirror those in pci.h, so they can be used
 * interchangeably with their PCI_ counterparts.
 */
enum Model0_dma_data_direction {
 Model0_DMA_BIDIRECTIONAL = 0,
 Model0_DMA_TO_DEVICE = 1,
 Model0_DMA_FROM_DEVICE = 2,
 Model0_DMA_NONE = 3,
};




/**
 * List of possible attributes associated with a DMA mapping. The semantics
 * of each attribute should be defined in Documentation/DMA-attributes.txt.
 *
 * DMA_ATTR_WRITE_BARRIER: DMA to a memory region with this attribute
 * forces all pending DMA writes to complete.
 */

/*
 * DMA_ATTR_WEAK_ORDERING: Specifies that reads and writes to the mapping
 * may be weakly ordered, that is that reads and writes may pass each other.
 */

/*
 * DMA_ATTR_WRITE_COMBINE: Specifies that writes to the mapping may be
 * buffered to improve performance.
 */

/*
 * DMA_ATTR_NON_CONSISTENT: Lets the platform to choose to return either
 * consistent or non-consistent memory as it sees fit.
 */

/*
 * DMA_ATTR_NO_KERNEL_MAPPING: Lets the platform to avoid creating a kernel
 * virtual mapping for the allocated buffer.
 */

/*
 * DMA_ATTR_SKIP_CPU_SYNC: Allows platform code to skip synchronization of
 * the CPU cache for the given buffer assuming that it has been already
 * transferred to 'device' domain.
 */

/*
 * DMA_ATTR_FORCE_CONTIGUOUS: Forces contiguous allocation of the buffer
 * in physical memory.
 */

/*
 * DMA_ATTR_ALLOC_SINGLE_PAGES: This is a hint to the DMA-mapping subsystem
 * that it's probably not worth the time to try to allocate memory to in a way
 * that gives better TLB efficiency.
 */


/*
 * A dma_addr_t can hold any valid DMA or bus address for the platform.
 * It can be given to a device to use as a DMA source or target.  A CPU cannot
 * reference a dma_addr_t directly because there may be translation between
 * its physical address space and the bus address space.
 */
struct Model0_dma_map_ops {
 void* (*Model0_alloc)(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
    Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_gfp,
    unsigned long Model0_attrs);
 void (*Model0_free)(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
         void *Model0_vaddr, Model0_dma_addr_t Model0_dma_handle,
         unsigned long Model0_attrs);
 int (*Model0_mmap)(struct Model0_device *, struct Model0_vm_area_struct *,
     void *, Model0_dma_addr_t, Model0_size_t,
     unsigned long Model0_attrs);

 int (*Model0_get_sgtable)(struct Model0_device *Model0_dev, struct Model0_sg_table *Model0_sgt, void *,
      Model0_dma_addr_t, Model0_size_t, unsigned long Model0_attrs);

 Model0_dma_addr_t (*Model0_map_page)(struct Model0_device *Model0_dev, struct Model0_page *Model0_page,
          unsigned long Model0_offset, Model0_size_t Model0_size,
          enum Model0_dma_data_direction Model0_dir,
          unsigned long Model0_attrs);
 void (*Model0_unmap_page)(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_dma_handle,
      Model0_size_t Model0_size, enum Model0_dma_data_direction Model0_dir,
      unsigned long Model0_attrs);
 /*
	 * map_sg returns 0 on error and a value > 0 on success.
	 * It should never return a value < 0.
	 */
 int (*Model0_map_sg)(struct Model0_device *Model0_dev, struct Model0_scatterlist *Model0_sg,
        int Model0_nents, enum Model0_dma_data_direction Model0_dir,
        unsigned long Model0_attrs);
 void (*Model0_unmap_sg)(struct Model0_device *Model0_dev,
    struct Model0_scatterlist *Model0_sg, int Model0_nents,
    enum Model0_dma_data_direction Model0_dir,
    unsigned long Model0_attrs);
 void (*Model0_sync_single_for_cpu)(struct Model0_device *Model0_dev,
        Model0_dma_addr_t Model0_dma_handle, Model0_size_t Model0_size,
        enum Model0_dma_data_direction Model0_dir);
 void (*Model0_sync_single_for_device)(struct Model0_device *Model0_dev,
           Model0_dma_addr_t Model0_dma_handle, Model0_size_t Model0_size,
           enum Model0_dma_data_direction Model0_dir);
 void (*Model0_sync_sg_for_cpu)(struct Model0_device *Model0_dev,
    struct Model0_scatterlist *Model0_sg, int Model0_nents,
    enum Model0_dma_data_direction Model0_dir);
 void (*Model0_sync_sg_for_device)(struct Model0_device *Model0_dev,
       struct Model0_scatterlist *Model0_sg, int Model0_nents,
       enum Model0_dma_data_direction Model0_dir);
 int (*Model0_mapping_error)(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_dma_addr);
 int (*Model0_dma_supported)(struct Model0_device *Model0_dev, Model0_u64 Model0_mask);
 int (*Model0_set_dma_mask)(struct Model0_device *Model0_dev, Model0_u64 Model0_mask);



 int Model0_is_phys;
};

extern struct Model0_dma_map_ops Model0_dma_noop_ops;





static inline __attribute__((no_instrument_function)) int Model0_valid_dma_direction(int Model0_dma_direction)
{
 return ((Model0_dma_direction == Model0_DMA_BIDIRECTIONAL) ||
  (Model0_dma_direction == Model0_DMA_TO_DEVICE) ||
  (Model0_dma_direction == Model0_DMA_FROM_DEVICE));
}

static inline __attribute__((no_instrument_function)) int Model0_is_device_dma_capable(struct Model0_device *Model0_dev)
{
 return Model0_dev->Model0_dma_mask != ((void *)0) && *Model0_dev->Model0_dma_mask != 0x0ULL;
}



/*
 * IOMMU interface. See Documentation/DMA-API-HOWTO.txt and
 * Documentation/DMA-API.txt for documentation.
 */
















struct Model0_device;
struct Model0_page;
struct Model0_scatterlist;

extern int Model0_swiotlb_force;

/*
 * Maximum allowable number of contiguous slabs to map,
 * must be a power of 2.  What is the appropriate value ?
 * The complexity of {map,unmap}_single is linearly dependent on this value.
 */


/*
 * log of the size of each IO TLB slab.  The number of slabs is command line
 * controllable.
 */


extern void Model0_swiotlb_init(int Model0_verbose);
int Model0_swiotlb_init_with_tbl(char *Model0_tlb, unsigned long Model0_nslabs, int Model0_verbose);
extern unsigned long Model0_swiotlb_nr_tbl(void);
unsigned long Model0_swiotlb_size_or_default(void);
extern int Model0_swiotlb_late_init_with_tbl(char *Model0_tlb, unsigned long Model0_nslabs);

/*
 * Enumeration for sync targets
 */
enum Model0_dma_sync_target {
 Model0_SYNC_FOR_CPU = 0,
 Model0_SYNC_FOR_DEVICE = 1,
};

/* define the last possible byte of physical address space as a mapping error */


extern Model0_phys_addr_t Model0_swiotlb_tbl_map_single(struct Model0_device *Model0_hwdev,
       Model0_dma_addr_t Model0_tbl_dma_addr,
       Model0_phys_addr_t Model0_phys, Model0_size_t Model0_size,
       enum Model0_dma_data_direction Model0_dir);

extern void Model0_swiotlb_tbl_unmap_single(struct Model0_device *Model0_hwdev,
         Model0_phys_addr_t Model0_tlb_addr,
         Model0_size_t Model0_size, enum Model0_dma_data_direction Model0_dir);

extern void Model0_swiotlb_tbl_sync_single(struct Model0_device *Model0_hwdev,
        Model0_phys_addr_t Model0_tlb_addr,
        Model0_size_t Model0_size, enum Model0_dma_data_direction Model0_dir,
        enum Model0_dma_sync_target Model0_target);

/* Accessory functions. */
extern void
*Model0_swiotlb_alloc_coherent(struct Model0_device *Model0_hwdev, Model0_size_t Model0_size,
   Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_flags);

extern void
Model0_swiotlb_free_coherent(struct Model0_device *Model0_hwdev, Model0_size_t Model0_size,
        void *Model0_vaddr, Model0_dma_addr_t Model0_dma_handle);

extern Model0_dma_addr_t Model0_swiotlb_map_page(struct Model0_device *Model0_dev, struct Model0_page *Model0_page,
       unsigned long Model0_offset, Model0_size_t Model0_size,
       enum Model0_dma_data_direction Model0_dir,
       unsigned long Model0_attrs);
extern void Model0_swiotlb_unmap_page(struct Model0_device *Model0_hwdev, Model0_dma_addr_t Model0_dev_addr,
          Model0_size_t Model0_size, enum Model0_dma_data_direction Model0_dir,
          unsigned long Model0_attrs);

extern int
Model0_swiotlb_map_sg(struct Model0_device *Model0_hwdev, struct Model0_scatterlist *Model0_sg, int Model0_nents,
        enum Model0_dma_data_direction Model0_dir);

extern void
Model0_swiotlb_unmap_sg(struct Model0_device *Model0_hwdev, struct Model0_scatterlist *Model0_sg, int Model0_nents,
   enum Model0_dma_data_direction Model0_dir);

extern int
Model0_swiotlb_map_sg_attrs(struct Model0_device *Model0_hwdev, struct Model0_scatterlist *Model0_sgl, int Model0_nelems,
       enum Model0_dma_data_direction Model0_dir,
       unsigned long Model0_attrs);

extern void
Model0_swiotlb_unmap_sg_attrs(struct Model0_device *Model0_hwdev, struct Model0_scatterlist *Model0_sgl,
         int Model0_nelems, enum Model0_dma_data_direction Model0_dir,
         unsigned long Model0_attrs);

extern void
Model0_swiotlb_sync_single_for_cpu(struct Model0_device *Model0_hwdev, Model0_dma_addr_t Model0_dev_addr,
       Model0_size_t Model0_size, enum Model0_dma_data_direction Model0_dir);

extern void
Model0_swiotlb_sync_sg_for_cpu(struct Model0_device *Model0_hwdev, struct Model0_scatterlist *Model0_sg,
   int Model0_nelems, enum Model0_dma_data_direction Model0_dir);

extern void
Model0_swiotlb_sync_single_for_device(struct Model0_device *Model0_hwdev, Model0_dma_addr_t Model0_dev_addr,
          Model0_size_t Model0_size, enum Model0_dma_data_direction Model0_dir);

extern void
Model0_swiotlb_sync_sg_for_device(struct Model0_device *Model0_hwdev, struct Model0_scatterlist *Model0_sg,
      int Model0_nelems, enum Model0_dma_data_direction Model0_dir);

extern int
Model0_swiotlb_dma_mapping_error(struct Model0_device *Model0_hwdev, Model0_dma_addr_t Model0_dma_addr);

extern int
Model0_swiotlb_dma_supported(struct Model0_device *Model0_hwdev, Model0_u64 Model0_mask);


extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_swiotlb_free(void);




extern void Model0_swiotlb_print_info(void);
extern int Model0_is_swiotlb_buffer(Model0_phys_addr_t Model0_paddr);


extern int Model0_swiotlb;
extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pci_swiotlb_detect_override(void);
extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pci_swiotlb_detect_4gb(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pci_swiotlb_init(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_pci_swiotlb_late_init(void);
static inline __attribute__((no_instrument_function)) void Model0_dma_mark_clean(void *Model0_addr, Model0_size_t Model0_size) {}

extern void *Model0_x86_swiotlb_alloc_coherent(struct Model0_device *Model0_hwdev, Model0_size_t Model0_size,
     Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_flags,
     unsigned long Model0_attrs);
extern void Model0_x86_swiotlb_free_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
     void *Model0_vaddr, Model0_dma_addr_t Model0_dma_addr,
     unsigned long Model0_attrs);



/*
 * Contiguous Memory Allocator for DMA mapping framework
 * Copyright (c) 2010-2011 by Samsung Electronics.
 * Written by:
 *	Marek Szyprowski <m.szyprowski@samsung.com>
 *	Michal Nazarewicz <mina86@mina86.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License as
 * published by the Free Software Foundation; either version 2 of the
 * License or (at your optional) any later version of the license.
 */

/*
 * Contiguous Memory Allocator
 *
 *   The Contiguous Memory Allocator (CMA) makes it possible to
 *   allocate big contiguous chunks of memory after the system has
 *   booted.
 *
 * Why is it needed?
 *
 *   Various devices on embedded systems have no scatter-getter and/or
 *   IO map support and require contiguous blocks of memory to
 *   operate.  They include devices such as cameras, hardware video
 *   coders, etc.
 *
 *   Such devices often require big memory buffers (a full HD frame
 *   is, for instance, more then 2 mega pixels large, i.e. more than 6
 *   MB of memory), which makes mechanisms such as kmalloc() or
 *   alloc_page() ineffective.
 *
 *   At the same time, a solution where a big memory region is
 *   reserved for a device is suboptimal since often more memory is
 *   reserved then strictly required and, moreover, the memory is
 *   inaccessible to page system even if device drivers don't use it.
 *
 *   CMA tries to solve this issue by operating on memory regions
 *   where only movable pages can be allocated from.  This way, kernel
 *   can use the memory for pagecache and when device driver requests
 *   it, allocated pages can be migrated.
 *
 * Driver usage
 *
 *   CMA should not be used by the device drivers directly. It is
 *   only a helper framework for dma-mapping subsystem.
 *
 *   For more information, see kernel-docs in drivers/base/dma-contiguous.c
 */





struct Model0_cma;
struct Model0_page;
static inline __attribute__((no_instrument_function)) struct Model0_cma *Model0_dev_get_cma_area(struct Model0_device *Model0_dev)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_dev_set_cma_area(struct Model0_device *Model0_dev, struct Model0_cma *Model0_cma) { }

static inline __attribute__((no_instrument_function)) void Model0_dma_contiguous_set_default(struct Model0_cma *Model0_cma) { }

static inline __attribute__((no_instrument_function)) void Model0_dma_contiguous_reserve(Model0_phys_addr_t Model0_limit) { }

static inline __attribute__((no_instrument_function)) int Model0_dma_contiguous_reserve_area(Model0_phys_addr_t Model0_size, Model0_phys_addr_t Model0_base,
           Model0_phys_addr_t Model0_limit, struct Model0_cma **Model0_res_cma,
           bool Model0_fixed)
{
 return -38;
}

static inline __attribute__((no_instrument_function))
int Model0_dma_declare_contiguous(struct Model0_device *Model0_dev, Model0_phys_addr_t Model0_size,
      Model0_phys_addr_t Model0_base, Model0_phys_addr_t Model0_limit)
{
 return -38;
}

static inline __attribute__((no_instrument_function))
struct Model0_page *Model0_dma_alloc_from_contiguous(struct Model0_device *Model0_dev, Model0_size_t Model0_count,
           unsigned int Model0_order)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function))
bool Model0_dma_release_from_contiguous(struct Model0_device *Model0_dev, struct Model0_page *Model0_pages,
     int Model0_count)
{
 return false;
}
extern int Model0_iommu_merge;
extern struct Model0_device Model0_x86_dma_fallback_dev;
extern int Model0_panic_on_overflow;

extern struct Model0_dma_map_ops *Model0_dma_ops;

static inline __attribute__((no_instrument_function)) struct Model0_dma_map_ops *Model0_get_dma_ops(struct Model0_device *Model0_dev)
{



 if (__builtin_expect(!!(!Model0_dev), 0) || !Model0_dev->Model0_archdata.Model0_dma_ops)
  return Model0_dma_ops;
 else
  return Model0_dev->Model0_archdata.Model0_dma_ops;

}

bool Model0_arch_dma_alloc_attrs(struct Model0_device **Model0_dev, Model0_gfp_t *Model0_gfp);



extern int Model0_dma_supported(struct Model0_device *Model0_hwdev, Model0_u64 Model0_mask);

extern void *Model0_dma_generic_alloc_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
     Model0_dma_addr_t *Model0_dma_addr, Model0_gfp_t Model0_flag,
     unsigned long Model0_attrs);

extern void Model0_dma_generic_free_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
          void *Model0_vaddr, Model0_dma_addr_t Model0_dma_addr,
          unsigned long Model0_attrs);







static inline __attribute__((no_instrument_function)) bool Model0_dma_capable(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_addr, Model0_size_t Model0_size)
{
 if (!Model0_dev->Model0_dma_mask)
  return 0;

 return Model0_addr + Model0_size - 1 <= *Model0_dev->Model0_dma_mask;
}

static inline __attribute__((no_instrument_function)) Model0_dma_addr_t Model0_phys_to_dma(struct Model0_device *Model0_dev, Model0_phys_addr_t Model0_paddr)
{
 return Model0_paddr;
}

static inline __attribute__((no_instrument_function)) Model0_phys_addr_t Model0_dma_to_phys(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_daddr)
{
 return Model0_daddr;
}


static inline __attribute__((no_instrument_function)) void
Model0_dma_cache_sync(struct Model0_device *Model0_dev, void *Model0_vaddr, Model0_size_t Model0_size,
 enum Model0_dma_data_direction Model0_dir)
{
 Model0_flush_write_buffers();
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_dma_alloc_coherent_mask(struct Model0_device *Model0_dev,
          Model0_gfp_t Model0_gfp)
{
 unsigned long Model0_dma_mask = 0;

 Model0_dma_mask = Model0_dev->Model0_coherent_dma_mask;
 if (!Model0_dma_mask)
  Model0_dma_mask = (Model0_gfp & (( Model0_gfp_t)0x01u)) ? (((24) == 64) ? ~0ULL : ((1ULL<<(24))-1)) : (((32) == 64) ? ~0ULL : ((1ULL<<(32))-1));

 return Model0_dma_mask;
}

static inline __attribute__((no_instrument_function)) Model0_gfp_t Model0_dma_alloc_coherent_gfp_flags(struct Model0_device *Model0_dev, Model0_gfp_t Model0_gfp)
{
 unsigned long Model0_dma_mask = Model0_dma_alloc_coherent_mask(Model0_dev, Model0_gfp);

 if (Model0_dma_mask <= (((24) == 64) ? ~0ULL : ((1ULL<<(24))-1)))
  Model0_gfp |= (( Model0_gfp_t)0x01u);

 if (Model0_dma_mask <= (((32) == 64) ? ~0ULL : ((1ULL<<(32))-1)) && !(Model0_gfp & (( Model0_gfp_t)0x01u)))
  Model0_gfp |= (( Model0_gfp_t)0x04u);

       return Model0_gfp;
}
static inline __attribute__((no_instrument_function)) Model0_dma_addr_t Model0_dma_map_single_attrs(struct Model0_device *Model0_dev, void *Model0_ptr,
           Model0_size_t Model0_size,
           enum Model0_dma_data_direction Model0_dir,
           unsigned long Model0_attrs)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);
 Model0_dma_addr_t Model0_addr;

 Model0_kmemcheck_mark_initialized(Model0_ptr, Model0_size);
 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (178), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 Model0_addr = Model0_ops->Model0_map_page(Model0_dev, (((struct Model0_page *)(0xffffea0000000000UL)) + (Model0___phys_addr_nodebug((unsigned long)(Model0_ptr)) >> 12)),
        ((unsigned long)(Model0_ptr) & ~(~(((1UL) << 12)-1))), Model0_size,
        Model0_dir, Model0_attrs);
 Model0_debug_dma_map_page(Model0_dev, (((struct Model0_page *)(0xffffea0000000000UL)) + (Model0___phys_addr_nodebug((unsigned long)(Model0_ptr)) >> 12)),
      ((unsigned long)(Model0_ptr) & ~(~(((1UL) << 12)-1))), Model0_size,
      Model0_dir, Model0_addr, true);
 return Model0_addr;
}

static inline __attribute__((no_instrument_function)) void Model0_dma_unmap_single_attrs(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_addr,
       Model0_size_t Model0_size,
       enum Model0_dma_data_direction Model0_dir,
       unsigned long Model0_attrs)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (195), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_unmap_page)
  Model0_ops->Model0_unmap_page(Model0_dev, Model0_addr, Model0_size, Model0_dir, Model0_attrs);
 Model0_debug_dma_unmap_page(Model0_dev, Model0_addr, Model0_size, Model0_dir, true);
}

/*
 * dma_maps_sg_attrs returns 0 on error and > 0 on success.
 * It should never return a value < 0.
 */
static inline __attribute__((no_instrument_function)) int Model0_dma_map_sg_attrs(struct Model0_device *Model0_dev, struct Model0_scatterlist *Model0_sg,
       int Model0_nents, enum Model0_dma_data_direction Model0_dir,
       unsigned long Model0_attrs)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);
 int Model0_i, Model0_ents;
 struct Model0_scatterlist *Model0_s;

 for (Model0_i = 0, Model0_s = (Model0_sg); Model0_i < (Model0_nents); Model0_i++, Model0_s = Model0_sg_next(Model0_s))
  Model0_kmemcheck_mark_initialized(Model0_sg_virt(Model0_s), Model0_s->Model0_length);
 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (215), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 Model0_ents = Model0_ops->Model0_map_sg(Model0_dev, Model0_sg, Model0_nents, Model0_dir, Model0_attrs);
 do { if (__builtin_expect(!!(Model0_ents < 0), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (217), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 Model0_debug_dma_map_sg(Model0_dev, Model0_sg, Model0_nents, Model0_ents, Model0_dir);

 return Model0_ents;
}

static inline __attribute__((no_instrument_function)) void Model0_dma_unmap_sg_attrs(struct Model0_device *Model0_dev, struct Model0_scatterlist *Model0_sg,
          int Model0_nents, enum Model0_dma_data_direction Model0_dir,
          unsigned long Model0_attrs)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (229), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 Model0_debug_dma_unmap_sg(Model0_dev, Model0_sg, Model0_nents, Model0_dir);
 if (Model0_ops->Model0_unmap_sg)
  Model0_ops->Model0_unmap_sg(Model0_dev, Model0_sg, Model0_nents, Model0_dir, Model0_attrs);
}

static inline __attribute__((no_instrument_function)) Model0_dma_addr_t Model0_dma_map_page(struct Model0_device *Model0_dev, struct Model0_page *Model0_page,
          Model0_size_t Model0_offset, Model0_size_t Model0_size,
          enum Model0_dma_data_direction Model0_dir)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);
 Model0_dma_addr_t Model0_addr;

 Model0_kmemcheck_mark_initialized(Model0_lowmem_page_address(Model0_page) + Model0_offset, Model0_size);
 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (243), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 Model0_addr = Model0_ops->Model0_map_page(Model0_dev, Model0_page, Model0_offset, Model0_size, Model0_dir, 0);
 Model0_debug_dma_map_page(Model0_dev, Model0_page, Model0_offset, Model0_size, Model0_dir, Model0_addr, false);

 return Model0_addr;
}

static inline __attribute__((no_instrument_function)) void Model0_dma_unmap_page(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_addr,
      Model0_size_t Model0_size, enum Model0_dma_data_direction Model0_dir)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (255), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_unmap_page)
  Model0_ops->Model0_unmap_page(Model0_dev, Model0_addr, Model0_size, Model0_dir, 0);
 Model0_debug_dma_unmap_page(Model0_dev, Model0_addr, Model0_size, Model0_dir, false);
}

static inline __attribute__((no_instrument_function)) void Model0_dma_sync_single_for_cpu(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_addr,
        Model0_size_t Model0_size,
        enum Model0_dma_data_direction Model0_dir)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (267), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_sync_single_for_cpu)
  Model0_ops->Model0_sync_single_for_cpu(Model0_dev, Model0_addr, Model0_size, Model0_dir);
 Model0_debug_dma_sync_single_for_cpu(Model0_dev, Model0_addr, Model0_size, Model0_dir);
}

static inline __attribute__((no_instrument_function)) void Model0_dma_sync_single_for_device(struct Model0_device *Model0_dev,
           Model0_dma_addr_t Model0_addr, Model0_size_t Model0_size,
           enum Model0_dma_data_direction Model0_dir)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (279), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_sync_single_for_device)
  Model0_ops->Model0_sync_single_for_device(Model0_dev, Model0_addr, Model0_size, Model0_dir);
 Model0_debug_dma_sync_single_for_device(Model0_dev, Model0_addr, Model0_size, Model0_dir);
}

static inline __attribute__((no_instrument_function)) void Model0_dma_sync_single_range_for_cpu(struct Model0_device *Model0_dev,
       Model0_dma_addr_t Model0_addr,
       unsigned long Model0_offset,
       Model0_size_t Model0_size,
       enum Model0_dma_data_direction Model0_dir)
{
 const struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (293), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_sync_single_for_cpu)
  Model0_ops->Model0_sync_single_for_cpu(Model0_dev, Model0_addr + Model0_offset, Model0_size, Model0_dir);
 Model0_debug_dma_sync_single_range_for_cpu(Model0_dev, Model0_addr, Model0_offset, Model0_size, Model0_dir);
}

static inline __attribute__((no_instrument_function)) void Model0_dma_sync_single_range_for_device(struct Model0_device *Model0_dev,
          Model0_dma_addr_t Model0_addr,
          unsigned long Model0_offset,
          Model0_size_t Model0_size,
          enum Model0_dma_data_direction Model0_dir)
{
 const struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (307), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_sync_single_for_device)
  Model0_ops->Model0_sync_single_for_device(Model0_dev, Model0_addr + Model0_offset, Model0_size, Model0_dir);
 Model0_debug_dma_sync_single_range_for_device(Model0_dev, Model0_addr, Model0_offset, Model0_size, Model0_dir);
}

static inline __attribute__((no_instrument_function)) void
Model0_dma_sync_sg_for_cpu(struct Model0_device *Model0_dev, struct Model0_scatterlist *Model0_sg,
      int Model0_nelems, enum Model0_dma_data_direction Model0_dir)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (319), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_sync_sg_for_cpu)
  Model0_ops->Model0_sync_sg_for_cpu(Model0_dev, Model0_sg, Model0_nelems, Model0_dir);
 Model0_debug_dma_sync_sg_for_cpu(Model0_dev, Model0_sg, Model0_nelems, Model0_dir);
}

static inline __attribute__((no_instrument_function)) void
Model0_dma_sync_sg_for_device(struct Model0_device *Model0_dev, struct Model0_scatterlist *Model0_sg,
         int Model0_nelems, enum Model0_dma_data_direction Model0_dir)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_valid_dma_direction(Model0_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (331), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_sync_sg_for_device)
  Model0_ops->Model0_sync_sg_for_device(Model0_dev, Model0_sg, Model0_nelems, Model0_dir);
 Model0_debug_dma_sync_sg_for_device(Model0_dev, Model0_sg, Model0_nelems, Model0_dir);

}






extern int Model0_dma_common_mmap(struct Model0_device *Model0_dev, struct Model0_vm_area_struct *Model0_vma,
      void *Model0_cpu_addr, Model0_dma_addr_t Model0_dma_addr, Model0_size_t Model0_size);

void *Model0_dma_common_contiguous_remap(struct Model0_page *Model0_page, Model0_size_t Model0_size,
   unsigned long Model0_vm_flags,
   Model0_pgprot_t Model0_prot, const void *Model0_caller);

void *Model0_dma_common_pages_remap(struct Model0_page **Model0_pages, Model0_size_t Model0_size,
   unsigned long Model0_vm_flags, Model0_pgprot_t Model0_prot,
   const void *Model0_caller);
void Model0_dma_common_free_remap(void *Model0_cpu_addr, Model0_size_t Model0_size, unsigned long Model0_vm_flags);

/**
 * dma_mmap_attrs - map a coherent DMA allocation into user space
 * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 * @vma: vm_area_struct describing requested user mapping
 * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
 * @handle: device-view address returned from dma_alloc_attrs
 * @size: size of memory originally requested in dma_alloc_attrs
 * @attrs: attributes of mapping properties requested in dma_alloc_attrs
 *
 * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
 * into user space.  The coherent DMA buffer must not be freed by the
 * driver until the user space mapping has been released.
 */
static inline __attribute__((no_instrument_function)) int
Model0_dma_mmap_attrs(struct Model0_device *Model0_dev, struct Model0_vm_area_struct *Model0_vma, void *Model0_cpu_addr,
        Model0_dma_addr_t Model0_dma_addr, Model0_size_t Model0_size, unsigned long Model0_attrs)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);
 do { if (__builtin_expect(!!(!Model0_ops), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (373), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_mmap)
  return Model0_ops->Model0_mmap(Model0_dev, Model0_vma, Model0_cpu_addr, Model0_dma_addr, Model0_size, Model0_attrs);
 return Model0_dma_common_mmap(Model0_dev, Model0_vma, Model0_cpu_addr, Model0_dma_addr, Model0_size);
}



int
Model0_dma_common_get_sgtable(struct Model0_device *Model0_dev, struct Model0_sg_table *Model0_sgt,
         void *Model0_cpu_addr, Model0_dma_addr_t Model0_dma_addr, Model0_size_t Model0_size);

static inline __attribute__((no_instrument_function)) int
Model0_dma_get_sgtable_attrs(struct Model0_device *Model0_dev, struct Model0_sg_table *Model0_sgt, void *Model0_cpu_addr,
        Model0_dma_addr_t Model0_dma_addr, Model0_size_t Model0_size,
        unsigned long Model0_attrs)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);
 do { if (__builtin_expect(!!(!Model0_ops), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (391), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_ops->Model0_get_sgtable)
  return Model0_ops->Model0_get_sgtable(Model0_dev, Model0_sgt, Model0_cpu_addr, Model0_dma_addr, Model0_size,
     Model0_attrs);
 return Model0_dma_common_get_sgtable(Model0_dev, Model0_sgt, Model0_cpu_addr, Model0_dma_addr, Model0_size);
}







static inline __attribute__((no_instrument_function)) void *Model0_dma_alloc_attrs(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
           Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_flag,
           unsigned long Model0_attrs)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);
 void *Model0_cpu_addr;

 do { if (__builtin_expect(!!(!Model0_ops), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (411), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);

 if ((0))
  return Model0_cpu_addr;

 if (!Model0_arch_dma_alloc_attrs(&Model0_dev, &Model0_flag))
  return ((void *)0);
 if (!Model0_ops->Model0_alloc)
  return ((void *)0);

 Model0_cpu_addr = Model0_ops->Model0_alloc(Model0_dev, Model0_size, Model0_dma_handle, Model0_flag, Model0_attrs);
 Model0_debug_dma_alloc_coherent(Model0_dev, Model0_size, *Model0_dma_handle, Model0_cpu_addr);
 return Model0_cpu_addr;
}

static inline __attribute__((no_instrument_function)) void Model0_dma_free_attrs(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
         void *Model0_cpu_addr, Model0_dma_addr_t Model0_dma_handle,
         unsigned long Model0_attrs)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 do { if (__builtin_expect(!!(!Model0_ops), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (432), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 ({ int Model0___ret_warn_on = !!(({ unsigned long Model0__flags; do { ({ unsigned long Model0___dummy; typeof(Model0__flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0__flags = Model0_arch_local_save_flags(); } while (0); ({ ({ unsigned long Model0___dummy; typeof(Model0__flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_arch_irqs_disabled_flags(Model0__flags); }); })); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/dma-mapping.h", 433); __builtin_expect(!!(Model0___ret_warn_on), 0); });

 if ((0))
  return;

 if (!Model0_ops->Model0_free || !Model0_cpu_addr)
  return;

 Model0_debug_dma_free_coherent(Model0_dev, Model0_size, Model0_cpu_addr, Model0_dma_handle);
 Model0_ops->Model0_free(Model0_dev, Model0_size, Model0_cpu_addr, Model0_dma_handle, Model0_attrs);
}

static inline __attribute__((no_instrument_function)) void *Model0_dma_alloc_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
  Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_flag)
{
 return Model0_dma_alloc_attrs(Model0_dev, Model0_size, Model0_dma_handle, Model0_flag, 0);
}

static inline __attribute__((no_instrument_function)) void Model0_dma_free_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
  void *Model0_cpu_addr, Model0_dma_addr_t Model0_dma_handle)
{
 return Model0_dma_free_attrs(Model0_dev, Model0_size, Model0_cpu_addr, Model0_dma_handle, 0);
}

static inline __attribute__((no_instrument_function)) void *Model0_dma_alloc_noncoherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
  Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_gfp)
{
 return Model0_dma_alloc_attrs(Model0_dev, Model0_size, Model0_dma_handle, Model0_gfp,
          (1UL << 3));
}

static inline __attribute__((no_instrument_function)) void Model0_dma_free_noncoherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
  void *Model0_cpu_addr, Model0_dma_addr_t Model0_dma_handle)
{
 Model0_dma_free_attrs(Model0_dev, Model0_size, Model0_cpu_addr, Model0_dma_handle,
         (1UL << 3));
}

static inline __attribute__((no_instrument_function)) int Model0_dma_mapping_error(struct Model0_device *Model0_dev, Model0_dma_addr_t Model0_dma_addr)
{
 Model0_debug_dma_mapping_error(Model0_dev, Model0_dma_addr);

 if (Model0_get_dma_ops(Model0_dev)->Model0_mapping_error)
  return Model0_get_dma_ops(Model0_dev)->Model0_mapping_error(Model0_dev, Model0_dma_addr);


 return Model0_dma_addr == 0;



}
static inline __attribute__((no_instrument_function)) int Model0_dma_set_mask(struct Model0_device *Model0_dev, Model0_u64 Model0_mask)
{
 struct Model0_dma_map_ops *Model0_ops = Model0_get_dma_ops(Model0_dev);

 if (Model0_ops->Model0_set_dma_mask)
  return Model0_ops->Model0_set_dma_mask(Model0_dev, Model0_mask);

 if (!Model0_dev->Model0_dma_mask || !Model0_dma_supported(Model0_dev, Model0_mask))
  return -5;
 *Model0_dev->Model0_dma_mask = Model0_mask;
 return 0;
}


static inline __attribute__((no_instrument_function)) Model0_u64 Model0_dma_get_mask(struct Model0_device *Model0_dev)
{
 if (Model0_dev && Model0_dev->Model0_dma_mask && *Model0_dev->Model0_dma_mask)
  return *Model0_dev->Model0_dma_mask;
 return (((32) == 64) ? ~0ULL : ((1ULL<<(32))-1));
}




static inline __attribute__((no_instrument_function)) int Model0_dma_set_coherent_mask(struct Model0_device *Model0_dev, Model0_u64 Model0_mask)
{
 if (!Model0_dma_supported(Model0_dev, Model0_mask))
  return -5;
 Model0_dev->Model0_coherent_dma_mask = Model0_mask;
 return 0;
}


/*
 * Set both the DMA mask and the coherent DMA mask to the same thing.
 * Note that we don't check the return value from dma_set_coherent_mask()
 * as the DMA API guarantees that the coherent DMA mask can be set to
 * the same or smaller than the streaming DMA mask.
 */
static inline __attribute__((no_instrument_function)) int Model0_dma_set_mask_and_coherent(struct Model0_device *Model0_dev, Model0_u64 Model0_mask)
{
 int Model0_rc = Model0_dma_set_mask(Model0_dev, Model0_mask);
 if (Model0_rc == 0)
  Model0_dma_set_coherent_mask(Model0_dev, Model0_mask);
 return Model0_rc;
}

/*
 * Similar to the above, except it deals with the case where the device
 * does not have dev->dma_mask appropriately setup.
 */
static inline __attribute__((no_instrument_function)) int Model0_dma_coerce_mask_and_coherent(struct Model0_device *Model0_dev, Model0_u64 Model0_mask)
{
 Model0_dev->Model0_dma_mask = &Model0_dev->Model0_coherent_dma_mask;
 return Model0_dma_set_mask_and_coherent(Model0_dev, Model0_mask);
}

extern Model0_u64 Model0_dma_get_required_mask(struct Model0_device *Model0_dev);


static inline __attribute__((no_instrument_function)) void Model0_arch_setup_dma_ops(struct Model0_device *Model0_dev, Model0_u64 Model0_dma_base,
          Model0_u64 Model0_size, const struct Model0_iommu_ops *Model0_iommu,
          bool Model0_coherent) { }



static inline __attribute__((no_instrument_function)) void Model0_arch_teardown_dma_ops(struct Model0_device *Model0_dev) { }


static inline __attribute__((no_instrument_function)) unsigned int Model0_dma_get_max_seg_size(struct Model0_device *Model0_dev)
{
 if (Model0_dev->Model0_dma_parms && Model0_dev->Model0_dma_parms->Model0_max_segment_size)
  return Model0_dev->Model0_dma_parms->Model0_max_segment_size;
 return 0x00010000;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_dma_set_max_seg_size(struct Model0_device *Model0_dev,
      unsigned int Model0_size)
{
 if (Model0_dev->Model0_dma_parms) {
  Model0_dev->Model0_dma_parms->Model0_max_segment_size = Model0_size;
  return 0;
 }
 return -5;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_dma_get_seg_boundary(struct Model0_device *Model0_dev)
{
 if (Model0_dev->Model0_dma_parms && Model0_dev->Model0_dma_parms->Model0_segment_boundary_mask)
  return Model0_dev->Model0_dma_parms->Model0_segment_boundary_mask;
 return (((32) == 64) ? ~0ULL : ((1ULL<<(32))-1));
}

static inline __attribute__((no_instrument_function)) int Model0_dma_set_seg_boundary(struct Model0_device *Model0_dev, unsigned long Model0_mask)
{
 if (Model0_dev->Model0_dma_parms) {
  Model0_dev->Model0_dma_parms->Model0_segment_boundary_mask = Model0_mask;
  return 0;
 }
 return -5;
}


static inline __attribute__((no_instrument_function)) unsigned long Model0_dma_max_pfn(struct Model0_device *Model0_dev)
{
 return *Model0_dev->Model0_dma_mask >> 12;
}


static inline __attribute__((no_instrument_function)) void *Model0_dma_zalloc_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
     Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_flag)
{
 void *Model0_ret = Model0_dma_alloc_coherent(Model0_dev, Model0_size, Model0_dma_handle,
           Model0_flag | (( Model0_gfp_t)0x8000u));
 return Model0_ret;
}


static inline __attribute__((no_instrument_function)) int Model0_dma_get_cache_alignment(void)
{



 return 1;
}


/* flags for the coherent memory api */
static inline __attribute__((no_instrument_function)) int
Model0_dma_declare_coherent_memory(struct Model0_device *Model0_dev, Model0_phys_addr_t Model0_phys_addr,
       Model0_dma_addr_t Model0_device_addr, Model0_size_t Model0_size, int Model0_flags)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void
Model0_dma_release_declared_memory(struct Model0_device *Model0_dev)
{
}

static inline __attribute__((no_instrument_function)) void *
Model0_dma_mark_declared_memory_occupied(struct Model0_device *Model0_dev,
      Model0_dma_addr_t Model0_device_addr, Model0_size_t Model0_size)
{
 return Model0_ERR_PTR(-16);
}


/*
 * Managed DMA API
 */
extern void *Model0_dmam_alloc_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
     Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_gfp);
extern void Model0_dmam_free_coherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size, void *Model0_vaddr,
          Model0_dma_addr_t Model0_dma_handle);
extern void *Model0_dmam_alloc_noncoherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
        Model0_dma_addr_t *Model0_dma_handle, Model0_gfp_t Model0_gfp);
extern void Model0_dmam_free_noncoherent(struct Model0_device *Model0_dev, Model0_size_t Model0_size, void *Model0_vaddr,
      Model0_dma_addr_t Model0_dma_handle);







static inline __attribute__((no_instrument_function)) int Model0_dmam_declare_coherent_memory(struct Model0_device *Model0_dev,
    Model0_phys_addr_t Model0_phys_addr, Model0_dma_addr_t Model0_device_addr,
    Model0_size_t Model0_size, Model0_gfp_t Model0_gfp)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_dmam_release_declared_memory(struct Model0_device *Model0_dev)
{
}


static inline __attribute__((no_instrument_function)) void *Model0_dma_alloc_wc(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
     Model0_dma_addr_t *Model0_dma_addr, Model0_gfp_t Model0_gfp)
{
 return Model0_dma_alloc_attrs(Model0_dev, Model0_size, Model0_dma_addr, Model0_gfp,
          (1UL << 2));
}




static inline __attribute__((no_instrument_function)) void Model0_dma_free_wc(struct Model0_device *Model0_dev, Model0_size_t Model0_size,
          void *Model0_cpu_addr, Model0_dma_addr_t Model0_dma_addr)
{
 return Model0_dma_free_attrs(Model0_dev, Model0_size, Model0_cpu_addr, Model0_dma_addr,
         (1UL << 2));
}




static inline __attribute__((no_instrument_function)) int Model0_dma_mmap_wc(struct Model0_device *Model0_dev,
         struct Model0_vm_area_struct *Model0_vma,
         void *Model0_cpu_addr, Model0_dma_addr_t Model0_dma_addr,
         Model0_size_t Model0_size)
{
 return Model0_dma_mmap_attrs(Model0_dev, Model0_vma, Model0_cpu_addr, Model0_dma_addr, Model0_size,
         (1UL << 2));
}
/*
 * Network device features.
 *
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 */





typedef Model0_u64 Model0_netdev_features_t;

enum {
 Model0_NETIF_F_SG_BIT, /* Scatter/gather IO. */
 Model0_NETIF_F_IP_CSUM_BIT, /* Can checksum TCP/UDP over IPv4. */
 Model0___UNUSED_NETIF_F_1,
 Model0_NETIF_F_HW_CSUM_BIT, /* Can checksum all the packets. */
 Model0_NETIF_F_IPV6_CSUM_BIT, /* Can checksum TCP/UDP over IPV6 */
 Model0_NETIF_F_HIGHDMA_BIT, /* Can DMA to high memory. */
 Model0_NETIF_F_FRAGLIST_BIT, /* Scatter/gather IO. */
 Model0_NETIF_F_HW_VLAN_CTAG_TX_BIT, /* Transmit VLAN CTAG HW acceleration */
 Model0_NETIF_F_HW_VLAN_CTAG_RX_BIT, /* Receive VLAN CTAG HW acceleration */
 Model0_NETIF_F_HW_VLAN_CTAG_FILTER_BIT,/* Receive filtering on VLAN CTAGs */
 Model0_NETIF_F_VLAN_CHALLENGED_BIT, /* Device cannot handle VLAN packets */
 Model0_NETIF_F_GSO_BIT, /* Enable software GSO. */
 Model0_NETIF_F_LLTX_BIT, /* LockLess TX - deprecated. Please */
     /* do not use LLTX in new drivers */
 Model0_NETIF_F_NETNS_LOCAL_BIT, /* Does not change network namespaces */
 Model0_NETIF_F_GRO_BIT, /* Generic receive offload */
 Model0_NETIF_F_LRO_BIT, /* large receive offload */

 /**/Model0_NETIF_F_GSO_SHIFT, /* keep the order of SKB_GSO_* bits */
 Model0_NETIF_F_TSO_BIT /* ... TCPv4 segmentation */
  = Model0_NETIF_F_GSO_SHIFT,
 Model0_NETIF_F_UFO_BIT, /* ... UDPv4 fragmentation */
 Model0_NETIF_F_GSO_ROBUST_BIT, /* ... ->SKB_GSO_DODGY */
 Model0_NETIF_F_TSO_ECN_BIT, /* ... TCP ECN support */
 Model0_NETIF_F_TSO_MANGLEID_BIT, /* ... IPV4 ID mangling allowed */
 Model0_NETIF_F_TSO6_BIT, /* ... TCPv6 segmentation */
 Model0_NETIF_F_FSO_BIT, /* ... FCoE segmentation */
 Model0_NETIF_F_GSO_GRE_BIT, /* ... GRE with TSO */
 Model0_NETIF_F_GSO_GRE_CSUM_BIT, /* ... GRE with csum with TSO */
 Model0_NETIF_F_GSO_IPXIP4_BIT, /* ... IP4 or IP6 over IP4 with TSO */
 Model0_NETIF_F_GSO_IPXIP6_BIT, /* ... IP4 or IP6 over IP6 with TSO */
 Model0_NETIF_F_GSO_UDP_TUNNEL_BIT, /* ... UDP TUNNEL with TSO */
 Model0_NETIF_F_GSO_UDP_TUNNEL_CSUM_BIT,/* ... UDP TUNNEL with TSO & CSUM */
 Model0_NETIF_F_GSO_PARTIAL_BIT, /* ... Only segment inner-most L4
					 *     in hardware and all other
					 *     headers in software.
					 */
 Model0_NETIF_F_GSO_TUNNEL_REMCSUM_BIT, /* ... TUNNEL with TSO & REMCSUM */
 Model0_NETIF_F_GSO_SCTP_BIT, /* ... SCTP fragmentation */
 /**/Model0_NETIF_F_GSO_LAST = /* last bit, see GSO_MASK */
  Model0_NETIF_F_GSO_SCTP_BIT,

 Model0_NETIF_F_FCOE_CRC_BIT, /* FCoE CRC32 */
 Model0_NETIF_F_SCTP_CRC_BIT, /* SCTP checksum offload */
 Model0_NETIF_F_FCOE_MTU_BIT, /* Supports max FCoE MTU, 2158 bytes*/
 Model0_NETIF_F_NTUPLE_BIT, /* N-tuple filters supported */
 Model0_NETIF_F_RXHASH_BIT, /* Receive hashing offload */
 Model0_NETIF_F_RXCSUM_BIT, /* Receive checksumming offload */
 Model0_NETIF_F_NOCACHE_COPY_BIT, /* Use no-cache copyfromuser */
 Model0_NETIF_F_LOOPBACK_BIT, /* Enable loopback */
 Model0_NETIF_F_RXFCS_BIT, /* Append FCS to skb pkt data */
 Model0_NETIF_F_RXALL_BIT, /* Receive errored frames too */
 Model0_NETIF_F_HW_VLAN_STAG_TX_BIT, /* Transmit VLAN STAG HW acceleration */
 Model0_NETIF_F_HW_VLAN_STAG_RX_BIT, /* Receive VLAN STAG HW acceleration */
 Model0_NETIF_F_HW_VLAN_STAG_FILTER_BIT,/* Receive filtering on VLAN STAGs */
 Model0_NETIF_F_HW_L2FW_DOFFLOAD_BIT, /* Allow L2 Forwarding in Hardware */
 Model0_NETIF_F_BUSY_POLL_BIT, /* Busy poll */

 Model0_NETIF_F_HW_TC_BIT, /* Offload TC infrastructure */

 /*
	 * Add your fresh new feature above and remember to update
	 * netdev_features_strings[] in net/core/ethtool.c and maybe
	 * some feature mask #defines below. Please also describe it
	 * in Documentation/networking/netdev-features.txt.
	 */

 /**/Model0_NETDEV_FEATURE_COUNT
};

/* copy'n'paste compression ;) */
/* Features valid for ethtool to change */
/* = all defined minus driver/device-class-related */



/* remember that ((t)1 << t_BITS) is undefined in C99 */




/* Segmentation offload feature mask */



/* List of IP checksum features. Note that NETIF_F_ HW_CSUM should not be
 * set in features when NETIF_F_IP_CSUM or NETIF_F_IPV6_CSUM are set--
 * this would be contradictory
 */
/* List of features with software fallbacks. */



/*
 * If one device supports one of these features, then enable them
 * for all in netdev_increment_features.
 */




/*
 * If one device doesn't support one of these features, then disable it
 * for all in netdev_increment_features.
 */


/*
 * If upper/master device has these features disabled, they must be disabled
 * on all lower/slave devices as well.
 */


/* changeable features with no special hardware requirements */





/*
 *	Types and definitions for AF_INET6 
 *	Linux INET6 implementation 
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>	
 *
 *	Sources:
 *	IPv6 Program Interfaces for BSD Systems
 *      <draft-ietf-ipngwg-bsd-api-05.txt>
 *
 *	Advanced Sockets API for IPv6
 *	<draft-stevens-advanced-api-00.txt>
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */




/*
 *	Types and definitions for AF_INET6 
 *	Linux INET6 implementation 
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>	
 *
 *	Sources:
 *	IPv6 Program Interfaces for BSD Systems
 *      <draft-ietf-ipngwg-bsd-api-05.txt>
 *
 *	Advanced Sockets API for IPv6
 *	<draft-stevens-advanced-api-00.txt>
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */







/*
 *	IPv6 address structure
 */


struct Model0_in6_addr {
 union {
  __u8 Model0_u6_addr8[16];

  Model0___be16 Model0_u6_addr16[8];
  Model0___be32 Model0_u6_addr32[4];

 } Model0_in6_u;





};



struct Model0_sockaddr_in6 {
 unsigned short int Model0_sin6_family; /* AF_INET6 */
 Model0___be16 Model0_sin6_port; /* Transport layer port # */
 Model0___be32 Model0_sin6_flowinfo; /* IPv6 flow information */
 struct Model0_in6_addr Model0_sin6_addr; /* IPv6 address */
 __u32 Model0_sin6_scope_id; /* scope id (new in RFC2553) */
};



struct Model0_ipv6_mreq {
 /* IPv6 multicast address of group */
 struct Model0_in6_addr Model0_ipv6mr_multiaddr;

 /* local IPv6 address of interface */
 int Model0_ipv6mr_ifindex;
};




struct Model0_in6_flowlabel_req {
 struct Model0_in6_addr Model0_flr_dst;
 Model0___be32 Model0_flr_label;
 __u8 Model0_flr_action;
 __u8 Model0_flr_share;
 Model0___u16 Model0_flr_flags;
 Model0___u16 Model0_flr_expires;
 Model0___u16 Model0_flr_linger;
 __u32 Model0___flr_pad;
 /* Options in format of IPV6_PKTOPTIONS */
};
/*
 *	Bitmask constant declarations to help applications select out the 
 *	flow label and priority fields.
 *
 *	Note that this are in host byte order while the flowinfo field of
 *	sockaddr_in6 is in network byte order.
 */




/* These definitions are obsolete */
/*
 *	IPV6 extension headers
 */
/*
 *	IPv6 TLV options.
 */







/*
 *	IPV6 socket options
 */
/* IPV6_MTU_DISCOVER values */




/* same as IPV6_PMTUDISC_PROBE, provided for symetry with IPv4
 * also see comments on IP_PMTUDISC_INTERFACE
 */

/* weaker version of IPV6_PMTUDISC_INTERFACE, which allows packets to
 * get fragmented if they exceed the interface mtu
 */


/* Flowlabel */
/*
 * Multicast:
 * Following socket options are shared between IPv4 and IPv6.
 *
 * MCAST_JOIN_GROUP		42
 * MCAST_BLOCK_SOURCE		43
 * MCAST_UNBLOCK_SOURCE		44
 * MCAST_LEAVE_GROUP		45
 * MCAST_JOIN_SOURCE_GROUP	46
 * MCAST_LEAVE_SOURCE_GROUP	47
 * MCAST_MSFILTER		48
 */

/*
 * Advanced API (RFC3542) (1)
 *
 * Note: IPV6_RECVRTHDRDSTOPTS does not exist. see net/ipv6/datagram.c.
 */
/*
 * Netfilter (1)
 *
 * Following socket options are used in ip6_tables;
 * see include/linux/netfilter_ipv6/ip6_tables.h.
 *
 * IP6T_SO_SET_REPLACE / IP6T_SO_GET_INFO		64
 * IP6T_SO_SET_ADD_COUNTERS / IP6T_SO_GET_ENTRIES	65
 */

/*
 * Advanced API (RFC3542) (2)
 */



/*
 * Netfilter (2)
 *
 * Following socket options are used in ip6_tables;
 * see include/linux/netfilter_ipv6/ip6_tables.h.
 *
 * IP6T_SO_GET_REVISION_MATCH	68
 * IP6T_SO_GET_REVISION_TARGET	69
 * IP6T_SO_ORIGINAL_DST		80
 */


/* RFC5014: Source address selection */
/* RFC5082: Generalized Ttl Security Mechanism */







/*
 * Multicast Routing:
 * see include/uapi/linux/mroute6.h.
 *
 * MRT6_BASE			200
 * ...
 * MRT6_MAX
 */

/* IPv6 Wildcard Address (::) and Loopback Address (::1) defined in RFC2553
 * NOTE: Be aware the IN6ADDR_* constants and in6addr_* externals are defined
 * in network byte order, not in host byte order as are the IPv4 equivalents
 */
extern const struct Model0_in6_addr Model0_in6addr_any;

extern const struct Model0_in6_addr Model0_in6addr_loopback;

extern const struct Model0_in6_addr Model0_in6addr_linklocal_allnodes;


extern const struct Model0_in6_addr Model0_in6addr_linklocal_allrouters;


extern const struct Model0_in6_addr Model0_in6addr_interfacelocal_allnodes;


extern const struct Model0_in6_addr Model0_in6addr_interfacelocal_allrouters;


extern const struct Model0_in6_addr Model0_in6addr_sitelocal_allrouters;
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the Ethernet IEEE 802.3 interface.
 *
 * Version:	@(#)if_ether.h	1.0.1a	02/08/94
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Donald Becker, <becker@super.org>
 *		Alan Cox, <alan@lxorguk.ukuu.org.uk>
 *		Steve Whitehouse, <gw7rrm@eeshack3.swan.ac.uk>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






/*
 *	IEEE 802.3 Ethernet magic constants.  The frame sizes omit the preamble
 *	and FCS/CRC (frame check sequence).
 */
/*
 *	These are the defined Ethernet Protocol ID's.
 */
/*
 *	Non DIX types. Won't clash for 1500 types.
 */
/*
 *	This is an Ethernet frame header.
 */

struct Model0_ethhdr {
 unsigned char Model0_h_dest[6]; /* destination eth addr	*/
 unsigned char Model0_h_source[6]; /* source ether addr	*/
 Model0___be16 Model0_h_proto; /* packet type ID field	*/
} __attribute__((packed));

/**
 * struct flow_dissector_key_control:
 * @thoff: Transport header offset
 */
struct Model0_flow_dissector_key_control {
 Model0_u16 Model0_thoff;
 Model0_u16 Model0_addr_type;
 Model0_u32 Model0_flags;
};





/**
 * struct flow_dissector_key_basic:
 * @thoff: Transport header offset
 * @n_proto: Network header protocol (eg. IPv4/IPv6)
 * @ip_proto: Transport header protocol (eg. TCP/UDP)
 */
struct Model0_flow_dissector_key_basic {
 Model0___be16 Model0_n_proto;
 Model0_u8 Model0_ip_proto;
 Model0_u8 Model0_padding;
};

struct Model0_flow_dissector_key_tags {
 Model0_u32 Model0_vlan_id:12,
  Model0_flow_label:20;
};

struct Model0_flow_dissector_key_keyid {
 Model0___be32 Model0_keyid;
};

/**
 * struct flow_dissector_key_ipv4_addrs:
 * @src: source ip address
 * @dst: destination ip address
 */
struct Model0_flow_dissector_key_ipv4_addrs {
 /* (src,dst) must be grouped, in the same way than in IP header */
 Model0___be32 Model0_src;
 Model0___be32 Model0_dst;
};

/**
 * struct flow_dissector_key_ipv6_addrs:
 * @src: source ip address
 * @dst: destination ip address
 */
struct Model0_flow_dissector_key_ipv6_addrs {
 /* (src,dst) must be grouped, in the same way than in IP header */
 struct Model0_in6_addr Model0_src;
 struct Model0_in6_addr Model0_dst;
};

/**
 * struct flow_dissector_key_tipc_addrs:
 * @srcnode: source node address
 */
struct Model0_flow_dissector_key_tipc_addrs {
 Model0___be32 Model0_srcnode;
};

/**
 * struct flow_dissector_key_addrs:
 * @v4addrs: IPv4 addresses
 * @v6addrs: IPv6 addresses
 */
struct Model0_flow_dissector_key_addrs {
 union {
  struct Model0_flow_dissector_key_ipv4_addrs Model0_v4addrs;
  struct Model0_flow_dissector_key_ipv6_addrs Model0_v6addrs;
  struct Model0_flow_dissector_key_tipc_addrs Model0_tipcaddrs;
 };
};

/**
 * flow_dissector_key_tp_ports:
 *	@ports: port numbers of Transport header
 *		src: source port number
 *		dst: destination port number
 */
struct Model0_flow_dissector_key_ports {
 union {
  Model0___be32 Model0_ports;
  struct {
   Model0___be16 Model0_src;
   Model0___be16 Model0_dst;
  };
 };
};


/**
 * struct flow_dissector_key_eth_addrs:
 * @src: source Ethernet address
 * @dst: destination Ethernet address
 */
struct Model0_flow_dissector_key_eth_addrs {
 /* (dst,src) must be grouped, in the same way than in ETH header */
 unsigned char Model0_dst[6];
 unsigned char Model0_src[6];
};

enum Model0_flow_dissector_key_id {
 Model0_FLOW_DISSECTOR_KEY_CONTROL, /* struct flow_dissector_key_control */
 Model0_FLOW_DISSECTOR_KEY_BASIC, /* struct flow_dissector_key_basic */
 Model0_FLOW_DISSECTOR_KEY_IPV4_ADDRS, /* struct flow_dissector_key_ipv4_addrs */
 Model0_FLOW_DISSECTOR_KEY_IPV6_ADDRS, /* struct flow_dissector_key_ipv6_addrs */
 Model0_FLOW_DISSECTOR_KEY_PORTS, /* struct flow_dissector_key_ports */
 Model0_FLOW_DISSECTOR_KEY_ETH_ADDRS, /* struct flow_dissector_key_eth_addrs */
 Model0_FLOW_DISSECTOR_KEY_TIPC_ADDRS, /* struct flow_dissector_key_tipc_addrs */
 Model0_FLOW_DISSECTOR_KEY_VLANID, /* struct flow_dissector_key_flow_tags */
 Model0_FLOW_DISSECTOR_KEY_FLOW_LABEL, /* struct flow_dissector_key_flow_tags */
 Model0_FLOW_DISSECTOR_KEY_GRE_KEYID, /* struct flow_dissector_key_keyid */
 Model0_FLOW_DISSECTOR_KEY_MPLS_ENTROPY, /* struct flow_dissector_key_keyid */

 Model0_FLOW_DISSECTOR_KEY_MAX,
};






struct Model0_flow_dissector_key {
 enum Model0_flow_dissector_key_id Model0_key_id;
 Model0_size_t Model0_offset; /* offset of struct flow_dissector_key_*
			  in target the struct */
};

struct Model0_flow_dissector {
 unsigned int Model0_used_keys; /* each bit repesents presence of one key id */
 unsigned short int Model0_offset[Model0_FLOW_DISSECTOR_KEY_MAX];
};

struct Model0_flow_keys {
 struct Model0_flow_dissector_key_control Model0_control;

 struct Model0_flow_dissector_key_basic Model0_basic;
 struct Model0_flow_dissector_key_tags Model0_tags;
 struct Model0_flow_dissector_key_keyid Model0_keyid;
 struct Model0_flow_dissector_key_ports Model0_ports;
 struct Model0_flow_dissector_key_addrs Model0_addrs;
};




Model0___be32 Model0_flow_get_u32_src(const struct Model0_flow_keys *Model0_flow);
Model0___be32 Model0_flow_get_u32_dst(const struct Model0_flow_keys *Model0_flow);

extern struct Model0_flow_dissector Model0_flow_keys_dissector;
extern struct Model0_flow_dissector Model0_flow_keys_buf_dissector;

/* struct flow_keys_digest:
 *
 * This structure is used to hold a digest of the full flow keys. This is a
 * larger "hash" of a flow to allow definitively matching specific flows where
 * the 32 bit skb->hash is not large enough. The size is limited to 16 bytes so
 * that it can by used in CB of skb (see sch_choke for an example).
 */

struct Model0_flow_keys_digest {
 Model0_u8 Model0_data[16];
};

void Model0_make_flow_keys_digest(struct Model0_flow_keys_digest *Model0_digest,
      const struct Model0_flow_keys *Model0_flow);

static inline __attribute__((no_instrument_function)) bool Model0_flow_keys_have_l4(struct Model0_flow_keys *Model0_keys)
{
 return (Model0_keys->Model0_ports.Model0_ports || Model0_keys->Model0_tags.Model0_flow_label);
}

Model0_u32 Model0_flow_hash_from_keys(struct Model0_flow_keys *Model0_keys);

static inline __attribute__((no_instrument_function)) bool Model0_dissector_uses_key(const struct Model0_flow_dissector *Model0_flow_dissector,
          enum Model0_flow_dissector_key_id Model0_key_id)
{
 return Model0_flow_dissector->Model0_used_keys & (1 << Model0_key_id);
}

static inline __attribute__((no_instrument_function)) void *Model0_skb_flow_dissector_target(struct Model0_flow_dissector *Model0_flow_dissector,
           enum Model0_flow_dissector_key_id Model0_key_id,
           void *Model0_target_container)
{
 return ((char *)Model0_target_container) + Model0_flow_dissector->Model0_offset[Model0_key_id];
}
/*
 * Function declerations and data structures related to the splice
 * implementation.
 *
 * Copyright (C) 2007 Jens Axboe <jens.axboe@oracle.com>
 *
 */




/**
 *	struct pipe_buffer - a linux kernel pipe buffer
 *	@page: the page containing the data for the pipe buffer
 *	@offset: offset of data inside the @page
 *	@len: length of data inside the @page
 *	@ops: operations associated with this buffer. See @pipe_buf_operations.
 *	@flags: pipe buffer flags. See above.
 *	@private: private data owned by the ops.
 **/
struct Model0_pipe_buffer {
 struct Model0_page *Model0_page;
 unsigned int Model0_offset, Model0_len;
 const struct Model0_pipe_buf_operations *Model0_ops;
 unsigned int Model0_flags;
 unsigned long Model0_private;
};

/**
 *	struct pipe_inode_info - a linux kernel pipe
 *	@mutex: mutex protecting the whole thing
 *	@wait: reader/writer wait point in case of empty/full pipe
 *	@nrbufs: the number of non-empty pipe buffers in this pipe
 *	@buffers: total number of buffers (should be a power of 2)
 *	@curbuf: the current pipe buffer entry
 *	@tmp_page: cached released page
 *	@readers: number of current readers of this pipe
 *	@writers: number of current writers of this pipe
 *	@files: number of struct file referring this pipe (protected by ->i_lock)
 *	@waiting_writers: number of writers blocked waiting for room
 *	@r_counter: reader counter
 *	@w_counter: writer counter
 *	@fasync_readers: reader side fasync
 *	@fasync_writers: writer side fasync
 *	@bufs: the circular array of pipe buffers
 *	@user: the user who created this pipe
 **/
struct Model0_pipe_inode_info {
 struct Model0_mutex Model0_mutex;
 Model0_wait_queue_head_t Model0_wait;
 unsigned int Model0_nrbufs, Model0_curbuf, Model0_buffers;
 unsigned int Model0_readers;
 unsigned int Model0_writers;
 unsigned int Model0_files;
 unsigned int Model0_waiting_writers;
 unsigned int Model0_r_counter;
 unsigned int Model0_w_counter;
 struct Model0_page *Model0_tmp_page;
 struct Model0_fasync_struct *Model0_fasync_readers;
 struct Model0_fasync_struct *Model0_fasync_writers;
 struct Model0_pipe_buffer *Model0_bufs;
 struct Model0_user_struct *Model0_user;
};

/*
 * Note on the nesting of these functions:
 *
 * ->confirm()
 *	->steal()
 *	...
 *	->map()
 *	...
 *	->unmap()
 *
 * That is, ->map() must be called on a confirmed buffer,
 * same goes for ->steal(). See below for the meaning of each
 * operation. Also see kerneldoc in fs/pipe.c for the pipe
 * and generic variants of these hooks.
 */
struct Model0_pipe_buf_operations {
 /*
	 * This is set to 1, if the generic pipe read/write may coalesce
	 * data into an existing buffer. If this is set to 0, a new pipe
	 * page segment is always used for new data.
	 */
 int Model0_can_merge;

 /*
	 * ->confirm() verifies that the data in the pipe buffer is there
	 * and that the contents are good. If the pages in the pipe belong
	 * to a file system, we may need to wait for IO completion in this
	 * hook. Returns 0 for good, or a negative error value in case of
	 * error.
	 */
 int (*Model0_confirm)(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *);

 /*
	 * When the contents of this pipe buffer has been completely
	 * consumed by a reader, ->release() is called.
	 */
 void (*Model0_release)(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *);

 /*
	 * Attempt to take ownership of the pipe buffer and its contents.
	 * ->steal() returns 0 for success, in which case the contents
	 * of the pipe (the buf->page) is locked and now completely owned
	 * by the caller. The page may then be transferred to a different
	 * mapping, the most often used case is insertion into different
	 * file address space cache.
	 */
 int (*Model0_steal)(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *);

 /*
	 * Get a reference to the pipe buffer.
	 */
 void (*Model0_get)(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *);
};

/* Differs from PIPE_BUF in that PIPE_SIZE is the length of the actual
   memory allocation, whereas PIPE_BUF makes atomicity guarantees.  */


/* Pipe lock and unlock operations */
void Model0_pipe_lock(struct Model0_pipe_inode_info *);
void Model0_pipe_unlock(struct Model0_pipe_inode_info *);
void Model0_pipe_double_lock(struct Model0_pipe_inode_info *, struct Model0_pipe_inode_info *);

extern unsigned int Model0_pipe_max_size, Model0_pipe_min_size;
extern unsigned long Model0_pipe_user_pages_hard;
extern unsigned long Model0_pipe_user_pages_soft;
int Model0_pipe_proc_fn(struct Model0_ctl_table *, int, void *, Model0_size_t *, Model0_loff_t *);


/* Drop the inode semaphore and wait for a pipe event, atomically */
void Model0_pipe_wait(struct Model0_pipe_inode_info *Model0_pipe);

struct Model0_pipe_inode_info *Model0_alloc_pipe_info(void);
void Model0_free_pipe_info(struct Model0_pipe_inode_info *);

/* Generic pipe buffer ops functions */
void Model0_generic_pipe_buf_get(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *);
int Model0_generic_pipe_buf_confirm(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *);
int Model0_generic_pipe_buf_steal(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *);
void Model0_generic_pipe_buf_release(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *);

extern const struct Model0_pipe_buf_operations Model0_nosteal_pipe_buf_ops;

/* for F_SETPIPE_SZ and F_GETPIPE_SZ */
long Model0_pipe_fcntl(struct Model0_file *, unsigned int, unsigned long Model0_arg);
struct Model0_pipe_inode_info *Model0_get_pipe_info(struct Model0_file *Model0_file);

int Model0_create_pipe_files(struct Model0_file **, int);

/*
 * Flags passed in from splice/tee/vmsplice
 */


     /* we may still block on the fd we splice */
     /* from/to, of course */



/*
 * Passed to the actors
 */
struct Model0_splice_desc {
 Model0_size_t Model0_total_len; /* remaining length */
 unsigned int Model0_len; /* current length */
 unsigned int Model0_flags; /* splice flags */
 /*
	 * actor() private data
	 */
 union {
  void *Model0_userptr; /* memory to write to */
  struct Model0_file *Model0_file; /* file to read/write */
  void *Model0_data; /* cookie */
 } Model0_u;
 Model0_loff_t Model0_pos; /* file position */
 Model0_loff_t *Model0_opos; /* sendfile: output position */
 Model0_size_t Model0_num_spliced; /* number of bytes already spliced */
 bool Model0_need_wakeup; /* need to wake up writer */
};

struct Model0_partial_page {
 unsigned int Model0_offset;
 unsigned int Model0_len;
 unsigned long Model0_private;
};

/*
 * Passed to splice_to_pipe
 */
struct Model0_splice_pipe_desc {
 struct Model0_page **Model0_pages; /* page map */
 struct Model0_partial_page *Model0_partial; /* pages[] may not be contig */
 int Model0_nr_pages; /* number of populated pages in map */
 unsigned int Model0_nr_pages_max; /* pages[] & partial[] arrays size */
 unsigned int Model0_flags; /* splice flags */
 const struct Model0_pipe_buf_operations *Model0_ops;/* ops associated with output pipe */
 void (*Model0_spd_release)(struct Model0_splice_pipe_desc *, unsigned int);
};

typedef int (Model0_splice_actor)(struct Model0_pipe_inode_info *, struct Model0_pipe_buffer *,
      struct Model0_splice_desc *);
typedef int (Model0_splice_direct_actor)(struct Model0_pipe_inode_info *,
      struct Model0_splice_desc *);

extern Model0_ssize_t Model0_splice_from_pipe(struct Model0_pipe_inode_info *, struct Model0_file *,
    Model0_loff_t *, Model0_size_t, unsigned int,
    Model0_splice_actor *);
extern Model0_ssize_t Model0___splice_from_pipe(struct Model0_pipe_inode_info *,
      struct Model0_splice_desc *, Model0_splice_actor *);
extern Model0_ssize_t Model0_splice_to_pipe(struct Model0_pipe_inode_info *,
         struct Model0_splice_pipe_desc *);
extern Model0_ssize_t Model0_splice_direct_to_actor(struct Model0_file *, struct Model0_splice_desc *,
          Model0_splice_direct_actor *);

/*
 * for dynamic pipe sizing
 */
extern int Model0_splice_grow_spd(const struct Model0_pipe_inode_info *, struct Model0_splice_pipe_desc *);
extern void Model0_splice_shrink_spd(struct Model0_splice_pipe_desc *);
extern void Model0_spd_release_page(struct Model0_splice_pipe_desc *, unsigned int);

extern const struct Model0_pipe_buf_operations Model0_page_cache_pipe_buf_ops;






struct Model0_sockaddr_pkt {
 unsigned short Model0_spkt_family;
 unsigned char Model0_spkt_device[14];
 Model0___be16 Model0_spkt_protocol;
};

struct Model0_sockaddr_ll {
 unsigned short Model0_sll_family;
 Model0___be16 Model0_sll_protocol;
 int Model0_sll_ifindex;
 unsigned short Model0_sll_hatype;
 unsigned char Model0_sll_pkttype;
 unsigned char Model0_sll_halen;
 unsigned char Model0_sll_addr[8];
};

/* Packet types */
/* Unused, PACKET_FASTROUTE and PACKET_LOOPBACK are invisible to user space */


/* Packet socket options */




/* Value 4 is still used by obsolete turbo-packet. */
struct Model0_tpacket_stats {
 unsigned int Model0_tp_packets;
 unsigned int Model0_tp_drops;
};

struct Model0_tpacket_stats_v3 {
 unsigned int Model0_tp_packets;
 unsigned int Model0_tp_drops;
 unsigned int Model0_tp_freeze_q_cnt;
};

struct Model0_tpacket_rollover_stats {
 __u64 __attribute__((aligned(8))) Model0_tp_all;
 __u64 __attribute__((aligned(8))) Model0_tp_huge;
 __u64 __attribute__((aligned(8))) Model0_tp_failed;
};

union Model0_tpacket_stats_u {
 struct Model0_tpacket_stats Model0_stats1;
 struct Model0_tpacket_stats_v3 Model0_stats3;
};

struct Model0_tpacket_auxdata {
 __u32 Model0_tp_status;
 __u32 Model0_tp_len;
 __u32 Model0_tp_snaplen;
 Model0___u16 Model0_tp_mac;
 Model0___u16 Model0_tp_net;
 Model0___u16 Model0_tp_vlan_tci;
 Model0___u16 Model0_tp_vlan_tpid;
};

/* Rx ring - header status */
/* Tx ring - header status */





/* Rx and Tx ring - header status */




/* Rx ring - feature request bits */


struct Model0_tpacket_hdr {
 unsigned long Model0_tp_status;
 unsigned int Model0_tp_len;
 unsigned int Model0_tp_snaplen;
 unsigned short Model0_tp_mac;
 unsigned short Model0_tp_net;
 unsigned int Model0_tp_sec;
 unsigned int Model0_tp_usec;
};





struct Model0_tpacket2_hdr {
 __u32 Model0_tp_status;
 __u32 Model0_tp_len;
 __u32 Model0_tp_snaplen;
 Model0___u16 Model0_tp_mac;
 Model0___u16 Model0_tp_net;
 __u32 Model0_tp_sec;
 __u32 Model0_tp_nsec;
 Model0___u16 Model0_tp_vlan_tci;
 Model0___u16 Model0_tp_vlan_tpid;
 __u8 Model0_tp_padding[4];
};

struct Model0_tpacket_hdr_variant1 {
 __u32 Model0_tp_rxhash;
 __u32 Model0_tp_vlan_tci;
 Model0___u16 Model0_tp_vlan_tpid;
 Model0___u16 Model0_tp_padding;
};

struct Model0_tpacket3_hdr {
 __u32 Model0_tp_next_offset;
 __u32 Model0_tp_sec;
 __u32 Model0_tp_nsec;
 __u32 Model0_tp_snaplen;
 __u32 Model0_tp_len;
 __u32 Model0_tp_status;
 Model0___u16 Model0_tp_mac;
 Model0___u16 Model0_tp_net;
 /* pkt_hdr variants */
 union {
  struct Model0_tpacket_hdr_variant1 Model0_hv1;
 };
 __u8 Model0_tp_padding[8];
};

struct Model0_tpacket_bd_ts {
 unsigned int Model0_ts_sec;
 union {
  unsigned int Model0_ts_usec;
  unsigned int Model0_ts_nsec;
 };
};

struct Model0_tpacket_hdr_v1 {
 __u32 Model0_block_status;
 __u32 Model0_num_pkts;
 __u32 Model0_offset_to_first_pkt;

 /* Number of valid bytes (including padding)
	 * blk_len <= tp_block_size
	 */
 __u32 Model0_blk_len;

 /*
	 * Quite a few uses of sequence number:
	 * 1. Make sure cache flush etc worked.
	 *    Well, one can argue - why not use the increasing ts below?
	 *    But look at 2. below first.
	 * 2. When you pass around blocks to other user space decoders,
	 *    you can see which blk[s] is[are] outstanding etc.
	 * 3. Validate kernel code.
	 */
 __u64 __attribute__((aligned(8))) Model0_seq_num;

 /*
	 * ts_last_pkt:
	 *
	 * Case 1.	Block has 'N'(N >=1) packets and TMO'd(timed out)
	 *		ts_last_pkt == 'time-stamp of last packet' and NOT the
	 *		time when the timer fired and the block was closed.
	 *		By providing the ts of the last packet we can absolutely
	 *		guarantee that time-stamp wise, the first packet in the
	 *		next block will never precede the last packet of the
	 *		previous block.
	 * Case 2.	Block has zero packets and TMO'd
	 *		ts_last_pkt = time when the timer fired and the block
	 *		was closed.
	 * Case 3.	Block has 'N' packets and NO TMO.
	 *		ts_last_pkt = time-stamp of the last pkt in the block.
	 *
	 * ts_first_pkt:
	 *		Is always the time-stamp when the block was opened.
	 *		Case a)	ZERO packets
	 *			No packets to deal with but atleast you know the
	 *			time-interval of this block.
	 *		Case b) Non-zero packets
	 *			Use the ts of the first packet in the block.
	 *
	 */
 struct Model0_tpacket_bd_ts Model0_ts_first_pkt, Model0_ts_last_pkt;
};

union Model0_tpacket_bd_header_u {
 struct Model0_tpacket_hdr_v1 Model0_bh1;
};

struct Model0_tpacket_block_desc {
 __u32 Model0_version;
 __u32 Model0_offset_to_priv;
 union Model0_tpacket_bd_header_u Model0_hdr;
};




enum Model0_tpacket_versions {
 Model0_TPACKET_V1,
 Model0_TPACKET_V2,
 Model0_TPACKET_V3
};

/*
   Frame structure:

   - Start. Frame must be aligned to TPACKET_ALIGNMENT=16
   - struct tpacket_hdr
   - pad to TPACKET_ALIGNMENT=16
   - struct sockaddr_ll
   - Gap, chosen so that packet data (Start+tp_net) alignes to TPACKET_ALIGNMENT=16
   - Start+tp_mac: [ Optional MAC header ]
   - Start+tp_net: Packet data, aligned to TPACKET_ALIGNMENT=16.
   - Pad to align to TPACKET_ALIGNMENT=16
 */

struct Model0_tpacket_req {
 unsigned int Model0_tp_block_size; /* Minimal size of contiguous block */
 unsigned int Model0_tp_block_nr; /* Number of blocks */
 unsigned int Model0_tp_frame_size; /* Size of frame */
 unsigned int Model0_tp_frame_nr; /* Total number of frames */
};

struct Model0_tpacket_req3 {
 unsigned int Model0_tp_block_size; /* Minimal size of contiguous block */
 unsigned int Model0_tp_block_nr; /* Number of blocks */
 unsigned int Model0_tp_frame_size; /* Size of frame */
 unsigned int Model0_tp_frame_nr; /* Total number of frames */
 unsigned int Model0_tp_retire_blk_tov; /* timeout in msecs */
 unsigned int Model0_tp_sizeof_priv; /* offset to private data area */
 unsigned int Model0_tp_feature_req_word;
};

union Model0_tpacket_req_u {
 struct Model0_tpacket_req Model0_req;
 struct Model0_tpacket_req3 Model0_req3;
};

struct Model0_packet_mreq {
 int Model0_mr_ifindex;
 unsigned short Model0_mr_type;
 unsigned short Model0_mr_alen;
 unsigned char Model0_mr_address[8];
};
/*
 *
 *	Generic internet FLOW.
 *
 */
/*
 * ifindex generation is per-net namespace, and loopback is
 * always the 1st device in ns (see net_dev_init), thus any
 * loopback device should get ifindex 1
 */



struct Model0_flowi_tunnel {
 Model0___be64 Model0_tun_id;
};

struct Model0_flowi_common {
 int Model0_flowic_oif;
 int Model0_flowic_iif;
 __u32 Model0_flowic_mark;
 __u8 Model0_flowic_tos;
 __u8 Model0_flowic_scope;
 __u8 Model0_flowic_proto;
 __u8 Model0_flowic_flags;




 __u32 Model0_flowic_secid;
 struct Model0_flowi_tunnel Model0_flowic_tun_key;
};

union Model0_flowi_uli {
 struct {
  Model0___be16 Model0_dport;
  Model0___be16 Model0_sport;
 } Model0_ports;

 struct {
  __u8 Model0_type;
  __u8 Model0_code;
 } Model0_icmpt;

 struct {
  Model0___le16 Model0_dport;
  Model0___le16 Model0_sport;
 } Model0_dnports;

 Model0___be32 Model0_spi;
 Model0___be32 Model0_gre_key;

 struct {
  __u8 Model0_type;
 } Model0_mht;
};

struct Model0_flowi4 {
 struct Model0_flowi_common Model0___fl_common;
 /* (saddr,daddr) must be grouped, same order as in IP header */
 Model0___be32 Model0_saddr;
 Model0___be32 Model0_daddr;

 union Model0_flowi_uli Model0_uli;







} __attribute__((__aligned__(64/8)));

static inline __attribute__((no_instrument_function)) void Model0_flowi4_init_output(struct Model0_flowi4 *Model0_fl4, int Model0_oif,
          __u32 Model0_mark, __u8 Model0_tos, __u8 Model0_scope,
          __u8 Model0_proto, __u8 Model0_flags,
          Model0___be32 Model0_daddr, Model0___be32 Model0_saddr,
          Model0___be16 Model0_dport, Model0___be16 Model0_sport)
{
 Model0_fl4->Model0___fl_common.Model0_flowic_oif = Model0_oif;
 Model0_fl4->Model0___fl_common.Model0_flowic_iif = 1;
 Model0_fl4->Model0___fl_common.Model0_flowic_mark = Model0_mark;
 Model0_fl4->Model0___fl_common.Model0_flowic_tos = Model0_tos;
 Model0_fl4->Model0___fl_common.Model0_flowic_scope = Model0_scope;
 Model0_fl4->Model0___fl_common.Model0_flowic_proto = Model0_proto;
 Model0_fl4->Model0___fl_common.Model0_flowic_flags = Model0_flags;
 Model0_fl4->Model0___fl_common.Model0_flowic_secid = 0;
 Model0_fl4->Model0___fl_common.Model0_flowic_tun_key.Model0_tun_id = 0;
 Model0_fl4->Model0_daddr = Model0_daddr;
 Model0_fl4->Model0_saddr = Model0_saddr;
 Model0_fl4->Model0_uli.Model0_ports.Model0_dport = Model0_dport;
 Model0_fl4->Model0_uli.Model0_ports.Model0_sport = Model0_sport;
}

/* Reset some input parameters after previous lookup */
static inline __attribute__((no_instrument_function)) void Model0_flowi4_update_output(struct Model0_flowi4 *Model0_fl4, int Model0_oif, __u8 Model0_tos,
     Model0___be32 Model0_daddr, Model0___be32 Model0_saddr)
{
 Model0_fl4->Model0___fl_common.Model0_flowic_oif = Model0_oif;
 Model0_fl4->Model0___fl_common.Model0_flowic_tos = Model0_tos;
 Model0_fl4->Model0_daddr = Model0_daddr;
 Model0_fl4->Model0_saddr = Model0_saddr;
}


struct Model0_flowi6 {
 struct Model0_flowi_common Model0___fl_common;
 struct Model0_in6_addr Model0_daddr;
 struct Model0_in6_addr Model0_saddr;
 /* Note: flowi6_tos is encoded in flowlabel, too. */
 Model0___be32 Model0_flowlabel;
 union Model0_flowi_uli Model0_uli;







} __attribute__((__aligned__(64/8)));

struct Model0_flowidn {
 struct Model0_flowi_common Model0___fl_common;






 Model0___le16 Model0_daddr;
 Model0___le16 Model0_saddr;
 union Model0_flowi_uli Model0_uli;


} __attribute__((__aligned__(64/8)));

struct Model0_flowi {
 union {
  struct Model0_flowi_common Model0___fl_common;
  struct Model0_flowi4 Model0_ip4;
  struct Model0_flowi6 Model0_ip6;
  struct Model0_flowidn Model0_dn;
 } Model0_u;
} __attribute__((__aligned__(64/8)));

static inline __attribute__((no_instrument_function)) struct Model0_flowi *Model0_flowi4_to_flowi(struct Model0_flowi4 *Model0_fl4)
{
 return ({ const typeof( ((struct Model0_flowi *)0)->Model0_u.Model0_ip4 ) *Model0___mptr = (Model0_fl4); (struct Model0_flowi *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_flowi, Model0_u.Model0_ip4) );});
}

static inline __attribute__((no_instrument_function)) struct Model0_flowi *Model0_flowi6_to_flowi(struct Model0_flowi6 *Model0_fl6)
{
 return ({ const typeof( ((struct Model0_flowi *)0)->Model0_u.Model0_ip6 ) *Model0___mptr = (Model0_fl6); (struct Model0_flowi *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_flowi, Model0_u.Model0_ip6) );});
}

static inline __attribute__((no_instrument_function)) struct Model0_flowi *Model0_flowidn_to_flowi(struct Model0_flowidn *Model0_fldn)
{
 return ({ const typeof( ((struct Model0_flowi *)0)->Model0_u.Model0_dn ) *Model0___mptr = (Model0_fldn); (struct Model0_flowi *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_flowi, Model0_u.Model0_dn) );});
}

typedef unsigned long Model0_flow_compare_t;

static inline __attribute__((no_instrument_function)) Model0_size_t Model0_flow_key_size(Model0_u16 Model0_family)
{
 switch (Model0_family) {
 case 2:
  do { bool Model0___cond = !(!(sizeof(struct Model0_flowi4) % sizeof(Model0_flow_compare_t))); extern void Model0___compiletime_assert_203(void) ; if (Model0___cond) Model0___compiletime_assert_203(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
  return sizeof(struct Model0_flowi4) / sizeof(Model0_flow_compare_t);
 case 10:
  do { bool Model0___cond = !(!(sizeof(struct Model0_flowi6) % sizeof(Model0_flow_compare_t))); extern void Model0___compiletime_assert_206(void) ; if (Model0___cond) Model0___compiletime_assert_206(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
  return sizeof(struct Model0_flowi6) / sizeof(Model0_flow_compare_t);
 case 12:
  do { bool Model0___cond = !(!(sizeof(struct Model0_flowidn) % sizeof(Model0_flow_compare_t))); extern void Model0___compiletime_assert_209(void) ; if (Model0___cond) Model0___compiletime_assert_209(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
  return sizeof(struct Model0_flowidn) / sizeof(Model0_flow_compare_t);
 }
 return 0;
}





struct Model0_net;
struct Model0_sock;
struct Model0_flow_cache_ops;

struct Model0_flow_cache_object {
 const struct Model0_flow_cache_ops *Model0_ops;
};

struct Model0_flow_cache_ops {
 struct Model0_flow_cache_object *(*Model0_get)(struct Model0_flow_cache_object *);
 int (*Model0_check)(struct Model0_flow_cache_object *);
 void (*Model0_delete)(struct Model0_flow_cache_object *);
};

typedef struct Model0_flow_cache_object *(*Model0_flow_resolve_t)(
  struct Model0_net *Model0_net, const struct Model0_flowi *Model0_key, Model0_u16 Model0_family,
  Model0_u8 Model0_dir, struct Model0_flow_cache_object *Model0_oldobj, void *Model0_ctx);

struct Model0_flow_cache_object *Model0_flow_cache_lookup(struct Model0_net *Model0_net,
         const struct Model0_flowi *Model0_key, Model0_u16 Model0_family,
         Model0_u8 Model0_dir, Model0_flow_resolve_t Model0_resolver,
         void *Model0_ctx);
int Model0_flow_cache_init(struct Model0_net *Model0_net);
void Model0_flow_cache_fini(struct Model0_net *Model0_net);

void Model0_flow_cache_flush(struct Model0_net *Model0_net);
void Model0_flow_cache_flush_deferred(struct Model0_net *Model0_net);
extern Model0_atomic_t Model0_flow_cache_genid;

__u32 Model0___get_hash_from_flowi6(const struct Model0_flowi6 *Model0_fl6, struct Model0_flow_keys *Model0_keys);

static inline __attribute__((no_instrument_function)) __u32 Model0_get_hash_from_flowi6(const struct Model0_flowi6 *Model0_fl6)
{
 struct Model0_flow_keys Model0_keys;

 return Model0___get_hash_from_flowi6(Model0_fl6, &Model0_keys);
}

__u32 Model0___get_hash_from_flowi4(const struct Model0_flowi4 *Model0_fl4, struct Model0_flow_keys *Model0_keys);

static inline __attribute__((no_instrument_function)) __u32 Model0_get_hash_from_flowi4(const struct Model0_flowi4 *Model0_fl4)
{
 struct Model0_flow_keys Model0_keys;

 return Model0___get_hash_from_flowi4(Model0_fl4, &Model0_keys);
}

/* The interface for checksum offload between the stack and networking drivers
 * is as follows...
 *
 * A. IP checksum related features
 *
 * Drivers advertise checksum offload capabilities in the features of a device.
 * From the stack's point of view these are capabilities offered by the driver,
 * a driver typically only advertises features that it is capable of offloading
 * to its device.
 *
 * The checksum related features are:
 *
 *	NETIF_F_HW_CSUM	- The driver (or its device) is able to compute one
 *			  IP (one's complement) checksum for any combination
 *			  of protocols or protocol layering. The checksum is
 *			  computed and set in a packet per the CHECKSUM_PARTIAL
 *			  interface (see below).
 *
 *	NETIF_F_IP_CSUM - Driver (device) is only able to checksum plain
 *			  TCP or UDP packets over IPv4. These are specifically
 *			  unencapsulated packets of the form IPv4|TCP or
 *			  IPv4|UDP where the Protocol field in the IPv4 header
 *			  is TCP or UDP. The IPv4 header may contain IP options
 *			  This feature cannot be set in features for a device
 *			  with NETIF_F_HW_CSUM also set. This feature is being
 *			  DEPRECATED (see below).
 *
 *	NETIF_F_IPV6_CSUM - Driver (device) is only able to checksum plain
 *			  TCP or UDP packets over IPv6. These are specifically
 *			  unencapsulated packets of the form IPv6|TCP or
 *			  IPv4|UDP where the Next Header field in the IPv6
 *			  header is either TCP or UDP. IPv6 extension headers
 *			  are not supported with this feature. This feature
 *			  cannot be set in features for a device with
 *			  NETIF_F_HW_CSUM also set. This feature is being
 *			  DEPRECATED (see below).
 *
 *	NETIF_F_RXCSUM - Driver (device) performs receive checksum offload.
 *			 This flag is used only used to disable the RX checksum
 *			 feature for a device. The stack will accept receive
 *			 checksum indication in packets received on a device
 *			 regardless of whether NETIF_F_RXCSUM is set.
 *
 * B. Checksumming of received packets by device. Indication of checksum
 *    verification is in set skb->ip_summed. Possible values are:
 *
 * CHECKSUM_NONE:
 *
 *   Device did not checksum this packet e.g. due to lack of capabilities.
 *   The packet contains full (though not verified) checksum in packet but
 *   not in skb->csum. Thus, skb->csum is undefined in this case.
 *
 * CHECKSUM_UNNECESSARY:
 *
 *   The hardware you're dealing with doesn't calculate the full checksum
 *   (as in CHECKSUM_COMPLETE), but it does parse headers and verify checksums
 *   for specific protocols. For such packets it will set CHECKSUM_UNNECESSARY
 *   if their checksums are okay. skb->csum is still undefined in this case
 *   though. A driver or device must never modify the checksum field in the
 *   packet even if checksum is verified.
 *
 *   CHECKSUM_UNNECESSARY is applicable to following protocols:
 *     TCP: IPv6 and IPv4.
 *     UDP: IPv4 and IPv6. A device may apply CHECKSUM_UNNECESSARY to a
 *       zero UDP checksum for either IPv4 or IPv6, the networking stack
 *       may perform further validation in this case.
 *     GRE: only if the checksum is present in the header.
 *     SCTP: indicates the CRC in SCTP header has been validated.
 *
 *   skb->csum_level indicates the number of consecutive checksums found in
 *   the packet minus one that have been verified as CHECKSUM_UNNECESSARY.
 *   For instance if a device receives an IPv6->UDP->GRE->IPv4->TCP packet
 *   and a device is able to verify the checksums for UDP (possibly zero),
 *   GRE (checksum flag is set), and TCP-- skb->csum_level would be set to
 *   two. If the device were only able to verify the UDP checksum and not
 *   GRE, either because it doesn't support GRE checksum of because GRE
 *   checksum is bad, skb->csum_level would be set to zero (TCP checksum is
 *   not considered in this case).
 *
 * CHECKSUM_COMPLETE:
 *
 *   This is the most generic way. The device supplied checksum of the _whole_
 *   packet as seen by netif_rx() and fills out in skb->csum. Meaning, the
 *   hardware doesn't need to parse L3/L4 headers to implement this.
 *
 *   Note: Even if device supports only some protocols, but is able to produce
 *   skb->csum, it MUST use CHECKSUM_COMPLETE, not CHECKSUM_UNNECESSARY.
 *
 * CHECKSUM_PARTIAL:
 *
 *   A checksum is set up to be offloaded to a device as described in the
 *   output description for CHECKSUM_PARTIAL. This may occur on a packet
 *   received directly from another Linux OS, e.g., a virtualized Linux kernel
 *   on the same host, or it may be set in the input path in GRO or remote
 *   checksum offload. For the purposes of checksum verification, the checksum
 *   referred to by skb->csum_start + skb->csum_offset and any preceding
 *   checksums in the packet are considered verified. Any checksums in the
 *   packet that are after the checksum being offloaded are not considered to
 *   be verified.
 *
 * C. Checksumming on transmit for non-GSO. The stack requests checksum offload
 *    in the skb->ip_summed for a packet. Values are:
 *
 * CHECKSUM_PARTIAL:
 *
 *   The driver is required to checksum the packet as seen by hard_start_xmit()
 *   from skb->csum_start up to the end, and to record/write the checksum at
 *   offset skb->csum_start + skb->csum_offset. A driver may verify that the
 *   csum_start and csum_offset values are valid values given the length and
 *   offset of the packet, however they should not attempt to validate that the
 *   checksum refers to a legitimate transport layer checksum-- it is the
 *   purview of the stack to validate that csum_start and csum_offset are set
 *   correctly.
 *
 *   When the stack requests checksum offload for a packet, the driver MUST
 *   ensure that the checksum is set correctly. A driver can either offload the
 *   checksum calculation to the device, or call skb_checksum_help (in the case
 *   that the device does not support offload for a particular checksum).
 *
 *   NETIF_F_IP_CSUM and NETIF_F_IPV6_CSUM are being deprecated in favor of
 *   NETIF_F_HW_CSUM. New devices should use NETIF_F_HW_CSUM to indicate
 *   checksum offload capability. If a	device has limited checksum capabilities
 *   (for instance can only perform NETIF_F_IP_CSUM or NETIF_F_IPV6_CSUM as
 *   described above) a helper function can be called to resolve
 *   CHECKSUM_PARTIAL. The helper functions are skb_csum_off_chk*. The helper
 *   function takes a spec argument that describes the protocol layer that is
 *   supported for checksum offload and can be called for each packet. If a
 *   packet does not match the specification for offload, skb_checksum_help
 *   is called to resolve the checksum.
 *
 * CHECKSUM_NONE:
 *
 *   The skb was already checksummed by the protocol, or a checksum is not
 *   required.
 *
 * CHECKSUM_UNNECESSARY:
 *
 *   This has the same meaning on as CHECKSUM_NONE for checksum offload on
 *   output.
 *
 * CHECKSUM_COMPLETE:
 *   Not used in checksum output. If a driver observes a packet with this value
 *   set in skbuff, if should treat as CHECKSUM_NONE being set.
 *
 * D. Non-IP checksum (CRC) offloads
 *
 *   NETIF_F_SCTP_CRC - This feature indicates that a device is capable of
 *     offloading the SCTP CRC in a packet. To perform this offload the stack
 *     will set ip_summed to CHECKSUM_PARTIAL and set csum_start and csum_offset
 *     accordingly. Note the there is no indication in the skbuff that the
 *     CHECKSUM_PARTIAL refers to an SCTP checksum, a driver that supports
 *     both IP checksum offload and SCTP CRC offload must verify which offload
 *     is configured for a packet presumably by inspecting packet headers.
 *
 *   NETIF_F_FCOE_CRC - This feature indicates that a device is capable of
 *     offloading the FCOE CRC in a packet. To perform this offload the stack
 *     will set ip_summed to CHECKSUM_PARTIAL and set csum_start and csum_offset
 *     accordingly. Note the there is no indication in the skbuff that the
 *     CHECKSUM_PARTIAL refers to an FCOE checksum, a driver that supports
 *     both IP checksum offload and FCOE CRC offload must verify which offload
 *     is configured for a packet presumably by inspecting packet headers.
 *
 * E. Checksumming on output with GSO.
 *
 * In the case of a GSO packet (skb_is_gso(skb) is true), checksum offload
 * is implied by the SKB_GSO_* flags in gso_type. Most obviously, if the
 * gso_type is SKB_GSO_TCPV4 or SKB_GSO_TCPV6, TCP checksum offload as
 * part of the GSO operation is implied. If a checksum is being offloaded
 * with GSO then ip_summed is CHECKSUM_PARTIAL, csum_start and csum_offset
 * are set to refer to the outermost checksum being offload (two offloaded
 * checksums are possible with UDP encapsulation).
 */

/* Don't change this without changing skb_csum_unnecessary! */





/* Maximum value in skb->csum_level */
/* return minimum truesize of one skb containing X bytes of data */




struct Model0_net_device;
struct Model0_scatterlist;
struct Model0_pipe_inode_info;
struct Model0_iov_iter;
struct Model0_napi_struct;


struct Model0_nf_conntrack {
 Model0_atomic_t Model0_use;
};
struct Model0_sk_buff_head {
 /* These two members must be first. */
 struct Model0_sk_buff *Model0_next;
 struct Model0_sk_buff *Model0_prev;

 __u32 Model0_qlen;
 Model0_spinlock_t Model0_lock;
};

struct Model0_sk_buff;

/* To allow 64K frame to be packed as single skb without frag_list we
 * require 64K/PAGE_SIZE pages plus 1 additional page to allow for
 * buffers which do not start on a page boundary.
 *
 * Since GRO uses frags we allocate at least 16 regardless of page
 * size.
 */





extern int Model0_sysctl_max_skb_frags;

/* Set skb_shinfo(skb)->gso_size to this in case you want skb_segment to
 * segment using its current segmentation instead.
 */


typedef struct Model0_skb_frag_struct Model0_skb_frag_t;

struct Model0_skb_frag_struct {
 struct {
  struct Model0_page *Model0_p;
 } Model0_page;

 __u32 Model0_page_offset;
 __u32 Model0_size;




};

static inline __attribute__((no_instrument_function)) unsigned int Model0_skb_frag_size(const Model0_skb_frag_t *Model0_frag)
{
 return Model0_frag->Model0_size;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_frag_size_set(Model0_skb_frag_t *Model0_frag, unsigned int Model0_size)
{
 Model0_frag->Model0_size = Model0_size;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_frag_size_add(Model0_skb_frag_t *Model0_frag, int Model0_delta)
{
 Model0_frag->Model0_size += Model0_delta;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_frag_size_sub(Model0_skb_frag_t *Model0_frag, int Model0_delta)
{
 Model0_frag->Model0_size -= Model0_delta;
}



/**
 * struct skb_shared_hwtstamps - hardware time stamps
 * @hwtstamp:	hardware time stamp transformed into duration
 *		since arbitrary point in time
 *
 * Software time stamps generated by ktime_get_real() are stored in
 * skb->tstamp.
 *
 * hwtstamps can only be compared against other hwtstamps from
 * the same device.
 *
 * This structure is attached to packets as part of the
 * &skb_shared_info. Use skb_hwtstamps() to get a pointer.
 */
struct Model0_skb_shared_hwtstamps {
 Model0_ktime_t Model0_hwtstamp;
};

/* Definitions for tx_flags in struct skb_shared_info */
enum {
 /* generate hardware time stamp */
 Model0_SKBTX_HW_TSTAMP = 1 << 0,

 /* generate software time stamp when queueing packet to NIC */
 Model0_SKBTX_SW_TSTAMP = 1 << 1,

 /* device driver is going to provide hardware time stamp */
 Model0_SKBTX_IN_PROGRESS = 1 << 2,

 /* device driver supports TX zero-copy buffers */
 Model0_SKBTX_DEV_ZEROCOPY = 1 << 3,

 /* generate wifi status information (where possible) */
 Model0_SKBTX_WIFI_STATUS = 1 << 4,

 /* This indicates at least one fragment might be overwritten
	 * (as in vmsplice(), sendfile() ...)
	 * If we need to compute a TX checksum, we'll need to copy
	 * all frags to avoid possible bad checksum
	 */
 Model0_SKBTX_SHARED_FRAG = 1 << 5,

 /* generate software time stamp when entering packet scheduling */
 Model0_SKBTX_SCHED_TSTAMP = 1 << 6,
};





/*
 * The callback notifies userspace to release buffers when skb DMA is done in
 * lower device, the skb last reference should be 0 when calling this.
 * The zerocopy_success argument is true if zero copy transmit occurred,
 * false on data copy or out of memory error caused by data copy attempt.
 * The ctx field is used to track device context.
 * The desc field is used to track userspace buffer index.
 */
struct Model0_ubuf_info {
 void (*Model0_callback)(struct Model0_ubuf_info *, bool Model0_zerocopy_success);
 void *Model0_ctx;
 unsigned long Model0_desc;
};

/* This data is invariant across clones and lives at
 * the end of the header data, ie. at skb->end.
 */
struct Model0_skb_shared_info {
 unsigned char Model0_nr_frags;
 __u8 Model0_tx_flags;
 unsigned short Model0_gso_size;
 /* Warning: this field is not always filled in (UFO)! */
 unsigned short Model0_gso_segs;
 unsigned short Model0_gso_type;
 struct Model0_sk_buff *Model0_frag_list;
 struct Model0_skb_shared_hwtstamps Model0_hwtstamps;
 Model0_u32 Model0_tskey;
 Model0___be32 Model0_ip6_frag_id;

 /*
	 * Warning : all fields before dataref are cleared in __alloc_skb()
	 */
 Model0_atomic_t Model0_dataref;

 /* Intermediate layers must ensure that destructor_arg
	 * remains valid until skb destructor */
 void * Model0_destructor_arg;

 /* must be last field, see pskb_expand_head() */
 Model0_skb_frag_t Model0_frags[(65536/((1UL) << 12) + 1)];
};

/* We divide dataref into two halves.  The higher 16 bits hold references
 * to the payload part of skb->data.  The lower 16 bits hold references to
 * the entire skb->data.  A clone of a headerless skb holds the length of
 * the header in skb->hdr_len.
 *
 * All users must obey the rule that the skb->data reference count must be
 * greater than or equal to the payload reference count.
 *
 * Holding a reference to the payload part means that the user does not
 * care about modifications to the header part of skb->data.
 */




enum {
 Model0_SKB_FCLONE_UNAVAILABLE, /* skb has no fclone (from head_cache) */
 Model0_SKB_FCLONE_ORIG, /* orig skb (from fclone_cache) */
 Model0_SKB_FCLONE_CLONE, /* companion fclone skb (from fclone_cache) */
};

enum {
 Model0_SKB_GSO_TCPV4 = 1 << 0,
 Model0_SKB_GSO_UDP = 1 << 1,

 /* This indicates the skb is from an untrusted source. */
 Model0_SKB_GSO_DODGY = 1 << 2,

 /* This indicates the tcp segment has CWR set. */
 Model0_SKB_GSO_TCP_ECN = 1 << 3,

 Model0_SKB_GSO_TCP_FIXEDID = 1 << 4,

 Model0_SKB_GSO_TCPV6 = 1 << 5,

 Model0_SKB_GSO_FCOE = 1 << 6,

 Model0_SKB_GSO_GRE = 1 << 7,

 Model0_SKB_GSO_GRE_CSUM = 1 << 8,

 Model0_SKB_GSO_IPXIP4 = 1 << 9,

 Model0_SKB_GSO_IPXIP6 = 1 << 10,

 Model0_SKB_GSO_UDP_TUNNEL = 1 << 11,

 Model0_SKB_GSO_UDP_TUNNEL_CSUM = 1 << 12,

 Model0_SKB_GSO_PARTIAL = 1 << 13,

 Model0_SKB_GSO_TUNNEL_REMCSUM = 1 << 14,

 Model0_SKB_GSO_SCTP = 1 << 15,
};






typedef unsigned int Model0_sk_buff_data_t;




/**
 * struct skb_mstamp - multi resolution time stamps
 * @stamp_us: timestamp in us resolution
 * @stamp_jiffies: timestamp in jiffies
 */
struct Model0_skb_mstamp {
 union {
  Model0_u64 Model0_v64;
  struct {
   Model0_u32 Model0_stamp_us;
   Model0_u32 Model0_stamp_jiffies;
  };
 };
};

/**
 * skb_mstamp_get - get current timestamp
 * @cl: place to store timestamps
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_mstamp_get(struct Model0_skb_mstamp *Model0_cl)
{
 Model0_u64 Model0_val = Model0_local_clock();

 ({ Model0_uint32_t Model0___base = (1000L); Model0_uint32_t Model0___rem; Model0___rem = ((Model0_uint64_t)(Model0_val)) % Model0___base; (Model0_val) = ((Model0_uint64_t)(Model0_val)) / Model0___base; Model0___rem; });
 Model0_cl->Model0_stamp_us = (Model0_u32)Model0_val;
 Model0_cl->Model0_stamp_jiffies = (Model0_u32)Model0_jiffies;
}

/**
 * skb_mstamp_delta - compute the difference in usec between two skb_mstamp
 * @t1: pointer to newest sample
 * @t0: pointer to oldest sample
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_skb_mstamp_us_delta(const struct Model0_skb_mstamp *Model0_t1,
          const struct Model0_skb_mstamp *Model0_t0)
{
 Model0_s32 Model0_delta_us = Model0_t1->Model0_stamp_us - Model0_t0->Model0_stamp_us;
 Model0_u32 Model0_delta_jiffies = Model0_t1->Model0_stamp_jiffies - Model0_t0->Model0_stamp_jiffies;

 /* If delta_us is negative, this might be because interval is too big,
	 * or local_clock() drift is too big : fallback using jiffies.
	 */
 if (Model0_delta_us <= 0 ||
     Model0_delta_jiffies >= (((int)(~0U>>1)) / (1000000L / 1000)))

  Model0_delta_us = Model0_jiffies_to_usecs(Model0_delta_jiffies);

 return Model0_delta_us;
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_mstamp_after(const struct Model0_skb_mstamp *Model0_t1,
        const struct Model0_skb_mstamp *Model0_t0)
{
 Model0_s32 Model0_diff = Model0_t1->Model0_stamp_jiffies - Model0_t0->Model0_stamp_jiffies;

 if (!Model0_diff)
  Model0_diff = Model0_t1->Model0_stamp_us - Model0_t0->Model0_stamp_us;
 return Model0_diff > 0;
}

/** 
 *	struct sk_buff - socket buffer
 *	@next: Next buffer in list
 *	@prev: Previous buffer in list
 *	@tstamp: Time we arrived/left
 *	@rbnode: RB tree node, alternative to next/prev for netem/tcp
 *	@sk: Socket we are owned by
 *	@dev: Device we arrived on/are leaving by
 *	@cb: Control buffer. Free for use by every layer. Put private vars here
 *	@_skb_refdst: destination entry (with norefcount bit)
 *	@sp: the security path, used for xfrm
 *	@len: Length of actual data
 *	@data_len: Data length
 *	@mac_len: Length of link layer header
 *	@hdr_len: writable header length of cloned skb
 *	@csum: Checksum (must include start/offset pair)
 *	@csum_start: Offset from skb->head where checksumming should start
 *	@csum_offset: Offset from csum_start where checksum should be stored
 *	@priority: Packet queueing priority
 *	@ignore_df: allow local fragmentation
 *	@cloned: Head may be cloned (check refcnt to be sure)
 *	@ip_summed: Driver fed us an IP checksum
 *	@nohdr: Payload reference only, must not modify header
 *	@nfctinfo: Relationship of this skb to the connection
 *	@pkt_type: Packet class
 *	@fclone: skbuff clone status
 *	@ipvs_property: skbuff is owned by ipvs
 *	@peeked: this packet has been seen already, so stats have been
 *		done for it, don't do them again
 *	@nf_trace: netfilter packet trace flag
 *	@protocol: Packet protocol from driver
 *	@destructor: Destruct function
 *	@nfct: Associated connection, if any
 *	@nf_bridge: Saved data about a bridged frame - see br_netfilter.c
 *	@skb_iif: ifindex of device we arrived on
 *	@tc_index: Traffic control index
 *	@tc_verd: traffic control verdict
 *	@hash: the packet hash
 *	@queue_mapping: Queue mapping for multiqueue devices
 *	@xmit_more: More SKBs are pending for this queue
 *	@ndisc_nodetype: router type (from link layer)
 *	@ooo_okay: allow the mapping of a socket to a queue to be changed
 *	@l4_hash: indicate hash is a canonical 4-tuple hash over transport
 *		ports.
 *	@sw_hash: indicates hash was computed in software stack
 *	@wifi_acked_valid: wifi_acked was set
 *	@wifi_acked: whether frame was acked on wifi or not
 *	@no_fcs:  Request NIC to treat last 4 bytes as Ethernet FCS
  *	@napi_id: id of the NAPI struct this skb came from
 *	@secmark: security marking
 *	@offload_fwd_mark: fwding offload mark
 *	@mark: Generic packet mark
 *	@vlan_proto: vlan encapsulation protocol
 *	@vlan_tci: vlan tag control information
 *	@inner_protocol: Protocol (encapsulation)
 *	@inner_transport_header: Inner transport layer header (encapsulation)
 *	@inner_network_header: Network layer header (encapsulation)
 *	@inner_mac_header: Link layer header (encapsulation)
 *	@transport_header: Transport layer header
 *	@network_header: Network layer header
 *	@mac_header: Link layer header
 *	@tail: Tail pointer
 *	@end: End pointer
 *	@head: Head of buffer
 *	@data: Data head pointer
 *	@truesize: Buffer size
 *	@users: User count - see {datagram,tcp}.c
 */

struct Model0_sk_buff {
 union {
  struct {
   /* These two members must be first. */
   struct Model0_sk_buff *Model0_next;
   struct Model0_sk_buff *Model0_prev;

   union {
    Model0_ktime_t Model0_tstamp;
    struct Model0_skb_mstamp Model0_skb_mstamp;
   };
  };
  struct Model0_rb_node Model0_rbnode; /* used in netem & tcp stack */
 };
 struct Model0_sock *Model0_sk;
 struct Model0_net_device *Model0_dev;

 /*
	 * This is the control buffer. It is free to use for every
	 * layer. Please put your private variables there. If you
	 * want to keep them across layers you have to do a skb_clone()
	 * first. This is owned by whoever has the skb queued ATM.
	 */
 char Model0_cb[48] __attribute__((aligned(8)));

 unsigned long Model0__skb_refdst;
 void (*Model0_destructor)(struct Model0_sk_buff *Model0_skb);

 struct Model0_sec_path *Model0_sp;


 struct Model0_nf_conntrack *Model0_nfct;




 unsigned int Model0_len,
    Model0_data_len;
 Model0___u16 Model0_mac_len,
    Model0_hdr_len;

 /* Following fields are _not_ copied in __copy_skb_header()
	 * Note that queue_mapping is here mostly to fill a hole.
	 */
                                 ;
 Model0___u16 Model0_queue_mapping;
 __u8 Model0_cloned:1,
    Model0_nohdr:1,
    Model0_fclone:2,
    Model0_peeked:1,
    Model0_head_frag:1,
    Model0_xmit_more:1;
 /* one bit hole */
                               ;

 /* fields enclosed in headers_start/headers_end are copied
	 * using a single memcpy() in __copy_skb_header()
	 */
 /* private: */
 __u32 Model0_headers_start[0];
 /* public: */

/* if you move pkt_type around you also must adapt those constants */







 __u8 Model0___pkt_type_offset[0];
 __u8 Model0_pkt_type:3;
 __u8 Model0_pfmemalloc:1;
 __u8 Model0_ignore_df:1;
 __u8 Model0_nfctinfo:3;

 __u8 Model0_nf_trace:1;
 __u8 Model0_ip_summed:2;
 __u8 Model0_ooo_okay:1;
 __u8 Model0_l4_hash:1;
 __u8 Model0_sw_hash:1;
 __u8 Model0_wifi_acked_valid:1;
 __u8 Model0_wifi_acked:1;

 __u8 Model0_no_fcs:1;
 /* Indicates the inner headers are valid in the skbuff. */
 __u8 Model0_encapsulation:1;
 __u8 Model0_encap_hdr_csum:1;
 __u8 Model0_csum_valid:1;
 __u8 Model0_csum_complete_sw:1;
 __u8 Model0_csum_level:2;
 __u8 Model0_csum_bad:1;


 __u8 Model0_ndisc_nodetype:2;

 __u8 Model0_ipvs_property:1;
 __u8 Model0_inner_protocol_type:1;
 __u8 Model0_remcsum_offload:1;
 /* 3 or 5 bit hole */


 Model0___u16 Model0_tc_index; /* traffic control index */

 Model0___u16 Model0_tc_verd; /* traffic control verdict */



 union {
  Model0___wsum Model0_csum;
  struct {
   Model0___u16 Model0_csum_start;
   Model0___u16 Model0_csum_offset;
  };
 };
 __u32 Model0_priority;
 int Model0_skb_iif;
 __u32 Model0_hash;
 Model0___be16 Model0_vlan_proto;
 Model0___u16 Model0_vlan_tci;

 union {
  unsigned int Model0_napi_id;
  unsigned int Model0_sender_cpu;
 };

 union {

  __u32 Model0_secmark;




 };

 union {
  __u32 Model0_mark;
  __u32 Model0_reserved_tailroom;
 };

 union {
  Model0___be16 Model0_inner_protocol;
  __u8 Model0_inner_ipproto;
 };

 Model0___u16 Model0_inner_transport_header;
 Model0___u16 Model0_inner_network_header;
 Model0___u16 Model0_inner_mac_header;

 Model0___be16 Model0_protocol;
 Model0___u16 Model0_transport_header;
 Model0___u16 Model0_network_header;
 Model0___u16 Model0_mac_header;

 /* private: */
 __u32 Model0_headers_end[0];
 /* public: */

 /* These elements must be at the end, see alloc_skb() for details.  */
 Model0_sk_buff_data_t Model0_tail;
 Model0_sk_buff_data_t Model0_end;
 unsigned char *Model0_head,
    *Model0_data;
 unsigned int Model0_truesize;
 Model0_atomic_t Model0_users;
};


/*
 *	Handling routines are only of interest to the kernel
 */







/* Returns true if the skb was allocated from PFMEMALLOC reserves */
static inline __attribute__((no_instrument_function)) bool Model0_skb_pfmemalloc(const struct Model0_sk_buff *Model0_skb)
{
 return __builtin_expect(!!(Model0_skb->Model0_pfmemalloc), 0);
}

/*
 * skb might have a dst pointer attached, refcounted or not.
 * _skb_refdst low order bit is set if refcount was _not_ taken
 */



/**
 * skb_dst - returns skb dst_entry
 * @skb: buffer
 *
 * Returns skb dst_entry, regardless of reference taken or not.
 */
static inline __attribute__((no_instrument_function)) struct Model0_dst_entry *Model0_skb_dst(const struct Model0_sk_buff *Model0_skb)
{
 /* If refdst was not refcounted, check we still are in a 
	 * rcu_read_lock section
	 */
 ({ int Model0___ret_warn_on = !!((Model0_skb->Model0__skb_refdst & 1UL) && !Model0_rcu_read_lock_held() && !Model0_rcu_read_lock_bh_held()); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/skbuff.h", 838); __builtin_expect(!!(Model0___ret_warn_on), 0); });


 return (struct Model0_dst_entry *)(Model0_skb->Model0__skb_refdst & ~(1UL));
}

/**
 * skb_dst_set - sets skb dst
 * @skb: buffer
 * @dst: dst entry
 *
 * Sets skb dst, assuming a reference was taken on dst and should
 * be released by skb_dst_drop()
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_dst_set(struct Model0_sk_buff *Model0_skb, struct Model0_dst_entry *Model0_dst)
{
 Model0_skb->Model0__skb_refdst = (unsigned long)Model0_dst;
}

/**
 * skb_dst_set_noref - sets skb dst, hopefully, without taking reference
 * @skb: buffer
 * @dst: dst entry
 *
 * Sets skb dst, assuming a reference was not taken on dst.
 * If dst entry is cached, we do not take reference and dst_release
 * will be avoided by refdst_drop. If dst entry is not cached, we take
 * reference, so that last dst_release can destroy the dst immediately.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_dst_set_noref(struct Model0_sk_buff *Model0_skb, struct Model0_dst_entry *Model0_dst)
{
 ({ int Model0___ret_warn_on = !!(!Model0_rcu_read_lock_held() && !Model0_rcu_read_lock_bh_held()); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/skbuff.h", 867); __builtin_expect(!!(Model0___ret_warn_on), 0); });
 Model0_skb->Model0__skb_refdst = (unsigned long)Model0_dst | 1UL;
}

/**
 * skb_dst_is_noref - Test if skb dst isn't refcounted
 * @skb: buffer
 */
static inline __attribute__((no_instrument_function)) bool Model0_skb_dst_is_noref(const struct Model0_sk_buff *Model0_skb)
{
 return (Model0_skb->Model0__skb_refdst & 1UL) && Model0_skb_dst(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0_skb_rtable(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_rtable *)Model0_skb_dst(Model0_skb);
}

/* For mangling skb->pkt_type from user space side from applications
 * such as nft, tc, etc, we only allow a conservative subset of
 * possible pkt_types to be set.
*/
static inline __attribute__((no_instrument_function)) bool Model0_skb_pkt_type_ok(Model0_u32 Model0_ptype)
{
 return Model0_ptype <= 3;
}

void Model0_kfree_skb(struct Model0_sk_buff *Model0_skb);
void Model0_kfree_skb_list(struct Model0_sk_buff *Model0_segs);
void Model0_skb_tx_error(struct Model0_sk_buff *Model0_skb);
void Model0_consume_skb(struct Model0_sk_buff *Model0_skb);
void Model0___kfree_skb(struct Model0_sk_buff *Model0_skb);
extern struct Model0_kmem_cache *Model0_skbuff_head_cache;

void Model0_kfree_skb_partial(struct Model0_sk_buff *Model0_skb, bool Model0_head_stolen);
bool Model0_skb_try_coalesce(struct Model0_sk_buff *Model0_to, struct Model0_sk_buff *Model0_from,
        bool *Model0_fragstolen, int *Model0_delta_truesize);

struct Model0_sk_buff *Model0___alloc_skb(unsigned int Model0_size, Model0_gfp_t Model0_priority, int Model0_flags,
       int Model0_node);
struct Model0_sk_buff *Model0___build_skb(void *Model0_data, unsigned int Model0_frag_size);
struct Model0_sk_buff *Model0_build_skb(void *Model0_data, unsigned int Model0_frag_size);
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_alloc_skb(unsigned int Model0_size,
     Model0_gfp_t Model0_priority)
{
 return Model0___alloc_skb(Model0_size, Model0_priority, 0, (-1));
}

struct Model0_sk_buff *Model0_alloc_skb_with_frags(unsigned long Model0_header_len,
         unsigned long Model0_data_len,
         int Model0_max_page_order,
         int *Model0_errcode,
         Model0_gfp_t Model0_gfp_mask);

/* Layout of fast clones : [skb1][skb2][fclone_ref] */
struct Model0_sk_buff_fclones {
 struct Model0_sk_buff Model0_skb1;

 struct Model0_sk_buff Model0_skb2;

 Model0_atomic_t Model0_fclone_ref;
};

/**
 *	skb_fclone_busy - check if fclone is busy
 *	@skb: buffer
 *
 * Returns true if skb is a fast clone, and its clone is not freed.
 * Some drivers call skb_orphan() in their ndo_start_xmit(),
 * so we also check that this didnt happen.
 */
static inline __attribute__((no_instrument_function)) bool Model0_skb_fclone_busy(const struct Model0_sock *Model0_sk,
       const struct Model0_sk_buff *Model0_skb)
{
 const struct Model0_sk_buff_fclones *Model0_fclones;

 Model0_fclones = ({ const typeof( ((struct Model0_sk_buff_fclones *)0)->Model0_skb1 ) *Model0___mptr = (Model0_skb); (struct Model0_sk_buff_fclones *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_sk_buff_fclones, Model0_skb1) );});

 return Model0_skb->Model0_fclone == Model0_SKB_FCLONE_ORIG &&
        Model0_atomic_read(&Model0_fclones->Model0_fclone_ref) > 1 &&
        Model0_fclones->Model0_skb2.Model0_sk == Model0_sk;
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_alloc_skb_fclone(unsigned int Model0_size,
            Model0_gfp_t Model0_priority)
{
 return Model0___alloc_skb(Model0_size, Model0_priority, 0x01, (-1));
}

struct Model0_sk_buff *Model0___alloc_skb_head(Model0_gfp_t Model0_priority, int Model0_node);
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_alloc_skb_head(Model0_gfp_t Model0_priority)
{
 return Model0___alloc_skb_head(Model0_priority, -1);
}

struct Model0_sk_buff *Model0_skb_morph(struct Model0_sk_buff *Model0_dst, struct Model0_sk_buff *Model0_src);
int Model0_skb_copy_ubufs(struct Model0_sk_buff *Model0_skb, Model0_gfp_t Model0_gfp_mask);
struct Model0_sk_buff *Model0_skb_clone(struct Model0_sk_buff *Model0_skb, Model0_gfp_t Model0_priority);
struct Model0_sk_buff *Model0_skb_copy(const struct Model0_sk_buff *Model0_skb, Model0_gfp_t Model0_priority);
struct Model0_sk_buff *Model0___pskb_copy_fclone(struct Model0_sk_buff *Model0_skb, int Model0_headroom,
       Model0_gfp_t Model0_gfp_mask, bool Model0_fclone);
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0___pskb_copy(struct Model0_sk_buff *Model0_skb, int Model0_headroom,
       Model0_gfp_t Model0_gfp_mask)
{
 return Model0___pskb_copy_fclone(Model0_skb, Model0_headroom, Model0_gfp_mask, false);
}

int Model0_pskb_expand_head(struct Model0_sk_buff *Model0_skb, int Model0_nhead, int Model0_ntail, Model0_gfp_t Model0_gfp_mask);
struct Model0_sk_buff *Model0_skb_realloc_headroom(struct Model0_sk_buff *Model0_skb,
         unsigned int Model0_headroom);
struct Model0_sk_buff *Model0_skb_copy_expand(const struct Model0_sk_buff *Model0_skb, int Model0_newheadroom,
    int Model0_newtailroom, Model0_gfp_t Model0_priority);
int Model0_skb_to_sgvec_nomark(struct Model0_sk_buff *Model0_skb, struct Model0_scatterlist *Model0_sg,
   int Model0_offset, int Model0_len);
int Model0_skb_to_sgvec(struct Model0_sk_buff *Model0_skb, struct Model0_scatterlist *Model0_sg, int Model0_offset,
   int Model0_len);
int Model0_skb_cow_data(struct Model0_sk_buff *Model0_skb, int Model0_tailbits, struct Model0_sk_buff **Model0_trailer);
int Model0_skb_pad(struct Model0_sk_buff *Model0_skb, int Model0_pad);


int Model0_skb_append_datato_frags(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
       int Model0_getfrag(void *Model0_from, char *Model0_to, int Model0_offset,
     int Model0_len, int Model0_odd, struct Model0_sk_buff *Model0_skb),
       void *Model0_from, int Model0_length);

int Model0_skb_append_pagefrags(struct Model0_sk_buff *Model0_skb, struct Model0_page *Model0_page,
    int Model0_offset, Model0_size_t Model0_size);

struct Model0_skb_seq_state {
 __u32 Model0_lower_offset;
 __u32 Model0_upper_offset;
 __u32 Model0_frag_idx;
 __u32 Model0_stepped_offset;
 struct Model0_sk_buff *Model0_root_skb;
 struct Model0_sk_buff *Model0_cur_skb;
 __u8 *Model0_frag_data;
};

void Model0_skb_prepare_seq_read(struct Model0_sk_buff *Model0_skb, unsigned int Model0_from,
     unsigned int Model0_to, struct Model0_skb_seq_state *Model0_st);
unsigned int Model0_skb_seq_read(unsigned int Model0_consumed, const Model0_u8 **Model0_data,
     struct Model0_skb_seq_state *Model0_st);
void Model0_skb_abort_seq_read(struct Model0_skb_seq_state *Model0_st);

unsigned int Model0_skb_find_text(struct Model0_sk_buff *Model0_skb, unsigned int Model0_from,
      unsigned int Model0_to, struct Model0_ts_config *Model0_config);

/*
 * Packet hash types specify the type of hash in skb_set_hash.
 *
 * Hash types refer to the protocol layer addresses which are used to
 * construct a packet's hash. The hashes are used to differentiate or identify
 * flows of the protocol layer for the hash type. Hash types are either
 * layer-2 (L2), layer-3 (L3), or layer-4 (L4).
 *
 * Properties of hashes:
 *
 * 1) Two packets in different flows have different hash values
 * 2) Two packets in the same flow should have the same hash value
 *
 * A hash at a higher layer is considered to be more specific. A driver should
 * set the most specific hash possible.
 *
 * A driver cannot indicate a more specific hash than the layer at which a hash
 * was computed. For instance an L3 hash cannot be set as an L4 hash.
 *
 * A driver may indicate a hash level which is less specific than the
 * actual layer the hash was computed on. For instance, a hash computed
 * at L4 may be considered an L3 hash. This should only be done if the
 * driver can't unambiguously determine that the HW computed the hash at
 * the higher layer. Note that the "should" in the second property above
 * permits this.
 */
enum Model0_pkt_hash_types {
 Model0_PKT_HASH_TYPE_NONE, /* Undefined type */
 Model0_PKT_HASH_TYPE_L2, /* Input: src_MAC, dest_MAC */
 Model0_PKT_HASH_TYPE_L3, /* Input: src_IP, dst_IP */
 Model0_PKT_HASH_TYPE_L4, /* Input: src_IP, dst_IP, src_port, dst_port */
};

static inline __attribute__((no_instrument_function)) void Model0_skb_clear_hash(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_hash = 0;
 Model0_skb->Model0_sw_hash = 0;
 Model0_skb->Model0_l4_hash = 0;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_clear_hash_if_not_l4(struct Model0_sk_buff *Model0_skb)
{
 if (!Model0_skb->Model0_l4_hash)
  Model0_skb_clear_hash(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void
Model0___skb_set_hash(struct Model0_sk_buff *Model0_skb, __u32 Model0_hash, bool Model0_is_sw, bool Model0_is_l4)
{
 Model0_skb->Model0_l4_hash = Model0_is_l4;
 Model0_skb->Model0_sw_hash = Model0_is_sw;
 Model0_skb->Model0_hash = Model0_hash;
}

static inline __attribute__((no_instrument_function)) void
Model0_skb_set_hash(struct Model0_sk_buff *Model0_skb, __u32 Model0_hash, enum Model0_pkt_hash_types Model0_type)
{
 /* Used by drivers to set hash from HW */
 Model0___skb_set_hash(Model0_skb, Model0_hash, false, Model0_type == Model0_PKT_HASH_TYPE_L4);
}

static inline __attribute__((no_instrument_function)) void
Model0___skb_set_sw_hash(struct Model0_sk_buff *Model0_skb, __u32 Model0_hash, bool Model0_is_l4)
{
 Model0___skb_set_hash(Model0_skb, Model0_hash, true, Model0_is_l4);
}

void Model0___skb_get_hash(struct Model0_sk_buff *Model0_skb);
Model0_u32 Model0___skb_get_hash_symmetric(struct Model0_sk_buff *Model0_skb);
Model0_u32 Model0_skb_get_poff(const struct Model0_sk_buff *Model0_skb);
Model0_u32 Model0___skb_get_poff(const struct Model0_sk_buff *Model0_skb, void *Model0_data,
     const struct Model0_flow_keys *Model0_keys, int Model0_hlen);
Model0___be32 Model0___skb_flow_get_ports(const struct Model0_sk_buff *Model0_skb, int Model0_thoff, Model0_u8 Model0_ip_proto,
       void *Model0_data, int Model0_hlen_proto);

static inline __attribute__((no_instrument_function)) Model0___be32 Model0_skb_flow_get_ports(const struct Model0_sk_buff *Model0_skb,
     int Model0_thoff, Model0_u8 Model0_ip_proto)
{
 return Model0___skb_flow_get_ports(Model0_skb, Model0_thoff, Model0_ip_proto, ((void *)0), 0);
}

void Model0_skb_flow_dissector_init(struct Model0_flow_dissector *Model0_flow_dissector,
        const struct Model0_flow_dissector_key *Model0_key,
        unsigned int Model0_key_count);

bool Model0___skb_flow_dissect(const struct Model0_sk_buff *Model0_skb,
   struct Model0_flow_dissector *Model0_flow_dissector,
   void *Model0_target_container,
   void *Model0_data, Model0___be16 Model0_proto, int Model0_nhoff, int Model0_hlen,
   unsigned int Model0_flags);

static inline __attribute__((no_instrument_function)) bool Model0_skb_flow_dissect(const struct Model0_sk_buff *Model0_skb,
        struct Model0_flow_dissector *Model0_flow_dissector,
        void *Model0_target_container, unsigned int Model0_flags)
{
 return Model0___skb_flow_dissect(Model0_skb, Model0_flow_dissector, Model0_target_container,
      ((void *)0), 0, 0, 0, Model0_flags);
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_flow_dissect_flow_keys(const struct Model0_sk_buff *Model0_skb,
           struct Model0_flow_keys *Model0_flow,
           unsigned int Model0_flags)
{
 memset(Model0_flow, 0, sizeof(*Model0_flow));
 return Model0___skb_flow_dissect(Model0_skb, &Model0_flow_keys_dissector, Model0_flow,
      ((void *)0), 0, 0, 0, Model0_flags);
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_flow_dissect_flow_keys_buf(struct Model0_flow_keys *Model0_flow,
        void *Model0_data, Model0___be16 Model0_proto,
        int Model0_nhoff, int Model0_hlen,
        unsigned int Model0_flags)
{
 memset(Model0_flow, 0, sizeof(*Model0_flow));
 return Model0___skb_flow_dissect(((void *)0), &Model0_flow_keys_buf_dissector, Model0_flow,
      Model0_data, Model0_proto, Model0_nhoff, Model0_hlen, Model0_flags);
}

static inline __attribute__((no_instrument_function)) __u32 Model0_skb_get_hash(struct Model0_sk_buff *Model0_skb)
{
 if (!Model0_skb->Model0_l4_hash && !Model0_skb->Model0_sw_hash)
  Model0___skb_get_hash(Model0_skb);

 return Model0_skb->Model0_hash;
}

__u32 Model0___skb_get_hash_flowi6(struct Model0_sk_buff *Model0_skb, const struct Model0_flowi6 *Model0_fl6);

static inline __attribute__((no_instrument_function)) __u32 Model0_skb_get_hash_flowi6(struct Model0_sk_buff *Model0_skb, const struct Model0_flowi6 *Model0_fl6)
{
 if (!Model0_skb->Model0_l4_hash && !Model0_skb->Model0_sw_hash) {
  struct Model0_flow_keys Model0_keys;
  __u32 Model0_hash = Model0___get_hash_from_flowi6(Model0_fl6, &Model0_keys);

  Model0___skb_set_sw_hash(Model0_skb, Model0_hash, Model0_flow_keys_have_l4(&Model0_keys));
 }

 return Model0_skb->Model0_hash;
}

__u32 Model0___skb_get_hash_flowi4(struct Model0_sk_buff *Model0_skb, const struct Model0_flowi4 *Model0_fl);

static inline __attribute__((no_instrument_function)) __u32 Model0_skb_get_hash_flowi4(struct Model0_sk_buff *Model0_skb, const struct Model0_flowi4 *Model0_fl4)
{
 if (!Model0_skb->Model0_l4_hash && !Model0_skb->Model0_sw_hash) {
  struct Model0_flow_keys Model0_keys;
  __u32 Model0_hash = Model0___get_hash_from_flowi4(Model0_fl4, &Model0_keys);

  Model0___skb_set_sw_hash(Model0_skb, Model0_hash, Model0_flow_keys_have_l4(&Model0_keys));
 }

 return Model0_skb->Model0_hash;
}

__u32 Model0_skb_get_hash_perturb(const struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_perturb);

static inline __attribute__((no_instrument_function)) __u32 Model0_skb_get_hash_raw(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_hash;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_copy_hash(struct Model0_sk_buff *Model0_to, const struct Model0_sk_buff *Model0_from)
{
 Model0_to->Model0_hash = Model0_from->Model0_hash;
 Model0_to->Model0_sw_hash = Model0_from->Model0_sw_hash;
 Model0_to->Model0_l4_hash = Model0_from->Model0_l4_hash;
};


static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_end_pointer(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_end;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_skb_end_offset(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_end;
}
/* Internal */


static inline __attribute__((no_instrument_function)) struct Model0_skb_shared_hwtstamps *Model0_skb_hwtstamps(struct Model0_sk_buff *Model0_skb)
{
 return &((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_hwtstamps;
}

/**
 *	skb_queue_empty - check if a queue is empty
 *	@list: queue head
 *
 *	Returns true if the queue is empty, false otherwise.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_queue_empty(const struct Model0_sk_buff_head *Model0_list)
{
 return Model0_list->Model0_next == (const struct Model0_sk_buff *) Model0_list;
}

/**
 *	skb_queue_is_last - check if skb is the last entry in the queue
 *	@list: queue head
 *	@skb: buffer
 *
 *	Returns true if @skb is the last buffer on the list.
 */
static inline __attribute__((no_instrument_function)) bool Model0_skb_queue_is_last(const struct Model0_sk_buff_head *Model0_list,
         const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_next == (const struct Model0_sk_buff *) Model0_list;
}

/**
 *	skb_queue_is_first - check if skb is the first entry in the queue
 *	@list: queue head
 *	@skb: buffer
 *
 *	Returns true if @skb is the first buffer on the list.
 */
static inline __attribute__((no_instrument_function)) bool Model0_skb_queue_is_first(const struct Model0_sk_buff_head *Model0_list,
          const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_prev == (const struct Model0_sk_buff *) Model0_list;
}

/**
 *	skb_queue_next - return the next packet in the queue
 *	@list: queue head
 *	@skb: current buffer
 *
 *	Return the next packet in @list after @skb.  It is only valid to
 *	call this if skb_queue_is_last() evaluates to false.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_skb_queue_next(const struct Model0_sk_buff_head *Model0_list,
          const struct Model0_sk_buff *Model0_skb)
{
 /* This BUG_ON may seem severe, but if we just return then we
	 * are going to dereference garbage.
	 */
 do { if (__builtin_expect(!!(Model0_skb_queue_is_last(Model0_list, Model0_skb)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1263), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 return Model0_skb->Model0_next;
}

/**
 *	skb_queue_prev - return the prev packet in the queue
 *	@list: queue head
 *	@skb: current buffer
 *
 *	Return the prev packet in @list before @skb.  It is only valid to
 *	call this if skb_queue_is_first() evaluates to false.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_skb_queue_prev(const struct Model0_sk_buff_head *Model0_list,
          const struct Model0_sk_buff *Model0_skb)
{
 /* This BUG_ON may seem severe, but if we just return then we
	 * are going to dereference garbage.
	 */
 do { if (__builtin_expect(!!(Model0_skb_queue_is_first(Model0_list, Model0_skb)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1281), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 return Model0_skb->Model0_prev;
}

/**
 *	skb_get - reference buffer
 *	@skb: buffer to reference
 *
 *	Makes another reference to a socket buffer and returns a pointer
 *	to the buffer.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_skb_get(struct Model0_sk_buff *Model0_skb)
{
 Model0_atomic_inc(&Model0_skb->Model0_users);
 return Model0_skb;
}

/*
 * If users == 1, we are the only owner and are can avoid redundant
 * atomic change.
 */

/**
 *	skb_cloned - is the buffer a clone
 *	@skb: buffer to check
 *
 *	Returns true if the buffer was generated with skb_clone() and is
 *	one of multiple shared copies of the buffer. Cloned buffers are
 *	shared data so must not be written to under normal circumstances.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_cloned(const struct Model0_sk_buff *Model0_skb)
{
#if CY_ABSTRACT4
    return false; //TODO: Model0_skb->Model0_cloned should always be false
#else
 return Model0_skb->Model0_cloned &&
        (Model0_atomic_read(&((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_dataref) & ((1 << 16) - 1)) != 1;
#endif
}

static inline __attribute__((no_instrument_function)) int Model0_skb_unclone(struct Model0_sk_buff *Model0_skb, Model0_gfp_t Model0_pri)
{
 do { if (Model0_gfpflags_allow_blocking(Model0_pri)) do { Model0__cond_resched(); } while (0); } while (0);

 if (Model0_skb_cloned(Model0_skb))
  return Model0_pskb_expand_head(Model0_skb, 0, 0, Model0_pri);

 return 0;
}

/**
 *	skb_header_cloned - is the header a clone
 *	@skb: buffer to check
 *
 *	Returns true if modifying the header part of the buffer requires
 *	the data to be copied.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_header_cloned(const struct Model0_sk_buff *Model0_skb)
{
 int Model0_dataref;

 if (!Model0_skb->Model0_cloned)
  return 0;

 Model0_dataref = Model0_atomic_read(&((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_dataref);
 Model0_dataref = (Model0_dataref & ((1 << 16) - 1)) - (Model0_dataref >> 16);
 return Model0_dataref != 1;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_header_unclone(struct Model0_sk_buff *Model0_skb, Model0_gfp_t Model0_pri)
{
 do { if (Model0_gfpflags_allow_blocking(Model0_pri)) do { Model0__cond_resched(); } while (0); } while (0);

 if (Model0_skb_header_cloned(Model0_skb))
  return Model0_pskb_expand_head(Model0_skb, 0, 0, Model0_pri);

 return 0;
}

/**
 *	skb_header_release - release reference to header
 *	@skb: buffer to operate on
 *
 *	Drop a reference to the header part of the buffer.  This is done
 *	by acquiring a payload reference.  You must not read from the header
 *	part of skb->data after this.
 *	Note : Check if you can use __skb_header_release() instead.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_header_release(struct Model0_sk_buff *Model0_skb)
{
 do { if (__builtin_expect(!!(Model0_skb->Model0_nohdr), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1367), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 Model0_skb->Model0_nohdr = 1;
 Model0_atomic_add(1 << 16, &((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_dataref);
}

/**
 *	__skb_header_release - release reference to header
 *	@skb: buffer to operate on
 *
 *	Variant of skb_header_release() assuming skb is private to caller.
 *	We can avoid one atomic operation.
 */
static inline __attribute__((no_instrument_function)) void Model0___skb_header_release(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_nohdr = 1;
 Model0_atomic_set(&((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_dataref, 1 + (1 << 16));
}


/**
 *	skb_shared - is the buffer shared
 *	@skb: buffer to check
 *
 *	Returns true if more than one person has a reference to this
 *	buffer.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_shared(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_atomic_read(&Model0_skb->Model0_users) != 1;
}

/**
 *	skb_share_check - check if buffer is shared and if so clone it
 *	@skb: buffer to check
 *	@pri: priority for memory allocation
 *
 *	If the buffer is shared the buffer is cloned and the old copy
 *	drops a reference. A new clone with a single reference is returned.
 *	If the buffer is not shared the original buffer is returned. When
 *	being called from interrupt status or with spinlocks held pri must
 *	be GFP_ATOMIC.
 *
 *	NULL is returned on a memory allocation failure.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_skb_share_check(struct Model0_sk_buff *Model0_skb, Model0_gfp_t Model0_pri)
{
 do { if (Model0_gfpflags_allow_blocking(Model0_pri)) do { Model0__cond_resched(); } while (0); } while (0);
 if (Model0_skb_shared(Model0_skb)) {
  struct Model0_sk_buff *Model0_nskb = Model0_skb_clone(Model0_skb, Model0_pri);

  if (__builtin_expect(!!(Model0_nskb), 1))
   Model0_consume_skb(Model0_skb);
  else
   Model0_kfree_skb(Model0_skb);
  Model0_skb = Model0_nskb;
 }
 return Model0_skb;
}

/*
 *	Copy shared buffers into a new sk_buff. We effectively do COW on
 *	packets to handle cases where we have a local reader and forward
 *	and a couple of other messy ones. The normal one is tcpdumping
 *	a packet thats being forwarded.
 */

/**
 *	skb_unshare - make a copy of a shared buffer
 *	@skb: buffer to check
 *	@pri: priority for memory allocation
 *
 *	If the socket buffer is a clone then this function creates a new
 *	copy of the data, drops a reference count on the old copy and returns
 *	the new copy with the reference count at 1. If the buffer is not a clone
 *	the original buffer is returned. When called with a spinlock held or
 *	from interrupt state @pri must be %GFP_ATOMIC
 *
 *	%NULL is returned on a memory allocation failure.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_skb_unshare(struct Model0_sk_buff *Model0_skb,
       Model0_gfp_t Model0_pri)
{
 do { if (Model0_gfpflags_allow_blocking(Model0_pri)) do { Model0__cond_resched(); } while (0); } while (0);
 if (Model0_skb_cloned(Model0_skb)) {
  struct Model0_sk_buff *Model0_nskb = Model0_skb_copy(Model0_skb, Model0_pri);

  /* Free our shared copy */
  if (__builtin_expect(!!(Model0_nskb), 1))
   Model0_consume_skb(Model0_skb);
  else
   Model0_kfree_skb(Model0_skb);
  Model0_skb = Model0_nskb;
 }
 return Model0_skb;
}

/**
 *	skb_peek - peek at the head of an &sk_buff_head
 *	@list_: list to peek at
 *
 *	Peek an &sk_buff. Unlike most other operations you _MUST_
 *	be careful with this one. A peek leaves the buffer on the
 *	list and someone else may run off with it. You must hold
 *	the appropriate locks or have a private queue to do this.
 *
 *	Returns %NULL for an empty list or a pointer to the head element.
 *	The reference count is not incremented and the reference is therefore
 *	volatile. Use with caution.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_skb_peek(const struct Model0_sk_buff_head *Model0_list_)
{
 struct Model0_sk_buff *Model0_skb = Model0_list_->Model0_next;

 if (Model0_skb == (struct Model0_sk_buff *)Model0_list_)
  Model0_skb = ((void *)0);
 return Model0_skb;
}

/**
 *	skb_peek_next - peek skb following the given one from a queue
 *	@skb: skb to start from
 *	@list_: list to peek at
 *
 *	Returns %NULL when the end of the list is met or a pointer to the
 *	next element. The reference count is not incremented and the
 *	reference is therefore volatile. Use with caution.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_skb_peek_next(struct Model0_sk_buff *Model0_skb,
  const struct Model0_sk_buff_head *Model0_list_)
{
 struct Model0_sk_buff *Model0_next = Model0_skb->Model0_next;

 if (Model0_next == (struct Model0_sk_buff *)Model0_list_)
  Model0_next = ((void *)0);
 return Model0_next;
}

/**
 *	skb_peek_tail - peek at the tail of an &sk_buff_head
 *	@list_: list to peek at
 *
 *	Peek an &sk_buff. Unlike most other operations you _MUST_
 *	be careful with this one. A peek leaves the buffer on the
 *	list and someone else may run off with it. You must hold
 *	the appropriate locks or have a private queue to do this.
 *
 *	Returns %NULL for an empty list or a pointer to the tail element.
 *	The reference count is not incremented and the reference is therefore
 *	volatile. Use with caution.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_skb_peek_tail(const struct Model0_sk_buff_head *Model0_list_)
{
 struct Model0_sk_buff *Model0_skb = Model0_list_->Model0_prev;

 if (Model0_skb == (struct Model0_sk_buff *)Model0_list_)
  Model0_skb = ((void *)0);
 return Model0_skb;

}

/**
 *	skb_queue_len	- get queue length
 *	@list_: list to measure
 *
 *	Return the length of an &sk_buff queue.
 */
static inline __attribute__((no_instrument_function)) __u32 Model0_skb_queue_len(const struct Model0_sk_buff_head *Model0_list_)
{
 return Model0_list_->Model0_qlen;
}

/**
 *	__skb_queue_head_init - initialize non-spinlock portions of sk_buff_head
 *	@list: queue to initialize
 *
 *	This initializes only the list and queue length aspects of
 *	an sk_buff_head object.  This allows to initialize the list
 *	aspects of an sk_buff_head without reinitializing things like
 *	the spinlock.  It can also be used for on-stack sk_buff_head
 *	objects where the spinlock is known to not be used.
 */
static inline __attribute__((no_instrument_function)) void Model0___skb_queue_head_init(struct Model0_sk_buff_head *Model0_list)
{
 Model0_list->Model0_prev = Model0_list->Model0_next = (struct Model0_sk_buff *)Model0_list;
 Model0_list->Model0_qlen = 0;
}

/*
 * This function creates a split out lock class for each invocation;
 * this is needed for now since a whole lot of users of the skb-queue
 * infrastructure in drivers have different locking usage (in hardirq)
 * than the networking core (in softirq only). In the long run either the
 * network layer or drivers should need annotation to consolidate the
 * main types of usage into 3 classes.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_queue_head_init(struct Model0_sk_buff_head *Model0_list)
{
 do { Model0_spinlock_check(&Model0_list->Model0_lock); do { *(&(&Model0_list->Model0_lock)->Model0_rlock) = (Model0_raw_spinlock_t) { .Model0_raw_lock = { { (0) } }, }; } while (0); } while (0);
 Model0___skb_queue_head_init(Model0_list);
}

static inline __attribute__((no_instrument_function)) void Model0_skb_queue_head_init_class(struct Model0_sk_buff_head *Model0_list,
  struct Model0_lock_class_key *Model0_class)
{
 Model0_skb_queue_head_init(Model0_list);
 do { (void)(Model0_class); } while (0);
}

/*
 *	Insert an sk_buff on a list.
 *
 *	The "__skb_xxxx()" functions are the non-atomic ones that
 *	can only be called with interrupts disabled.
 */
void Model0_skb_insert(struct Model0_sk_buff *old, struct Model0_sk_buff *Model0_newsk,
  struct Model0_sk_buff_head *Model0_list);
static inline __attribute__((no_instrument_function)) void Model0___skb_insert(struct Model0_sk_buff *Model0_newsk,
    struct Model0_sk_buff *Model0_prev, struct Model0_sk_buff *Model0_next,
    struct Model0_sk_buff_head *Model0_list)
{
 Model0_newsk->Model0_next = Model0_next;
 Model0_newsk->Model0_prev = Model0_prev;
 Model0_next->Model0_prev = Model0_prev->Model0_next = Model0_newsk;
 Model0_list->Model0_qlen++;
}

static inline __attribute__((no_instrument_function)) void Model0___skb_queue_splice(const struct Model0_sk_buff_head *Model0_list,
          struct Model0_sk_buff *Model0_prev,
          struct Model0_sk_buff *Model0_next)
{
 struct Model0_sk_buff *Model0_first = Model0_list->Model0_next;
 struct Model0_sk_buff *Model0_last = Model0_list->Model0_prev;

 Model0_first->Model0_prev = Model0_prev;
 Model0_prev->Model0_next = Model0_first;

 Model0_last->Model0_next = Model0_next;
 Model0_next->Model0_prev = Model0_last;
}

/**
 *	skb_queue_splice - join two skb lists, this is designed for stacks
 *	@list: the new list to add
 *	@head: the place to add it in the first list
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_queue_splice(const struct Model0_sk_buff_head *Model0_list,
        struct Model0_sk_buff_head *Model0_head)
{
 if (!Model0_skb_queue_empty(Model0_list)) {
  Model0___skb_queue_splice(Model0_list, (struct Model0_sk_buff *) Model0_head, Model0_head->Model0_next);
  Model0_head->Model0_qlen += Model0_list->Model0_qlen;
 }
}

/**
 *	skb_queue_splice_init - join two skb lists and reinitialise the emptied list
 *	@list: the new list to add
 *	@head: the place to add it in the first list
 *
 *	The list at @list is reinitialised
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_queue_splice_init(struct Model0_sk_buff_head *Model0_list,
      struct Model0_sk_buff_head *Model0_head)
{
 if (!Model0_skb_queue_empty(Model0_list)) {
  Model0___skb_queue_splice(Model0_list, (struct Model0_sk_buff *) Model0_head, Model0_head->Model0_next);
  Model0_head->Model0_qlen += Model0_list->Model0_qlen;
  Model0___skb_queue_head_init(Model0_list);
 }
}

/**
 *	skb_queue_splice_tail - join two skb lists, each list being a queue
 *	@list: the new list to add
 *	@head: the place to add it in the first list
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_queue_splice_tail(const struct Model0_sk_buff_head *Model0_list,
      struct Model0_sk_buff_head *Model0_head)
{
 if (!Model0_skb_queue_empty(Model0_list)) {
  Model0___skb_queue_splice(Model0_list, Model0_head->Model0_prev, (struct Model0_sk_buff *) Model0_head);
  Model0_head->Model0_qlen += Model0_list->Model0_qlen;
 }
}

/**
 *	skb_queue_splice_tail_init - join two skb lists and reinitialise the emptied list
 *	@list: the new list to add
 *	@head: the place to add it in the first list
 *
 *	Each of the lists is a queue.
 *	The list at @list is reinitialised
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_queue_splice_tail_init(struct Model0_sk_buff_head *Model0_list,
           struct Model0_sk_buff_head *Model0_head)
{
 if (!Model0_skb_queue_empty(Model0_list)) {
  Model0___skb_queue_splice(Model0_list, Model0_head->Model0_prev, (struct Model0_sk_buff *) Model0_head);
  Model0_head->Model0_qlen += Model0_list->Model0_qlen;
  Model0___skb_queue_head_init(Model0_list);
 }
}

/**
 *	__skb_queue_after - queue a buffer at the list head
 *	@list: list to use
 *	@prev: place after this buffer
 *	@newsk: buffer to queue
 *
 *	Queue a buffer int the middle of a list. This function takes no locks
 *	and you must therefore hold required locks before calling it.
 *
 *	A buffer cannot be placed on two lists at the same time.
 */
static inline __attribute__((no_instrument_function)) void Model0___skb_queue_after(struct Model0_sk_buff_head *Model0_list,
         struct Model0_sk_buff *Model0_prev,
         struct Model0_sk_buff *Model0_newsk)
{
 Model0___skb_insert(Model0_newsk, Model0_prev, Model0_prev->Model0_next, Model0_list);
}

void Model0_skb_append(struct Model0_sk_buff *old, struct Model0_sk_buff *Model0_newsk,
  struct Model0_sk_buff_head *Model0_list);

static inline __attribute__((no_instrument_function)) void Model0___skb_queue_before(struct Model0_sk_buff_head *Model0_list,
          struct Model0_sk_buff *Model0_next,
          struct Model0_sk_buff *Model0_newsk)
{
 Model0___skb_insert(Model0_newsk, Model0_next->Model0_prev, Model0_next, Model0_list);
}

/**
 *	__skb_queue_head - queue a buffer at the list head
 *	@list: list to use
 *	@newsk: buffer to queue
 *
 *	Queue a buffer at the start of a list. This function takes no locks
 *	and you must therefore hold required locks before calling it.
 *
 *	A buffer cannot be placed on two lists at the same time.
 */
void Model0_skb_queue_head(struct Model0_sk_buff_head *Model0_list, struct Model0_sk_buff *Model0_newsk);
static inline __attribute__((no_instrument_function)) void Model0___skb_queue_head(struct Model0_sk_buff_head *Model0_list,
        struct Model0_sk_buff *Model0_newsk)
{
 Model0___skb_queue_after(Model0_list, (struct Model0_sk_buff *)Model0_list, Model0_newsk);
}

/**
 *	__skb_queue_tail - queue a buffer at the list tail
 *	@list: list to use
 *	@newsk: buffer to queue
 *
 *	Queue a buffer at the end of a list. This function takes no locks
 *	and you must therefore hold required locks before calling it.
 *
 *	A buffer cannot be placed on two lists at the same time.
 */
void Model0_skb_queue_tail(struct Model0_sk_buff_head *Model0_list, struct Model0_sk_buff *Model0_newsk);
static inline __attribute__((no_instrument_function)) void Model0___skb_queue_tail(struct Model0_sk_buff_head *Model0_list,
       struct Model0_sk_buff *Model0_newsk)
{
 Model0___skb_queue_before(Model0_list, (struct Model0_sk_buff *)Model0_list, Model0_newsk);
}

/*
 * remove sk_buff from list. _Must_ be called atomically, and with
 * the list known..
 */
void Model0_skb_unlink(struct Model0_sk_buff *Model0_skb, struct Model0_sk_buff_head *Model0_list);
static inline __attribute__((no_instrument_function)) void Model0___skb_unlink(struct Model0_sk_buff *Model0_skb, struct Model0_sk_buff_head *Model0_list)
{
 struct Model0_sk_buff *Model0_next, *Model0_prev;

 Model0_list->Model0_qlen--;
 Model0_next = Model0_skb->Model0_next;
 Model0_prev = Model0_skb->Model0_prev;
 Model0_skb->Model0_next = Model0_skb->Model0_prev = ((void *)0);
 Model0_next->Model0_prev = Model0_prev;
 Model0_prev->Model0_next = Model0_next;
}

/**
 *	__skb_dequeue - remove from the head of the queue
 *	@list: list to dequeue from
 *
 *	Remove the head of the list. This function does not take any locks
 *	so must be used with appropriate locks held only. The head item is
 *	returned or %NULL if the list is empty.
 */
struct Model0_sk_buff *Model0_skb_dequeue(struct Model0_sk_buff_head *Model0_list);
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0___skb_dequeue(struct Model0_sk_buff_head *Model0_list)
{
 struct Model0_sk_buff *Model0_skb = Model0_skb_peek(Model0_list);
 if (Model0_skb)
  Model0___skb_unlink(Model0_skb, Model0_list);
 return Model0_skb;
}

/**
 *	__skb_dequeue_tail - remove from the tail of the queue
 *	@list: list to dequeue from
 *
 *	Remove the tail of the list. This function does not take any locks
 *	so must be used with appropriate locks held only. The tail item is
 *	returned or %NULL if the list is empty.
 */
struct Model0_sk_buff *Model0_skb_dequeue_tail(struct Model0_sk_buff_head *Model0_list);
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0___skb_dequeue_tail(struct Model0_sk_buff_head *Model0_list)
{
 struct Model0_sk_buff *Model0_skb = Model0_skb_peek_tail(Model0_list);
 if (Model0_skb)
  Model0___skb_unlink(Model0_skb, Model0_list);
 return Model0_skb;
}


static inline __attribute__((no_instrument_function)) bool Model0_skb_is_nonlinear(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_data_len;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_skb_headlen(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_len - Model0_skb->Model0_data_len;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_pagelen(const struct Model0_sk_buff *Model0_skb)
{
 int Model0_i, Model0_len = 0;

 for (Model0_i = (int)((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_nr_frags - 1; Model0_i >= 0; Model0_i--)
  Model0_len += Model0_skb_frag_size(&((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_frags[Model0_i]);
 return Model0_len + Model0_skb_headlen(Model0_skb);
}

/**
 * __skb_fill_page_desc - initialise a paged fragment in an skb
 * @skb: buffer containing fragment to be initialised
 * @i: paged fragment index to initialise
 * @page: the page to use for this fragment
 * @off: the offset to the data with @page
 * @size: the length of the data
 *
 * Initialises the @i'th fragment of @skb to point to &size bytes at
 * offset @off within @page.
 *
 * Does not take any additional reference on the fragment.
 */
static inline __attribute__((no_instrument_function)) void Model0___skb_fill_page_desc(struct Model0_sk_buff *Model0_skb, int Model0_i,
     struct Model0_page *Model0_page, int Model0_off, int Model0_size)
{
 Model0_skb_frag_t *Model0_frag = &((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_frags[Model0_i];

 /*
	 * Propagate page pfmemalloc to the skb if we can. The problem is
	 * that not all callers have unique ownership of the page but rely
	 * on page_is_pfmemalloc doing the right thing(tm).
	 */
 Model0_frag->Model0_page.Model0_p = Model0_page;
 Model0_frag->Model0_page_offset = Model0_off;
 Model0_skb_frag_size_set(Model0_frag, Model0_size);

 Model0_page = Model0_compound_head(Model0_page);
 if (Model0_page_is_pfmemalloc(Model0_page))
  Model0_skb->Model0_pfmemalloc = true;
}

/**
 * skb_fill_page_desc - initialise a paged fragment in an skb
 * @skb: buffer containing fragment to be initialised
 * @i: paged fragment index to initialise
 * @page: the page to use for this fragment
 * @off: the offset to the data with @page
 * @size: the length of the data
 *
 * As per __skb_fill_page_desc() -- initialises the @i'th fragment of
 * @skb to point to @size bytes at offset @off within @page. In
 * addition updates @skb such that @i is the last fragment.
 *
 * Does not take any additional reference on the fragment.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_fill_page_desc(struct Model0_sk_buff *Model0_skb, int Model0_i,
          struct Model0_page *Model0_page, int Model0_off, int Model0_size)
{
 Model0___skb_fill_page_desc(Model0_skb, Model0_i, Model0_page, Model0_off, Model0_size);
 ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_nr_frags = Model0_i + 1;
}

void Model0_skb_add_rx_frag(struct Model0_sk_buff *Model0_skb, int Model0_i, struct Model0_page *Model0_page, int Model0_off,
       int Model0_size, unsigned int Model0_truesize);

void Model0_skb_coalesce_rx_frag(struct Model0_sk_buff *Model0_skb, int Model0_i, int Model0_size,
     unsigned int Model0_truesize);






static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_tail_pointer(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_tail;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_tail_pointer(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_tail = Model0_skb->Model0_data - Model0_skb->Model0_head;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_tail_pointer(struct Model0_sk_buff *Model0_skb, const int Model0_offset)
{
 Model0_skb_reset_tail_pointer(Model0_skb);
 Model0_skb->Model0_tail += Model0_offset;
}
/*
 *	Add data to an sk_buff
 */
unsigned char *Model0_pskb_put(struct Model0_sk_buff *Model0_skb, struct Model0_sk_buff *Model0_tail, int Model0_len);
unsigned char *Model0_skb_put(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len);
static inline __attribute__((no_instrument_function)) unsigned char *Model0___skb_put(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 unsigned char *Model0_tmp = Model0_skb_tail_pointer(Model0_skb);
 do { if (__builtin_expect(!!(Model0_skb_is_nonlinear(Model0_skb)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1909), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 Model0_skb->Model0_tail += Model0_len;
 Model0_skb->Model0_len += Model0_len;
 return Model0_tmp;
}

unsigned char *Model0_skb_push(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len);
static inline __attribute__((no_instrument_function)) unsigned char *Model0___skb_push(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 Model0_skb->Model0_data -= Model0_len;
 Model0_skb->Model0_len += Model0_len;
 return Model0_skb->Model0_data;
}

unsigned char *Model0_skb_pull(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len);
static inline __attribute__((no_instrument_function)) unsigned char *Model0___skb_pull(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 Model0_skb->Model0_len -= Model0_len;
 do { if (__builtin_expect(!!(Model0_skb->Model0_len < Model0_skb->Model0_data_len), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1927), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 return Model0_skb->Model0_data += Model0_len;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_pull_inline(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 return __builtin_expect(!!(Model0_len > Model0_skb->Model0_len), 0) ? ((void *)0) : Model0___skb_pull(Model0_skb, Model0_len);
}

unsigned char *Model0___pskb_pull_tail(struct Model0_sk_buff *Model0_skb, int Model0_delta);

static inline __attribute__((no_instrument_function)) unsigned char *Model0___pskb_pull(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 if (Model0_len > Model0_skb_headlen(Model0_skb) &&
     !Model0___pskb_pull_tail(Model0_skb, Model0_len - Model0_skb_headlen(Model0_skb)))
  return ((void *)0);
 Model0_skb->Model0_len -= Model0_len;
 return Model0_skb->Model0_data += Model0_len;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_pskb_pull(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 return __builtin_expect(!!(Model0_len > Model0_skb->Model0_len), 0) ? ((void *)0) : Model0___pskb_pull(Model0_skb, Model0_len);
}

static inline __attribute__((no_instrument_function)) int Model0_pskb_may_pull(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 if (__builtin_expect(!!(Model0_len <= Model0_skb_headlen(Model0_skb)), 1))
  return 1;
 if (__builtin_expect(!!(Model0_len > Model0_skb->Model0_len), 0))
  return 0;
 return Model0___pskb_pull_tail(Model0_skb, Model0_len - Model0_skb_headlen(Model0_skb)) != ((void *)0);
}

/**
 *	skb_headroom - bytes at buffer head
 *	@skb: buffer to check
 *
 *	Return the number of bytes of free space at the head of an &sk_buff.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_skb_headroom(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_data - Model0_skb->Model0_head;
}

/**
 *	skb_tailroom - bytes at buffer end
 *	@skb: buffer to check
 *
 *	Return the number of bytes of free space at the tail of an sk_buff
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_tailroom(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_is_nonlinear(Model0_skb) ? 0 : Model0_skb->Model0_end - Model0_skb->Model0_tail;
}

/**
 *	skb_availroom - bytes at buffer end
 *	@skb: buffer to check
 *
 *	Return the number of bytes of free space at the tail of an sk_buff
 *	allocated by sk_stream_alloc()
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_availroom(const struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb_is_nonlinear(Model0_skb))
  return 0;

 return Model0_skb->Model0_end - Model0_skb->Model0_tail - Model0_skb->Model0_reserved_tailroom;
}

/**
 *	skb_reserve - adjust headroom
 *	@skb: buffer to alter
 *	@len: bytes to move
 *
 *	Increase the headroom of an empty &sk_buff by reducing the tail
 *	room. This is only allowed for an empty buffer.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_reserve(struct Model0_sk_buff *Model0_skb, int Model0_len)
{
 Model0_skb->Model0_data += Model0_len;
 Model0_skb->Model0_tail += Model0_len;
}

/**
 *	skb_tailroom_reserve - adjust reserved_tailroom
 *	@skb: buffer to alter
 *	@mtu: maximum amount of headlen permitted
 *	@needed_tailroom: minimum amount of reserved_tailroom
 *
 *	Set reserved_tailroom so that headlen can be as large as possible but
 *	not larger than mtu and tailroom cannot be smaller than
 *	needed_tailroom.
 *	The required headroom should already have been reserved before using
 *	this function.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_tailroom_reserve(struct Model0_sk_buff *Model0_skb, unsigned int Model0_mtu,
     unsigned int Model0_needed_tailroom)
{
 do { if (__builtin_expect(!!(Model0_skb_is_nonlinear(Model0_skb)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (2027), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_mtu < Model0_skb_tailroom(Model0_skb) - Model0_needed_tailroom)
  /* use at most mtu */
  Model0_skb->Model0_reserved_tailroom = Model0_skb_tailroom(Model0_skb) - Model0_mtu;
 else
  /* use up to all available space */
  Model0_skb->Model0_reserved_tailroom = Model0_needed_tailroom;
}




static inline __attribute__((no_instrument_function)) void Model0_skb_set_inner_protocol(struct Model0_sk_buff *Model0_skb,
       Model0___be16 Model0_protocol)
{
 Model0_skb->Model0_inner_protocol = Model0_protocol;
 Model0_skb->Model0_inner_protocol_type = 0;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_inner_ipproto(struct Model0_sk_buff *Model0_skb,
      __u8 Model0_ipproto)
{
 Model0_skb->Model0_inner_ipproto = Model0_ipproto;
 Model0_skb->Model0_inner_protocol_type = 1;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_inner_headers(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_inner_mac_header = Model0_skb->Model0_mac_header;
 Model0_skb->Model0_inner_network_header = Model0_skb->Model0_network_header;
 Model0_skb->Model0_inner_transport_header = Model0_skb->Model0_transport_header;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_mac_len(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_mac_len = Model0_skb->Model0_network_header - Model0_skb->Model0_mac_header;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_inner_transport_header(const struct Model0_sk_buff
       *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_inner_transport_header;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_inner_transport_offset(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_inner_transport_header(Model0_skb) - Model0_skb->Model0_data;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_inner_transport_header(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_inner_transport_header = Model0_skb->Model0_data - Model0_skb->Model0_head;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_inner_transport_header(struct Model0_sk_buff *Model0_skb,
         const int Model0_offset)
{
 Model0_skb_reset_inner_transport_header(Model0_skb);
 Model0_skb->Model0_inner_transport_header += Model0_offset;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_inner_network_header(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_inner_network_header;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_inner_network_header(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_inner_network_header = Model0_skb->Model0_data - Model0_skb->Model0_head;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_inner_network_header(struct Model0_sk_buff *Model0_skb,
      const int Model0_offset)
{
 Model0_skb_reset_inner_network_header(Model0_skb);
 Model0_skb->Model0_inner_network_header += Model0_offset;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_inner_mac_header(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_inner_mac_header;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_inner_mac_header(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_inner_mac_header = Model0_skb->Model0_data - Model0_skb->Model0_head;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_inner_mac_header(struct Model0_sk_buff *Model0_skb,
         const int Model0_offset)
{
 Model0_skb_reset_inner_mac_header(Model0_skb);
 Model0_skb->Model0_inner_mac_header += Model0_offset;
}
static inline __attribute__((no_instrument_function)) bool Model0_skb_transport_header_was_set(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_transport_header != (typeof(Model0_skb->Model0_transport_header))~0U;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_transport_header(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_transport_header;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_transport_header(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_transport_header = Model0_skb->Model0_data - Model0_skb->Model0_head;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_transport_header(struct Model0_sk_buff *Model0_skb,
         const int Model0_offset)
{
 Model0_skb_reset_transport_header(Model0_skb);
 Model0_skb->Model0_transport_header += Model0_offset;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_network_header(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_network_header;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_network_header(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_network_header = Model0_skb->Model0_data - Model0_skb->Model0_head;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_network_header(struct Model0_sk_buff *Model0_skb, const int Model0_offset)
{
 Model0_skb_reset_network_header(Model0_skb);
 Model0_skb->Model0_network_header += Model0_offset;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_mac_header(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_mac_header;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_mac_header_was_set(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_mac_header != (typeof(Model0_skb->Model0_mac_header))~0U;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_reset_mac_header(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_mac_header = Model0_skb->Model0_data - Model0_skb->Model0_head;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_mac_header(struct Model0_sk_buff *Model0_skb, const int Model0_offset)
{
 Model0_skb_reset_mac_header(Model0_skb);
 Model0_skb->Model0_mac_header += Model0_offset;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_pop_mac_header(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_mac_header = Model0_skb->Model0_network_header;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_probe_transport_header(struct Model0_sk_buff *Model0_skb,
           const int Model0_offset_hint)
{
 struct Model0_flow_keys Model0_keys;

 if (Model0_skb_transport_header_was_set(Model0_skb))
  return;
 else if (Model0_skb_flow_dissect_flow_keys(Model0_skb, &Model0_keys, 0))
  Model0_skb_set_transport_header(Model0_skb, Model0_keys.Model0_control.Model0_thoff);
 else
  Model0_skb_set_transport_header(Model0_skb, Model0_offset_hint);
}

static inline __attribute__((no_instrument_function)) void Model0_skb_mac_header_rebuild(struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb_mac_header_was_set(Model0_skb)) {
  const unsigned char *Model0_old_mac = Model0_skb_mac_header(Model0_skb);

  Model0_skb_set_mac_header(Model0_skb, -Model0_skb->Model0_mac_len);
  Model0_memmove(Model0_skb_mac_header(Model0_skb), Model0_old_mac, Model0_skb->Model0_mac_len);
 }
}

static inline __attribute__((no_instrument_function)) int Model0_skb_checksum_start_offset(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_csum_start - Model0_skb_headroom(Model0_skb);
}

static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_checksum_start(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_head + Model0_skb->Model0_csum_start;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_transport_offset(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_transport_header(Model0_skb) - Model0_skb->Model0_data;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_skb_network_header_len(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_transport_header - Model0_skb->Model0_network_header;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_skb_inner_network_header_len(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_inner_transport_header - Model0_skb->Model0_inner_network_header;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_network_offset(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_network_header(Model0_skb) - Model0_skb->Model0_data;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_inner_network_offset(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_inner_network_header(Model0_skb) - Model0_skb->Model0_data;
}

static inline __attribute__((no_instrument_function)) int Model0_pskb_network_may_pull(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 return Model0_pskb_may_pull(Model0_skb, Model0_skb_network_offset(Model0_skb) + Model0_len);
}

/*
 * CPUs often take a performance hit when accessing unaligned memory
 * locations. The actual performance hit varies, it can be small if the
 * hardware handles it or large if we have to take an exception and fix it
 * in software.
 *
 * Since an ethernet header is 14 bytes network drivers often end up with
 * the IP header at an unaligned offset. The IP header can be aligned by
 * shifting the start of the packet by 2 bytes. Drivers should do this
 * with:
 *
 * skb_reserve(skb, NET_IP_ALIGN);
 *
 * The downside to this alignment of the IP header is that the DMA is now
 * unaligned. On some architectures the cost of an unaligned DMA is high
 * and this cost outweighs the gains made by aligning the IP header.
 *
 * Since this trade off varies between architectures, we allow NET_IP_ALIGN
 * to be overridden.
 */




/*
 * The networking layer reserves some headroom in skb data (via
 * dev_alloc_skb). This is used to avoid having to reallocate skb data when
 * the header has to grow. In the default case, if the header has to grow
 * 32 bytes or less we avoid the reallocation.
 *
 * Unfortunately this headroom changes the DMA alignment of the resulting
 * network packet. As for NET_IP_ALIGN, this unaligned DMA is expensive
 * on some architectures. An architecture can override this value,
 * perhaps setting it to a cacheline in size (since that will maintain
 * cacheline alignment of the DMA). It must be a power of 2.
 *
 * Various parts of the networking layer expect at least 32 bytes of
 * headroom, you should not reduce this.
 *
 * Using max(32, L1_CACHE_BYTES) makes sense (especially with RPS)
 * to reduce average number of cache lines per packet.
 * get_rps_cpus() for example only access one 64 bytes aligned block :
 * NET_IP_ALIGN(2) + ethernet_header(14) + IP_header(20/40) + ports(8)
 */




int Model0____pskb_trim(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len);

static inline __attribute__((no_instrument_function)) void Model0___skb_trim(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 if (__builtin_expect(!!(Model0_skb_is_nonlinear(Model0_skb)), 0)) {
  ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/skbuff.h", 2301); __builtin_expect(!!(Model0___ret_warn_on), 0); });
  return;
 }
 Model0_skb->Model0_len = Model0_len;
 Model0_skb_set_tail_pointer(Model0_skb, Model0_len);
}

void Model0_skb_trim(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len);

static inline __attribute__((no_instrument_function)) int Model0___pskb_trim(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 if (Model0_skb->Model0_data_len)
  return Model0____pskb_trim(Model0_skb, Model0_len);
 Model0___skb_trim(Model0_skb, Model0_len);
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_pskb_trim(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 return (Model0_len < Model0_skb->Model0_len) ? Model0___pskb_trim(Model0_skb, Model0_len) : 0;
}

/**
 *	pskb_trim_unique - remove end from a paged unique (not cloned) buffer
 *	@skb: buffer to alter
 *	@len: new length
 *
 *	This is identical to pskb_trim except that the caller knows that
 *	the skb is not cloned so we should never get an error due to out-
 *	of-memory.
 */
static inline __attribute__((no_instrument_function)) void Model0_pskb_trim_unique(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 int err = Model0_pskb_trim(Model0_skb, Model0_len);
 do { if (__builtin_expect(!!(err), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (2335), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
}

/**
 *	skb_orphan - orphan a buffer
 *	@skb: buffer to orphan
 *
 *	If a buffer currently has an owner then we call the owner's
 *	destructor function and make the @skb unowned. The buffer continues
 *	to exist but is no longer charged to its former owner.
 */
void Model0_sock_rfree(struct Model0_sk_buff *Model0_skb);
static inline __attribute__((no_instrument_function)) void Model0_skb_orphan(struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb->Model0_destructor) {
  Model0_skb->Model0_destructor(Model0_skb); //we ignore skb->destructor
  Model0_skb->Model0_destructor = ((void *)0);
  Model0_skb->Model0_sk = ((void *)0);
 } else {
#if CY_ABSTRACT1
     //TODO: confirm the correctness. Actual logic is to print BUG_ON, but that's not expected
     //In what case, it will trigger this branch?
     /*
     printf("Bug_ON inside Model0_skb_orphan\n");
     Model0_sock_rfree(Model0_skb);
     Model0_skb->Model0_destructor = ((void *)0);
     Model0_skb->Model0_sk = ((void *)0);
     */
#else
  do { if (__builtin_expect(!!(Model0_skb->Model0_sk), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (2353), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
#endif
 }
}

/**
 *	skb_orphan_frags - orphan the frags contained in a buffer
 *	@skb: buffer to orphan frags from
 *	@gfp_mask: allocation mask for replacement pages
 *
 *	For each frag in the SKB which needs a destructor (i.e. has an
 *	owner) create a copy of that frag and release the original
 *	page by calling the destructor.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_orphan_frags(struct Model0_sk_buff *Model0_skb, Model0_gfp_t Model0_gfp_mask)
{
 if (__builtin_expect(!!(!(((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_tx_flags & Model0_SKBTX_DEV_ZEROCOPY)), 1))
  return 0;
 return Model0_skb_copy_ubufs(Model0_skb, Model0_gfp_mask);
}

/**
 *	__skb_queue_purge - empty a list
 *	@list: list to empty
 *
 *	Delete all buffers on an &sk_buff list. Each buffer is removed from
 *	the list and one reference dropped. This function does not take the
 *	list lock and the caller must hold the relevant locks to use it.
 */
void Model0_skb_queue_purge(struct Model0_sk_buff_head *Model0_list);
static inline __attribute__((no_instrument_function)) void Model0___skb_queue_purge(struct Model0_sk_buff_head *Model0_list)
{
 struct Model0_sk_buff *Model0_skb;
 while ((Model0_skb = Model0___skb_dequeue(Model0_list)) != ((void *)0))
  Model0_kfree_skb(Model0_skb);
}

void *Model0_netdev_alloc_frag(unsigned int Model0_fragsz);

struct Model0_sk_buff *Model0___netdev_alloc_skb(struct Model0_net_device *Model0_dev, unsigned int Model0_length,
       Model0_gfp_t Model0_gfp_mask);

/**
 *	netdev_alloc_skb - allocate an skbuff for rx on a specific device
 *	@dev: network device to receive on
 *	@length: length to allocate
 *
 *	Allocate a new &sk_buff and assign it a usage count of one. The
 *	buffer has unspecified headroom built in. Users should allocate
 *	the headroom they think they need without accounting for the
 *	built in space. The built in space is used for optimisations.
 *
 *	%NULL is returned if there is no free memory. Although this function
 *	allocates memory it can be called from an interrupt.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_netdev_alloc_skb(struct Model0_net_device *Model0_dev,
            unsigned int Model0_length)
{
 return Model0___netdev_alloc_skb(Model0_dev, Model0_length, ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)));
}

/* legacy helper around __netdev_alloc_skb() */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0___dev_alloc_skb(unsigned int Model0_length,
           Model0_gfp_t Model0_gfp_mask)
{
 return Model0___netdev_alloc_skb(((void *)0), Model0_length, Model0_gfp_mask);
}

/* legacy helper around netdev_alloc_skb() */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_dev_alloc_skb(unsigned int Model0_length)
{
 return Model0_netdev_alloc_skb(((void *)0), Model0_length);
}


static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0___netdev_alloc_skb_ip_align(struct Model0_net_device *Model0_dev,
  unsigned int Model0_length, Model0_gfp_t Model0_gfp)
{
 struct Model0_sk_buff *Model0_skb = Model0___netdev_alloc_skb(Model0_dev, Model0_length + 0, Model0_gfp);

 if (0 && Model0_skb)
  Model0_skb_reserve(Model0_skb, 0);
 return Model0_skb;
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_netdev_alloc_skb_ip_align(struct Model0_net_device *Model0_dev,
  unsigned int Model0_length)
{
 return Model0___netdev_alloc_skb_ip_align(Model0_dev, Model0_length, ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)));
}

static inline __attribute__((no_instrument_function)) void Model0_skb_free_frag(void *Model0_addr)
{
 Model0___free_page_frag(Model0_addr);
}

void *Model0_napi_alloc_frag(unsigned int Model0_fragsz);
struct Model0_sk_buff *Model0___napi_alloc_skb(struct Model0_napi_struct *Model0_napi,
     unsigned int Model0_length, Model0_gfp_t Model0_gfp_mask);
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_napi_alloc_skb(struct Model0_napi_struct *Model0_napi,
          unsigned int Model0_length)
{
 return Model0___napi_alloc_skb(Model0_napi, Model0_length, ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)));
}
void Model0_napi_consume_skb(struct Model0_sk_buff *Model0_skb, int Model0_budget);

void Model0___kfree_skb_flush(void);
void Model0___kfree_skb_defer(struct Model0_sk_buff *Model0_skb);

/**
 * __dev_alloc_pages - allocate page for network Rx
 * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx
 * @order: size of the allocation
 *
 * Allocate a new page.
 *
 * %NULL is returned if there is no free memory.
*/
static inline __attribute__((no_instrument_function)) struct Model0_page *Model0___dev_alloc_pages(Model0_gfp_t Model0_gfp_mask,
          unsigned int Model0_order)
{
 /* This piece of code contains several assumptions.
	 * 1.  This is for device Rx, therefor a cold page is preferred.
	 * 2.  The expectation is the user wants a compound page.
	 * 3.  If requesting a order 0 page it will not be compound
	 *     due to the check to see if order has a value in prep_new_page
	 * 4.  __GFP_MEMALLOC is ignored if __GFP_NOMEMALLOC is set due to
	 *     code in gfp_to_alloc_flags that should be enforcing this.
	 */
 Model0_gfp_mask |= (( Model0_gfp_t)0x100u) | (( Model0_gfp_t)0x4000u) | (( Model0_gfp_t)0x2000u);

 return Model0_alloc_pages_node((-1), Model0_gfp_mask, Model0_order);
}

static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_dev_alloc_pages(unsigned int Model0_order)
{
 return Model0___dev_alloc_pages(((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)) | (( Model0_gfp_t)0x200u), Model0_order);
}

/**
 * __dev_alloc_page - allocate a page for network Rx
 * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx
 *
 * Allocate a new page.
 *
 * %NULL is returned if there is no free memory.
 */
static inline __attribute__((no_instrument_function)) struct Model0_page *Model0___dev_alloc_page(Model0_gfp_t Model0_gfp_mask)
{
 return Model0___dev_alloc_pages(Model0_gfp_mask, 0);
}

static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_dev_alloc_page(void)
{
 return Model0_dev_alloc_pages(0);
}

/**
 *	skb_propagate_pfmemalloc - Propagate pfmemalloc if skb is allocated after RX page
 *	@page: The page that was allocated from skb_alloc_page
 *	@skb: The skb that may need pfmemalloc set
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_propagate_pfmemalloc(struct Model0_page *Model0_page,
          struct Model0_sk_buff *Model0_skb)
{
 if (Model0_page_is_pfmemalloc(Model0_page))
  Model0_skb->Model0_pfmemalloc = true;
}

/**
 * skb_frag_page - retrieve the page referred to by a paged fragment
 * @frag: the paged fragment
 *
 * Returns the &struct page associated with @frag.
 */
static inline __attribute__((no_instrument_function)) struct Model0_page *Model0_skb_frag_page(const Model0_skb_frag_t *Model0_frag)
{
 return Model0_frag->Model0_page.Model0_p;
}

/**
 * __skb_frag_ref - take an addition reference on a paged fragment.
 * @frag: the paged fragment
 *
 * Takes an additional reference on the paged fragment @frag.
 */
static inline __attribute__((no_instrument_function)) void Model0___skb_frag_ref(Model0_skb_frag_t *Model0_frag)
{
 Model0_get_page(Model0_skb_frag_page(Model0_frag));
}

/**
 * skb_frag_ref - take an addition reference on a paged fragment of an skb.
 * @skb: the buffer
 * @f: the fragment offset.
 *
 * Takes an additional reference on the @f'th paged fragment of @skb.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_frag_ref(struct Model0_sk_buff *Model0_skb, int Model0_f)
{
 Model0___skb_frag_ref(&((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_frags[Model0_f]);
}

/**
 * __skb_frag_unref - release a reference on a paged fragment.
 * @frag: the paged fragment
 *
 * Releases a reference on the paged fragment @frag.
 */
static inline __attribute__((no_instrument_function)) void Model0___skb_frag_unref(Model0_skb_frag_t *Model0_frag)
{
 Model0_put_page(Model0_skb_frag_page(Model0_frag));
}

/**
 * skb_frag_unref - release a reference on a paged fragment of an skb.
 * @skb: the buffer
 * @f: the fragment offset
 *
 * Releases a reference on the @f'th paged fragment of @skb.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_frag_unref(struct Model0_sk_buff *Model0_skb, int Model0_f)
{
 Model0___skb_frag_unref(&((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_frags[Model0_f]);
}

/**
 * skb_frag_address - gets the address of the data contained in a paged fragment
 * @frag: the paged fragment buffer
 *
 * Returns the address of the data within @frag. The page must already
 * be mapped.
 */
static inline __attribute__((no_instrument_function)) void *Model0_skb_frag_address(const Model0_skb_frag_t *Model0_frag)
{
 return Model0_lowmem_page_address(Model0_skb_frag_page(Model0_frag)) + Model0_frag->Model0_page_offset;
}

/**
 * skb_frag_address_safe - gets the address of the data contained in a paged fragment
 * @frag: the paged fragment buffer
 *
 * Returns the address of the data within @frag. Checks that the page
 * is mapped and returns %NULL otherwise.
 */
static inline __attribute__((no_instrument_function)) void *Model0_skb_frag_address_safe(const Model0_skb_frag_t *Model0_frag)
{
 void *Model0_ptr = Model0_lowmem_page_address(Model0_skb_frag_page(Model0_frag));
 if (__builtin_expect(!!(!Model0_ptr), 0))
  return ((void *)0);

 return Model0_ptr + Model0_frag->Model0_page_offset;
}

/**
 * __skb_frag_set_page - sets the page contained in a paged fragment
 * @frag: the paged fragment
 * @page: the page to set
 *
 * Sets the fragment @frag to contain @page.
 */
static inline __attribute__((no_instrument_function)) void Model0___skb_frag_set_page(Model0_skb_frag_t *Model0_frag, struct Model0_page *Model0_page)
{
 Model0_frag->Model0_page.Model0_p = Model0_page;
}

/**
 * skb_frag_set_page - sets the page contained in a paged fragment of an skb
 * @skb: the buffer
 * @f: the fragment offset
 * @page: the page to set
 *
 * Sets the @f'th fragment of @skb to contain @page.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_frag_set_page(struct Model0_sk_buff *Model0_skb, int Model0_f,
         struct Model0_page *Model0_page)
{
 Model0___skb_frag_set_page(&((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_frags[Model0_f], Model0_page);
}

bool Model0_skb_page_frag_refill(unsigned int Model0_sz, struct Model0_page_frag *Model0_pfrag, Model0_gfp_t Model0_prio);

/**
 * skb_frag_dma_map - maps a paged fragment via the DMA API
 * @dev: the device to map the fragment to
 * @frag: the paged fragment to map
 * @offset: the offset within the fragment (starting at the
 *          fragment's own offset)
 * @size: the number of bytes to map
 * @dir: the direction of the mapping (%PCI_DMA_*)
 *
 * Maps the page associated with @frag to @device.
 */
static inline __attribute__((no_instrument_function)) Model0_dma_addr_t Model0_skb_frag_dma_map(struct Model0_device *Model0_dev,
       const Model0_skb_frag_t *Model0_frag,
       Model0_size_t Model0_offset, Model0_size_t Model0_size,
       enum Model0_dma_data_direction Model0_dir)
{
 return Model0_dma_map_page(Model0_dev, Model0_skb_frag_page(Model0_frag),
       Model0_frag->Model0_page_offset + Model0_offset, Model0_size, Model0_dir);
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_pskb_copy(struct Model0_sk_buff *Model0_skb,
     Model0_gfp_t Model0_gfp_mask)
{
 return Model0___pskb_copy(Model0_skb, Model0_skb_headroom(Model0_skb), Model0_gfp_mask);
}


static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_pskb_copy_for_clone(struct Model0_sk_buff *Model0_skb,
        Model0_gfp_t Model0_gfp_mask)
{
 return Model0___pskb_copy_fclone(Model0_skb, Model0_skb_headroom(Model0_skb), Model0_gfp_mask, true);
}


/**
 *	skb_clone_writable - is the header of a clone writable
 *	@skb: buffer to check
 *	@len: length up to which to write
 *
 *	Returns true if modifying the header part of the cloned buffer
 *	does not requires the data to be copied.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_clone_writable(const struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 return !Model0_skb_header_cloned(Model0_skb) &&
        Model0_skb_headroom(Model0_skb) + Model0_len <= Model0_skb->Model0_hdr_len;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_try_make_writable(struct Model0_sk_buff *Model0_skb,
     unsigned int Model0_write_len)
{
 return Model0_skb_cloned(Model0_skb) && !Model0_skb_clone_writable(Model0_skb, Model0_write_len) &&
        Model0_pskb_expand_head(Model0_skb, 0, 0, ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)));
}

static inline __attribute__((no_instrument_function)) int Model0___skb_cow(struct Model0_sk_buff *Model0_skb, unsigned int Model0_headroom,
       int Model0_cloned)
{
 int Model0_delta = 0;

 if (Model0_headroom > Model0_skb_headroom(Model0_skb))
  Model0_delta = Model0_headroom - Model0_skb_headroom(Model0_skb);

 if (Model0_delta || Model0_cloned)
  return Model0_pskb_expand_head(Model0_skb, ((((Model0_delta)) + ((typeof((Model0_delta)))((({ typeof(32) Model0__max1 = (32); typeof((1 << (6))) Model0__max2 = ((1 << (6))); (void) (&Model0__max1 == &Model0__max2); Model0__max1 > Model0__max2 ? Model0__max1 : Model0__max2; }))) - 1)) & ~((typeof((Model0_delta)))((({ typeof(32) Model0__max1 = (32); typeof((1 << (6))) Model0__max2 = ((1 << (6))); (void) (&Model0__max1 == &Model0__max2); Model0__max1 > Model0__max2 ? Model0__max1 : Model0__max2; }))) - 1)), 0,
     ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)));
 return 0;
}

/**
 *	skb_cow - copy header of skb when it is required
 *	@skb: buffer to cow
 *	@headroom: needed headroom
 *
 *	If the skb passed lacks sufficient headroom or its data part
 *	is shared, data is reallocated. If reallocation fails, an error
 *	is returned and original skb is not changed.
 *
 *	The result is skb with writable area skb->head...skb->tail
 *	and at least @headroom of space at head.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_cow(struct Model0_sk_buff *Model0_skb, unsigned int Model0_headroom)
{
 return Model0___skb_cow(Model0_skb, Model0_headroom, Model0_skb_cloned(Model0_skb));
}

/**
 *	skb_cow_head - skb_cow but only making the head writable
 *	@skb: buffer to cow
 *	@headroom: needed headroom
 *
 *	This function is identical to skb_cow except that we replace the
 *	skb_cloned check by skb_header_cloned.  It should be used when
 *	you only need to push on some header and do not need to modify
 *	the data.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_cow_head(struct Model0_sk_buff *Model0_skb, unsigned int Model0_headroom)
{
 return Model0___skb_cow(Model0_skb, Model0_headroom, Model0_skb_header_cloned(Model0_skb));
}

/**
 *	skb_padto	- pad an skbuff up to a minimal size
 *	@skb: buffer to pad
 *	@len: minimal length
 *
 *	Pads up a buffer to ensure the trailing bytes exist and are
 *	blanked. If the buffer already contains sufficient data it
 *	is untouched. Otherwise it is extended. Returns zero on
 *	success. The skb is freed on error.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_padto(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 unsigned int Model0_size = Model0_skb->Model0_len;
 if (__builtin_expect(!!(Model0_size >= Model0_len), 1))
  return 0;
 return Model0_skb_pad(Model0_skb, Model0_len - Model0_size);
}

/**
 *	skb_put_padto - increase size and pad an skbuff up to a minimal size
 *	@skb: buffer to pad
 *	@len: minimal length
 *
 *	Pads up a buffer to ensure the trailing bytes exist and are
 *	blanked. If the buffer already contains sufficient data it
 *	is untouched. Otherwise it is extended. Returns zero on
 *	success. The skb is freed on error.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_put_padto(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 unsigned int Model0_size = Model0_skb->Model0_len;

 if (__builtin_expect(!!(Model0_size < Model0_len), 0)) {
  Model0_len -= Model0_size;
  if (Model0_skb_pad(Model0_skb, Model0_len))
   return -12;
  Model0___skb_put(Model0_skb, Model0_len);
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_add_data(struct Model0_sk_buff *Model0_skb,
          struct Model0_iov_iter *Model0_from, int Model0_copy)
{
 const int Model0_off = Model0_skb->Model0_len;

 if (Model0_skb->Model0_ip_summed == 0) {
  Model0___wsum Model0_csum = 0;
  if (Model0_csum_and_copy_from_iter(Model0_skb_put(Model0_skb, Model0_copy), Model0_copy,
         &Model0_csum, Model0_from) == Model0_copy) {
   Model0_skb->Model0_csum = Model0_csum_block_add(Model0_skb->Model0_csum, Model0_csum, Model0_off);
   return 0;
  }
 } else if (Model0_copy_from_iter(Model0_skb_put(Model0_skb, Model0_copy), Model0_copy, Model0_from) == Model0_copy)
  return 0;

 Model0___skb_trim(Model0_skb, Model0_off);
 return -14;
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_can_coalesce(struct Model0_sk_buff *Model0_skb, int Model0_i,
        const struct Model0_page *Model0_page, int Model0_off)
{
 if (Model0_i) {
  const struct Model0_skb_frag_struct *Model0_frag = &((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_frags[Model0_i - 1];

  return Model0_page == Model0_skb_frag_page(Model0_frag) &&
         Model0_off == Model0_frag->Model0_page_offset + Model0_skb_frag_size(Model0_frag);
 }
 return false;
}

static inline __attribute__((no_instrument_function)) int Model0___skb_linearize(struct Model0_sk_buff *Model0_skb)
{
 return Model0___pskb_pull_tail(Model0_skb, Model0_skb->Model0_data_len) ? 0 : -12;
}

/**
 *	skb_linearize - convert paged skb to linear one
 *	@skb: buffer to linarize
 *
 *	If there is no free memory -ENOMEM is returned, otherwise zero
 *	is returned and the old skb data released.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_linearize(struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_is_nonlinear(Model0_skb) ? Model0___skb_linearize(Model0_skb) : 0;
}

/**
 * skb_has_shared_frag - can any frag be overwritten
 * @skb: buffer to test
 *
 * Return true if the skb has at least one frag that might be modified
 * by an external entity (as in vmsplice()/sendfile())
 */
static inline __attribute__((no_instrument_function)) bool Model0_skb_has_shared_frag(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_is_nonlinear(Model0_skb) &&
        ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_tx_flags & Model0_SKBTX_SHARED_FRAG;
}

/**
 *	skb_linearize_cow - make sure skb is linear and writable
 *	@skb: buffer to process
 *
 *	If there is no free memory -ENOMEM is returned, otherwise zero
 *	is returned and the old skb data released.
 */
static inline __attribute__((no_instrument_function)) int Model0_skb_linearize_cow(struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_is_nonlinear(Model0_skb) || Model0_skb_cloned(Model0_skb) ?
        Model0___skb_linearize(Model0_skb) : 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model0___skb_postpull_rcsum(struct Model0_sk_buff *Model0_skb, const void *Model0_start, unsigned int Model0_len,
       unsigned int Model0_off)
{
 if (Model0_skb->Model0_ip_summed == 2)
  Model0_skb->Model0_csum = Model0_csum_block_sub(Model0_skb->Model0_csum,
        Model0_csum_partial(Model0_start, Model0_len, 0), Model0_off);
 else if (Model0_skb->Model0_ip_summed == 3 &&
   Model0_skb_checksum_start_offset(Model0_skb) < 0)
  Model0_skb->Model0_ip_summed = 0;
}

/**
 *	skb_postpull_rcsum - update checksum for received skb after pull
 *	@skb: buffer to update
 *	@start: start of data before pull
 *	@len: length of data pulled
 *
 *	After doing a pull on a received packet, you need to call this to
 *	update the CHECKSUM_COMPLETE checksum, or set ip_summed to
 *	CHECKSUM_NONE so that it can be recomputed from scratch.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_postpull_rcsum(struct Model0_sk_buff *Model0_skb,
          const void *Model0_start, unsigned int Model0_len)
{
 Model0___skb_postpull_rcsum(Model0_skb, Model0_start, Model0_len, 0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model0___skb_postpush_rcsum(struct Model0_sk_buff *Model0_skb, const void *Model0_start, unsigned int Model0_len,
       unsigned int Model0_off)
{
 if (Model0_skb->Model0_ip_summed == 2)
  Model0_skb->Model0_csum = Model0_csum_block_add(Model0_skb->Model0_csum,
        Model0_csum_partial(Model0_start, Model0_len, 0), Model0_off);
}

/**
 *	skb_postpush_rcsum - update checksum for received skb after push
 *	@skb: buffer to update
 *	@start: start of data after push
 *	@len: length of data pushed
 *
 *	After doing a push on a received packet, you need to call this to
 *	update the CHECKSUM_COMPLETE checksum.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_postpush_rcsum(struct Model0_sk_buff *Model0_skb,
          const void *Model0_start, unsigned int Model0_len)
{
 Model0___skb_postpush_rcsum(Model0_skb, Model0_start, Model0_len, 0);
}

unsigned char *Model0_skb_pull_rcsum(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len);

/**
 *	skb_push_rcsum - push skb and update receive checksum
 *	@skb: buffer to update
 *	@len: length of data pulled
 *
 *	This function performs an skb_push on the packet and updates
 *	the CHECKSUM_COMPLETE checksum.  It should be used on
 *	receive path processing instead of skb_push unless you know
 *	that the checksum difference is zero (e.g., a valid IP header)
 *	or you are setting ip_summed to CHECKSUM_NONE.
 */
static inline __attribute__((no_instrument_function)) unsigned char *Model0_skb_push_rcsum(struct Model0_sk_buff *Model0_skb,
         unsigned int Model0_len)
{
 Model0_skb_push(Model0_skb, Model0_len);
 Model0_skb_postpush_rcsum(Model0_skb, Model0_skb->Model0_data, Model0_len);
 return Model0_skb->Model0_data;
}

/**
 *	pskb_trim_rcsum - trim received skb and update checksum
 *	@skb: buffer to trim
 *	@len: new length
 *
 *	This is exactly the same as pskb_trim except that it ensures the
 *	checksum of received packets are still valid after the operation.
 */

static inline __attribute__((no_instrument_function)) int Model0_pskb_trim_rcsum(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 if (__builtin_expect(!!(Model0_len >= Model0_skb->Model0_len), 1))
  return 0;
 if (Model0_skb->Model0_ip_summed == 2)
  Model0_skb->Model0_ip_summed = 0;
 return Model0___pskb_trim(Model0_skb, Model0_len);
}
static inline __attribute__((no_instrument_function)) bool Model0_skb_has_frag_list(const struct Model0_sk_buff *Model0_skb)
{
 return ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_frag_list != ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_skb_frag_list_init(struct Model0_sk_buff *Model0_skb)
{
 ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_frag_list = ((void *)0);
}





int Model0___skb_wait_for_more_packets(struct Model0_sock *Model0_sk, int *err, long *Model0_timeo_p,
    const struct Model0_sk_buff *Model0_skb);
struct Model0_sk_buff *Model0___skb_try_recv_datagram(struct Model0_sock *Model0_sk, unsigned Model0_flags,
     int *Model0_peeked, int *Model0_off, int *err,
     struct Model0_sk_buff **Model0_last);
struct Model0_sk_buff *Model0___skb_recv_datagram(struct Model0_sock *Model0_sk, unsigned Model0_flags,
        int *Model0_peeked, int *Model0_off, int *err);
struct Model0_sk_buff *Model0_skb_recv_datagram(struct Model0_sock *Model0_sk, unsigned Model0_flags, int Model0_noblock,
      int *err);
unsigned int Model0_datagram_poll(struct Model0_file *Model0_file, struct Model0_socket *Model0_sock,
      struct Model0_poll_table_struct *Model0_wait);
int Model0_skb_copy_datagram_iter(const struct Model0_sk_buff *Model0_from, int Model0_offset,
      struct Model0_iov_iter *Model0_to, int Model0_size);
static inline __attribute__((no_instrument_function)) int Model0_skb_copy_datagram_msg(const struct Model0_sk_buff *Model0_from, int Model0_offset,
     struct Model0_msghdr *Model0_msg, int Model0_size)
{
 return Model0_skb_copy_datagram_iter(Model0_from, Model0_offset, &Model0_msg->Model0_msg_iter, Model0_size);
}
int Model0_skb_copy_and_csum_datagram_msg(struct Model0_sk_buff *Model0_skb, int Model0_hlen,
       struct Model0_msghdr *Model0_msg);
int Model0_skb_copy_datagram_from_iter(struct Model0_sk_buff *Model0_skb, int Model0_offset,
     struct Model0_iov_iter *Model0_from, int Model0_len);
int Model0_zerocopy_sg_from_iter(struct Model0_sk_buff *Model0_skb, struct Model0_iov_iter *Model0_frm);
void Model0_skb_free_datagram(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
void Model0___skb_free_datagram_locked(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, int Model0_len);
static inline __attribute__((no_instrument_function)) void Model0_skb_free_datagram_locked(struct Model0_sock *Model0_sk,
         struct Model0_sk_buff *Model0_skb)
{
 Model0___skb_free_datagram_locked(Model0_sk, Model0_skb, 0);
}
int Model0_skb_kill_datagram(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, unsigned int Model0_flags);
int Model0_skb_copy_bits(const struct Model0_sk_buff *Model0_skb, int Model0_offset, void *Model0_to, int Model0_len);
int Model0_skb_store_bits(struct Model0_sk_buff *Model0_skb, int Model0_offset, const void *Model0_from, int Model0_len);
Model0___wsum Model0_skb_copy_and_csum_bits(const struct Model0_sk_buff *Model0_skb, int Model0_offset, Model0_u8 *Model0_to,
         int Model0_len, Model0___wsum Model0_csum);
Model0_ssize_t Model0_skb_socket_splice(struct Model0_sock *Model0_sk,
     struct Model0_pipe_inode_info *Model0_pipe,
     struct Model0_splice_pipe_desc *Model0_spd);
int Model0_skb_splice_bits(struct Model0_sk_buff *Model0_skb, struct Model0_sock *Model0_sk, unsigned int Model0_offset,
      struct Model0_pipe_inode_info *Model0_pipe, unsigned int Model0_len,
      unsigned int Model0_flags,
      Model0_ssize_t (*Model0_splice_cb)(struct Model0_sock *,
      struct Model0_pipe_inode_info *,
      struct Model0_splice_pipe_desc *));
void Model0_skb_copy_and_csum_dev(const struct Model0_sk_buff *Model0_skb, Model0_u8 *Model0_to);
unsigned int Model0_skb_zerocopy_headlen(const struct Model0_sk_buff *Model0_from);
int Model0_skb_zerocopy(struct Model0_sk_buff *Model0_to, struct Model0_sk_buff *Model0_from,
   int Model0_len, int Model0_hlen);
void Model0_skb_split(struct Model0_sk_buff *Model0_skb, struct Model0_sk_buff *Model0_skb1, const Model0_u32 Model0_len);
int Model0_skb_shift(struct Model0_sk_buff *Model0_tgt, struct Model0_sk_buff *Model0_skb, int Model0_shiftlen);
void Model0_skb_scrub_packet(struct Model0_sk_buff *Model0_skb, bool Model0_xnet);
unsigned int Model0_skb_gso_transport_seglen(const struct Model0_sk_buff *Model0_skb);
bool Model0_skb_gso_validate_mtu(const struct Model0_sk_buff *Model0_skb, unsigned int Model0_mtu);
struct Model0_sk_buff *Model0_skb_segment(struct Model0_sk_buff *Model0_skb, Model0_netdev_features_t Model0_features);
struct Model0_sk_buff *Model0_skb_vlan_untag(struct Model0_sk_buff *Model0_skb);
int Model0_skb_ensure_writable(struct Model0_sk_buff *Model0_skb, int Model0_write_len);
int Model0_skb_vlan_pop(struct Model0_sk_buff *Model0_skb);
int Model0_skb_vlan_push(struct Model0_sk_buff *Model0_skb, Model0___be16 Model0_vlan_proto, Model0_u16 Model0_vlan_tci);
struct Model0_sk_buff *Model0_pskb_extract(struct Model0_sk_buff *Model0_skb, int Model0_off, int Model0_to_copy,
        Model0_gfp_t Model0_gfp);

static inline __attribute__((no_instrument_function)) int Model0_memcpy_from_msg(void *Model0_data, struct Model0_msghdr *Model0_msg, int Model0_len)
{
 return Model0_copy_from_iter(Model0_data, Model0_len, &Model0_msg->Model0_msg_iter) == Model0_len ? 0 : -14;
}

static inline __attribute__((no_instrument_function)) int Model0_memcpy_to_msg(struct Model0_msghdr *Model0_msg, void *Model0_data, int Model0_len)
{
 return Model0_copy_to_iter(Model0_data, Model0_len, &Model0_msg->Model0_msg_iter) == Model0_len ? 0 : -14;
}

struct Model0_skb_checksum_ops {
 Model0___wsum (*Model0_update)(const void *Model0_mem, int Model0_len, Model0___wsum Model0_wsum);
 Model0___wsum (*Model0_combine)(Model0___wsum Model0_csum, Model0___wsum Model0_csum2, int Model0_offset, int Model0_len);
};

Model0___wsum Model0___skb_checksum(const struct Model0_sk_buff *Model0_skb, int Model0_offset, int Model0_len,
        Model0___wsum Model0_csum, const struct Model0_skb_checksum_ops *Model0_ops);
Model0___wsum Model0_skb_checksum(const struct Model0_sk_buff *Model0_skb, int Model0_offset, int Model0_len,
      Model0___wsum Model0_csum);

static inline __attribute__((no_instrument_function)) void * __attribute__((warn_unused_result))
Model0___skb_header_pointer(const struct Model0_sk_buff *Model0_skb, int Model0_offset,
       int Model0_len, void *Model0_data, int Model0_hlen, void *Model0_buffer)
{
 if (Model0_hlen - Model0_offset >= Model0_len)
  return Model0_data + Model0_offset;

 if (!Model0_skb ||
     Model0_skb_copy_bits(Model0_skb, Model0_offset, Model0_buffer, Model0_len) < 0)
  return ((void *)0);

 return Model0_buffer;
}

static inline __attribute__((no_instrument_function)) void * __attribute__((warn_unused_result))
Model0_skb_header_pointer(const struct Model0_sk_buff *Model0_skb, int Model0_offset, int Model0_len, void *Model0_buffer)
{
 return Model0___skb_header_pointer(Model0_skb, Model0_offset, Model0_len, Model0_skb->Model0_data,
        Model0_skb_headlen(Model0_skb), Model0_buffer);
}

/**
 *	skb_needs_linearize - check if we need to linearize a given skb
 *			      depending on the given device features.
 *	@skb: socket buffer to check
 *	@features: net device features
 *
 *	Returns true if either:
 *	1. skb has frag_list and the device doesn't support FRAGLIST, or
 *	2. skb is fragmented and the device does not support SG.
 */
static inline __attribute__((no_instrument_function)) bool Model0_skb_needs_linearize(struct Model0_sk_buff *Model0_skb,
           Model0_netdev_features_t Model0_features)
{
 return Model0_skb_is_nonlinear(Model0_skb) &&
        ((Model0_skb_has_frag_list(Model0_skb) && !(Model0_features & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_FRAGLIST_BIT)))) ||
  (((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_nr_frags && !(Model0_features & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_SG_BIT)))));
}

static inline __attribute__((no_instrument_function)) void Model0_skb_copy_from_linear_data(const struct Model0_sk_buff *Model0_skb,
          void *Model0_to,
          const unsigned int Model0_len)
{
 ({ Model0_size_t Model0___len = (Model0_len); void *Model0___ret; if (__builtin_constant_p(Model0_len) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_to), (Model0_skb->Model0_data), Model0___len); else Model0___ret = __builtin_memcpy((Model0_to), (Model0_skb->Model0_data), Model0___len); Model0___ret; });
}

static inline __attribute__((no_instrument_function)) void Model0_skb_copy_from_linear_data_offset(const struct Model0_sk_buff *Model0_skb,
          const int Model0_offset, void *Model0_to,
          const unsigned int Model0_len)
{
 ({ Model0_size_t Model0___len = (Model0_len); void *Model0___ret; if (__builtin_constant_p(Model0_len) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_to), (Model0_skb->Model0_data + Model0_offset), Model0___len); else Model0___ret = __builtin_memcpy((Model0_to), (Model0_skb->Model0_data + Model0_offset), Model0___len); Model0___ret; });
}

static inline __attribute__((no_instrument_function)) void Model0_skb_copy_to_linear_data(struct Model0_sk_buff *Model0_skb,
        const void *Model0_from,
        const unsigned int Model0_len)
{
 ({ Model0_size_t Model0___len = (Model0_len); void *Model0___ret; if (__builtin_constant_p(Model0_len) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_skb->Model0_data), (Model0_from), Model0___len); else Model0___ret = __builtin_memcpy((Model0_skb->Model0_data), (Model0_from), Model0___len); Model0___ret; });
}

static inline __attribute__((no_instrument_function)) void Model0_skb_copy_to_linear_data_offset(struct Model0_sk_buff *Model0_skb,
        const int Model0_offset,
        const void *Model0_from,
        const unsigned int Model0_len)
{
 ({ Model0_size_t Model0___len = (Model0_len); void *Model0___ret; if (__builtin_constant_p(Model0_len) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_skb->Model0_data + Model0_offset), (Model0_from), Model0___len); else Model0___ret = __builtin_memcpy((Model0_skb->Model0_data + Model0_offset), (Model0_from), Model0___len); Model0___ret; });
}

void Model0_skb_init(void);

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_skb_get_ktime(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_tstamp;
}

/**
 *	skb_get_timestamp - get timestamp from a skb
 *	@skb: skb to get stamp from
 *	@stamp: pointer to struct timeval to store stamp in
 *
 *	Timestamps are stored in the skb as offsets to a base timestamp.
 *	This function converts the offset back to a struct timeval and stores
 *	it in stamp.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_get_timestamp(const struct Model0_sk_buff *Model0_skb,
         struct Model0_timeval *Model0_stamp)
{
 *Model0_stamp = Model0_ns_to_timeval((Model0_skb->Model0_tstamp).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) void Model0_skb_get_timestampns(const struct Model0_sk_buff *Model0_skb,
           struct Model0_timespec *Model0_stamp)
{
 *Model0_stamp = Model0_ns_to_timespec((Model0_skb->Model0_tstamp).Model0_tv64);
}

static inline __attribute__((no_instrument_function)) void Model0___net_timestamp(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_tstamp = Model0_ktime_get_real();
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_net_timedelta(Model0_ktime_t Model0_t)
{
 return ({ (Model0_ktime_t){ .Model0_tv64 = (Model0_ktime_get_real()).Model0_tv64 - (Model0_t).Model0_tv64 }; });
}

static inline __attribute__((no_instrument_function)) Model0_ktime_t Model0_net_invalid_timestamp(void)
{
 return Model0_ktime_set(0, 0);
}

struct Model0_sk_buff *Model0_skb_clone_sk(struct Model0_sk_buff *Model0_skb);
static inline __attribute__((no_instrument_function)) void Model0_skb_clone_tx_timestamp(struct Model0_sk_buff *Model0_skb)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_defer_rx_timestamp(struct Model0_sk_buff *Model0_skb)
{
 return false;
}



/**
 * skb_complete_tx_timestamp() - deliver cloned skb with tx timestamps
 *
 * PHY drivers may accept clones of transmitted packets for
 * timestamping via their phy_driver.txtstamp method. These drivers
 * must call this function to return the skb back to the stack with a
 * timestamp.
 *
 * @skb: clone of the the original outgoing packet
 * @hwtstamps: hardware time stamps
 *
 */
void Model0_skb_complete_tx_timestamp(struct Model0_sk_buff *Model0_skb,
          struct Model0_skb_shared_hwtstamps *Model0_hwtstamps);

void Model0___skb_tstamp_tx(struct Model0_sk_buff *Model0_orig_skb,
       struct Model0_skb_shared_hwtstamps *Model0_hwtstamps,
       struct Model0_sock *Model0_sk, int Model0_tstype);

/**
 * skb_tstamp_tx - queue clone of skb with send time stamps
 * @orig_skb:	the original outgoing packet
 * @hwtstamps:	hardware time stamps, may be NULL if not available
 *
 * If the skb has a socket associated, then this function clones the
 * skb (thus sharing the actual data and optional structures), stores
 * the optional hardware time stamping information (if non NULL) or
 * generates a software time stamp (otherwise), then queues the clone
 * to the error queue of the socket.  Errors are silently ignored.
 */
void Model0_skb_tstamp_tx(struct Model0_sk_buff *Model0_orig_skb,
     struct Model0_skb_shared_hwtstamps *Model0_hwtstamps);

static inline __attribute__((no_instrument_function)) void Model0_sw_tx_timestamp(struct Model0_sk_buff *Model0_skb)
{
 if (((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_tx_flags & Model0_SKBTX_SW_TSTAMP &&
     !(((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_tx_flags & Model0_SKBTX_IN_PROGRESS))
  Model0_skb_tstamp_tx(Model0_skb, ((void *)0));
}

/**
 * skb_tx_timestamp() - Driver hook for transmit timestamping
 *
 * Ethernet MAC Drivers should call this function in their hard_xmit()
 * function immediately before giving the sk_buff to the MAC hardware.
 *
 * Specifically, one should make absolutely sure that this function is
 * called before TX completion of this packet can trigger.  Otherwise
 * the packet could potentially already be freed.
 *
 * @skb: A socket buffer.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_tx_timestamp(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb_clone_tx_timestamp(Model0_skb);
 Model0_sw_tx_timestamp(Model0_skb);
}

/**
 * skb_complete_wifi_ack - deliver skb with wifi status
 *
 * @skb: the original outgoing packet
 * @acked: ack status
 *
 */
void Model0_skb_complete_wifi_ack(struct Model0_sk_buff *Model0_skb, bool Model0_acked);

Model0___sum16 Model0___skb_checksum_complete_head(struct Model0_sk_buff *Model0_skb, int Model0_len);
Model0___sum16 Model0___skb_checksum_complete(struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) int Model0_skb_csum_unnecessary(const struct Model0_sk_buff *Model0_skb)
{
#if CY_ABSTRACT1
     return 1; //csum is always unnecessary
#else
 return ((Model0_skb->Model0_ip_summed == 1) ||
  Model0_skb->Model0_csum_valid ||
  (Model0_skb->Model0_ip_summed == 3 &&
   Model0_skb_checksum_start_offset(Model0_skb) >= 0));
#endif
}

/**
 *	skb_checksum_complete - Calculate checksum of an entire packet
 *	@skb: packet to process
 *
 *	This function calculates the checksum over the entire packet plus
 *	the value of skb->csum.  The latter can be used to supply the
 *	checksum of a pseudo header as used by TCP/UDP.  It returns the
 *	checksum.
 *
 *	For protocols that contain complete checksums such as ICMP/TCP/UDP,
 *	this function can be used to verify that checksum on received
 *	packets.  In that case the function should return zero if the
 *	checksum is correct.  In particular, this function will return zero
 *	if skb->ip_summed is CHECKSUM_UNNECESSARY which indicates that the
 *	hardware has already verified the correctness of the checksum.
 */
static inline __attribute__((no_instrument_function)) Model0___sum16 Model0_skb_checksum_complete(struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_csum_unnecessary(Model0_skb) ?
        0 : Model0___skb_checksum_complete(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0___skb_decr_checksum_unnecessary(struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb->Model0_ip_summed == 1) {
  if (Model0_skb->Model0_csum_level == 0)
   Model0_skb->Model0_ip_summed = 0;
  else
   Model0_skb->Model0_csum_level--;
 }
}

static inline __attribute__((no_instrument_function)) void Model0___skb_incr_checksum_unnecessary(struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb->Model0_ip_summed == 1) {
  if (Model0_skb->Model0_csum_level < 3)
   Model0_skb->Model0_csum_level++;
 } else if (Model0_skb->Model0_ip_summed == 0) {
  Model0_skb->Model0_ip_summed = 1;
  Model0_skb->Model0_csum_level = 0;
 }
}

static inline __attribute__((no_instrument_function)) void Model0___skb_mark_checksum_bad(struct Model0_sk_buff *Model0_skb)
{
 /* Mark current checksum as bad (typically called from GRO
	 * path). In the case that ip_summed is CHECKSUM_NONE
	 * this must be the first checksum encountered in the packet.
	 * When ip_summed is CHECKSUM_UNNECESSARY, this is the first
	 * checksum after the last one validated. For UDP, a zero
	 * checksum can not be marked as bad.
	 */

 if (Model0_skb->Model0_ip_summed == 0 ||
     Model0_skb->Model0_ip_summed == 1)
  Model0_skb->Model0_csum_bad = 1;
}

/* Check if we need to perform checksum complete validation.
 *
 * Returns true if checksum complete is needed, false otherwise
 * (either checksum is unnecessary or zero checksum is allowed).
 */
static inline __attribute__((no_instrument_function)) bool Model0___skb_checksum_validate_needed(struct Model0_sk_buff *Model0_skb,
        bool Model0_zero_okay,
        Model0___sum16 Model0_check)
{
 if (Model0_skb_csum_unnecessary(Model0_skb) || (Model0_zero_okay && !Model0_check)) {
  Model0_skb->Model0_csum_valid = 1;
  Model0___skb_decr_checksum_unnecessary(Model0_skb);
  return false;
 }

 return true;
}

/* For small packets <= CHECKSUM_BREAK peform checksum complete directly
 * in checksum_init.
 */


/* Unset checksum-complete
 *
 * Unset checksum complete can be done when packet is being modified
 * (uncompressed for instance) and checksum-complete value is
 * invalidated.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_checksum_complete_unset(struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb->Model0_ip_summed == 2)
  Model0_skb->Model0_ip_summed = 0;
}

/* Validate (init) checksum based on checksum complete.
 *
 * Return values:
 *   0: checksum is validated or try to in skb_checksum_complete. In the latter
 *	case the ip_summed will not be CHECKSUM_UNNECESSARY and the pseudo
 *	checksum is stored in skb->csum for use in __skb_checksum_complete
 *   non-zero: value of invalid checksum
 *
 */
static inline __attribute__((no_instrument_function)) Model0___sum16 Model0___skb_checksum_validate_complete(struct Model0_sk_buff *Model0_skb,
             bool Model0_complete,
             Model0___wsum Model0_psum)
{
 if (Model0_skb->Model0_ip_summed == 2) {
  if (!Model0_csum_fold(Model0_csum_add(Model0_psum, Model0_skb->Model0_csum))) {
   Model0_skb->Model0_csum_valid = 1;
   return 0;
  }
 } else if (Model0_skb->Model0_csum_bad) {
  /* ip_summed == CHECKSUM_NONE in this case */
  return ( Model0___sum16)1;
 }

 Model0_skb->Model0_csum = Model0_psum;

 if (Model0_complete || Model0_skb->Model0_len <= 76) {
  Model0___sum16 Model0_csum;

  Model0_csum = Model0___skb_checksum_complete(Model0_skb);
  Model0_skb->Model0_csum_valid = !Model0_csum;
  return Model0_csum;
 }

 return 0;
}

static inline __attribute__((no_instrument_function)) Model0___wsum Model0_null_compute_pseudo(struct Model0_sk_buff *Model0_skb, int Model0_proto)
{
 return 0;
}

/* Perform checksum validate (init). Note that this is a macro since we only
 * want to calculate the pseudo header which is an input function if necessary.
 * First we try to validate without any computation (checksum unnecessary) and
 * then calculate based on checksum complete calling the function to compute
 * pseudo header.
 *
 * Return values:
 *   0: checksum is validated or try to in skb_checksum_complete
 *   non-zero: value of invalid checksum
 */
static inline __attribute__((no_instrument_function)) bool Model0___skb_checksum_convert_check(struct Model0_sk_buff *Model0_skb)
{
 return (Model0_skb->Model0_ip_summed == 0 &&
  Model0_skb->Model0_csum_valid && !Model0_skb->Model0_csum_bad);
}

static inline __attribute__((no_instrument_function)) void Model0___skb_checksum_convert(struct Model0_sk_buff *Model0_skb,
       Model0___sum16 Model0_check, Model0___wsum Model0_pseudo)
{
 Model0_skb->Model0_csum = ~Model0_pseudo;
 Model0_skb->Model0_ip_summed = 2;
}
static inline __attribute__((no_instrument_function)) void Model0_skb_remcsum_adjust_partial(struct Model0_sk_buff *Model0_skb, void *Model0_ptr,
           Model0_u16 Model0_start, Model0_u16 Model0_offset)
{
 Model0_skb->Model0_ip_summed = 3;
 Model0_skb->Model0_csum_start = ((unsigned char *)Model0_ptr + Model0_start) - Model0_skb->Model0_head;
 Model0_skb->Model0_csum_offset = Model0_offset - Model0_start;
}

/* Update skbuf and packet to reflect the remote checksum offload operation.
 * When called, ptr indicates the starting point for skb->csum when
 * ip_summed is CHECKSUM_COMPLETE. If we need create checksum complete
 * here, skb_postpull_rcsum is done so skb->csum start is ptr.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_remcsum_process(struct Model0_sk_buff *Model0_skb, void *Model0_ptr,
           int Model0_start, int Model0_offset, bool Model0_nopartial)
{
 Model0___wsum Model0_delta;

 if (!Model0_nopartial) {
  Model0_skb_remcsum_adjust_partial(Model0_skb, Model0_ptr, Model0_start, Model0_offset);
  return;
 }

  if (__builtin_expect(!!(Model0_skb->Model0_ip_summed != 2), 0)) {
  Model0___skb_checksum_complete(Model0_skb);
  Model0_skb_postpull_rcsum(Model0_skb, Model0_skb->Model0_data, Model0_ptr - (void *)Model0_skb->Model0_data);
 }

 Model0_delta = Model0_remcsum_adjust(Model0_ptr, Model0_skb->Model0_csum, Model0_start, Model0_offset);

 /* Adjust skb->csum since we changed the packet */
 Model0_skb->Model0_csum = Model0_csum_add(Model0_skb->Model0_csum, Model0_delta);
}


void Model0_nf_conntrack_destroy(struct Model0_nf_conntrack *Model0_nfct);
static inline __attribute__((no_instrument_function)) void Model0_nf_conntrack_put(struct Model0_nf_conntrack *Model0_nfct)
{
 if (Model0_nfct && Model0_atomic_dec_and_test(&Model0_nfct->Model0_use))
  Model0_nf_conntrack_destroy(Model0_nfct);
}
static inline __attribute__((no_instrument_function)) void Model0_nf_conntrack_get(struct Model0_nf_conntrack *Model0_nfct)
{
 if (Model0_nfct)
  Model0_atomic_inc(&Model0_nfct->Model0_use);
}
static inline __attribute__((no_instrument_function)) void Model0_nf_reset(struct Model0_sk_buff *Model0_skb)
{

 Model0_nf_conntrack_put(Model0_skb->Model0_nfct);
 Model0_skb->Model0_nfct = ((void *)0);





}

static inline __attribute__((no_instrument_function)) void Model0_nf_reset_trace(struct Model0_sk_buff *Model0_skb)
{



}

/* Note: This doesn't put any conntrack and bridge info in dst. */
static inline __attribute__((no_instrument_function)) void Model0___nf_copy(struct Model0_sk_buff *Model0_dst, const struct Model0_sk_buff *Model0_src,
        bool Model0_copy)
{

 Model0_dst->Model0_nfct = Model0_src->Model0_nfct;
 Model0_nf_conntrack_get(Model0_src->Model0_nfct);
 if (Model0_copy)
  Model0_dst->Model0_nfctinfo = Model0_src->Model0_nfctinfo;
}

static inline __attribute__((no_instrument_function)) void Model0_nf_copy(struct Model0_sk_buff *Model0_dst, const struct Model0_sk_buff *Model0_src)
{

 Model0_nf_conntrack_put(Model0_dst->Model0_nfct);




 Model0___nf_copy(Model0_dst, Model0_src, true);
}


static inline __attribute__((no_instrument_function)) void Model0_skb_copy_secmark(struct Model0_sk_buff *Model0_to, const struct Model0_sk_buff *Model0_from)
{
 Model0_to->Model0_secmark = Model0_from->Model0_secmark;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_init_secmark(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_secmark = 0;
}
static inline __attribute__((no_instrument_function)) bool Model0_skb_irq_freeable(const struct Model0_sk_buff *Model0_skb)
{
 return !Model0_skb->Model0_destructor &&

  !Model0_skb->Model0_sp &&


  !Model0_skb->Model0_nfct &&

  !Model0_skb->Model0__skb_refdst &&
  !Model0_skb_has_frag_list(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_queue_mapping(struct Model0_sk_buff *Model0_skb, Model0_u16 Model0_queue_mapping)
{
 Model0_skb->Model0_queue_mapping = Model0_queue_mapping;
}

static inline __attribute__((no_instrument_function)) Model0_u16 Model0_skb_get_queue_mapping(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_queue_mapping;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_copy_queue_mapping(struct Model0_sk_buff *Model0_to, const struct Model0_sk_buff *Model0_from)
{
 Model0_to->Model0_queue_mapping = Model0_from->Model0_queue_mapping;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_record_rx_queue(struct Model0_sk_buff *Model0_skb, Model0_u16 Model0_rx_queue)
{
 Model0_skb->Model0_queue_mapping = Model0_rx_queue + 1;
}

static inline __attribute__((no_instrument_function)) Model0_u16 Model0_skb_get_rx_queue(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_queue_mapping - 1;
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_rx_queue_recorded(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_queue_mapping != 0;
}

static inline __attribute__((no_instrument_function)) struct Model0_sec_path *Model0_skb_sec_path(struct Model0_sk_buff *Model0_skb)
{

 return Model0_skb->Model0_sp;



}

/* Keeps track of mac header offset relative to skb->head.
 * It is useful for TSO of Tunneling protocol. e.g. GRE.
 * For non-tunnel skb it points to skb_mac_header() and for
 * tunnel skb it points to outer mac header.
 * Keeps track of level of encapsulation of network headers.
 */
struct Model0_skb_gso_cb {
 union {
  int Model0_mac_offset;
  int Model0_data_offset;
 };
 int Model0_encap_level;
 Model0___wsum Model0_csum;
 Model0___u16 Model0_csum_start;
};



static inline __attribute__((no_instrument_function)) int Model0_skb_tnl_header_len(const struct Model0_sk_buff *Model0_inner_skb)
{
 return (Model0_skb_mac_header(Model0_inner_skb) - Model0_inner_skb->Model0_head) -
  ((struct Model0_skb_gso_cb *)((Model0_inner_skb)->Model0_cb + 32))->Model0_mac_offset;
}

static inline __attribute__((no_instrument_function)) int Model0_gso_pskb_expand_head(struct Model0_sk_buff *Model0_skb, int Model0_extra)
{
 int Model0_new_headroom, Model0_headroom;
 int Model0_ret;

 Model0_headroom = Model0_skb_headroom(Model0_skb);
 Model0_ret = Model0_pskb_expand_head(Model0_skb, Model0_extra, 0, ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)));
 if (Model0_ret)
  return Model0_ret;

 Model0_new_headroom = Model0_skb_headroom(Model0_skb);
 ((struct Model0_skb_gso_cb *)((Model0_skb)->Model0_cb + 32))->Model0_mac_offset += (Model0_new_headroom - Model0_headroom);
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_gso_reset_checksum(struct Model0_sk_buff *Model0_skb, Model0___wsum Model0_res)
{
 /* Do not update partial checksums if remote checksum is enabled. */
 if (Model0_skb->Model0_remcsum_offload)
  return;

 ((struct Model0_skb_gso_cb *)((Model0_skb)->Model0_cb + 32))->Model0_csum = Model0_res;
 ((struct Model0_skb_gso_cb *)((Model0_skb)->Model0_cb + 32))->Model0_csum_start = Model0_skb_checksum_start(Model0_skb) - Model0_skb->Model0_head;
}

/* Compute the checksum for a gso segment. First compute the checksum value
 * from the start of transport header to SKB_GSO_CB(skb)->csum_start, and
 * then add in skb->csum (checksum from csum_start to end of packet).
 * skb->csum and csum_start are then updated to reflect the checksum of the
 * resultant packet starting from the transport header-- the resultant checksum
 * is in the res argument (i.e. normally zero or ~ of checksum of a pseudo
 * header.
 */
static inline __attribute__((no_instrument_function)) Model0___sum16 Model0_gso_make_checksum(struct Model0_sk_buff *Model0_skb, Model0___wsum Model0_res)
{
 unsigned char *Model0_csum_start = Model0_skb_transport_header(Model0_skb);
 int Model0_plen = (Model0_skb->Model0_head + ((struct Model0_skb_gso_cb *)((Model0_skb)->Model0_cb + 32))->Model0_csum_start) - Model0_csum_start;
 Model0___wsum Model0_partial = ((struct Model0_skb_gso_cb *)((Model0_skb)->Model0_cb + 32))->Model0_csum;

 ((struct Model0_skb_gso_cb *)((Model0_skb)->Model0_cb + 32))->Model0_csum = Model0_res;
 ((struct Model0_skb_gso_cb *)((Model0_skb)->Model0_cb + 32))->Model0_csum_start = Model0_csum_start - Model0_skb->Model0_head;

 return Model0_csum_fold(Model0_csum_partial(Model0_csum_start, Model0_plen, Model0_partial));
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_is_gso(const struct Model0_sk_buff *Model0_skb)
{
 return ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_gso_size;
}

/* Note: Should be called only if skb_is_gso(skb) is true */
static inline __attribute__((no_instrument_function)) bool Model0_skb_is_gso_v6(const struct Model0_sk_buff *Model0_skb)
{
 return ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_gso_type & Model0_SKB_GSO_TCPV6;
}

void Model0___skb_warn_lro_forwarding(const struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) bool Model0_skb_warn_if_lro(const struct Model0_sk_buff *Model0_skb)
{
 /* LRO sets gso_size but not gso_type, whereas if GSO is really
	 * wanted then gso_type will be set. */
 const struct Model0_skb_shared_info *Model0_shinfo = ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)));

 if (Model0_skb_is_nonlinear(Model0_skb) && Model0_shinfo->Model0_gso_size != 0 &&
     __builtin_expect(!!(Model0_shinfo->Model0_gso_type == 0), 0)) {
  Model0___skb_warn_lro_forwarding(Model0_skb);
  return true;
 }
 return false;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_forward_csum(struct Model0_sk_buff *Model0_skb)
{
 /* Unfortunately we don't support this one.  Any brave souls? */
 if (Model0_skb->Model0_ip_summed == 2)
  Model0_skb->Model0_ip_summed = 0;
}

/**
 * skb_checksum_none_assert - make sure skb ip_summed is CHECKSUM_NONE
 * @skb: skb to check
 *
 * fresh skbs have their ip_summed set to CHECKSUM_NONE.
 * Instead of forcing ip_summed to CHECKSUM_NONE, we can
 * use this helper, to document places where we make this assertion.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_checksum_none_assert(const struct Model0_sk_buff *Model0_skb)
{



}

bool Model0_skb_partial_csum_set(struct Model0_sk_buff *Model0_skb, Model0_u16 Model0_start, Model0_u16 Model0_off);

int Model0_skb_checksum_setup(struct Model0_sk_buff *Model0_skb, bool Model0_recalculate);
struct Model0_sk_buff *Model0_skb_checksum_trimmed(struct Model0_sk_buff *Model0_skb,
         unsigned int Model0_transport_len,
         Model0___sum16(*Model0_skb_chkf)(struct Model0_sk_buff *Model0_skb));

/**
 * skb_head_is_locked - Determine if the skb->head is locked down
 * @skb: skb to check
 *
 * The head on skbs build around a head frag can be removed if they are
 * not cloned.  This function returns true if the skb head is locked down
 * due to either being allocated via kmalloc, or by being a clone with
 * multiple references to the head.
 */
static inline __attribute__((no_instrument_function)) bool Model0_skb_head_is_locked(const struct Model0_sk_buff *Model0_skb)
{
 return !Model0_skb->Model0_head_frag || Model0_skb_cloned(Model0_skb);
}

/**
 * skb_gso_network_seglen - Return length of individual segments of a gso packet
 *
 * @skb: GSO skb
 *
 * skb_gso_network_seglen is used to determine the real size of the
 * individual segments, including Layer3 (IP, IPv6) and L4 headers (TCP/UDP).
 *
 * The MAC/L2 header is not accounted for.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_skb_gso_network_seglen(const struct Model0_sk_buff *Model0_skb)
{
 unsigned int Model0_hdr_len = Model0_skb_transport_header(Model0_skb) -
          Model0_skb_network_header(Model0_skb);
 return Model0_hdr_len + Model0_skb_gso_transport_seglen(Model0_skb);
}

/* Local Checksum Offload.
 * Compute outer checksum based on the assumption that the
 * inner checksum will be offloaded later.
 * See Documentation/networking/checksum-offloads.txt for
 * explanation of how this works.
 * Fill in outer checksum adjustment (e.g. with sum of outer
 * pseudo-header) before calling.
 * Also ensure that inner checksum is in linear data area.
 */
static inline __attribute__((no_instrument_function)) Model0___wsum Model0_lco_csum(struct Model0_sk_buff *Model0_skb)
{
 unsigned char *Model0_csum_start = Model0_skb_checksum_start(Model0_skb);
 unsigned char *Model0_l4_hdr = Model0_skb_transport_header(Model0_skb);
 Model0___wsum Model0_partial;

 /* Start with complement of inner checksum adjustment */
 Model0_partial = ~Model0_csum_unfold(*( Model0___sum16 *)(Model0_csum_start +
          Model0_skb->Model0_csum_offset));

 /* Add in checksum of our headers (incl. outer checksum
	 * adjustment filled in by caller) and return result.
	 */
 return Model0_csum_partial(Model0_l4_hdr, Model0_csum_start - Model0_l4_hdr, Model0_partial);
}


static inline __attribute__((no_instrument_function)) struct Model0_ethhdr *Model0_eth_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_ethhdr *)Model0_skb_mac_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_ethhdr *Model0_inner_eth_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_ethhdr *)Model0_skb_inner_mac_header(Model0_skb);
}

int Model0_eth_header_parse(const struct Model0_sk_buff *Model0_skb, unsigned char *Model0_haddr);

extern Model0_ssize_t Model0_sysfs_format_mac(char *Model0_buf, const unsigned char *Model0_addr, int Model0_len);





/* All structures exposed to userland should be defined such that they
 * have the same layout for 32-bit and 64-bit userland.
 */

/**
 * struct ethtool_cmd - DEPRECATED, link control and status
 * This structure is DEPRECATED, please use struct ethtool_link_settings.
 * @cmd: Command number = %ETHTOOL_GSET or %ETHTOOL_SSET
 * @supported: Bitmask of %SUPPORTED_* flags for the link modes,
 *	physical connectors and other link features for which the
 *	interface supports autonegotiation or auto-detection.
 *	Read-only.
 * @advertising: Bitmask of %ADVERTISED_* flags for the link modes,
 *	physical connectors and other link features that are
 *	advertised through autonegotiation or enabled for
 *	auto-detection.
 * @speed: Low bits of the speed, 1Mb units, 0 to INT_MAX or SPEED_UNKNOWN
 * @duplex: Duplex mode; one of %DUPLEX_*
 * @port: Physical connector type; one of %PORT_*
 * @phy_address: MDIO address of PHY (transceiver); 0 or 255 if not
 *	applicable.  For clause 45 PHYs this is the PRTAD.
 * @transceiver: Historically used to distinguish different possible
 *	PHY types, but not in a consistent way.  Deprecated.
 * @autoneg: Enable/disable autonegotiation and auto-detection;
 *	either %AUTONEG_DISABLE or %AUTONEG_ENABLE
 * @mdio_support: Bitmask of %ETH_MDIO_SUPPORTS_* flags for the MDIO
 *	protocols supported by the interface; 0 if unknown.
 *	Read-only.
 * @maxtxpkt: Historically used to report TX IRQ coalescing; now
 *	obsoleted by &struct ethtool_coalesce.  Read-only; deprecated.
 * @maxrxpkt: Historically used to report RX IRQ coalescing; now
 *	obsoleted by &struct ethtool_coalesce.  Read-only; deprecated.
 * @speed_hi: High bits of the speed, 1Mb units, 0 to INT_MAX or SPEED_UNKNOWN
 * @eth_tp_mdix: Ethernet twisted-pair MDI(-X) status; one of
 *	%ETH_TP_MDI_*.  If the status is unknown or not applicable, the
 *	value will be %ETH_TP_MDI_INVALID.  Read-only.
 * @eth_tp_mdix_ctrl: Ethernet twisted pair MDI(-X) control; one of
 *	%ETH_TP_MDI_*.  If MDI(-X) control is not implemented, reads
 *	yield %ETH_TP_MDI_INVALID and writes may be ignored or rejected.
 *	When written successfully, the link should be renegotiated if
 *	necessary.
 * @lp_advertising: Bitmask of %ADVERTISED_* flags for the link modes
 *	and other link features that the link partner advertised
 *	through autonegotiation; 0 if unknown or not applicable.
 *	Read-only.
 *
 * The link speed in Mbps is split between @speed and @speed_hi.  Use
 * the ethtool_cmd_speed() and ethtool_cmd_speed_set() functions to
 * access it.
 *
 * If autonegotiation is disabled, the speed and @duplex represent the
 * fixed link mode and are writable if the driver supports multiple
 * link modes.  If it is enabled then they are read-only; if the link
 * is up they represent the negotiated link mode; if the link is down,
 * the speed is 0, %SPEED_UNKNOWN or the highest enabled speed and
 * @duplex is %DUPLEX_UNKNOWN or the best enabled duplex mode.
 *
 * Some hardware interfaces may have multiple PHYs and/or physical
 * connectors fitted or do not allow the driver to detect which are
 * fitted.  For these interfaces @port and/or @phy_address may be
 * writable, possibly dependent on @autoneg being %AUTONEG_DISABLE.
 * Otherwise, attempts to write different values may be ignored or
 * rejected.
 *
 * Users should assume that all fields not marked read-only are
 * writable and subject to validation by the driver.  They should use
 * %ETHTOOL_GSET to get the current values before making specific
 * changes and then applying them with %ETHTOOL_SSET.
 *
 * Drivers that implement set_settings() should validate all fields
 * other than @cmd that are not described as read-only or deprecated,
 * and must ignore all fields described as read-only.
 *
 * Deprecated fields should be ignored by both users and drivers.
 */
struct Model0_ethtool_cmd {
 __u32 Model0_cmd;
 __u32 Model0_supported;
 __u32 Model0_advertising;
 Model0___u16 Model0_speed;
 __u8 Model0_duplex;
 __u8 Model0_port;
 __u8 Model0_phy_address;
 __u8 Model0_transceiver;
 __u8 Model0_autoneg;
 __u8 Model0_mdio_support;
 __u32 Model0_maxtxpkt;
 __u32 Model0_maxrxpkt;
 Model0___u16 Model0_speed_hi;
 __u8 Model0_eth_tp_mdix;
 __u8 Model0_eth_tp_mdix_ctrl;
 __u32 Model0_lp_advertising;
 __u32 Model0_reserved[2];
};

static inline __attribute__((no_instrument_function)) void Model0_ethtool_cmd_speed_set(struct Model0_ethtool_cmd *Model0_ep,
      __u32 Model0_speed)
{

 Model0_ep->Model0_speed = (Model0___u16)Model0_speed;
 Model0_ep->Model0_speed_hi = (Model0___u16)(Model0_speed >> 16);
}

static inline __attribute__((no_instrument_function)) __u32 Model0_ethtool_cmd_speed(const struct Model0_ethtool_cmd *Model0_ep)
{
 return (Model0_ep->Model0_speed_hi << 16) | Model0_ep->Model0_speed;
}

/* Device supports clause 22 register access to PHY or peripherals
 * using the interface defined in <linux/mii.h>.  This should not be
 * set if there are known to be no such peripherals present or if
 * the driver only emulates clause 22 registers for compatibility.
 */


/* Device supports clause 45 register access to PHY or peripherals
 * using the interface defined in <linux/mii.h> and <linux/mdio.h>.
 * This should not be set if there are known to be no such peripherals
 * present.
 */






/**
 * struct ethtool_drvinfo - general driver and device information
 * @cmd: Command number = %ETHTOOL_GDRVINFO
 * @driver: Driver short name.  This should normally match the name
 *	in its bus driver structure (e.g. pci_driver::name).  Must
 *	not be an empty string.
 * @version: Driver version string; may be an empty string
 * @fw_version: Firmware version string; may be an empty string
 * @erom_version: Expansion ROM version string; may be an empty string
 * @bus_info: Device bus address.  This should match the dev_name()
 *	string for the underlying bus device, if there is one.  May be
 *	an empty string.
 * @n_priv_flags: Number of flags valid for %ETHTOOL_GPFLAGS and
 *	%ETHTOOL_SPFLAGS commands; also the number of strings in the
 *	%ETH_SS_PRIV_FLAGS set
 * @n_stats: Number of u64 statistics returned by the %ETHTOOL_GSTATS
 *	command; also the number of strings in the %ETH_SS_STATS set
 * @testinfo_len: Number of results returned by the %ETHTOOL_TEST
 *	command; also the number of strings in the %ETH_SS_TEST set
 * @eedump_len: Size of EEPROM accessible through the %ETHTOOL_GEEPROM
 *	and %ETHTOOL_SEEPROM commands, in bytes
 * @regdump_len: Size of register dump returned by the %ETHTOOL_GREGS
 *	command, in bytes
 *
 * Users can use the %ETHTOOL_GSSET_INFO command to get the number of
 * strings in any string set (from Linux 2.6.34).
 *
 * Drivers should set at most @driver, @version, @fw_version and
 * @bus_info in their get_drvinfo() implementation.  The ethtool
 * core fills in the other fields using other driver operations.
 */
struct Model0_ethtool_drvinfo {
 __u32 Model0_cmd;
 char Model0_driver[32];
 char Model0_version[32];
 char Model0_fw_version[32];
 char Model0_bus_info[32];
 char Model0_erom_version[32];
 char Model0_reserved2[12];
 __u32 Model0_n_priv_flags;
 __u32 Model0_n_stats;
 __u32 Model0_testinfo_len;
 __u32 Model0_eedump_len;
 __u32 Model0_regdump_len;
};



/**
 * struct ethtool_wolinfo - Wake-On-Lan configuration
 * @cmd: Command number = %ETHTOOL_GWOL or %ETHTOOL_SWOL
 * @supported: Bitmask of %WAKE_* flags for supported Wake-On-Lan modes.
 *	Read-only.
 * @wolopts: Bitmask of %WAKE_* flags for enabled Wake-On-Lan modes.
 * @sopass: SecureOn(tm) password; meaningful only if %WAKE_MAGICSECURE
 *	is set in @wolopts.
 */
struct Model0_ethtool_wolinfo {
 __u32 Model0_cmd;
 __u32 Model0_supported;
 __u32 Model0_wolopts;
 __u8 Model0_sopass[6];
};

/* for passing single values */
struct Model0_ethtool_value {
 __u32 Model0_cmd;
 __u32 Model0_data;
};

enum Model0_tunable_id {
 Model0_ETHTOOL_ID_UNSPEC,
 Model0_ETHTOOL_RX_COPYBREAK,
 Model0_ETHTOOL_TX_COPYBREAK,
 /*
	 * Add your fresh new tubale attribute above and remember to update
	 * tunable_strings[] in net/core/ethtool.c
	 */
 Model0___ETHTOOL_TUNABLE_COUNT,
};

enum Model0_tunable_type_id {
 Model0_ETHTOOL_TUNABLE_UNSPEC,
 Model0_ETHTOOL_TUNABLE_U8,
 Model0_ETHTOOL_TUNABLE_U16,
 Model0_ETHTOOL_TUNABLE_U32,
 Model0_ETHTOOL_TUNABLE_U64,
 Model0_ETHTOOL_TUNABLE_STRING,
 Model0_ETHTOOL_TUNABLE_S8,
 Model0_ETHTOOL_TUNABLE_S16,
 Model0_ETHTOOL_TUNABLE_S32,
 Model0_ETHTOOL_TUNABLE_S64,
};

struct Model0_ethtool_tunable {
 __u32 Model0_cmd;
 __u32 Model0_id;
 __u32 Model0_type_id;
 __u32 Model0_len;
 void *Model0_data[0];
};

/**
 * struct ethtool_regs - hardware register dump
 * @cmd: Command number = %ETHTOOL_GREGS
 * @version: Dump format version.  This is driver-specific and may
 *	distinguish different chips/revisions.  Drivers must use new
 *	version numbers whenever the dump format changes in an
 *	incompatible way.
 * @len: On entry, the real length of @data.  On return, the number of
 *	bytes used.
 * @data: Buffer for the register dump
 *
 * Users should use %ETHTOOL_GDRVINFO to find the maximum length of
 * a register dump for the interface.  They must allocate the buffer
 * immediately following this structure.
 */
struct Model0_ethtool_regs {
 __u32 Model0_cmd;
 __u32 Model0_version;
 __u32 Model0_len;
 __u8 Model0_data[0];
};

/**
 * struct ethtool_eeprom - EEPROM dump
 * @cmd: Command number = %ETHTOOL_GEEPROM, %ETHTOOL_GMODULEEEPROM or
 *	%ETHTOOL_SEEPROM
 * @magic: A 'magic cookie' value to guard against accidental changes.
 *	The value passed in to %ETHTOOL_SEEPROM must match the value
 *	returned by %ETHTOOL_GEEPROM for the same device.  This is
 *	unused when @cmd is %ETHTOOL_GMODULEEEPROM.
 * @offset: Offset within the EEPROM to begin reading/writing, in bytes
 * @len: On entry, number of bytes to read/write.  On successful
 *	return, number of bytes actually read/written.  In case of
 *	error, this may indicate at what point the error occurred.
 * @data: Buffer to read/write from
 *
 * Users may use %ETHTOOL_GDRVINFO or %ETHTOOL_GMODULEINFO to find
 * the length of an on-board or module EEPROM, respectively.  They
 * must allocate the buffer immediately following this structure.
 */
struct Model0_ethtool_eeprom {
 __u32 Model0_cmd;
 __u32 Model0_magic;
 __u32 Model0_offset;
 __u32 Model0_len;
 __u8 Model0_data[0];
};

/**
 * struct ethtool_eee - Energy Efficient Ethernet information
 * @cmd: ETHTOOL_{G,S}EEE
 * @supported: Mask of %SUPPORTED_* flags for the speed/duplex combinations
 *	for which there is EEE support.
 * @advertised: Mask of %ADVERTISED_* flags for the speed/duplex combinations
 *	advertised as eee capable.
 * @lp_advertised: Mask of %ADVERTISED_* flags for the speed/duplex
 *	combinations advertised by the link partner as eee capable.
 * @eee_active: Result of the eee auto negotiation.
 * @eee_enabled: EEE configured mode (enabled/disabled).
 * @tx_lpi_enabled: Whether the interface should assert its tx lpi, given
 *	that eee was negotiated.
 * @tx_lpi_timer: Time in microseconds the interface delays prior to asserting
 *	its tx lpi (after reaching 'idle' state). Effective only when eee
 *	was negotiated and tx_lpi_enabled was set.
 */
struct Model0_ethtool_eee {
 __u32 Model0_cmd;
 __u32 Model0_supported;
 __u32 Model0_advertised;
 __u32 Model0_lp_advertised;
 __u32 Model0_eee_active;
 __u32 Model0_eee_enabled;
 __u32 Model0_tx_lpi_enabled;
 __u32 Model0_tx_lpi_timer;
 __u32 Model0_reserved[2];
};

/**
 * struct ethtool_modinfo - plugin module eeprom information
 * @cmd: %ETHTOOL_GMODULEINFO
 * @type: Standard the module information conforms to %ETH_MODULE_SFF_xxxx
 * @eeprom_len: Length of the eeprom
 *
 * This structure is used to return the information to
 * properly size memory for a subsequent call to %ETHTOOL_GMODULEEEPROM.
 * The type code indicates the eeprom data format
 */
struct Model0_ethtool_modinfo {
 __u32 Model0_cmd;
 __u32 Model0_type;
 __u32 Model0_eeprom_len;
 __u32 Model0_reserved[8];
};

/**
 * struct ethtool_coalesce - coalescing parameters for IRQs and stats updates
 * @cmd: ETHTOOL_{G,S}COALESCE
 * @rx_coalesce_usecs: How many usecs to delay an RX interrupt after
 *	a packet arrives.
 * @rx_max_coalesced_frames: Maximum number of packets to receive
 *	before an RX interrupt.
 * @rx_coalesce_usecs_irq: Same as @rx_coalesce_usecs, except that
 *	this value applies while an IRQ is being serviced by the host.
 * @rx_max_coalesced_frames_irq: Same as @rx_max_coalesced_frames,
 *	except that this value applies while an IRQ is being serviced
 *	by the host.
 * @tx_coalesce_usecs: How many usecs to delay a TX interrupt after
 *	a packet is sent.
 * @tx_max_coalesced_frames: Maximum number of packets to be sent
 *	before a TX interrupt.
 * @tx_coalesce_usecs_irq: Same as @tx_coalesce_usecs, except that
 *	this value applies while an IRQ is being serviced by the host.
 * @tx_max_coalesced_frames_irq: Same as @tx_max_coalesced_frames,
 *	except that this value applies while an IRQ is being serviced
 *	by the host.
 * @stats_block_coalesce_usecs: How many usecs to delay in-memory
 *	statistics block updates.  Some drivers do not have an
 *	in-memory statistic block, and in such cases this value is
 *	ignored.  This value must not be zero.
 * @use_adaptive_rx_coalesce: Enable adaptive RX coalescing.
 * @use_adaptive_tx_coalesce: Enable adaptive TX coalescing.
 * @pkt_rate_low: Threshold for low packet rate (packets per second).
 * @rx_coalesce_usecs_low: How many usecs to delay an RX interrupt after
 *	a packet arrives, when the packet rate is below @pkt_rate_low.
 * @rx_max_coalesced_frames_low: Maximum number of packets to be received
 *	before an RX interrupt, when the packet rate is below @pkt_rate_low.
 * @tx_coalesce_usecs_low: How many usecs to delay a TX interrupt after
 *	a packet is sent, when the packet rate is below @pkt_rate_low.
 * @tx_max_coalesced_frames_low: Maximum nuumber of packets to be sent before
 *	a TX interrupt, when the packet rate is below @pkt_rate_low.
 * @pkt_rate_high: Threshold for high packet rate (packets per second).
 * @rx_coalesce_usecs_high: How many usecs to delay an RX interrupt after
 *	a packet arrives, when the packet rate is above @pkt_rate_high.
 * @rx_max_coalesced_frames_high: Maximum number of packets to be received
 *	before an RX interrupt, when the packet rate is above @pkt_rate_high.
 * @tx_coalesce_usecs_high: How many usecs to delay a TX interrupt after
 *	a packet is sent, when the packet rate is above @pkt_rate_high.
 * @tx_max_coalesced_frames_high: Maximum number of packets to be sent before
 *	a TX interrupt, when the packet rate is above @pkt_rate_high.
 * @rate_sample_interval: How often to do adaptive coalescing packet rate
 *	sampling, measured in seconds.  Must not be zero.
 *
 * Each pair of (usecs, max_frames) fields specifies that interrupts
 * should be coalesced until
 *	(usecs > 0 && time_since_first_completion >= usecs) ||
 *	(max_frames > 0 && completed_frames >= max_frames)
 *
 * It is illegal to set both usecs and max_frames to zero as this
 * would cause interrupts to never be generated.  To disable
 * coalescing, set usecs = 0 and max_frames = 1.
 *
 * Some implementations ignore the value of max_frames and use the
 * condition time_since_first_completion >= usecs
 *
 * This is deprecated.  Drivers for hardware that does not support
 * counting completions should validate that max_frames == !rx_usecs.
 *
 * Adaptive RX/TX coalescing is an algorithm implemented by some
 * drivers to improve latency under low packet rates and improve
 * throughput under high packet rates.  Some drivers only implement
 * one of RX or TX adaptive coalescing.  Anything not implemented by
 * the driver causes these values to be silently ignored.
 *
 * When the packet rate is below @pkt_rate_high but above
 * @pkt_rate_low (both measured in packets per second) the
 * normal {rx,tx}_* coalescing parameters are used.
 */
struct Model0_ethtool_coalesce {
 __u32 Model0_cmd;
 __u32 Model0_rx_coalesce_usecs;
 __u32 Model0_rx_max_coalesced_frames;
 __u32 Model0_rx_coalesce_usecs_irq;
 __u32 Model0_rx_max_coalesced_frames_irq;
 __u32 Model0_tx_coalesce_usecs;
 __u32 Model0_tx_max_coalesced_frames;
 __u32 Model0_tx_coalesce_usecs_irq;
 __u32 Model0_tx_max_coalesced_frames_irq;
 __u32 Model0_stats_block_coalesce_usecs;
 __u32 Model0_use_adaptive_rx_coalesce;
 __u32 Model0_use_adaptive_tx_coalesce;
 __u32 Model0_pkt_rate_low;
 __u32 Model0_rx_coalesce_usecs_low;
 __u32 Model0_rx_max_coalesced_frames_low;
 __u32 Model0_tx_coalesce_usecs_low;
 __u32 Model0_tx_max_coalesced_frames_low;
 __u32 Model0_pkt_rate_high;
 __u32 Model0_rx_coalesce_usecs_high;
 __u32 Model0_rx_max_coalesced_frames_high;
 __u32 Model0_tx_coalesce_usecs_high;
 __u32 Model0_tx_max_coalesced_frames_high;
 __u32 Model0_rate_sample_interval;
};

/**
 * struct ethtool_ringparam - RX/TX ring parameters
 * @cmd: Command number = %ETHTOOL_GRINGPARAM or %ETHTOOL_SRINGPARAM
 * @rx_max_pending: Maximum supported number of pending entries per
 *	RX ring.  Read-only.
 * @rx_mini_max_pending: Maximum supported number of pending entries
 *	per RX mini ring.  Read-only.
 * @rx_jumbo_max_pending: Maximum supported number of pending entries
 *	per RX jumbo ring.  Read-only.
 * @tx_max_pending: Maximum supported number of pending entries per
 *	TX ring.  Read-only.
 * @rx_pending: Current maximum number of pending entries per RX ring
 * @rx_mini_pending: Current maximum number of pending entries per RX
 *	mini ring
 * @rx_jumbo_pending: Current maximum number of pending entries per RX
 *	jumbo ring
 * @tx_pending: Current maximum supported number of pending entries
 *	per TX ring
 *
 * If the interface does not have separate RX mini and/or jumbo rings,
 * @rx_mini_max_pending and/or @rx_jumbo_max_pending will be 0.
 *
 * There may also be driver-dependent minimum values for the number
 * of entries per ring.
 */
struct Model0_ethtool_ringparam {
 __u32 Model0_cmd;
 __u32 Model0_rx_max_pending;
 __u32 Model0_rx_mini_max_pending;
 __u32 Model0_rx_jumbo_max_pending;
 __u32 Model0_tx_max_pending;
 __u32 Model0_rx_pending;
 __u32 Model0_rx_mini_pending;
 __u32 Model0_rx_jumbo_pending;
 __u32 Model0_tx_pending;
};

/**
 * struct ethtool_channels - configuring number of network channel
 * @cmd: ETHTOOL_{G,S}CHANNELS
 * @max_rx: Read only. Maximum number of receive channel the driver support.
 * @max_tx: Read only. Maximum number of transmit channel the driver support.
 * @max_other: Read only. Maximum number of other channel the driver support.
 * @max_combined: Read only. Maximum number of combined channel the driver
 *	support. Set of queues RX, TX or other.
 * @rx_count: Valid values are in the range 1 to the max_rx.
 * @tx_count: Valid values are in the range 1 to the max_tx.
 * @other_count: Valid values are in the range 1 to the max_other.
 * @combined_count: Valid values are in the range 1 to the max_combined.
 *
 * This can be used to configure RX, TX and other channels.
 */

struct Model0_ethtool_channels {
 __u32 Model0_cmd;
 __u32 Model0_max_rx;
 __u32 Model0_max_tx;
 __u32 Model0_max_other;
 __u32 Model0_max_combined;
 __u32 Model0_rx_count;
 __u32 Model0_tx_count;
 __u32 Model0_other_count;
 __u32 Model0_combined_count;
};

/**
 * struct ethtool_pauseparam - Ethernet pause (flow control) parameters
 * @cmd: Command number = %ETHTOOL_GPAUSEPARAM or %ETHTOOL_SPAUSEPARAM
 * @autoneg: Flag to enable autonegotiation of pause frame use
 * @rx_pause: Flag to enable reception of pause frames
 * @tx_pause: Flag to enable transmission of pause frames
 *
 * Drivers should reject a non-zero setting of @autoneg when
 * autoneogotiation is disabled (or not supported) for the link.
 *
 * If the link is autonegotiated, drivers should use
 * mii_advertise_flowctrl() or similar code to set the advertised
 * pause frame capabilities based on the @rx_pause and @tx_pause flags,
 * even if @autoneg is zero.  They should also allow the advertised
 * pause frame capabilities to be controlled directly through the
 * advertising field of &struct ethtool_cmd.
 *
 * If @autoneg is non-zero, the MAC is configured to send and/or
 * receive pause frames according to the result of autonegotiation.
 * Otherwise, it is configured directly based on the @rx_pause and
 * @tx_pause flags.
 */
struct Model0_ethtool_pauseparam {
 __u32 Model0_cmd;
 __u32 Model0_autoneg;
 __u32 Model0_rx_pause;
 __u32 Model0_tx_pause;
};



/**
 * enum ethtool_stringset - string set ID
 * @ETH_SS_TEST: Self-test result names, for use with %ETHTOOL_TEST
 * @ETH_SS_STATS: Statistic names, for use with %ETHTOOL_GSTATS
 * @ETH_SS_PRIV_FLAGS: Driver private flag names, for use with
 *	%ETHTOOL_GPFLAGS and %ETHTOOL_SPFLAGS
 * @ETH_SS_NTUPLE_FILTERS: Previously used with %ETHTOOL_GRXNTUPLE;
 *	now deprecated
 * @ETH_SS_FEATURES: Device feature names
 * @ETH_SS_RSS_HASH_FUNCS: RSS hush function names
 * @ETH_SS_PHY_STATS: Statistic names, for use with %ETHTOOL_GPHYSTATS
 */
enum Model0_ethtool_stringset {
 Model0_ETH_SS_TEST = 0,
 Model0_ETH_SS_STATS,
 Model0_ETH_SS_PRIV_FLAGS,
 Model0_ETH_SS_NTUPLE_FILTERS,
 Model0_ETH_SS_FEATURES,
 Model0_ETH_SS_RSS_HASH_FUNCS,
 Model0_ETH_SS_TUNABLES,
 Model0_ETH_SS_PHY_STATS,
};

/**
 * struct ethtool_gstrings - string set for data tagging
 * @cmd: Command number = %ETHTOOL_GSTRINGS
 * @string_set: String set ID; one of &enum ethtool_stringset
 * @len: On return, the number of strings in the string set
 * @data: Buffer for strings.  Each string is null-padded to a size of
 *	%ETH_GSTRING_LEN.
 *
 * Users must use %ETHTOOL_GSSET_INFO to find the number of strings in
 * the string set.  They must allocate a buffer of the appropriate
 * size immediately following this structure.
 */
struct Model0_ethtool_gstrings {
 __u32 Model0_cmd;
 __u32 Model0_string_set;
 __u32 Model0_len;
 __u8 Model0_data[0];
};

/**
 * struct ethtool_sset_info - string set information
 * @cmd: Command number = %ETHTOOL_GSSET_INFO
 * @sset_mask: On entry, a bitmask of string sets to query, with bits
 *	numbered according to &enum ethtool_stringset.  On return, a
 *	bitmask of those string sets queried that are supported.
 * @data: Buffer for string set sizes.  On return, this contains the
 *	size of each string set that was queried and supported, in
 *	order of ID.
 *
 * Example: The user passes in @sset_mask = 0x7 (sets 0, 1, 2) and on
 * return @sset_mask == 0x6 (sets 1, 2).  Then @data[0] contains the
 * size of set 1 and @data[1] contains the size of set 2.
 *
 * Users must allocate a buffer of the appropriate size (4 * number of
 * sets queried) immediately following this structure.
 */
struct Model0_ethtool_sset_info {
 __u32 Model0_cmd;
 __u32 Model0_reserved;
 __u64 Model0_sset_mask;
 __u32 Model0_data[0];
};

/**
 * enum ethtool_test_flags - flags definition of ethtool_test
 * @ETH_TEST_FL_OFFLINE: if set perform online and offline tests, otherwise
 *	only online tests.
 * @ETH_TEST_FL_FAILED: Driver set this flag if test fails.
 * @ETH_TEST_FL_EXTERNAL_LB: Application request to perform external loopback
 *	test.
 * @ETH_TEST_FL_EXTERNAL_LB_DONE: Driver performed the external loopback test
 */

enum Model0_ethtool_test_flags {
 Model0_ETH_TEST_FL_OFFLINE = (1 << 0),
 Model0_ETH_TEST_FL_FAILED = (1 << 1),
 Model0_ETH_TEST_FL_EXTERNAL_LB = (1 << 2),
 Model0_ETH_TEST_FL_EXTERNAL_LB_DONE = (1 << 3),
};

/**
 * struct ethtool_test - device self-test invocation
 * @cmd: Command number = %ETHTOOL_TEST
 * @flags: A bitmask of flags from &enum ethtool_test_flags.  Some
 *	flags may be set by the user on entry; others may be set by
 *	the driver on return.
 * @len: On return, the number of test results
 * @data: Array of test results
 *
 * Users must use %ETHTOOL_GSSET_INFO or %ETHTOOL_GDRVINFO to find the
 * number of test results that will be returned.  They must allocate a
 * buffer of the appropriate size (8 * number of results) immediately
 * following this structure.
 */
struct Model0_ethtool_test {
 __u32 Model0_cmd;
 __u32 Model0_flags;
 __u32 Model0_reserved;
 __u32 Model0_len;
 __u64 Model0_data[0];
};

/**
 * struct ethtool_stats - device-specific statistics
 * @cmd: Command number = %ETHTOOL_GSTATS
 * @n_stats: On return, the number of statistics
 * @data: Array of statistics
 *
 * Users must use %ETHTOOL_GSSET_INFO or %ETHTOOL_GDRVINFO to find the
 * number of statistics that will be returned.  They must allocate a
 * buffer of the appropriate size (8 * number of statistics)
 * immediately following this structure.
 */
struct Model0_ethtool_stats {
 __u32 Model0_cmd;
 __u32 Model0_n_stats;
 __u64 Model0_data[0];
};

/**
 * struct ethtool_perm_addr - permanent hardware address
 * @cmd: Command number = %ETHTOOL_GPERMADDR
 * @size: On entry, the size of the buffer.  On return, the size of the
 *	address.  The command fails if the buffer is too small.
 * @data: Buffer for the address
 *
 * Users must allocate the buffer immediately following this structure.
 * A buffer size of %MAX_ADDR_LEN should be sufficient for any address
 * type.
 */
struct Model0_ethtool_perm_addr {
 __u32 Model0_cmd;
 __u32 Model0_size;
 __u8 Model0_data[0];
};

/* boolean flags controlling per-interface behavior characteristics.
 * When reading, the flag indicates whether or not a certain behavior
 * is enabled/present.  When writing, the flag indicates whether
 * or not the driver should turn on (set) or off (clear) a behavior.
 *
 * Some behaviors may read-only (unconditionally absent or present).
 * If such is the case, return EINVAL in the set-flags operation if the
 * flag differs from the read-only value.
 */
enum Model0_ethtool_flags {
 Model0_ETH_FLAG_TXVLAN = (1 << 7), /* TX VLAN offload enabled */
 Model0_ETH_FLAG_RXVLAN = (1 << 8), /* RX VLAN offload enabled */
 Model0_ETH_FLAG_LRO = (1 << 15), /* LRO is enabled */
 Model0_ETH_FLAG_NTUPLE = (1 << 27), /* N-tuple filters enabled */
 Model0_ETH_FLAG_RXHASH = (1 << 28),
};

/* The following structures are for supporting RX network flow
 * classification and RX n-tuple configuration. Note, all multibyte
 * fields, e.g., ip4src, ip4dst, psrc, pdst, spi, etc. are expected to
 * be in network byte order.
 */

/**
 * struct ethtool_tcpip4_spec - flow specification for TCP/IPv4 etc.
 * @ip4src: Source host
 * @ip4dst: Destination host
 * @psrc: Source port
 * @pdst: Destination port
 * @tos: Type-of-service
 *
 * This can be used to specify a TCP/IPv4, UDP/IPv4 or SCTP/IPv4 flow.
 */
struct Model0_ethtool_tcpip4_spec {
 Model0___be32 Model0_ip4src;
 Model0___be32 Model0_ip4dst;
 Model0___be16 Model0_psrc;
 Model0___be16 Model0_pdst;
 __u8 Model0_tos;
};

/**
 * struct ethtool_ah_espip4_spec - flow specification for IPsec/IPv4
 * @ip4src: Source host
 * @ip4dst: Destination host
 * @spi: Security parameters index
 * @tos: Type-of-service
 *
 * This can be used to specify an IPsec transport or tunnel over IPv4.
 */
struct Model0_ethtool_ah_espip4_spec {
 Model0___be32 Model0_ip4src;
 Model0___be32 Model0_ip4dst;
 Model0___be32 Model0_spi;
 __u8 Model0_tos;
};



/**
 * struct ethtool_usrip4_spec - general flow specification for IPv4
 * @ip4src: Source host
 * @ip4dst: Destination host
 * @l4_4_bytes: First 4 bytes of transport (layer 4) header
 * @tos: Type-of-service
 * @ip_ver: Value must be %ETH_RX_NFC_IP4; mask must be 0
 * @proto: Transport protocol number; mask must be 0
 */
struct Model0_ethtool_usrip4_spec {
 Model0___be32 Model0_ip4src;
 Model0___be32 Model0_ip4dst;
 Model0___be32 Model0_l4_4_bytes;
 __u8 Model0_tos;
 __u8 Model0_ip_ver;
 __u8 Model0_proto;
};

/**
 * struct ethtool_tcpip6_spec - flow specification for TCP/IPv6 etc.
 * @ip6src: Source host
 * @ip6dst: Destination host
 * @psrc: Source port
 * @pdst: Destination port
 * @tclass: Traffic Class
 *
 * This can be used to specify a TCP/IPv6, UDP/IPv6 or SCTP/IPv6 flow.
 */
struct Model0_ethtool_tcpip6_spec {
 Model0___be32 Model0_ip6src[4];
 Model0___be32 Model0_ip6dst[4];
 Model0___be16 Model0_psrc;
 Model0___be16 Model0_pdst;
 __u8 Model0_tclass;
};

/**
 * struct ethtool_ah_espip6_spec - flow specification for IPsec/IPv6
 * @ip6src: Source host
 * @ip6dst: Destination host
 * @spi: Security parameters index
 * @tclass: Traffic Class
 *
 * This can be used to specify an IPsec transport or tunnel over IPv6.
 */
struct Model0_ethtool_ah_espip6_spec {
 Model0___be32 Model0_ip6src[4];
 Model0___be32 Model0_ip6dst[4];
 Model0___be32 Model0_spi;
 __u8 Model0_tclass;
};

/**
 * struct ethtool_usrip6_spec - general flow specification for IPv6
 * @ip6src: Source host
 * @ip6dst: Destination host
 * @l4_4_bytes: First 4 bytes of transport (layer 4) header
 * @tclass: Traffic Class
 * @l4_proto: Transport protocol number (nexthdr after any Extension Headers)
 */
struct Model0_ethtool_usrip6_spec {
 Model0___be32 Model0_ip6src[4];
 Model0___be32 Model0_ip6dst[4];
 Model0___be32 Model0_l4_4_bytes;
 __u8 Model0_tclass;
 __u8 Model0_l4_proto;
};

union Model0_ethtool_flow_union {
 struct Model0_ethtool_tcpip4_spec Model0_tcp_ip4_spec;
 struct Model0_ethtool_tcpip4_spec Model0_udp_ip4_spec;
 struct Model0_ethtool_tcpip4_spec Model0_sctp_ip4_spec;
 struct Model0_ethtool_ah_espip4_spec Model0_ah_ip4_spec;
 struct Model0_ethtool_ah_espip4_spec Model0_esp_ip4_spec;
 struct Model0_ethtool_usrip4_spec Model0_usr_ip4_spec;
 struct Model0_ethtool_tcpip6_spec Model0_tcp_ip6_spec;
 struct Model0_ethtool_tcpip6_spec Model0_udp_ip6_spec;
 struct Model0_ethtool_tcpip6_spec Model0_sctp_ip6_spec;
 struct Model0_ethtool_ah_espip6_spec Model0_ah_ip6_spec;
 struct Model0_ethtool_ah_espip6_spec Model0_esp_ip6_spec;
 struct Model0_ethtool_usrip6_spec Model0_usr_ip6_spec;
 struct Model0_ethhdr Model0_ether_spec;
 __u8 Model0_hdata[52];
};

/**
 * struct ethtool_flow_ext - additional RX flow fields
 * @h_dest: destination MAC address
 * @vlan_etype: VLAN EtherType
 * @vlan_tci: VLAN tag control information
 * @data: user defined data
 *
 * Note, @vlan_etype, @vlan_tci, and @data are only valid if %FLOW_EXT
 * is set in &struct ethtool_rx_flow_spec @flow_type.
 * @h_dest is valid if %FLOW_MAC_EXT is set.
 */
struct Model0_ethtool_flow_ext {
 __u8 Model0_padding[2];
 unsigned char Model0_h_dest[6];
 Model0___be16 Model0_vlan_etype;
 Model0___be16 Model0_vlan_tci;
 Model0___be32 Model0_data[2];
};

/**
 * struct ethtool_rx_flow_spec - classification rule for RX flows
 * @flow_type: Type of match to perform, e.g. %TCP_V4_FLOW
 * @h_u: Flow fields to match (dependent on @flow_type)
 * @h_ext: Additional fields to match
 * @m_u: Masks for flow field bits to be matched
 * @m_ext: Masks for additional field bits to be matched
 *	Note, all additional fields must be ignored unless @flow_type
 *	includes the %FLOW_EXT or %FLOW_MAC_EXT flag
 *	(see &struct ethtool_flow_ext description).
 * @ring_cookie: RX ring/queue index to deliver to, or %RX_CLS_FLOW_DISC
 *	if packets should be discarded
 * @location: Location of rule in the table.  Locations must be
 *	numbered such that a flow matching multiple rules will be
 *	classified according to the first (lowest numbered) rule.
 */
struct Model0_ethtool_rx_flow_spec {
 __u32 Model0_flow_type;
 union Model0_ethtool_flow_union Model0_h_u;
 struct Model0_ethtool_flow_ext Model0_h_ext;
 union Model0_ethtool_flow_union Model0_m_u;
 struct Model0_ethtool_flow_ext Model0_m_ext;
 __u64 Model0_ring_cookie;
 __u32 Model0_location;
};

/* How rings are layed out when accessing virtual functions or
 * offloaded queues is device specific. To allow users to do flow
 * steering and specify these queues the ring cookie is partitioned
 * into a 32bit queue index with an 8 bit virtual function id.
 * This also leaves the 3bytes for further specifiers. It is possible
 * future devices may support more than 256 virtual functions if
 * devices start supporting PCIe w/ARI. However at the moment I
 * do not know of any devices that support this so I do not reserve
 * space for this at this time. If a future patch consumes the next
 * byte it should be aware of this possiblity.
 */



static inline __attribute__((no_instrument_function)) __u64 Model0_ethtool_get_flow_spec_ring(__u64 Model0_ring_cookie)
{
 return 0x00000000FFFFFFFFLL & Model0_ring_cookie;
};

static inline __attribute__((no_instrument_function)) __u64 Model0_ethtool_get_flow_spec_ring_vf(__u64 Model0_ring_cookie)
{
 return (0x000000FF00000000LL & Model0_ring_cookie) >>
    32;
};

/**
 * struct ethtool_rxnfc - command to get or set RX flow classification rules
 * @cmd: Specific command number - %ETHTOOL_GRXFH, %ETHTOOL_SRXFH,
 *	%ETHTOOL_GRXRINGS, %ETHTOOL_GRXCLSRLCNT, %ETHTOOL_GRXCLSRULE,
 *	%ETHTOOL_GRXCLSRLALL, %ETHTOOL_SRXCLSRLDEL or %ETHTOOL_SRXCLSRLINS
 * @flow_type: Type of flow to be affected, e.g. %TCP_V4_FLOW
 * @data: Command-dependent value
 * @fs: Flow classification rule
 * @rule_cnt: Number of rules to be affected
 * @rule_locs: Array of used rule locations
 *
 * For %ETHTOOL_GRXFH and %ETHTOOL_SRXFH, @data is a bitmask indicating
 * the fields included in the flow hash, e.g. %RXH_IP_SRC.  The following
 * structure fields must not be used.
 *
 * For %ETHTOOL_GRXRINGS, @data is set to the number of RX rings/queues
 * on return.
 *
 * For %ETHTOOL_GRXCLSRLCNT, @rule_cnt is set to the number of defined
 * rules on return.  If @data is non-zero on return then it is the
 * size of the rule table, plus the flag %RX_CLS_LOC_SPECIAL if the
 * driver supports any special location values.  If that flag is not
 * set in @data then special location values should not be used.
 *
 * For %ETHTOOL_GRXCLSRULE, @fs.@location specifies the location of an
 * existing rule on entry and @fs contains the rule on return.
 *
 * For %ETHTOOL_GRXCLSRLALL, @rule_cnt specifies the array size of the
 * user buffer for @rule_locs on entry.  On return, @data is the size
 * of the rule table, @rule_cnt is the number of defined rules, and
 * @rule_locs contains the locations of the defined rules.  Drivers
 * must use the second parameter to get_rxnfc() instead of @rule_locs.
 *
 * For %ETHTOOL_SRXCLSRLINS, @fs specifies the rule to add or update.
 * @fs.@location either specifies the location to use or is a special
 * location value with %RX_CLS_LOC_SPECIAL flag set.  On return,
 * @fs.@location is the actual rule location.
 *
 * For %ETHTOOL_SRXCLSRLDEL, @fs.@location specifies the location of an
 * existing rule on entry.
 *
 * A driver supporting the special location values for
 * %ETHTOOL_SRXCLSRLINS may add the rule at any suitable unused
 * location, and may remove a rule at a later location (lower
 * priority) that matches exactly the same set of flows.  The special
 * values are %RX_CLS_LOC_ANY, selecting any location;
 * %RX_CLS_LOC_FIRST, selecting the first suitable location (maximum
 * priority); and %RX_CLS_LOC_LAST, selecting the last suitable
 * location (minimum priority).  Additional special values may be
 * defined in future and drivers must return -%EINVAL for any
 * unrecognised value.
 */
struct Model0_ethtool_rxnfc {
 __u32 Model0_cmd;
 __u32 Model0_flow_type;
 __u64 Model0_data;
 struct Model0_ethtool_rx_flow_spec Model0_fs;
 __u32 Model0_rule_cnt;
 __u32 Model0_rule_locs[0];
};


/**
 * struct ethtool_rxfh_indir - command to get or set RX flow hash indirection
 * @cmd: Specific command number - %ETHTOOL_GRXFHINDIR or %ETHTOOL_SRXFHINDIR
 * @size: On entry, the array size of the user buffer, which may be zero.
 *	On return from %ETHTOOL_GRXFHINDIR, the array size of the hardware
 *	indirection table.
 * @ring_index: RX ring/queue index for each hash value
 *
 * For %ETHTOOL_GRXFHINDIR, a @size of zero means that only the size
 * should be returned.  For %ETHTOOL_SRXFHINDIR, a @size of zero means
 * the table should be reset to default values.  This last feature
 * is not supported by the original implementations.
 */
struct Model0_ethtool_rxfh_indir {
 __u32 Model0_cmd;
 __u32 Model0_size;
 __u32 Model0_ring_index[0];
};

/**
 * struct ethtool_rxfh - command to get/set RX flow hash indir or/and hash key.
 * @cmd: Specific command number - %ETHTOOL_GRSSH or %ETHTOOL_SRSSH
 * @rss_context: RSS context identifier.
 * @indir_size: On entry, the array size of the user buffer for the
 *	indirection table, which may be zero, or (for %ETHTOOL_SRSSH),
 *	%ETH_RXFH_INDIR_NO_CHANGE.  On return from %ETHTOOL_GRSSH,
 *	the array size of the hardware indirection table.
 * @key_size: On entry, the array size of the user buffer for the hash key,
 *	which may be zero.  On return from %ETHTOOL_GRSSH, the size of the
 *	hardware hash key.
 * @hfunc: Defines the current RSS hash function used by HW (or to be set to).
 *	Valid values are one of the %ETH_RSS_HASH_*.
 * @rsvd:	Reserved for future extensions.
 * @rss_config: RX ring/queue index for each hash value i.e., indirection table
 *	of @indir_size __u32 elements, followed by hash key of @key_size
 *	bytes.
 *
 * For %ETHTOOL_GRSSH, a @indir_size and key_size of zero means that only the
 * size should be returned.  For %ETHTOOL_SRSSH, an @indir_size of
 * %ETH_RXFH_INDIR_NO_CHANGE means that indir table setting is not requested
 * and a @indir_size of zero means the indir table should be reset to default
 * values. An hfunc of zero means that hash function setting is not requested.
 */
struct Model0_ethtool_rxfh {
 __u32 Model0_cmd;
 __u32 Model0_rss_context;
 __u32 Model0_indir_size;
 __u32 Model0_key_size;
 __u8 Model0_hfunc;
 __u8 Model0_rsvd8[3];
 __u32 Model0_rsvd32;
 __u32 Model0_rss_config[0];
};


/**
 * struct ethtool_rx_ntuple_flow_spec - specification for RX flow filter
 * @flow_type: Type of match to perform, e.g. %TCP_V4_FLOW
 * @h_u: Flow field values to match (dependent on @flow_type)
 * @m_u: Masks for flow field value bits to be ignored
 * @vlan_tag: VLAN tag to match
 * @vlan_tag_mask: Mask for VLAN tag bits to be ignored
 * @data: Driver-dependent data to match
 * @data_mask: Mask for driver-dependent data bits to be ignored
 * @action: RX ring/queue index to deliver to (non-negative) or other action
 *	(negative, e.g. %ETHTOOL_RXNTUPLE_ACTION_DROP)
 *
 * For flow types %TCP_V4_FLOW, %UDP_V4_FLOW and %SCTP_V4_FLOW, where
 * a field value and mask are both zero this is treated as if all mask
 * bits are set i.e. the field is ignored.
 */
struct Model0_ethtool_rx_ntuple_flow_spec {
 __u32 Model0_flow_type;
 union {
  struct Model0_ethtool_tcpip4_spec Model0_tcp_ip4_spec;
  struct Model0_ethtool_tcpip4_spec Model0_udp_ip4_spec;
  struct Model0_ethtool_tcpip4_spec Model0_sctp_ip4_spec;
  struct Model0_ethtool_ah_espip4_spec Model0_ah_ip4_spec;
  struct Model0_ethtool_ah_espip4_spec Model0_esp_ip4_spec;
  struct Model0_ethtool_usrip4_spec Model0_usr_ip4_spec;
  struct Model0_ethhdr Model0_ether_spec;
  __u8 Model0_hdata[72];
 } Model0_h_u, Model0_m_u;

 Model0___u16 Model0_vlan_tag;
 Model0___u16 Model0_vlan_tag_mask;
 __u64 Model0_data;
 __u64 Model0_data_mask;

 Model0___s32 Model0_action;


};

/**
 * struct ethtool_rx_ntuple - command to set or clear RX flow filter
 * @cmd: Command number - %ETHTOOL_SRXNTUPLE
 * @fs: Flow filter specification
 */
struct Model0_ethtool_rx_ntuple {
 __u32 Model0_cmd;
 struct Model0_ethtool_rx_ntuple_flow_spec Model0_fs;
};


enum Model0_ethtool_flash_op_type {
 Model0_ETHTOOL_FLASH_ALL_REGIONS = 0,
};

/* for passing firmware flashing related parameters */
struct Model0_ethtool_flash {
 __u32 Model0_cmd;
 __u32 Model0_region;
 char Model0_data[128];
};

/**
 * struct ethtool_dump - used for retrieving, setting device dump
 * @cmd: Command number - %ETHTOOL_GET_DUMP_FLAG, %ETHTOOL_GET_DUMP_DATA, or
 * 	%ETHTOOL_SET_DUMP
 * @version: FW version of the dump, filled in by driver
 * @flag: driver dependent flag for dump setting, filled in by driver during
 *        get and filled in by ethtool for set operation.
 *        flag must be initialized by macro ETH_FW_DUMP_DISABLE value when
 *        firmware dump is disabled.
 * @len: length of dump data, used as the length of the user buffer on entry to
 * 	 %ETHTOOL_GET_DUMP_DATA and this is returned as dump length by driver
 * 	 for %ETHTOOL_GET_DUMP_FLAG command
 * @data: data collected for get dump data operation
 */
struct Model0_ethtool_dump {
 __u32 Model0_cmd;
 __u32 Model0_version;
 __u32 Model0_flag;
 __u32 Model0_len;
 __u8 Model0_data[0];
};



/* for returning and changing feature sets */

/**
 * struct ethtool_get_features_block - block with state of 32 features
 * @available: mask of changeable features
 * @requested: mask of features requested to be enabled if possible
 * @active: mask of currently enabled features
 * @never_changed: mask of features not changeable for any device
 */
struct Model0_ethtool_get_features_block {
 __u32 Model0_available;
 __u32 Model0_requested;
 __u32 Model0_active;
 __u32 Model0_never_changed;
};

/**
 * struct ethtool_gfeatures - command to get state of device's features
 * @cmd: command number = %ETHTOOL_GFEATURES
 * @size: On entry, the number of elements in the features[] array;
 *	on return, the number of elements in features[] needed to hold
 *	all features
 * @features: state of features
 */
struct Model0_ethtool_gfeatures {
 __u32 Model0_cmd;
 __u32 Model0_size;
 struct Model0_ethtool_get_features_block Model0_features[0];
};

/**
 * struct ethtool_set_features_block - block with request for 32 features
 * @valid: mask of features to be changed
 * @requested: values of features to be changed
 */
struct Model0_ethtool_set_features_block {
 __u32 Model0_valid;
 __u32 Model0_requested;
};

/**
 * struct ethtool_sfeatures - command to request change in device's features
 * @cmd: command number = %ETHTOOL_SFEATURES
 * @size: array size of the features[] array
 * @features: feature change masks
 */
struct Model0_ethtool_sfeatures {
 __u32 Model0_cmd;
 __u32 Model0_size;
 struct Model0_ethtool_set_features_block Model0_features[0];
};

/**
 * struct ethtool_ts_info - holds a device's timestamping and PHC association
 * @cmd: command number = %ETHTOOL_GET_TS_INFO
 * @so_timestamping: bit mask of the sum of the supported SO_TIMESTAMPING flags
 * @phc_index: device index of the associated PHC, or -1 if there is none
 * @tx_types: bit mask of the supported hwtstamp_tx_types enumeration values
 * @rx_filters: bit mask of the supported hwtstamp_rx_filters enumeration values
 *
 * The bits in the 'tx_types' and 'rx_filters' fields correspond to
 * the 'hwtstamp_tx_types' and 'hwtstamp_rx_filters' enumeration values,
 * respectively.  For example, if the device supports HWTSTAMP_TX_ON,
 * then (1 << HWTSTAMP_TX_ON) in 'tx_types' will be set.
 *
 * Drivers should only report the filters they actually support without
 * upscaling in the SIOCSHWTSTAMP ioctl. If the SIOCSHWSTAMP request for
 * HWTSTAMP_FILTER_V1_SYNC is supported by HWTSTAMP_FILTER_V1_EVENT, then the
 * driver should only report HWTSTAMP_FILTER_V1_EVENT in this op.
 */
struct Model0_ethtool_ts_info {
 __u32 Model0_cmd;
 __u32 Model0_so_timestamping;
 Model0___s32 Model0_phc_index;
 __u32 Model0_tx_types;
 __u32 Model0_tx_reserved[3];
 __u32 Model0_rx_filters;
 __u32 Model0_rx_reserved[3];
};

/*
 * %ETHTOOL_SFEATURES changes features present in features[].valid to the
 * values of corresponding bits in features[].requested. Bits in .requested
 * not set in .valid or not changeable are ignored.
 *
 * Returns %EINVAL when .valid contains undefined or never-changeable bits
 * or size is not equal to required number of features words (32-bit blocks).
 * Returns >= 0 if request was completed; bits set in the value mean:
 *   %ETHTOOL_F_UNSUPPORTED - there were bits set in .valid that are not
 *	changeable (not present in %ETHTOOL_GFEATURES' features[].available)
 *	those bits were ignored.
 *   %ETHTOOL_F_WISH - some or all changes requested were recorded but the
 *      resulting state of bits masked by .valid is not equal to .requested.
 *      Probably there are other device-specific constraints on some features
 *      in the set. When %ETHTOOL_F_UNSUPPORTED is set, .valid is considered
 *      here as though ignored bits were cleared.
 *   %ETHTOOL_F_COMPAT - some or all changes requested were made by calling
 *      compatibility functions. Requested offload state cannot be properly
 *      managed by kernel.
 *
 * Meaning of bits in the masks are obtained by %ETHTOOL_GSSET_INFO (number of
 * bits in the arrays - always multiple of 32) and %ETHTOOL_GSTRINGS commands
 * for ETH_SS_FEATURES string set. First entry in the table corresponds to least
 * significant bit in features[0] fields. Empty strings mark undefined features.
 */
enum Model0_ethtool_sfeatures_retval_bits {
 Model0_ETHTOOL_F_UNSUPPORTED__BIT,
 Model0_ETHTOOL_F_WISH__BIT,
 Model0_ETHTOOL_F_COMPAT__BIT,
};







/**
 * struct ethtool_per_queue_op - apply sub command to the queues in mask.
 * @cmd: ETHTOOL_PERQUEUE
 * @sub_command: the sub command which apply to each queues
 * @queue_mask: Bitmap of the queues which sub command apply to
 * @data: A complete command structure following for each of the queues addressed
 */
struct Model0_ethtool_per_queue_op {
 __u32 Model0_cmd;
 __u32 Model0_sub_command;
 __u32 Model0_queue_mask[(((4096) + (32) - 1) / (32))];
 char Model0_data[];
};

/* CMDs currently supported */
/* Get link status for host, i.e. whether the interface *and* the
 * physical port (if there is one) are up (ethtool_value). */
/* compatibility with older code */



/* Link mode bit indices */
enum Model0_ethtool_link_mode_bit_indices {
 Model0_ETHTOOL_LINK_MODE_10baseT_Half_BIT = 0,
 Model0_ETHTOOL_LINK_MODE_10baseT_Full_BIT = 1,
 Model0_ETHTOOL_LINK_MODE_100baseT_Half_BIT = 2,
 Model0_ETHTOOL_LINK_MODE_100baseT_Full_BIT = 3,
 Model0_ETHTOOL_LINK_MODE_1000baseT_Half_BIT = 4,
 Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT = 5,
 Model0_ETHTOOL_LINK_MODE_Autoneg_BIT = 6,
 Model0_ETHTOOL_LINK_MODE_TP_BIT = 7,
 Model0_ETHTOOL_LINK_MODE_AUI_BIT = 8,
 Model0_ETHTOOL_LINK_MODE_MII_BIT = 9,
 Model0_ETHTOOL_LINK_MODE_FIBRE_BIT = 10,
 Model0_ETHTOOL_LINK_MODE_BNC_BIT = 11,
 Model0_ETHTOOL_LINK_MODE_10000baseT_Full_BIT = 12,
 Model0_ETHTOOL_LINK_MODE_Pause_BIT = 13,
 Model0_ETHTOOL_LINK_MODE_Asym_Pause_BIT = 14,
 Model0_ETHTOOL_LINK_MODE_2500baseX_Full_BIT = 15,
 Model0_ETHTOOL_LINK_MODE_Backplane_BIT = 16,
 Model0_ETHTOOL_LINK_MODE_1000baseKX_Full_BIT = 17,
 Model0_ETHTOOL_LINK_MODE_10000baseKX4_Full_BIT = 18,
 Model0_ETHTOOL_LINK_MODE_10000baseKR_Full_BIT = 19,
 Model0_ETHTOOL_LINK_MODE_10000baseR_FEC_BIT = 20,
 Model0_ETHTOOL_LINK_MODE_20000baseMLD2_Full_BIT = 21,
 Model0_ETHTOOL_LINK_MODE_20000baseKR2_Full_BIT = 22,
 Model0_ETHTOOL_LINK_MODE_40000baseKR4_Full_BIT = 23,
 Model0_ETHTOOL_LINK_MODE_40000baseCR4_Full_BIT = 24,
 Model0_ETHTOOL_LINK_MODE_40000baseSR4_Full_BIT = 25,
 Model0_ETHTOOL_LINK_MODE_40000baseLR4_Full_BIT = 26,
 Model0_ETHTOOL_LINK_MODE_56000baseKR4_Full_BIT = 27,
 Model0_ETHTOOL_LINK_MODE_56000baseCR4_Full_BIT = 28,
 Model0_ETHTOOL_LINK_MODE_56000baseSR4_Full_BIT = 29,
 Model0_ETHTOOL_LINK_MODE_56000baseLR4_Full_BIT = 30,
 Model0_ETHTOOL_LINK_MODE_25000baseCR_Full_BIT = 31,
 Model0_ETHTOOL_LINK_MODE_25000baseKR_Full_BIT = 32,
 Model0_ETHTOOL_LINK_MODE_25000baseSR_Full_BIT = 33,
 Model0_ETHTOOL_LINK_MODE_50000baseCR2_Full_BIT = 34,
 Model0_ETHTOOL_LINK_MODE_50000baseKR2_Full_BIT = 35,
 Model0_ETHTOOL_LINK_MODE_100000baseKR4_Full_BIT = 36,
 Model0_ETHTOOL_LINK_MODE_100000baseSR4_Full_BIT = 37,
 Model0_ETHTOOL_LINK_MODE_100000baseCR4_Full_BIT = 38,
 Model0_ETHTOOL_LINK_MODE_100000baseLR4_ER4_Full_BIT = 39,
 Model0_ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT = 40,

 /* Last allowed bit for __ETHTOOL_LINK_MODE_LEGACY_MASK is bit
	 * 31. Please do NOT define any SUPPORTED_* or ADVERTISED_*
	 * macro for bits > 31. The only way to use indices > 31 is to
	 * use the new ETHTOOL_GLINKSETTINGS/ETHTOOL_SLINKSETTINGS API.
	 */

 Model0___ETHTOOL_LINK_MODE_LAST
   = Model0_ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT,
};




/* DEPRECATED macros. Please migrate to
 * ETHTOOL_GLINKSETTINGS/ETHTOOL_SLINKSETTINGS API. Please do NOT
 * define any new SUPPORTED_* macro for bits > 31.
 */
/* Please do not define any new SUPPORTED_* macro for bits > 31, see
 * notice above.
 */

/*
 * DEPRECATED macros. Please migrate to
 * ETHTOOL_GLINKSETTINGS/ETHTOOL_SLINKSETTINGS API. Please do NOT
 * define any new ADERTISE_* macro for bits > 31.
 */
/* Please do not define any new ADVERTISED_* macro for bits > 31, see
 * notice above.
 */

/* The following are all involved in forcing a particular link
 * mode for the device for setting things.  When getting the
 * devices settings, these indicate the current mode and whether
 * it was forced up into this mode or autonegotiated.
 */

/* The forced speed, in units of 1Mb. All values 0 to INT_MAX are legal. */
static inline __attribute__((no_instrument_function)) int Model0_ethtool_validate_speed(__u32 Model0_speed)
{
 return Model0_speed <= ((int)(~0U>>1)) || Model0_speed == -1;
}

/* Duplex, half or full. */




static inline __attribute__((no_instrument_function)) int Model0_ethtool_validate_duplex(__u8 Model0_duplex)
{
 switch (Model0_duplex) {
 case 0x00:
 case 0x01:
 case 0xff:
  return 1;
 }

 return 0;
}

/* Which connector port. */
/* Which transceiver to use. */






/* Enable or disable autonegotiation. */



/* MDI or MDI-X status/control - if MDI/MDI_X/AUTO is set then
 * the driver is required to renegotiate link
 */





/* Wake-On-Lan options. */
/* L2-L4 network traffic flow types */
/* Flag to enable additional fields in struct ethtool_rx_flow_spec */



/* L3-L4 network traffic flow hash options */
/* Special RX classification rule insert location values */





/* EEPROM Standards for plug in modules */
/* Reset flags */
/* The reset() operation must clear the flags for the components which
 * were actually reset.  On successful return, the flags indicate the
 * components which were not reset, either because they do not exist
 * in the hardware or because they cannot be reset independently.  The
 * driver must never reset any components that were not requested.
 */
enum Model0_ethtool_reset_flags {
 /* These flags represent components dedicated to the interface
	 * the command is addressed to.  Shift any flag left by
	 * ETH_RESET_SHARED_SHIFT to reset a shared component of the
	 * same type.
	 */
 Model0_ETH_RESET_MGMT = 1 << 0, /* Management processor */
 Model0_ETH_RESET_IRQ = 1 << 1, /* Interrupt requester */
 Model0_ETH_RESET_DMA = 1 << 2, /* DMA engine */
 Model0_ETH_RESET_FILTER = 1 << 3, /* Filtering/flow direction */
 Model0_ETH_RESET_OFFLOAD = 1 << 4, /* Protocol offload */
 Model0_ETH_RESET_MAC = 1 << 5, /* Media access controller */
 Model0_ETH_RESET_PHY = 1 << 6, /* Transceiver/PHY */
 Model0_ETH_RESET_RAM = 1 << 7, /* RAM shared between
						 * multiple components */

 Model0_ETH_RESET_DEDICATED = 0x0000ffff, /* All components dedicated to
						 * this interface */
 Model0_ETH_RESET_ALL = 0xffffffff, /* All components used by this
						 * interface, even if shared */
};



/**
 * struct ethtool_link_settings - link control and status
 *
 * IMPORTANT, Backward compatibility notice: When implementing new
 *	user-space tools, please first try %ETHTOOL_GLINKSETTINGS, and
 *	if it succeeds use %ETHTOOL_SLINKSETTINGS to change link
 *	settings; do not use %ETHTOOL_SSET if %ETHTOOL_GLINKSETTINGS
 *	succeeded: stick to %ETHTOOL_GLINKSETTINGS/%SLINKSETTINGS in
 *	that case.  Conversely, if %ETHTOOL_GLINKSETTINGS fails, use
 *	%ETHTOOL_GSET to query and %ETHTOOL_SSET to change link
 *	settings; do not use %ETHTOOL_SLINKSETTINGS if
 *	%ETHTOOL_GLINKSETTINGS failed: stick to
 *	%ETHTOOL_GSET/%ETHTOOL_SSET in that case.
 *
 * @cmd: Command number = %ETHTOOL_GLINKSETTINGS or %ETHTOOL_SLINKSETTINGS
 * @speed: Link speed (Mbps)
 * @duplex: Duplex mode; one of %DUPLEX_*
 * @port: Physical connector type; one of %PORT_*
 * @phy_address: MDIO address of PHY (transceiver); 0 or 255 if not
 *	applicable.  For clause 45 PHYs this is the PRTAD.
 * @autoneg: Enable/disable autonegotiation and auto-detection;
 *	either %AUTONEG_DISABLE or %AUTONEG_ENABLE
 * @mdio_support: Bitmask of %ETH_MDIO_SUPPORTS_* flags for the MDIO
 *	protocols supported by the interface; 0 if unknown.
 *	Read-only.
 * @eth_tp_mdix: Ethernet twisted-pair MDI(-X) status; one of
 *	%ETH_TP_MDI_*.  If the status is unknown or not applicable, the
 *	value will be %ETH_TP_MDI_INVALID.  Read-only.
 * @eth_tp_mdix_ctrl: Ethernet twisted pair MDI(-X) control; one of
 *	%ETH_TP_MDI_*.  If MDI(-X) control is not implemented, reads
 *	yield %ETH_TP_MDI_INVALID and writes may be ignored or rejected.
 *	When written successfully, the link should be renegotiated if
 *	necessary.
 * @link_mode_masks_nwords: Number of 32-bit words for each of the
 *	supported, advertising, lp_advertising link mode bitmaps. For
 *	%ETHTOOL_GLINKSETTINGS: on entry, number of words passed by user
 *	(>= 0); on return, if handshake in progress, negative if
 *	request size unsupported by kernel: absolute value indicates
 *	kernel expected size and all the other fields but cmd
 *	are 0; otherwise (handshake completed), strictly positive
 *	to indicate size used by kernel and cmd field stays
 *	%ETHTOOL_GLINKSETTINGS, all other fields populated by driver. For
 *	%ETHTOOL_SLINKSETTINGS: must be valid on entry, ie. a positive
 *	value returned previously by %ETHTOOL_GLINKSETTINGS, otherwise
 *	refused. For drivers: ignore this field (use kernel's
 *	__ETHTOOL_LINK_MODE_MASK_NBITS instead), any change to it will
 *	be overwritten by kernel.
 * @supported: Bitmap with each bit meaning given by
 *	%ethtool_link_mode_bit_indices for the link modes, physical
 *	connectors and other link features for which the interface
 *	supports autonegotiation or auto-detection.  Read-only.
 * @advertising: Bitmap with each bit meaning given by
 *	%ethtool_link_mode_bit_indices for the link modes, physical
 *	connectors and other link features that are advertised through
 *	autonegotiation or enabled for auto-detection.
 * @lp_advertising: Bitmap with each bit meaning given by
 *	%ethtool_link_mode_bit_indices for the link modes, and other
 *	link features that the link partner advertised through
 *	autonegotiation; 0 if unknown or not applicable.  Read-only.
 *
 * If autonegotiation is disabled, the speed and @duplex represent the
 * fixed link mode and are writable if the driver supports multiple
 * link modes.  If it is enabled then they are read-only; if the link
 * is up they represent the negotiated link mode; if the link is down,
 * the speed is 0, %SPEED_UNKNOWN or the highest enabled speed and
 * @duplex is %DUPLEX_UNKNOWN or the best enabled duplex mode.
 *
 * Some hardware interfaces may have multiple PHYs and/or physical
 * connectors fitted or do not allow the driver to detect which are
 * fitted.  For these interfaces @port and/or @phy_address may be
 * writable, possibly dependent on @autoneg being %AUTONEG_DISABLE.
 * Otherwise, attempts to write different values may be ignored or
 * rejected.
 *
 * Deprecated %ethtool_cmd fields transceiver, maxtxpkt and maxrxpkt
 * are not available in %ethtool_link_settings. Until all drivers are
 * converted to ignore them or to the new %ethtool_link_settings API,
 * for both queries and changes, users should always try
 * %ETHTOOL_GLINKSETTINGS first, and if it fails with -ENOTSUPP stick
 * only to %ETHTOOL_GSET and %ETHTOOL_SSET consistently. If it
 * succeeds, then users should stick to %ETHTOOL_GLINKSETTINGS and
 * %ETHTOOL_SLINKSETTINGS (which would support drivers implementing
 * either %ethtool_cmd or %ethtool_link_settings).
 *
 * Users should assume that all fields not marked read-only are
 * writable and subject to validation by the driver.  They should use
 * %ETHTOOL_GLINKSETTINGS to get the current values before making specific
 * changes and then applying them with %ETHTOOL_SLINKSETTINGS.
 *
 * Drivers that implement %get_link_ksettings and/or
 * %set_link_ksettings should ignore the @cmd
 * and @link_mode_masks_nwords fields (any change to them overwritten
 * by kernel), and rely only on kernel's internal
 * %__ETHTOOL_LINK_MODE_MASK_NBITS and
 * %ethtool_link_mode_mask_t. Drivers that implement
 * %set_link_ksettings() should validate all fields other than @cmd
 * and @link_mode_masks_nwords that are not described as read-only or
 * deprecated, and must ignore all fields described as read-only.
 */
struct Model0_ethtool_link_settings {
 __u32 Model0_cmd;
 __u32 Model0_speed;
 __u8 Model0_duplex;
 __u8 Model0_port;
 __u8 Model0_phy_address;
 __u8 Model0_autoneg;
 __u8 Model0_mdio_support;
 __u8 Model0_eth_tp_mdix;
 __u8 Model0_eth_tp_mdix_ctrl;
 Model0___s8 Model0_link_mode_masks_nwords;
 __u32 Model0_reserved[8];
 __u32 Model0_link_mode_masks[0];
 /* layout of link_mode_masks fields:
	 * __u32 map_supported[link_mode_masks_nwords];
	 * __u32 map_advertising[link_mode_masks_nwords];
	 * __u32 map_lp_advertising[link_mode_masks_nwords];
	 */
};



struct Model0_compat_ethtool_rx_flow_spec {
 Model0_u32 Model0_flow_type;
 union Model0_ethtool_flow_union Model0_h_u;
 struct Model0_ethtool_flow_ext Model0_h_ext;
 union Model0_ethtool_flow_union Model0_m_u;
 struct Model0_ethtool_flow_ext Model0_m_ext;
 Model0_compat_u64 Model0_ring_cookie;
 Model0_u32 Model0_location;
};

struct Model0_compat_ethtool_rxnfc {
 Model0_u32 Model0_cmd;
 Model0_u32 Model0_flow_type;
 Model0_compat_u64 Model0_data;
 struct Model0_compat_ethtool_rx_flow_spec Model0_fs;
 Model0_u32 Model0_rule_cnt;
 Model0_u32 Model0_rule_locs[0];
};





/**
 * enum ethtool_phys_id_state - indicator state for physical identification
 * @ETHTOOL_ID_INACTIVE: Physical ID indicator should be deactivated
 * @ETHTOOL_ID_ACTIVE: Physical ID indicator should be activated
 * @ETHTOOL_ID_ON: LED should be turned on (used iff %ETHTOOL_ID_ACTIVE
 *	is not supported)
 * @ETHTOOL_ID_OFF: LED should be turned off (used iff %ETHTOOL_ID_ACTIVE
 *	is not supported)
 */
enum Model0_ethtool_phys_id_state {
 Model0_ETHTOOL_ID_INACTIVE,
 Model0_ETHTOOL_ID_ACTIVE,
 Model0_ETHTOOL_ID_ON,
 Model0_ETHTOOL_ID_OFF
};

enum {
 Model0_ETH_RSS_HASH_TOP_BIT, /* Configurable RSS hash function - Toeplitz */
 Model0_ETH_RSS_HASH_XOR_BIT, /* Configurable RSS hash function - Xor */

 /*
	 * Add your fresh new hash function bits above and remember to update
	 * rss_hash_func_strings[] in ethtool.c
	 */
 Model0_ETH_RSS_HASH_FUNCS_COUNT
};
struct Model0_net_device;

/* Some generic methods drivers may use in their ethtool_ops */
Model0_u32 Model0_ethtool_op_get_link(struct Model0_net_device *Model0_dev);
int Model0_ethtool_op_get_ts_info(struct Model0_net_device *Model0_dev, struct Model0_ethtool_ts_info *Model0_eti);

/**
 * ethtool_rxfh_indir_default - get default value for RX flow hash indirection
 * @index: Index in RX flow hash indirection table
 * @n_rx_rings: Number of RX rings to use
 *
 * This function provides the default policy for RX flow hash indirection.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_ethtool_rxfh_indir_default(Model0_u32 Model0_index, Model0_u32 Model0_n_rx_rings)
{
 return Model0_index % Model0_n_rx_rings;
}

/* number of link mode bits/ulongs handled internally by kernel */



/* declare a link mode bitmap */



/* drivers must ignore base.cmd and base.link_mode_masks_nwords
 * fields, but they are allowed to overwrite them (will be ignored).
 */
struct Model0_ethtool_link_ksettings {
 struct Model0_ethtool_link_settings Model0_base;
 struct {
  unsigned long Model0_supported[((((Model0___ETHTOOL_LINK_MODE_LAST + 1)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
  unsigned long Model0_advertising[((((Model0___ETHTOOL_LINK_MODE_LAST + 1)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
  unsigned long Model0_lp_advertising[((((Model0___ETHTOOL_LINK_MODE_LAST + 1)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
 } Model0_link_modes;
};

/**
 * ethtool_link_ksettings_zero_link_mode - clear link_ksettings link mode mask
 *   @ptr : pointer to struct ethtool_link_ksettings
 *   @name : one of supported/advertising/lp_advertising
 */



/**
 * ethtool_link_ksettings_add_link_mode - set bit in link_ksettings
 * link mode mask
 *   @ptr : pointer to struct ethtool_link_ksettings
 *   @name : one of supported/advertising/lp_advertising
 *   @mode : one of the ETHTOOL_LINK_MODE_*_BIT
 * (not atomic, no bound checking)
 */



/**
 * ethtool_link_ksettings_test_link_mode - test bit in ksettings link mode mask
 *   @ptr : pointer to struct ethtool_link_ksettings
 *   @name : one of supported/advertising/lp_advertising
 *   @mode : one of the ETHTOOL_LINK_MODE_*_BIT
 * (not atomic, no bound checking)
 *
 * Returns true/false.
 */



extern int
Model0___ethtool_get_link_ksettings(struct Model0_net_device *Model0_dev,
        struct Model0_ethtool_link_ksettings *Model0_link_ksettings);

void Model0_ethtool_convert_legacy_u32_to_link_mode(unsigned long *Model0_dst,
          Model0_u32 Model0_legacy_u32);

/* return false if src had higher bits set. lower bits always updated. */
bool Model0_ethtool_convert_link_mode_to_legacy_u32(Model0_u32 *Model0_legacy_u32,
         const unsigned long *Model0_src);

/**
 * struct ethtool_ops - optional netdev operations
 * @get_settings: DEPRECATED, use %get_link_ksettings/%set_link_ksettings
 *	API. Get various device settings including Ethernet link
 *	settings. The @cmd parameter is expected to have been cleared
 *	before get_settings is called. Returns a negative error code
 *	or zero.
 * @set_settings: DEPRECATED, use %get_link_ksettings/%set_link_ksettings
 *	API. Set various device settings including Ethernet link
 *	settings.  Returns a negative error code or zero.
 * @get_drvinfo: Report driver/device information.  Should only set the
 *	@driver, @version, @fw_version and @bus_info fields.  If not
 *	implemented, the @driver and @bus_info fields will be filled in
 *	according to the netdev's parent device.
 * @get_regs_len: Get buffer length required for @get_regs
 * @get_regs: Get device registers
 * @get_wol: Report whether Wake-on-Lan is enabled
 * @set_wol: Turn Wake-on-Lan on or off.  Returns a negative error code
 *	or zero.
 * @get_msglevel: Report driver message level.  This should be the value
 *	of the @msg_enable field used by netif logging functions.
 * @set_msglevel: Set driver message level
 * @nway_reset: Restart autonegotiation.  Returns a negative error code
 *	or zero.
 * @get_link: Report whether physical link is up.  Will only be called if
 *	the netdev is up.  Should usually be set to ethtool_op_get_link(),
 *	which uses netif_carrier_ok().
 * @get_eeprom: Read data from the device EEPROM.
 *	Should fill in the magic field.  Don't need to check len for zero
 *	or wraparound.  Fill in the data argument with the eeprom values
 *	from offset to offset + len.  Update len to the amount read.
 *	Returns an error or zero.
 * @set_eeprom: Write data to the device EEPROM.
 *	Should validate the magic field.  Don't need to check len for zero
 *	or wraparound.  Update len to the amount written.  Returns an error
 *	or zero.
 * @get_coalesce: Get interrupt coalescing parameters.  Returns a negative
 *	error code or zero.
 * @set_coalesce: Set interrupt coalescing parameters.  Returns a negative
 *	error code or zero.
 * @get_ringparam: Report ring sizes
 * @set_ringparam: Set ring sizes.  Returns a negative error code or zero.
 * @get_pauseparam: Report pause parameters
 * @set_pauseparam: Set pause parameters.  Returns a negative error code
 *	or zero.
 * @self_test: Run specified self-tests
 * @get_strings: Return a set of strings that describe the requested objects
 * @set_phys_id: Identify the physical devices, e.g. by flashing an LED
 *	attached to it.  The implementation may update the indicator
 *	asynchronously or synchronously, but in either case it must return
 *	quickly.  It is initially called with the argument %ETHTOOL_ID_ACTIVE,
 *	and must either activate asynchronous updates and return zero, return
 *	a negative error or return a positive frequency for synchronous
 *	indication (e.g. 1 for one on/off cycle per second).  If it returns
 *	a frequency then it will be called again at intervals with the
 *	argument %ETHTOOL_ID_ON or %ETHTOOL_ID_OFF and should set the state of
 *	the indicator accordingly.  Finally, it is called with the argument
 *	%ETHTOOL_ID_INACTIVE and must deactivate the indicator.  Returns a
 *	negative error code or zero.
 * @get_ethtool_stats: Return extended statistics about the device.
 *	This is only useful if the device maintains statistics not
 *	included in &struct rtnl_link_stats64.
 * @begin: Function to be called before any other operation.  Returns a
 *	negative error code or zero.
 * @complete: Function to be called after any other operation except
 *	@begin.  Will be called even if the other operation failed.
 * @get_priv_flags: Report driver-specific feature flags.
 * @set_priv_flags: Set driver-specific feature flags.  Returns a negative
 *	error code or zero.
 * @get_sset_count: Get number of strings that @get_strings will write.
 * @get_rxnfc: Get RX flow classification rules.  Returns a negative
 *	error code or zero.
 * @set_rxnfc: Set RX flow classification rules.  Returns a negative
 *	error code or zero.
 * @flash_device: Write a firmware image to device's flash memory.
 *	Returns a negative error code or zero.
 * @reset: Reset (part of) the device, as specified by a bitmask of
 *	flags from &enum ethtool_reset_flags.  Returns a negative
 *	error code or zero.
 * @get_rxfh_key_size: Get the size of the RX flow hash key.
 *	Returns zero if not supported for this specific device.
 * @get_rxfh_indir_size: Get the size of the RX flow hash indirection table.
 *	Returns zero if not supported for this specific device.
 * @get_rxfh: Get the contents of the RX flow hash indirection table, hash key
 *	and/or hash function.
 *	Returns a negative error code or zero.
 * @set_rxfh: Set the contents of the RX flow hash indirection table, hash
 *	key, and/or hash function.  Arguments which are set to %NULL or zero
 *	will remain unchanged.
 *	Returns a negative error code or zero. An error code must be returned
 *	if at least one unsupported change was requested.
 * @get_channels: Get number of channels.
 * @set_channels: Set number of channels.  Returns a negative error code or
 *	zero.
 * @get_dump_flag: Get dump flag indicating current dump length, version,
 * 		   and flag of the device.
 * @get_dump_data: Get dump data.
 * @set_dump: Set dump specific flags to the device.
 * @get_ts_info: Get the time stamping and PTP hardware clock capabilities.
 *	Drivers supporting transmit time stamps in software should set this to
 *	ethtool_op_get_ts_info().
 * @get_module_info: Get the size and type of the eeprom contained within
 *	a plug-in module.
 * @get_module_eeprom: Get the eeprom information from the plug-in module
 * @get_eee: Get Energy-Efficient (EEE) supported and status.
 * @set_eee: Set EEE status (enable/disable) as well as LPI timers.
 * @get_per_queue_coalesce: Get interrupt coalescing parameters per queue.
 *	It must check that the given queue number is valid. If neither a RX nor
 *	a TX queue has this number, return -EINVAL. If only a RX queue or a TX
 *	queue has this number, set the inapplicable fields to ~0 and return 0.
 *	Returns a negative error code or zero.
 * @set_per_queue_coalesce: Set interrupt coalescing parameters per queue.
 *	It must check that the given queue number is valid. If neither a RX nor
 *	a TX queue has this number, return -EINVAL. If only a RX queue or a TX
 *	queue has this number, ignore the inapplicable fields.
 *	Returns a negative error code or zero.
 * @get_link_ksettings: When defined, takes precedence over the
 *	%get_settings method. Get various device settings
 *	including Ethernet link settings. The %cmd and
 *	%link_mode_masks_nwords fields should be ignored (use
 *	%__ETHTOOL_LINK_MODE_MASK_NBITS instead of the latter), any
 *	change to them will be overwritten by kernel. Returns a
 *	negative error code or zero.
 * @set_link_ksettings: When defined, takes precedence over the
 *	%set_settings method. Set various device settings including
 *	Ethernet link settings. The %cmd and %link_mode_masks_nwords
 *	fields should be ignored (use %__ETHTOOL_LINK_MODE_MASK_NBITS
 *	instead of the latter), any change to them will be overwritten
 *	by kernel. Returns a negative error code or zero.
 *
 * All operations are optional (i.e. the function pointer may be set
 * to %NULL) and callers must take this into account.  Callers must
 * hold the RTNL lock.
 *
 * See the structures used by these operations for further documentation.
 * Note that for all operations using a structure ending with a zero-
 * length array, the array is allocated separately in the kernel and
 * is passed to the driver as an additional parameter.
 *
 * See &struct net_device and &struct net_device_ops for documentation
 * of the generic netdev features interface.
 */
struct Model0_ethtool_ops {
 int (*Model0_get_settings)(struct Model0_net_device *, struct Model0_ethtool_cmd *);
 int (*Model0_set_settings)(struct Model0_net_device *, struct Model0_ethtool_cmd *);
 void (*Model0_get_drvinfo)(struct Model0_net_device *, struct Model0_ethtool_drvinfo *);
 int (*Model0_get_regs_len)(struct Model0_net_device *);
 void (*Model0_get_regs)(struct Model0_net_device *, struct Model0_ethtool_regs *, void *);
 void (*Model0_get_wol)(struct Model0_net_device *, struct Model0_ethtool_wolinfo *);
 int (*Model0_set_wol)(struct Model0_net_device *, struct Model0_ethtool_wolinfo *);
 Model0_u32 (*Model0_get_msglevel)(struct Model0_net_device *);
 void (*Model0_set_msglevel)(struct Model0_net_device *, Model0_u32);
 int (*Model0_nway_reset)(struct Model0_net_device *);
 Model0_u32 (*Model0_get_link)(struct Model0_net_device *);
 int (*Model0_get_eeprom_len)(struct Model0_net_device *);
 int (*Model0_get_eeprom)(struct Model0_net_device *,
         struct Model0_ethtool_eeprom *, Model0_u8 *);
 int (*Model0_set_eeprom)(struct Model0_net_device *,
         struct Model0_ethtool_eeprom *, Model0_u8 *);
 int (*Model0_get_coalesce)(struct Model0_net_device *, struct Model0_ethtool_coalesce *);
 int (*Model0_set_coalesce)(struct Model0_net_device *, struct Model0_ethtool_coalesce *);
 void (*Model0_get_ringparam)(struct Model0_net_device *,
     struct Model0_ethtool_ringparam *);
 int (*Model0_set_ringparam)(struct Model0_net_device *,
     struct Model0_ethtool_ringparam *);
 void (*Model0_get_pauseparam)(struct Model0_net_device *,
      struct Model0_ethtool_pauseparam*);
 int (*Model0_set_pauseparam)(struct Model0_net_device *,
      struct Model0_ethtool_pauseparam*);
 void (*Model0_self_test)(struct Model0_net_device *, struct Model0_ethtool_test *, Model0_u64 *);
 void (*Model0_get_strings)(struct Model0_net_device *, Model0_u32 Model0_stringset, Model0_u8 *);
 int (*Model0_set_phys_id)(struct Model0_net_device *, enum Model0_ethtool_phys_id_state);
 void (*Model0_get_ethtool_stats)(struct Model0_net_device *,
         struct Model0_ethtool_stats *, Model0_u64 *);
 int (*Model0_begin)(struct Model0_net_device *);
 void (*Model0_complete)(struct Model0_net_device *);
 Model0_u32 (*Model0_get_priv_flags)(struct Model0_net_device *);
 int (*Model0_set_priv_flags)(struct Model0_net_device *, Model0_u32);
 int (*Model0_get_sset_count)(struct Model0_net_device *, int);
 int (*Model0_get_rxnfc)(struct Model0_net_device *,
        struct Model0_ethtool_rxnfc *, Model0_u32 *Model0_rule_locs);
 int (*Model0_set_rxnfc)(struct Model0_net_device *, struct Model0_ethtool_rxnfc *);
 int (*Model0_flash_device)(struct Model0_net_device *, struct Model0_ethtool_flash *);
 int (*Model0_reset)(struct Model0_net_device *, Model0_u32 *);
 Model0_u32 (*Model0_get_rxfh_key_size)(struct Model0_net_device *);
 Model0_u32 (*Model0_get_rxfh_indir_size)(struct Model0_net_device *);
 int (*Model0_get_rxfh)(struct Model0_net_device *, Model0_u32 *Model0_indir, Model0_u8 *Model0_key,
       Model0_u8 *Model0_hfunc);
 int (*Model0_set_rxfh)(struct Model0_net_device *, const Model0_u32 *Model0_indir,
       const Model0_u8 *Model0_key, const Model0_u8 Model0_hfunc);
 void (*Model0_get_channels)(struct Model0_net_device *, struct Model0_ethtool_channels *);
 int (*Model0_set_channels)(struct Model0_net_device *, struct Model0_ethtool_channels *);
 int (*Model0_get_dump_flag)(struct Model0_net_device *, struct Model0_ethtool_dump *);
 int (*Model0_get_dump_data)(struct Model0_net_device *,
     struct Model0_ethtool_dump *, void *);
 int (*Model0_set_dump)(struct Model0_net_device *, struct Model0_ethtool_dump *);
 int (*Model0_get_ts_info)(struct Model0_net_device *, struct Model0_ethtool_ts_info *);
 int (*Model0_get_module_info)(struct Model0_net_device *,
       struct Model0_ethtool_modinfo *);
 int (*Model0_get_module_eeprom)(struct Model0_net_device *,
         struct Model0_ethtool_eeprom *, Model0_u8 *);
 int (*Model0_get_eee)(struct Model0_net_device *, struct Model0_ethtool_eee *);
 int (*Model0_set_eee)(struct Model0_net_device *, struct Model0_ethtool_eee *);
 int (*Model0_get_tunable)(struct Model0_net_device *,
          const struct Model0_ethtool_tunable *, void *);
 int (*Model0_set_tunable)(struct Model0_net_device *,
          const struct Model0_ethtool_tunable *, const void *);
 int (*Model0_get_per_queue_coalesce)(struct Model0_net_device *, Model0_u32,
       struct Model0_ethtool_coalesce *);
 int (*Model0_set_per_queue_coalesce)(struct Model0_net_device *, Model0_u32,
       struct Model0_ethtool_coalesce *);
 int (*Model0_get_link_ksettings)(struct Model0_net_device *,
          struct Model0_ethtool_link_ksettings *);
 int (*Model0_set_link_ksettings)(struct Model0_net_device *,
          const struct Model0_ethtool_link_ksettings *);
};
/*
 * Operations on the network namespace
 */



struct Model0_ctl_table_header;
struct Model0_prot_inuse;

struct Model0_netns_core {
 /* core sysctls */
 struct Model0_ctl_table_header *Model0_sysctl_hdr;

 int Model0_sysctl_somaxconn;

 struct Model0_prot_inuse *Model0_inuse;
};



/*
 *
 *		SNMP MIB entries for the IP subsystem.
 *		
 *		Alan Cox <gw4pts@gw4pts.ampr.org>
 *
 *		We don't chose to implement SNMP in the kernel (this would
 *		be silly as SNMP is a pain in the backside in places). We do
 *		however need to collect the MIB statistics and export them
 *		out of /proc (eventually)
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 *
 */






/*
 * Definitions for MIBs
 *
 * Author: Hideaki YOSHIFUJI <yoshfuji@linux-ipv6.org>
 */




/* ipstats mib definitions */
/*
 * RFC 1213:  MIB-II
 * RFC 2011 (updates 1213):  SNMPv2-MIB-IP
 * RFC 2863:  Interfaces Group MIB
 * RFC 2465:  IPv6 MIB: General Group
 * draft-ietf-ipv6-rfc2011-update-10.txt: MIB for IP: IP Statistics Tables
 */
enum
{
 Model0_IPSTATS_MIB_NUM = 0,
/* frequently written fields in fast path, kept in same cache line */
 Model0_IPSTATS_MIB_INPKTS, /* InReceives */
 Model0_IPSTATS_MIB_INOCTETS, /* InOctets */
 Model0_IPSTATS_MIB_INDELIVERS, /* InDelivers */
 Model0_IPSTATS_MIB_OUTFORWDATAGRAMS, /* OutForwDatagrams */
 Model0_IPSTATS_MIB_OUTPKTS, /* OutRequests */
 Model0_IPSTATS_MIB_OUTOCTETS, /* OutOctets */
/* other fields */
 Model0_IPSTATS_MIB_INHDRERRORS, /* InHdrErrors */
 Model0_IPSTATS_MIB_INTOOBIGERRORS, /* InTooBigErrors */
 Model0_IPSTATS_MIB_INNOROUTES, /* InNoRoutes */
 Model0_IPSTATS_MIB_INADDRERRORS, /* InAddrErrors */
 Model0_IPSTATS_MIB_INUNKNOWNPROTOS, /* InUnknownProtos */
 Model0_IPSTATS_MIB_INTRUNCATEDPKTS, /* InTruncatedPkts */
 Model0_IPSTATS_MIB_INDISCARDS, /* InDiscards */
 Model0_IPSTATS_MIB_OUTDISCARDS, /* OutDiscards */
 Model0_IPSTATS_MIB_OUTNOROUTES, /* OutNoRoutes */
 Model0_IPSTATS_MIB_REASMTIMEOUT, /* ReasmTimeout */
 Model0_IPSTATS_MIB_REASMREQDS, /* ReasmReqds */
 Model0_IPSTATS_MIB_REASMOKS, /* ReasmOKs */
 Model0_IPSTATS_MIB_REASMFAILS, /* ReasmFails */
 Model0_IPSTATS_MIB_FRAGOKS, /* FragOKs */
 Model0_IPSTATS_MIB_FRAGFAILS, /* FragFails */
 Model0_IPSTATS_MIB_FRAGCREATES, /* FragCreates */
 Model0_IPSTATS_MIB_INMCASTPKTS, /* InMcastPkts */
 Model0_IPSTATS_MIB_OUTMCASTPKTS, /* OutMcastPkts */
 Model0_IPSTATS_MIB_INBCASTPKTS, /* InBcastPkts */
 Model0_IPSTATS_MIB_OUTBCASTPKTS, /* OutBcastPkts */
 Model0_IPSTATS_MIB_INMCASTOCTETS, /* InMcastOctets */
 Model0_IPSTATS_MIB_OUTMCASTOCTETS, /* OutMcastOctets */
 Model0_IPSTATS_MIB_INBCASTOCTETS, /* InBcastOctets */
 Model0_IPSTATS_MIB_OUTBCASTOCTETS, /* OutBcastOctets */
 Model0_IPSTATS_MIB_CSUMERRORS, /* InCsumErrors */
 Model0_IPSTATS_MIB_NOECTPKTS, /* InNoECTPkts */
 Model0_IPSTATS_MIB_ECT1PKTS, /* InECT1Pkts */
 Model0_IPSTATS_MIB_ECT0PKTS, /* InECT0Pkts */
 Model0_IPSTATS_MIB_CEPKTS, /* InCEPkts */
 Model0___IPSTATS_MIB_MAX
};

/* icmp mib definitions */
/*
 * RFC 1213:  MIB-II ICMP Group
 * RFC 2011 (updates 1213):  SNMPv2 MIB for IP: ICMP group
 */
enum
{
 Model0_ICMP_MIB_NUM = 0,
 Model0_ICMP_MIB_INMSGS, /* InMsgs */
 Model0_ICMP_MIB_INERRORS, /* InErrors */
 Model0_ICMP_MIB_INDESTUNREACHS, /* InDestUnreachs */
 Model0_ICMP_MIB_INTIMEEXCDS, /* InTimeExcds */
 Model0_ICMP_MIB_INPARMPROBS, /* InParmProbs */
 Model0_ICMP_MIB_INSRCQUENCHS, /* InSrcQuenchs */
 Model0_ICMP_MIB_INREDIRECTS, /* InRedirects */
 Model0_ICMP_MIB_INECHOS, /* InEchos */
 Model0_ICMP_MIB_INECHOREPS, /* InEchoReps */
 Model0_ICMP_MIB_INTIMESTAMPS, /* InTimestamps */
 Model0_ICMP_MIB_INTIMESTAMPREPS, /* InTimestampReps */
 Model0_ICMP_MIB_INADDRMASKS, /* InAddrMasks */
 Model0_ICMP_MIB_INADDRMASKREPS, /* InAddrMaskReps */
 Model0_ICMP_MIB_OUTMSGS, /* OutMsgs */
 Model0_ICMP_MIB_OUTERRORS, /* OutErrors */
 Model0_ICMP_MIB_OUTDESTUNREACHS, /* OutDestUnreachs */
 Model0_ICMP_MIB_OUTTIMEEXCDS, /* OutTimeExcds */
 Model0_ICMP_MIB_OUTPARMPROBS, /* OutParmProbs */
 Model0_ICMP_MIB_OUTSRCQUENCHS, /* OutSrcQuenchs */
 Model0_ICMP_MIB_OUTREDIRECTS, /* OutRedirects */
 Model0_ICMP_MIB_OUTECHOS, /* OutEchos */
 Model0_ICMP_MIB_OUTECHOREPS, /* OutEchoReps */
 Model0_ICMP_MIB_OUTTIMESTAMPS, /* OutTimestamps */
 Model0_ICMP_MIB_OUTTIMESTAMPREPS, /* OutTimestampReps */
 Model0_ICMP_MIB_OUTADDRMASKS, /* OutAddrMasks */
 Model0_ICMP_MIB_OUTADDRMASKREPS, /* OutAddrMaskReps */
 Model0_ICMP_MIB_CSUMERRORS, /* InCsumErrors */
 Model0___ICMP_MIB_MAX
};



/* icmp6 mib definitions */
/*
 * RFC 2466:  ICMPv6-MIB
 */
enum
{
 Model0_ICMP6_MIB_NUM = 0,
 Model0_ICMP6_MIB_INMSGS, /* InMsgs */
 Model0_ICMP6_MIB_INERRORS, /* InErrors */
 Model0_ICMP6_MIB_OUTMSGS, /* OutMsgs */
 Model0_ICMP6_MIB_OUTERRORS, /* OutErrors */
 Model0_ICMP6_MIB_CSUMERRORS, /* InCsumErrors */
 Model0___ICMP6_MIB_MAX
};



/* tcp mib definitions */
/*
 * RFC 1213:  MIB-II TCP group
 * RFC 2012 (updates 1213):  SNMPv2-MIB-TCP
 */
enum
{
 Model0_TCP_MIB_NUM = 0,
 Model0_TCP_MIB_RTOALGORITHM, /* RtoAlgorithm */
 Model0_TCP_MIB_RTOMIN, /* RtoMin */
 Model0_TCP_MIB_RTOMAX, /* RtoMax */
 Model0_TCP_MIB_MAXCONN, /* MaxConn */
 Model0_TCP_MIB_ACTIVEOPENS, /* ActiveOpens */
 Model0_TCP_MIB_PASSIVEOPENS, /* PassiveOpens */
 Model0_TCP_MIB_ATTEMPTFAILS, /* AttemptFails */
 Model0_TCP_MIB_ESTABRESETS, /* EstabResets */
 Model0_TCP_MIB_CURRESTAB, /* CurrEstab */
 Model0_TCP_MIB_INSEGS, /* InSegs */
 Model0_TCP_MIB_OUTSEGS, /* OutSegs */
 Model0_TCP_MIB_RETRANSSEGS, /* RetransSegs */
 Model0_TCP_MIB_INERRS, /* InErrs */
 Model0_TCP_MIB_OUTRSTS, /* OutRsts */
 Model0_TCP_MIB_CSUMERRORS, /* InCsumErrors */
 Model0___TCP_MIB_MAX
};

/* udp mib definitions */
/*
 * RFC 1213:  MIB-II UDP group
 * RFC 2013 (updates 1213):  SNMPv2-MIB-UDP
 */
enum
{
 Model0_UDP_MIB_NUM = 0,
 Model0_UDP_MIB_INDATAGRAMS, /* InDatagrams */
 Model0_UDP_MIB_NOPORTS, /* NoPorts */
 Model0_UDP_MIB_INERRORS, /* InErrors */
 Model0_UDP_MIB_OUTDATAGRAMS, /* OutDatagrams */
 Model0_UDP_MIB_RCVBUFERRORS, /* RcvbufErrors */
 Model0_UDP_MIB_SNDBUFERRORS, /* SndbufErrors */
 Model0_UDP_MIB_CSUMERRORS, /* InCsumErrors */
 Model0_UDP_MIB_IGNOREDMULTI, /* IgnoredMulti */
 Model0___UDP_MIB_MAX
};

/* linux mib definitions */
enum
{
 Model0_LINUX_MIB_NUM = 0,
 Model0_LINUX_MIB_SYNCOOKIESSENT, /* SyncookiesSent */
 Model0_LINUX_MIB_SYNCOOKIESRECV, /* SyncookiesRecv */
 Model0_LINUX_MIB_SYNCOOKIESFAILED, /* SyncookiesFailed */
 Model0_LINUX_MIB_EMBRYONICRSTS, /* EmbryonicRsts */
 Model0_LINUX_MIB_PRUNECALLED, /* PruneCalled */
 Model0_LINUX_MIB_RCVPRUNED, /* RcvPruned */
 Model0_LINUX_MIB_OFOPRUNED, /* OfoPruned */
 Model0_LINUX_MIB_OUTOFWINDOWICMPS, /* OutOfWindowIcmps */
 Model0_LINUX_MIB_LOCKDROPPEDICMPS, /* LockDroppedIcmps */
 Model0_LINUX_MIB_ARPFILTER, /* ArpFilter */
 Model0_LINUX_MIB_TIMEWAITED, /* TimeWaited */
 Model0_LINUX_MIB_TIMEWAITRECYCLED, /* TimeWaitRecycled */
 Model0_LINUX_MIB_TIMEWAITKILLED, /* TimeWaitKilled */
 Model0_LINUX_MIB_PAWSPASSIVEREJECTED, /* PAWSPassiveRejected */
 Model0_LINUX_MIB_PAWSACTIVEREJECTED, /* PAWSActiveRejected */
 Model0_LINUX_MIB_PAWSESTABREJECTED, /* PAWSEstabRejected */
 Model0_LINUX_MIB_DELAYEDACKS, /* DelayedACKs */
 Model0_LINUX_MIB_DELAYEDACKLOCKED, /* DelayedACKLocked */
 Model0_LINUX_MIB_DELAYEDACKLOST, /* DelayedACKLost */
 Model0_LINUX_MIB_LISTENOVERFLOWS, /* ListenOverflows */
 Model0_LINUX_MIB_LISTENDROPS, /* ListenDrops */
 Model0_LINUX_MIB_TCPPREQUEUED, /* TCPPrequeued */
 Model0_LINUX_MIB_TCPDIRECTCOPYFROMBACKLOG, /* TCPDirectCopyFromBacklog */
 Model0_LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, /* TCPDirectCopyFromPrequeue */
 Model0_LINUX_MIB_TCPPREQUEUEDROPPED, /* TCPPrequeueDropped */
 Model0_LINUX_MIB_TCPHPHITS, /* TCPHPHits */
 Model0_LINUX_MIB_TCPHPHITSTOUSER, /* TCPHPHitsToUser */
 Model0_LINUX_MIB_TCPPUREACKS, /* TCPPureAcks */
 Model0_LINUX_MIB_TCPHPACKS, /* TCPHPAcks */
 Model0_LINUX_MIB_TCPRENORECOVERY, /* TCPRenoRecovery */
 Model0_LINUX_MIB_TCPSACKRECOVERY, /* TCPSackRecovery */
 Model0_LINUX_MIB_TCPSACKRENEGING, /* TCPSACKReneging */
 Model0_LINUX_MIB_TCPFACKREORDER, /* TCPFACKReorder */
 Model0_LINUX_MIB_TCPSACKREORDER, /* TCPSACKReorder */
 Model0_LINUX_MIB_TCPRENOREORDER, /* TCPRenoReorder */
 Model0_LINUX_MIB_TCPTSREORDER, /* TCPTSReorder */
 Model0_LINUX_MIB_TCPFULLUNDO, /* TCPFullUndo */
 Model0_LINUX_MIB_TCPPARTIALUNDO, /* TCPPartialUndo */
 Model0_LINUX_MIB_TCPDSACKUNDO, /* TCPDSACKUndo */
 Model0_LINUX_MIB_TCPLOSSUNDO, /* TCPLossUndo */
 Model0_LINUX_MIB_TCPLOSTRETRANSMIT, /* TCPLostRetransmit */
 Model0_LINUX_MIB_TCPRENOFAILURES, /* TCPRenoFailures */
 Model0_LINUX_MIB_TCPSACKFAILURES, /* TCPSackFailures */
 Model0_LINUX_MIB_TCPLOSSFAILURES, /* TCPLossFailures */
 Model0_LINUX_MIB_TCPFASTRETRANS, /* TCPFastRetrans */
 Model0_LINUX_MIB_TCPFORWARDRETRANS, /* TCPForwardRetrans */
 Model0_LINUX_MIB_TCPSLOWSTARTRETRANS, /* TCPSlowStartRetrans */
 Model0_LINUX_MIB_TCPTIMEOUTS, /* TCPTimeouts */
 Model0_LINUX_MIB_TCPLOSSPROBES, /* TCPLossProbes */
 Model0_LINUX_MIB_TCPLOSSPROBERECOVERY, /* TCPLossProbeRecovery */
 Model0_LINUX_MIB_TCPRENORECOVERYFAIL, /* TCPRenoRecoveryFail */
 Model0_LINUX_MIB_TCPSACKRECOVERYFAIL, /* TCPSackRecoveryFail */
 Model0_LINUX_MIB_TCPSCHEDULERFAILED, /* TCPSchedulerFailed */
 Model0_LINUX_MIB_TCPRCVCOLLAPSED, /* TCPRcvCollapsed */
 Model0_LINUX_MIB_TCPDSACKOLDSENT, /* TCPDSACKOldSent */
 Model0_LINUX_MIB_TCPDSACKOFOSENT, /* TCPDSACKOfoSent */
 Model0_LINUX_MIB_TCPDSACKRECV, /* TCPDSACKRecv */
 Model0_LINUX_MIB_TCPDSACKOFORECV, /* TCPDSACKOfoRecv */
 Model0_LINUX_MIB_TCPABORTONDATA, /* TCPAbortOnData */
 Model0_LINUX_MIB_TCPABORTONCLOSE, /* TCPAbortOnClose */
 Model0_LINUX_MIB_TCPABORTONMEMORY, /* TCPAbortOnMemory */
 Model0_LINUX_MIB_TCPABORTONTIMEOUT, /* TCPAbortOnTimeout */
 Model0_LINUX_MIB_TCPABORTONLINGER, /* TCPAbortOnLinger */
 Model0_LINUX_MIB_TCPABORTFAILED, /* TCPAbortFailed */
 Model0_LINUX_MIB_TCPMEMORYPRESSURES, /* TCPMemoryPressures */
 Model0_LINUX_MIB_TCPSACKDISCARD, /* TCPSACKDiscard */
 Model0_LINUX_MIB_TCPDSACKIGNOREDOLD, /* TCPSACKIgnoredOld */
 Model0_LINUX_MIB_TCPDSACKIGNOREDNOUNDO, /* TCPSACKIgnoredNoUndo */
 Model0_LINUX_MIB_TCPSPURIOUSRTOS, /* TCPSpuriousRTOs */
 Model0_LINUX_MIB_TCPMD5NOTFOUND, /* TCPMD5NotFound */
 Model0_LINUX_MIB_TCPMD5UNEXPECTED, /* TCPMD5Unexpected */
 Model0_LINUX_MIB_SACKSHIFTED,
 Model0_LINUX_MIB_SACKMERGED,
 Model0_LINUX_MIB_SACKSHIFTFALLBACK,
 Model0_LINUX_MIB_TCPBACKLOGDROP,
 Model0_LINUX_MIB_TCPMINTTLDROP, /* RFC 5082 */
 Model0_LINUX_MIB_TCPDEFERACCEPTDROP,
 Model0_LINUX_MIB_IPRPFILTER, /* IP Reverse Path Filter (rp_filter) */
 Model0_LINUX_MIB_TCPTIMEWAITOVERFLOW, /* TCPTimeWaitOverflow */
 Model0_LINUX_MIB_TCPREQQFULLDOCOOKIES, /* TCPReqQFullDoCookies */
 Model0_LINUX_MIB_TCPREQQFULLDROP, /* TCPReqQFullDrop */
 Model0_LINUX_MIB_TCPRETRANSFAIL, /* TCPRetransFail */
 Model0_LINUX_MIB_TCPRCVCOALESCE, /* TCPRcvCoalesce */
 Model0_LINUX_MIB_TCPOFOQUEUE, /* TCPOFOQueue */
 Model0_LINUX_MIB_TCPOFODROP, /* TCPOFODrop */
 Model0_LINUX_MIB_TCPOFOMERGE, /* TCPOFOMerge */
 Model0_LINUX_MIB_TCPCHALLENGEACK, /* TCPChallengeACK */
 Model0_LINUX_MIB_TCPSYNCHALLENGE, /* TCPSYNChallenge */
 Model0_LINUX_MIB_TCPFASTOPENACTIVE, /* TCPFastOpenActive */
 Model0_LINUX_MIB_TCPFASTOPENACTIVEFAIL, /* TCPFastOpenActiveFail */
 Model0_LINUX_MIB_TCPFASTOPENPASSIVE, /* TCPFastOpenPassive*/
 Model0_LINUX_MIB_TCPFASTOPENPASSIVEFAIL, /* TCPFastOpenPassiveFail */
 Model0_LINUX_MIB_TCPFASTOPENLISTENOVERFLOW, /* TCPFastOpenListenOverflow */
 Model0_LINUX_MIB_TCPFASTOPENCOOKIEREQD, /* TCPFastOpenCookieReqd */
 Model0_LINUX_MIB_TCPSPURIOUS_RTX_HOSTQUEUES, /* TCPSpuriousRtxHostQueues */
 Model0_LINUX_MIB_BUSYPOLLRXPACKETS, /* BusyPollRxPackets */
 Model0_LINUX_MIB_TCPAUTOCORKING, /* TCPAutoCorking */
 Model0_LINUX_MIB_TCPFROMZEROWINDOWADV, /* TCPFromZeroWindowAdv */
 Model0_LINUX_MIB_TCPTOZEROWINDOWADV, /* TCPToZeroWindowAdv */
 Model0_LINUX_MIB_TCPWANTZEROWINDOWADV, /* TCPWantZeroWindowAdv */
 Model0_LINUX_MIB_TCPSYNRETRANS, /* TCPSynRetrans */
 Model0_LINUX_MIB_TCPORIGDATASENT, /* TCPOrigDataSent */
 Model0_LINUX_MIB_TCPHYSTARTTRAINDETECT, /* TCPHystartTrainDetect */
 Model0_LINUX_MIB_TCPHYSTARTTRAINCWND, /* TCPHystartTrainCwnd */
 Model0_LINUX_MIB_TCPHYSTARTDELAYDETECT, /* TCPHystartDelayDetect */
 Model0_LINUX_MIB_TCPHYSTARTDELAYCWND, /* TCPHystartDelayCwnd */
 Model0_LINUX_MIB_TCPACKSKIPPEDSYNRECV, /* TCPACKSkippedSynRecv */
 Model0_LINUX_MIB_TCPACKSKIPPEDPAWS, /* TCPACKSkippedPAWS */
 Model0_LINUX_MIB_TCPACKSKIPPEDSEQ, /* TCPACKSkippedSeq */
 Model0_LINUX_MIB_TCPACKSKIPPEDFINWAIT2, /* TCPACKSkippedFinWait2 */
 Model0_LINUX_MIB_TCPACKSKIPPEDTIMEWAIT, /* TCPACKSkippedTimeWait */
 Model0_LINUX_MIB_TCPACKSKIPPEDCHALLENGE, /* TCPACKSkippedChallenge */
 Model0_LINUX_MIB_TCPWINPROBE, /* TCPWinProbe */
 Model0_LINUX_MIB_TCPKEEPALIVE, /* TCPKeepAlive */
 Model0_LINUX_MIB_TCPMTUPFAIL, /* TCPMTUPFail */
 Model0_LINUX_MIB_TCPMTUPSUCCESS, /* TCPMTUPSuccess */
 Model0___LINUX_MIB_MAX
};

/* linux Xfrm mib definitions */
enum
{
 Model0_LINUX_MIB_XFRMNUM = 0,
 Model0_LINUX_MIB_XFRMINERROR, /* XfrmInError */
 Model0_LINUX_MIB_XFRMINBUFFERERROR, /* XfrmInBufferError */
 Model0_LINUX_MIB_XFRMINHDRERROR, /* XfrmInHdrError */
 Model0_LINUX_MIB_XFRMINNOSTATES, /* XfrmInNoStates */
 Model0_LINUX_MIB_XFRMINSTATEPROTOERROR, /* XfrmInStateProtoError */
 Model0_LINUX_MIB_XFRMINSTATEMODEERROR, /* XfrmInStateModeError */
 Model0_LINUX_MIB_XFRMINSTATESEQERROR, /* XfrmInStateSeqError */
 Model0_LINUX_MIB_XFRMINSTATEEXPIRED, /* XfrmInStateExpired */
 Model0_LINUX_MIB_XFRMINSTATEMISMATCH, /* XfrmInStateMismatch */
 Model0_LINUX_MIB_XFRMINSTATEINVALID, /* XfrmInStateInvalid */
 Model0_LINUX_MIB_XFRMINTMPLMISMATCH, /* XfrmInTmplMismatch */
 Model0_LINUX_MIB_XFRMINNOPOLS, /* XfrmInNoPols */
 Model0_LINUX_MIB_XFRMINPOLBLOCK, /* XfrmInPolBlock */
 Model0_LINUX_MIB_XFRMINPOLERROR, /* XfrmInPolError */
 Model0_LINUX_MIB_XFRMOUTERROR, /* XfrmOutError */
 Model0_LINUX_MIB_XFRMOUTBUNDLEGENERROR, /* XfrmOutBundleGenError */
 Model0_LINUX_MIB_XFRMOUTBUNDLECHECKERROR, /* XfrmOutBundleCheckError */
 Model0_LINUX_MIB_XFRMOUTNOSTATES, /* XfrmOutNoStates */
 Model0_LINUX_MIB_XFRMOUTSTATEPROTOERROR, /* XfrmOutStateProtoError */
 Model0_LINUX_MIB_XFRMOUTSTATEMODEERROR, /* XfrmOutStateModeError */
 Model0_LINUX_MIB_XFRMOUTSTATESEQERROR, /* XfrmOutStateSeqError */
 Model0_LINUX_MIB_XFRMOUTSTATEEXPIRED, /* XfrmOutStateExpired */
 Model0_LINUX_MIB_XFRMOUTPOLBLOCK, /* XfrmOutPolBlock */
 Model0_LINUX_MIB_XFRMOUTPOLDEAD, /* XfrmOutPolDead */
 Model0_LINUX_MIB_XFRMOUTPOLERROR, /* XfrmOutPolError */
 Model0_LINUX_MIB_XFRMFWDHDRERROR, /* XfrmFwdHdrError*/
 Model0_LINUX_MIB_XFRMOUTSTATEINVALID, /* XfrmOutStateInvalid */
 Model0_LINUX_MIB_XFRMACQUIREERROR, /* XfrmAcquireError */
 Model0___LINUX_MIB_XFRMMAX
};


/*
 * Mibs are stored in array of unsigned long.
 */
/*
 * struct snmp_mib{}
 *  - list of entries for particular API (such as /proc/net/snmp)
 *  - name of entries.
 */
struct Model0_snmp_mib {
 const char *Model0_name;
 int Model0_entry;
};
/*
 * We use unsigned longs for most mibs but u64 for ipstats.
 */




/*
 * To properly implement 64bits network statistics on 32bit and 64bit hosts,
 * we provide a synchronization point, that is a noop on 64bit or UP kernels.
 *
 * Key points :
 * 1) Use a seqcount on SMP 32bits, with low overhead.
 * 2) Whole thing is a noop on 64bit arches or UP kernels.
 * 3) Write side must ensure mutual exclusion or one seqcount update could
 *    be lost, thus blocking readers forever.
 *    If this synchronization point is not a mutex, but a spinlock or
 *    spinlock_bh() or disable_bh() :
 * 3.1) Write side should not sleep.
 * 3.2) Write side should not allow preemption.
 * 3.3) If applicable, interrupts should be disabled.
 *
 * 4) If reader fetches several counters, there is no guarantee the whole values
 *    are consistent (remember point 1) : this is a noop on 64bit arches anyway)
 *
 * 5) readers are allowed to sleep or be preempted/interrupted : They perform
 *    pure reads. But if they have to fetch many values, it's better to not allow
 *    preemptions/interruptions to avoid many retries.
 *
 * 6) If counter might be written by an interrupt, readers should block interrupts.
 *    (On UP, there is no seqcount_t protection, a reader allowing interrupts could
 *     read partial values)
 *
 * 7) For irq and softirq uses, readers can use u64_stats_fetch_begin_irq() and
 *    u64_stats_fetch_retry_irq() helpers
 *
 * Usage :
 *
 * Stats producer (writer) should use following template granted it already got
 * an exclusive access to counters (a lock is already taken, or per cpu
 * data is used [in a non preemptable context])
 *
 *   spin_lock_bh(...) or other synchronization to get exclusive access
 *   ...
 *   u64_stats_update_begin(&stats->syncp);
 *   stats->bytes64 += len; // non atomic operation
 *   stats->packets64++;    // non atomic operation
 *   u64_stats_update_end(&stats->syncp);
 *
 * While a consumer (reader) should use following template to get consistent
 * snapshot for each variable (but no guarantee on several ones)
 *
 * u64 tbytes, tpackets;
 * unsigned int start;
 *
 * do {
 *         start = u64_stats_fetch_begin(&stats->syncp);
 *         tbytes = stats->bytes64; // non atomic operation
 *         tpackets = stats->packets64; // non atomic operation
 * } while (u64_stats_fetch_retry(&stats->syncp, start));
 *
 *
 * Example of use in drivers/net/loopback.c, using per_cpu containers,
 * in BH disabled context.
 */


struct Model0_u64_stats_sync {



};


static inline __attribute__((no_instrument_function)) void Model0_u64_stats_init(struct Model0_u64_stats_sync *Model0_syncp)
{



}

static inline __attribute__((no_instrument_function)) void Model0_u64_stats_update_begin(struct Model0_u64_stats_sync *Model0_syncp)
{



}

static inline __attribute__((no_instrument_function)) void Model0_u64_stats_update_end(struct Model0_u64_stats_sync *Model0_syncp)
{



}

static inline __attribute__((no_instrument_function)) void Model0_u64_stats_update_begin_raw(struct Model0_u64_stats_sync *Model0_syncp)
{



}

static inline __attribute__((no_instrument_function)) void Model0_u64_stats_update_end_raw(struct Model0_u64_stats_sync *Model0_syncp)
{



}

static inline __attribute__((no_instrument_function)) unsigned int Model0_u64_stats_fetch_begin(const struct Model0_u64_stats_sync *Model0_syncp)
{






 return 0;

}

static inline __attribute__((no_instrument_function)) bool Model0_u64_stats_fetch_retry(const struct Model0_u64_stats_sync *Model0_syncp,
      unsigned int Model0_start)
{






 return false;

}

/*
 * In case irq handlers can update u64 counters, readers can use following helpers
 * - SMP 32bit arches use seqcount protection, irq safe.
 * - UP 32bit must disable irqs.
 * - 64bit have no problem atomically reading u64 values, irq safe.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_u64_stats_fetch_begin_irq(const struct Model0_u64_stats_sync *Model0_syncp)
{






 return 0;

}

static inline __attribute__((no_instrument_function)) bool Model0_u64_stats_fetch_retry_irq(const struct Model0_u64_stats_sync *Model0_syncp,
      unsigned int Model0_start)
{






 return false;

}

/* IPstats */

struct Model0_ipstats_mib {
 /* mibs[] must be first field of struct ipstats_mib */
 Model0_u64 Model0_mibs[Model0___IPSTATS_MIB_MAX];
 struct Model0_u64_stats_sync Model0_syncp;
};

/* ICMP */

struct Model0_icmp_mib {
 unsigned long Model0_mibs[Model0___ICMP_MIB_MAX];
};


struct Model0_icmpmsg_mib {
 Model0_atomic_long_t Model0_mibs[512];
};

/* ICMP6 (IPv6-ICMP) */

/* per network ns counters */
struct Model0_icmpv6_mib {
 unsigned long Model0_mibs[Model0___ICMP6_MIB_MAX];
};
/* per device counters, (shared on all cpus) */
struct Model0_icmpv6_mib_device {
 Model0_atomic_long_t Model0_mibs[Model0___ICMP6_MIB_MAX];
};


/* per network ns counters */
struct Model0_icmpv6msg_mib {
 Model0_atomic_long_t Model0_mibs[512];
};
/* per device counters, (shared on all cpus) */
struct Model0_icmpv6msg_mib_device {
 Model0_atomic_long_t Model0_mibs[512];
};


/* TCP */

struct Model0_tcp_mib {
 unsigned long Model0_mibs[Model0___TCP_MIB_MAX];
};

/* UDP */

struct Model0_udp_mib {
 unsigned long Model0_mibs[Model0___UDP_MIB_MAX];
};

/* Linux */

struct Model0_linux_mib {
 unsigned long Model0_mibs[Model0___LINUX_MIB_MAX];
};

/* Linux Xfrm */

struct Model0_linux_xfrm_mib {
 unsigned long Model0_mibs[Model0___LINUX_MIB_XFRMMAX];
};

struct Model0_netns_mib {
 __typeof__(struct Model0_tcp_mib) *Model0_tcp_statistics;
 __typeof__(struct Model0_ipstats_mib) *Model0_ip_statistics;
 __typeof__(struct Model0_linux_mib) *Model0_net_statistics;
 __typeof__(struct Model0_udp_mib) *Model0_udp_statistics;
 __typeof__(struct Model0_udp_mib) *Model0_udplite_statistics;
 __typeof__(struct Model0_icmp_mib) *Model0_icmp_statistics;
 __typeof__(struct Model0_icmpmsg_mib) *Model0_icmpmsg_statistics;


 struct Model0_proc_dir_entry *Model0_proc_net_devsnmp6;
 __typeof__(struct Model0_udp_mib) *Model0_udp_stats_in6;
 __typeof__(struct Model0_udp_mib) *Model0_udplite_stats_in6;
 __typeof__(struct Model0_ipstats_mib) *Model0_ipv6_statistics;
 __typeof__(struct Model0_icmpv6_mib) *Model0_icmpv6_statistics;
 __typeof__(struct Model0_icmpv6msg_mib) *Model0_icmpv6msg_statistics;




};
/*
 * Unix network namespace
 */



struct Model0_ctl_table_header;
struct Model0_netns_unix {
 int Model0_sysctl_max_dgram_qlen;
 struct Model0_ctl_table_header *Model0_ctl;
};
/*
 * Packet network namespace
 */






struct Model0_netns_packet {
 struct Model0_mutex Model0_sklist_lock;
 struct Model0_hlist_head Model0_sklist;
};
/*
 * ipv4 in net namespaces
 */











struct Model0_netns_frags {
 /* The percpu_counter "mem" need to be cacheline aligned.
	 *  mem.count must not share cacheline with other writers
	 */
 struct Model0_percpu_counter Model0_mem __attribute__((__aligned__((1 << (6)))));

 /* sysctls */
 int Model0_timeout;
 int Model0_high_thresh;
 int Model0_low_thresh;
 int Model0_max_dist;
};

/**
 * fragment queue flags
 *
 * @INET_FRAG_FIRST_IN: first fragment has arrived
 * @INET_FRAG_LAST_IN: final fragment has arrived
 * @INET_FRAG_COMPLETE: frag queue has been processed and is due for destruction
 */
enum {
 Model0_INET_FRAG_FIRST_IN = (1UL << (0)),
 Model0_INET_FRAG_LAST_IN = (1UL << (1)),
 Model0_INET_FRAG_COMPLETE = (1UL << (2)),
};

/**
 * struct inet_frag_queue - fragment queue
 *
 * @lock: spinlock protecting the queue
 * @timer: queue expiration timer
 * @list: hash bucket list
 * @refcnt: reference count of the queue
 * @fragments: received fragments head
 * @fragments_tail: received fragments tail
 * @stamp: timestamp of the last received fragment
 * @len: total length of the original datagram
 * @meat: length of received fragments so far
 * @flags: fragment queue flags
 * @max_size: maximum received fragment size
 * @net: namespace that this frag belongs to
 * @list_evictor: list of queues to forcefully evict (e.g. due to low memory)
 */
struct Model0_inet_frag_queue {
 Model0_spinlock_t Model0_lock;
 struct Model0_timer_list Model0_timer;
 struct Model0_hlist_node Model0_list;
 Model0_atomic_t Model0_refcnt;
 struct Model0_sk_buff *Model0_fragments;
 struct Model0_sk_buff *Model0_fragments_tail;
 Model0_ktime_t Model0_stamp;
 int Model0_len;
 int Model0_meat;
 __u8 Model0_flags;
 Model0_u16 Model0_max_size;
 struct Model0_netns_frags *Model0_net;
 struct Model0_hlist_node Model0_list_evictor;
};



/* averaged:
 * max_depth = default ipfrag_high_thresh / INETFRAGS_HASHSZ /
 *	       rounded up (SKB_TRUELEN(0) + sizeof(struct ipq or
 *	       struct frag_queue))
 */


struct Model0_inet_frag_bucket {
 struct Model0_hlist_head Model0_chain;
 Model0_spinlock_t Model0_chain_lock;
};

struct Model0_inet_frags {
 struct Model0_inet_frag_bucket Model0_hash[1024];

 struct Model0_work_struct Model0_frags_work;
 unsigned int Model0_next_bucket;
 unsigned long Model0_last_rebuild_jiffies;
 bool Model0_rebuild;

 /* The first call to hashfn is responsible to initialize
	 * rnd. This is best done with net_get_random_once.
	 *
	 * rnd_seqlock is used to let hash insertion detect
	 * when it needs to re-lookup the hash chain to use.
	 */
 Model0_u32 Model0_rnd;
 Model0_seqlock_t Model0_rnd_seqlock;
 int Model0_qsize;

 unsigned int (*Model0_hashfn)(const struct Model0_inet_frag_queue *);
 bool (*Model0_match)(const struct Model0_inet_frag_queue *Model0_q,
      const void *Model0_arg);
 void (*Model0_constructor)(struct Model0_inet_frag_queue *Model0_q,
            const void *Model0_arg);
 void (*Model0_destructor)(struct Model0_inet_frag_queue *);
 void (*Model0_frag_expire)(unsigned long Model0_data);
 struct Model0_kmem_cache *Model0_frags_cachep;
 const char *Model0_frags_cache_name;
};

int Model0_inet_frags_init(struct Model0_inet_frags *);
void Model0_inet_frags_fini(struct Model0_inet_frags *);

static inline __attribute__((no_instrument_function)) int Model0_inet_frags_init_net(struct Model0_netns_frags *Model0_nf)
{
 return ({ static struct Model0_lock_class_key Model0___key; Model0___percpu_counter_init(&Model0_nf->Model0_mem, 0, ((( Model0_gfp_t)(0x400000u|0x2000000u)) | (( Model0_gfp_t)0x40u) | (( Model0_gfp_t)0x80u)), &Model0___key); });
}
static inline __attribute__((no_instrument_function)) void Model0_inet_frags_uninit_net(struct Model0_netns_frags *Model0_nf)
{
 Model0_percpu_counter_destroy(&Model0_nf->Model0_mem);
}

void Model0_inet_frags_exit_net(struct Model0_netns_frags *Model0_nf, struct Model0_inet_frags *Model0_f);

void Model0_inet_frag_kill(struct Model0_inet_frag_queue *Model0_q, struct Model0_inet_frags *Model0_f);
void Model0_inet_frag_destroy(struct Model0_inet_frag_queue *Model0_q, struct Model0_inet_frags *Model0_f);
struct Model0_inet_frag_queue *Model0_inet_frag_find(struct Model0_netns_frags *Model0_nf,
  struct Model0_inet_frags *Model0_f, void *Model0_key, unsigned int Model0_hash);

void Model0_inet_frag_maybe_warn_overflow(struct Model0_inet_frag_queue *Model0_q,
       const char *Model0_prefix);

static inline __attribute__((no_instrument_function)) void Model0_inet_frag_put(struct Model0_inet_frag_queue *Model0_q, struct Model0_inet_frags *Model0_f)
{
 if (Model0_atomic_dec_and_test(&Model0_q->Model0_refcnt))
  Model0_inet_frag_destroy(Model0_q, Model0_f);
}

static inline __attribute__((no_instrument_function)) bool Model0_inet_frag_evicting(struct Model0_inet_frag_queue *Model0_q)
{
 return !Model0_hlist_unhashed(&Model0_q->Model0_list_evictor);
}

/* Memory Tracking Functions. */

/* The default percpu_counter batch size is not big enough to scale to
 * fragmentation mem acct sizes.
 * The mem size of a 64K fragment is approx:
 *  (44 fragments * 2944 truesize) + frag_queue struct(200) = 129736 bytes
 */
static unsigned int Model0_frag_percpu_counter_batch = 130000;

static inline __attribute__((no_instrument_function)) int Model0_frag_mem_limit(struct Model0_netns_frags *Model0_nf)
{
 return Model0_percpu_counter_read(&Model0_nf->Model0_mem);
}

static inline __attribute__((no_instrument_function)) void Model0_sub_frag_mem_limit(struct Model0_netns_frags *Model0_nf, int Model0_i)
{
 Model0___percpu_counter_add(&Model0_nf->Model0_mem, -Model0_i, Model0_frag_percpu_counter_batch);
}

static inline __attribute__((no_instrument_function)) void Model0_add_frag_mem_limit(struct Model0_netns_frags *Model0_nf, int Model0_i)
{
 Model0___percpu_counter_add(&Model0_nf->Model0_mem, Model0_i, Model0_frag_percpu_counter_batch);
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_sum_frag_mem_limit(struct Model0_netns_frags *Model0_nf)
{
 unsigned int Model0_res;

 Model0_local_bh_disable();
 Model0_res = Model0_percpu_counter_sum_positive(&Model0_nf->Model0_mem);
 Model0_local_bh_enable();

 return Model0_res;
}

/* RFC 3168 support :
 * We want to check ECN values of all fragments, do detect invalid combinations.
 * In ipq->ecn, we store the OR value of each ip4_frag_ecn() fragment value.
 */





extern const Model0_u8 Model0_ip_frag_ecn_table[16];


struct Model0_tcpm_hash_bucket;
struct Model0_ctl_table_header;
struct Model0_ipv4_devconf;
struct Model0_fib_rules_ops;
struct Model0_hlist_head;
struct Model0_fib_table;
struct Model0_sock;
struct Model0_local_ports {
 Model0_seqlock_t Model0_lock;
 int Model0_range[2];
 bool Model0_warned;
};

struct Model0_ping_group_range {
 Model0_seqlock_t Model0_lock;
 Model0_kgid_t Model0_range[2];
};

struct Model0_netns_ipv4 {

 struct Model0_ctl_table_header *Model0_forw_hdr;
 struct Model0_ctl_table_header *Model0_frags_hdr;
 struct Model0_ctl_table_header *Model0_ipv4_hdr;
 struct Model0_ctl_table_header *Model0_route_hdr;
 struct Model0_ctl_table_header *Model0_xfrm4_hdr;

 struct Model0_ipv4_devconf *Model0_devconf_all;
 struct Model0_ipv4_devconf *Model0_devconf_dflt;

 struct Model0_fib_rules_ops *Model0_rules_ops;
 bool Model0_fib_has_custom_rules;
 struct Model0_fib_table *Model0_fib_local;
 struct Model0_fib_table *Model0_fib_main;
 struct Model0_fib_table *Model0_fib_default;




 struct Model0_hlist_head *Model0_fib_table_hash;
 bool Model0_fib_offload_disabled;
 struct Model0_sock *Model0_fibnl;

 struct Model0_sock * *Model0_icmp_sk;
 struct Model0_sock *Model0_mc_autojoin_sk;

 struct Model0_inet_peer_base *Model0_peers;
 struct Model0_sock * *Model0_tcp_sk;
 struct Model0_netns_frags Model0_frags;

 struct Model0_xt_table *Model0_iptable_filter;
 struct Model0_xt_table *Model0_iptable_mangle;
 struct Model0_xt_table *Model0_iptable_raw;
 struct Model0_xt_table *Model0_arptable_filter;

 struct Model0_xt_table *Model0_iptable_security;

 struct Model0_xt_table *Model0_nat_table;


 int Model0_sysctl_icmp_echo_ignore_all;
 int Model0_sysctl_icmp_echo_ignore_broadcasts;
 int Model0_sysctl_icmp_ignore_bogus_error_responses;
 int Model0_sysctl_icmp_ratelimit;
 int Model0_sysctl_icmp_ratemask;
 int Model0_sysctl_icmp_errors_use_inbound_ifaddr;

 struct Model0_local_ports Model0_ip_local_ports;

 int Model0_sysctl_tcp_ecn;
 int Model0_sysctl_tcp_ecn_fallback;

 int Model0_sysctl_ip_default_ttl;
 int Model0_sysctl_ip_no_pmtu_disc;
 int Model0_sysctl_ip_fwd_use_pmtu;
 int Model0_sysctl_ip_nonlocal_bind;
 /* Shall we try to damage output packets if routing dev changes? */
 int Model0_sysctl_ip_dynaddr;
 int Model0_sysctl_ip_early_demux;

 int Model0_sysctl_fwmark_reflect;
 int Model0_sysctl_tcp_fwmark_accept;



 int Model0_sysctl_tcp_mtu_probing;
 int Model0_sysctl_tcp_base_mss;
 int Model0_sysctl_tcp_probe_threshold;
 Model0_u32 Model0_sysctl_tcp_probe_interval;

 int Model0_sysctl_tcp_keepalive_time;
 int Model0_sysctl_tcp_keepalive_probes;
 int Model0_sysctl_tcp_keepalive_intvl;

 int Model0_sysctl_tcp_syn_retries;
 int Model0_sysctl_tcp_synack_retries;
 int Model0_sysctl_tcp_syncookies;
 int Model0_sysctl_tcp_reordering;
 int Model0_sysctl_tcp_retries1;
 int Model0_sysctl_tcp_retries2;
 int Model0_sysctl_tcp_orphan_retries;
 int Model0_sysctl_tcp_fin_timeout;
 unsigned int Model0_sysctl_tcp_notsent_lowat;

 int Model0_sysctl_igmp_max_memberships;
 int Model0_sysctl_igmp_max_msf;
 int Model0_sysctl_igmp_llm_reports;
 int Model0_sysctl_igmp_qrv;

 struct Model0_ping_group_range Model0_ping_group_range;

 Model0_atomic_t Model0_dev_addr_genid;


 unsigned long *Model0_sysctl_local_reserved_ports;




 struct Model0_mr_table *Model0_mrt;






 int Model0_sysctl_fib_multipath_use_neigh;

 Model0_atomic_t Model0_rt_genid;
};
/*
 * ipv6 in net namespaces
 */







struct Model0_ctl_table_header;

struct Model0_netns_sysctl_ipv6 {

 struct Model0_ctl_table_header *Model0_hdr;
 struct Model0_ctl_table_header *Model0_route_hdr;
 struct Model0_ctl_table_header *Model0_icmp_hdr;
 struct Model0_ctl_table_header *Model0_frags_hdr;
 struct Model0_ctl_table_header *Model0_xfrm6_hdr;

 int Model0_bindv6only;
 int Model0_flush_delay;
 int Model0_ip6_rt_max_size;
 int Model0_ip6_rt_gc_min_interval;
 int Model0_ip6_rt_gc_timeout;
 int Model0_ip6_rt_gc_interval;
 int Model0_ip6_rt_gc_elasticity;
 int Model0_ip6_rt_mtu_expires;
 int Model0_ip6_rt_min_advmss;
 int Model0_flowlabel_consistency;
 int Model0_auto_flowlabels;
 int Model0_icmpv6_time;
 int Model0_anycast_src_echo_reply;
 int Model0_ip_nonlocal_bind;
 int Model0_fwmark_reflect;
 int Model0_idgen_retries;
 int Model0_idgen_delay;
 int Model0_flowlabel_state_ranges;
};

struct Model0_netns_ipv6 {
 struct Model0_netns_sysctl_ipv6 Model0_sysctl;
 struct Model0_ipv6_devconf *Model0_devconf_all;
 struct Model0_ipv6_devconf *Model0_devconf_dflt;
 struct Model0_inet_peer_base *Model0_peers;
 struct Model0_netns_frags Model0_frags;

 struct Model0_xt_table *Model0_ip6table_filter;
 struct Model0_xt_table *Model0_ip6table_mangle;
 struct Model0_xt_table *Model0_ip6table_raw;

 struct Model0_xt_table *Model0_ip6table_security;

 struct Model0_xt_table *Model0_ip6table_nat;

 struct Model0_rt6_info *Model0_ip6_null_entry;
 struct Model0_rt6_statistics *Model0_rt6_stats;
 struct Model0_timer_list Model0_ip6_fib_timer;
 struct Model0_hlist_head *Model0_fib_table_hash;
 struct Model0_fib6_table *Model0_fib6_main_tbl;
 struct Model0_list_head Model0_fib6_walkers;
 struct Model0_dst_ops Model0_ip6_dst_ops;
 Model0_rwlock_t Model0_fib6_walker_lock;
 Model0_spinlock_t Model0_fib6_gc_lock;
 unsigned int Model0_ip6_rt_gc_expire;
 unsigned long Model0_ip6_rt_last_gc;






 struct Model0_sock **Model0_icmp_sk;
 struct Model0_sock *Model0_ndisc_sk;
 struct Model0_sock *Model0_tcp_sk;
 struct Model0_sock *Model0_igmp_sk;
 struct Model0_sock *Model0_mc_autojoin_sk;
 Model0_atomic_t Model0_dev_addr_genid;
 Model0_atomic_t Model0_fib6_sernum;
};


struct Model0_netns_nf_frag {
 struct Model0_netns_sysctl_ipv6 Model0_sysctl;
 struct Model0_netns_frags Model0_frags;
};
/*
 * ieee802154 6lowpan in net namespaces
 */






struct Model0_netns_sysctl_lowpan {

 struct Model0_ctl_table_header *Model0_frags_hdr;

};

struct Model0_netns_ieee802154_lowpan {
 struct Model0_netns_sysctl_lowpan Model0_sysctl;
 struct Model0_netns_frags Model0_frags;
};



struct Model0_sock;
struct Model0_proc_dir_entry;
struct Model0_sctp_mib;
struct Model0_ctl_table_header;

struct Model0_netns_sctp {
 __typeof__(struct Model0_sctp_mib) *Model0_sctp_statistics;


 struct Model0_proc_dir_entry *Model0_proc_net_sctp;


 struct Model0_ctl_table_header *Model0_sysctl_header;

 /* This is the global socket data structure used for responding to
	 * the Out-of-the-blue (OOTB) packets.  A control sock will be created
	 * for this socket at the initialization time.
	 */
 struct Model0_sock *Model0_ctl_sock;

 /* This is the global local address list.
	 * We actively maintain this complete list of addresses on
	 * the system by catching address add/delete events.
	 *
	 * It is a list of sctp_sockaddr_entry.
	 */
 struct Model0_list_head Model0_local_addr_list;
 struct Model0_list_head Model0_addr_waitq;
 struct Model0_timer_list Model0_addr_wq_timer;
 struct Model0_list_head Model0_auto_asconf_splist;
 /* Lock that protects both addr_waitq and auto_asconf_splist */
 Model0_spinlock_t Model0_addr_wq_lock;

 /* Lock that protects the local_addr_list writers */
 Model0_spinlock_t Model0_local_addr_lock;

 /* RFC2960 Section 14. Suggested SCTP Protocol Parameter Values
	 *
	 * The following protocol parameters are RECOMMENDED:
	 *
	 * RTO.Initial		    - 3	 seconds
	 * RTO.Min		    - 1	 second
	 * RTO.Max		   -  60 seconds
	 * RTO.Alpha		    - 1/8  (3 when converted to right shifts.)
	 * RTO.Beta		    - 1/4  (2 when converted to right shifts.)
	 */
 unsigned int Model0_rto_initial;
 unsigned int Model0_rto_min;
 unsigned int Model0_rto_max;

 /* Note: rto_alpha and rto_beta are really defined as inverse
	 * powers of two to facilitate integer operations.
	 */
 int Model0_rto_alpha;
 int Model0_rto_beta;

 /* Max.Burst		    - 4 */
 int Model0_max_burst;

 /* Whether Cookie Preservative is enabled(1) or not(0) */
 int Model0_cookie_preserve_enable;

 /* The namespace default hmac alg */
 char *Model0_sctp_hmac_alg;

 /* Valid.Cookie.Life	    - 60  seconds  */
 unsigned int Model0_valid_cookie_life;

 /* Delayed SACK timeout  200ms default*/
 unsigned int Model0_sack_timeout;

 /* HB.interval		    - 30 seconds  */
 unsigned int Model0_hb_interval;

 /* Association.Max.Retrans  - 10 attempts
	 * Path.Max.Retrans	    - 5	 attempts (per destination address)
	 * Max.Init.Retransmits	    - 8	 attempts
	 */
 int Model0_max_retrans_association;
 int Model0_max_retrans_path;
 int Model0_max_retrans_init;
 /* Potentially-Failed.Max.Retrans sysctl value
	 * taken from:
	 * http://tools.ietf.org/html/draft-nishida-tsvwg-sctp-failover-05
	 */
 int Model0_pf_retrans;

 /*
	 * Disable Potentially-Failed feature, the feature is enabled by default
	 * pf_enable	-  0  : disable pf
	 *		- >0  : enable pf
	 */
 int Model0_pf_enable;

 /*
	 * Policy for preforming sctp/socket accounting
	 * 0   - do socket level accounting, all assocs share sk_sndbuf
	 * 1   - do sctp accounting, each asoc may use sk_sndbuf bytes
	 */
 int Model0_sndbuf_policy;

 /*
	 * Policy for preforming sctp/socket accounting
	 * 0   - do socket level accounting, all assocs share sk_rcvbuf
	 * 1   - do sctp accounting, each asoc may use sk_rcvbuf bytes
	 */
 int Model0_rcvbuf_policy;

 int Model0_default_auto_asconf;

 /* Flag to indicate if addip is enabled. */
 int Model0_addip_enable;
 int Model0_addip_noauth;

 /* Flag to indicate if PR-SCTP is enabled. */
 int Model0_prsctp_enable;

 /* Flag to idicate if SCTP-AUTH is enabled */
 int Model0_auth_enable;

 /*
	 * Policy to control SCTP IPv4 address scoping
	 * 0   - Disable IPv4 address scoping
	 * 1   - Enable IPv4 address scoping
	 * 2   - Selectively allow only IPv4 private addresses
	 * 3   - Selectively allow only IPv4 link local address
	 */
 int Model0_scope_policy;

 /* Threshold for rwnd update SACKS.  Receive buffer shifted this many
	 * bits is an indicator of when to send and window update SACK.
	 */
 int Model0_rwnd_upd_shift;

 /* Threshold for autoclose timeout, in seconds. */
 unsigned long Model0_max_autoclose;
};



struct Model0_sock;

struct Model0_netns_dccp {
 struct Model0_sock *Model0_v4_ctl_sk;
 struct Model0_sock *Model0_v6_ctl_sk;
};












/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions of the Internet Protocol.
 *
 * Version:	@(#)in.h	1.0.1	04/21/93
 *
 * Authors:	Original taken from the GNU Project <netinet/in.h> file.
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions of the Internet Protocol.
 *
 * Version:	@(#)in.h	1.0.1	04/21/93
 *
 * Authors:	Original taken from the GNU Project <netinet/in.h> file.
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
/* Standard well-defined IP protocols.  */
enum {
  Model0_IPPROTO_IP = 0, /* Dummy protocol for TCP		*/

  Model0_IPPROTO_ICMP = 1, /* Internet Control Message Protocol	*/

  Model0_IPPROTO_IGMP = 2, /* Internet Group Management Protocol	*/

  Model0_IPPROTO_IPIP = 4, /* IPIP tunnels (older KA9Q tunnels use 94) */

  Model0_IPPROTO_TCP = 6, /* Transmission Control Protocol	*/

  Model0_IPPROTO_EGP = 8, /* Exterior Gateway Protocol		*/

  Model0_IPPROTO_PUP = 12, /* PUP protocol				*/

  Model0_IPPROTO_UDP = 17, /* User Datagram Protocol		*/

  Model0_IPPROTO_IDP = 22, /* XNS IDP protocol			*/

  Model0_IPPROTO_TP = 29, /* SO Transport Protocol Class 4	*/

  Model0_IPPROTO_DCCP = 33, /* Datagram Congestion Control Protocol */

  Model0_IPPROTO_IPV6 = 41, /* IPv6-in-IPv4 tunnelling		*/

  Model0_IPPROTO_RSVP = 46, /* RSVP Protocol			*/

  Model0_IPPROTO_GRE = 47, /* Cisco GRE tunnels (rfc 1701,1702)	*/

  Model0_IPPROTO_ESP = 50, /* Encapsulation Security Payload protocol */

  Model0_IPPROTO_AH = 51, /* Authentication Header protocol	*/

  Model0_IPPROTO_MTP = 92, /* Multicast Transport Protocol		*/

  Model0_IPPROTO_BEETPH = 94, /* IP option pseudo header for BEET	*/

  Model0_IPPROTO_ENCAP = 98, /* Encapsulation Header			*/

  Model0_IPPROTO_PIM = 103, /* Protocol Independent Multicast	*/

  Model0_IPPROTO_COMP = 108, /* Compression Header Protocol		*/

  Model0_IPPROTO_SCTP = 132, /* Stream Control Transport Protocol	*/

  Model0_IPPROTO_UDPLITE = 136, /* UDP-Lite (RFC 3828)			*/

  Model0_IPPROTO_MPLS = 137, /* MPLS in IP (RFC 4023)		*/

  Model0_IPPROTO_RAW = 255, /* Raw IP packets			*/

  Model0_IPPROTO_MAX
};



/* Internet address. */
struct Model0_in_addr {
 Model0___be32 Model0_s_addr;
};
/* BSD compatibility */


/* TProxy original addresses */
/* IP_MTU_DISCOVER values */




/* Always use interface mtu (ignores dst pmtu) but don't set DF flag.
 * Also incoming ICMP frag_needed notifications will be ignored on
 * this socket to prevent accepting spoofed ones.
 */

/* weaker version of IP_PMTUDISC_INTERFACE, which allos packets to get
 * fragmented if they exeed the interface mtu
 */
/* These need to appear somewhere around here */



/* Request struct for multicast socket ops */


struct Model0_ip_mreq {
 struct Model0_in_addr Model0_imr_multiaddr; /* IP multicast address of group */
 struct Model0_in_addr Model0_imr_interface; /* local IP address of interface */
};

struct Model0_ip_mreqn {
 struct Model0_in_addr Model0_imr_multiaddr; /* IP multicast address of group */
 struct Model0_in_addr Model0_imr_address; /* local IP address of interface */
 int Model0_imr_ifindex; /* Interface index */
};

struct Model0_ip_mreq_source {
 Model0___be32 Model0_imr_multiaddr;
 Model0___be32 Model0_imr_interface;
 Model0___be32 Model0_imr_sourceaddr;
};

struct Model0_ip_msfilter {
 Model0___be32 Model0_imsf_multiaddr;
 Model0___be32 Model0_imsf_interface;
 __u32 Model0_imsf_fmode;
 __u32 Model0_imsf_numsrc;
 Model0___be32 Model0_imsf_slist[1];
};





struct Model0_group_req {
 __u32 Model0_gr_interface; /* interface index */
 struct Model0___kernel_sockaddr_storage Model0_gr_group; /* group address */
};

struct Model0_group_source_req {
 __u32 Model0_gsr_interface; /* interface index */
 struct Model0___kernel_sockaddr_storage Model0_gsr_group; /* group address */
 struct Model0___kernel_sockaddr_storage Model0_gsr_source; /* source address */
};

struct Model0_group_filter {
 __u32 Model0_gf_interface; /* interface index */
 struct Model0___kernel_sockaddr_storage Model0_gf_group; /* multicast address */
 __u32 Model0_gf_fmode; /* filter mode */
 __u32 Model0_gf_numsrc; /* number of sources */
 struct Model0___kernel_sockaddr_storage Model0_gf_slist[1]; /* interface index */
};







struct Model0_in_pktinfo {
 int Model0_ipi_ifindex;
 struct Model0_in_addr Model0_ipi_spec_dst;
 struct Model0_in_addr Model0_ipi_addr;
};


/* Structure describing an Internet (IP) socket address. */


struct Model0_sockaddr_in {
  Model0___kernel_sa_family_t Model0_sin_family; /* Address family		*/
  Model0___be16 Model0_sin_port; /* Port number			*/
  struct Model0_in_addr Model0_sin_addr; /* Internet address		*/

  /* Pad to size of `struct sockaddr'. */
  unsigned char Model0___pad[16 - sizeof(short int) -
   sizeof(unsigned short int) - sizeof(struct Model0_in_addr)];
};




/*
 * Definitions of the bits in an Internet address integer.
 * On subnets, host and network parts are found according
 * to the subnet mask, not these masks.
 */
/* Address to accept any incoming messages. */


/* Address to send to all hosts. */


/* Address indicating an error return. */


/* Network number for local host loopback. */


/* Address to loopback in software to local host.  */



/* Defines for Multicast INADDR */






/* <asm/byteorder.h> contains the htonl type stuff.. */

static inline __attribute__((no_instrument_function)) int Model0_proto_ports_offset(int Model0_proto)
{
 switch (Model0_proto) {
 case Model0_IPPROTO_TCP:
 case Model0_IPPROTO_UDP:
 case Model0_IPPROTO_DCCP:
 case Model0_IPPROTO_ESP: /* SPI */
 case Model0_IPPROTO_SCTP:
 case Model0_IPPROTO_UDPLITE:
  return 0;
 case Model0_IPPROTO_AH: /* SPI */
  return 4;
 default:
  return -22;
 }
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_loopback(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xff000000))) ? ((__u32)( (((__u32)((0xff000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xff000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xff000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xff000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xff000000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0x7f000000))) ? ((__u32)( (((__u32)((0x7f000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x7f000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x7f000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x7f000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x7f000000))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_multicast(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xf0000000))) ? ((__u32)( (((__u32)((0xf0000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xf0000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xf0000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xf0000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xf0000000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xe0000000))) ? ((__u32)( (((__u32)((0xe0000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xe0000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xe0000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xe0000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xe0000000))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_local_multicast(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xffffff00))) ? ((__u32)( (((__u32)((0xffffff00)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffffff00)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffffff00)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffffff00)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xffffff00))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xe0000000))) ? ((__u32)( (((__u32)((0xe0000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xe0000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xe0000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xe0000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xe0000000))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_lbcast(Model0___be32 Model0_addr)
{
 /* limited broadcast */
 return Model0_addr == (( Model0___be32)(__builtin_constant_p((__u32)((((unsigned long int) 0xffffffff)))) ? ((__u32)( (((__u32)((((unsigned long int) 0xffffffff))) & (__u32)0x000000ffUL) << 24) | (((__u32)((((unsigned long int) 0xffffffff))) & (__u32)0x0000ff00UL) << 8) | (((__u32)((((unsigned long int) 0xffffffff))) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((((unsigned long int) 0xffffffff))) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((((unsigned long int) 0xffffffff)))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_zeronet(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xff000000))) ? ((__u32)( (((__u32)((0xff000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xff000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xff000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xff000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xff000000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0x00000000))) ? ((__u32)( (((__u32)((0x00000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x00000000))));
}

/* Special-Use IPv4 Addresses (RFC3330) */

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_private_10(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xff000000))) ? ((__u32)( (((__u32)((0xff000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xff000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xff000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xff000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xff000000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0x0a000000))) ? ((__u32)( (((__u32)((0x0a000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0a000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0a000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0a000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0a000000))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_private_172(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xfff00000))) ? ((__u32)( (((__u32)((0xfff00000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xfff00000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xfff00000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xfff00000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xfff00000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xac100000))) ? ((__u32)( (((__u32)((0xac100000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xac100000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xac100000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xac100000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xac100000))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_private_192(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xffff0000))) ? ((__u32)( (((__u32)((0xffff0000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffff0000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffff0000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffff0000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xffff0000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xc0a80000))) ? ((__u32)( (((__u32)((0xc0a80000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xc0a80000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xc0a80000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xc0a80000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xc0a80000))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_linklocal_169(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xffff0000))) ? ((__u32)( (((__u32)((0xffff0000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffff0000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffff0000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffff0000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xffff0000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xa9fe0000))) ? ((__u32)( (((__u32)((0xa9fe0000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xa9fe0000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xa9fe0000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xa9fe0000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xa9fe0000))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_anycast_6to4(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xffffff00))) ? ((__u32)( (((__u32)((0xffffff00)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffffff00)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffffff00)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffffff00)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xffffff00))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xc0586300))) ? ((__u32)( (((__u32)((0xc0586300)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xc0586300)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xc0586300)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xc0586300)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xc0586300))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_test_192(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xffffff00))) ? ((__u32)( (((__u32)((0xffffff00)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffffff00)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffffff00)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffffff00)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xffffff00))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xc0000200))) ? ((__u32)( (((__u32)((0xc0000200)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xc0000200)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xc0000200)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xc0000200)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xc0000200))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv4_is_test_198(Model0___be32 Model0_addr)
{
 return (Model0_addr & (( Model0___be32)(__builtin_constant_p((__u32)((0xfffe0000))) ? ((__u32)( (((__u32)((0xfffe0000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xfffe0000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xfffe0000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xfffe0000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xfffe0000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xc6120000))) ? ((__u32)( (((__u32)((0xc6120000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xc6120000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xc6120000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xc6120000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xc6120000))));
}


/* Responses from hook functions. */
/* we overload the higher bits for encoding auxiliary data such as the queue
 * number or errno values. Not nice, but better than additional function
 * arguments. */


/* extra verdict flags have mask 0x0000ff00 */


/* queue number (NF_QUEUE) or errno (NF_DROP) */







/* only for userspace compatibility */
enum Model0_nf_inet_hooks {
 Model0_NF_INET_PRE_ROUTING,
 Model0_NF_INET_LOCAL_IN,
 Model0_NF_INET_FORWARD,
 Model0_NF_INET_LOCAL_OUT,
 Model0_NF_INET_POST_ROUTING,
 Model0_NF_INET_NUMHOOKS
};

enum Model0_nf_dev_hooks {
 Model0_NF_NETDEV_INGRESS,
 Model0_NF_NETDEV_NUMHOOKS
};

enum {
 Model0_NFPROTO_UNSPEC = 0,
 Model0_NFPROTO_INET = 1,
 Model0_NFPROTO_IPV4 = 2,
 Model0_NFPROTO_ARP = 3,
 Model0_NFPROTO_NETDEV = 5,
 Model0_NFPROTO_BRIDGE = 7,
 Model0_NFPROTO_IPV6 = 10,
 Model0_NFPROTO_DECNET = 12,
 Model0_NFPROTO_NUMPROTO,
};

union Model0_nf_inet_addr {
 __u32 Model0_all[4];
 Model0___be32 Model0_ip;
 Model0___be32 Model0_ip6[4];
 struct Model0_in_addr Model0_in;
 struct Model0_in6_addr Model0_in6;
};

/* Largest hook number + 1, see uapi/linux/netfilter_decnet.h */

struct Model0_proc_dir_entry;
struct Model0_nf_logger;
struct Model0_nf_queue_handler;

struct Model0_netns_nf {

 struct Model0_proc_dir_entry *Model0_proc_netfilter;

 const struct Model0_nf_queue_handler *Model0_queue_handler;
 const struct Model0_nf_logger *Model0_nf_loggers[Model0_NFPROTO_NUMPROTO];

 struct Model0_ctl_table_header *Model0_nf_log_dir_header;

 struct Model0_list_head Model0_hooks[Model0_NFPROTO_NUMPROTO][8];
};






struct Model0_ebt_table;

struct Model0_netns_xt {
 struct Model0_list_head Model0_tables[Model0_NFPROTO_NUMPROTO];
 bool Model0_notrack_deprecated_warning;
 bool Model0_clusterip_deprecated_warning;






};











/*
 * Special version of lists, where end of list is not a NULL pointer,
 * but a 'nulls' marker, which can have many different values.
 * (up to 2^31 different values guaranteed on all platforms)
 *
 * In the standard hlist, termination of a list is the NULL pointer.
 * In this special 'nulls' variant, we use the fact that objects stored in
 * a list are aligned on a word (4 or 8 bytes alignment).
 * We therefore use the last significant bit of 'ptr' :
 * Set to 1 : This is a 'nulls' end-of-list marker (ptr >> 1)
 * Set to 0 : This is a pointer to some object (ptr)
 */

struct Model0_hlist_nulls_head {
 struct Model0_hlist_nulls_node *Model0_first;
};

struct Model0_hlist_nulls_node {
 struct Model0_hlist_nulls_node *Model0_next, **Model0_pprev;
};





/**
 * ptr_is_a_nulls - Test if a ptr is a nulls
 * @ptr: ptr to be tested
 *
 */
static inline __attribute__((no_instrument_function)) int Model0_is_a_nulls(const struct Model0_hlist_nulls_node *Model0_ptr)
{
 return ((unsigned long)Model0_ptr & 1);
}

/**
 * get_nulls_value - Get the 'nulls' value of the end of chain
 * @ptr: end of chain
 *
 * Should be called only if is_a_nulls(ptr);
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_get_nulls_value(const struct Model0_hlist_nulls_node *Model0_ptr)
{
 return ((unsigned long)Model0_ptr) >> 1;
}

static inline __attribute__((no_instrument_function)) int Model0_hlist_nulls_unhashed(const struct Model0_hlist_nulls_node *Model0_h)
{
 return !Model0_h->Model0_pprev;
}

static inline __attribute__((no_instrument_function)) int Model0_hlist_nulls_empty(const struct Model0_hlist_nulls_head *Model0_h)
{
 return Model0_is_a_nulls(({ union { typeof(Model0_h->Model0_first) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_h->Model0_first), Model0___u.Model0___c, sizeof(Model0_h->Model0_first)); else Model0___read_once_size_nocheck(&(Model0_h->Model0_first), Model0___u.Model0___c, sizeof(Model0_h->Model0_first)); Model0___u.Model0___val; }));
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_nulls_add_head(struct Model0_hlist_nulls_node *Model0_n,
     struct Model0_hlist_nulls_head *Model0_h)
{
 struct Model0_hlist_nulls_node *Model0_first = Model0_h->Model0_first;

 Model0_n->Model0_next = Model0_first;
 Model0_n->Model0_pprev = &Model0_h->Model0_first;
 Model0_h->Model0_first = Model0_n;
 if (!Model0_is_a_nulls(Model0_first))
  Model0_first->Model0_pprev = &Model0_n->Model0_next;
}

static inline __attribute__((no_instrument_function)) void Model0___hlist_nulls_del(struct Model0_hlist_nulls_node *Model0_n)
{
 struct Model0_hlist_nulls_node *Model0_next = Model0_n->Model0_next;
 struct Model0_hlist_nulls_node **Model0_pprev = Model0_n->Model0_pprev;

 ({ union { typeof(*Model0_pprev) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*Model0_pprev)) (Model0_next) }; Model0___write_once_size(&(*Model0_pprev), Model0___u.Model0___c, sizeof(*Model0_pprev)); Model0___u.Model0___val; });
 if (!Model0_is_a_nulls(Model0_next))
  Model0_next->Model0_pprev = Model0_pprev;
}

static inline __attribute__((no_instrument_function)) void Model0_hlist_nulls_del(struct Model0_hlist_nulls_node *Model0_n)
{
 Model0___hlist_nulls_del(Model0_n);
 Model0_n->Model0_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_nulls_for_each_entry	- iterate over list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 */






/**
 * hlist_nulls_for_each_entry_from - iterate over a hlist continuing from current point
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 *
 */







/* TCP tracking. */



/* This is exposed to userspace (ctnetlink) */
enum Model0_tcp_conntrack {
 Model0_TCP_CONNTRACK_NONE,
 Model0_TCP_CONNTRACK_SYN_SENT,
 Model0_TCP_CONNTRACK_SYN_RECV,
 Model0_TCP_CONNTRACK_ESTABLISHED,
 Model0_TCP_CONNTRACK_FIN_WAIT,
 Model0_TCP_CONNTRACK_CLOSE_WAIT,
 Model0_TCP_CONNTRACK_LAST_ACK,
 Model0_TCP_CONNTRACK_TIME_WAIT,
 Model0_TCP_CONNTRACK_CLOSE,
 Model0_TCP_CONNTRACK_LISTEN, /* obsolete */

 Model0_TCP_CONNTRACK_MAX,
 Model0_TCP_CONNTRACK_IGNORE,
 Model0_TCP_CONNTRACK_RETRANS,
 Model0_TCP_CONNTRACK_UNACK,
 Model0_TCP_CONNTRACK_TIMEOUT_MAX
};

/* Window scaling is advertised by the sender */


/* SACK is permitted by the sender */


/* This sender sent FIN first */


/* Be liberal in window checking */


/* Has unacknowledged data */


/* The field td_maxack has been set */


/* Marks possibility for expected RFC5961 challenge ACK */


struct Model0_nf_ct_tcp_flags {
 __u8 Model0_flags;
 __u8 Model0_mask;
};


struct Model0_ip_ct_tcp_state {
 Model0_u_int32_t Model0_td_end; /* max of seq + len */
 Model0_u_int32_t Model0_td_maxend; /* max of ack + max(win, 1) */
 Model0_u_int32_t Model0_td_maxwin; /* max(win) */
 Model0_u_int32_t Model0_td_maxack; /* max of ack */
 Model0_u_int8_t Model0_td_scale; /* window scale factor */
 Model0_u_int8_t Model0_flags; /* per direction options */
};

struct Model0_ip_ct_tcp {
 struct Model0_ip_ct_tcp_state Model0_seen[2]; /* connection parameters per direction */
 Model0_u_int8_t Model0_state; /* state of the connection (enum tcp_conntrack) */
 /* For detecting stale connections */
 Model0_u_int8_t Model0_last_dir; /* Direction of the last packet (enum ip_conntrack_dir) */
 Model0_u_int8_t Model0_retrans; /* Number of retransmitted packets */
 Model0_u_int8_t Model0_last_index; /* Index of the last packet */
 Model0_u_int32_t Model0_last_seq; /* Last sequence number seen in dir */
 Model0_u_int32_t Model0_last_ack; /* Last sequence number seen in opposite dir */
 Model0_u_int32_t Model0_last_end; /* Last seq + len */
 Model0_u_int16_t Model0_last_win; /* Last window advertisement seen in dir */
 /* For SYN packets while we may be out-of-sync */
 Model0_u_int8_t Model0_last_wscale; /* Last window scaling factor seen */
 Model0_u_int8_t Model0_last_flags; /* Last flags set */
};


struct Model0_ctl_table_header;
struct Model0_nf_conntrack_ecache;

struct Model0_nf_proto_net {

 struct Model0_ctl_table_header *Model0_ctl_table_header;
 struct Model0_ctl_table *Model0_ctl_table;

 struct Model0_ctl_table_header *Model0_ctl_compat_header;
 struct Model0_ctl_table *Model0_ctl_compat_table;


 unsigned int Model0_users;
};

struct Model0_nf_generic_net {
 struct Model0_nf_proto_net Model0_pn;
 unsigned int Model0_timeout;
};

struct Model0_nf_tcp_net {
 struct Model0_nf_proto_net Model0_pn;
 unsigned int Model0_timeouts[Model0_TCP_CONNTRACK_TIMEOUT_MAX];
 unsigned int Model0_tcp_loose;
 unsigned int Model0_tcp_be_liberal;
 unsigned int Model0_tcp_max_retrans;
};

enum Model0_udp_conntrack {
 Model0_UDP_CT_UNREPLIED,
 Model0_UDP_CT_REPLIED,
 Model0_UDP_CT_MAX
};

struct Model0_nf_udp_net {
 struct Model0_nf_proto_net Model0_pn;
 unsigned int Model0_timeouts[Model0_UDP_CT_MAX];
};

struct Model0_nf_icmp_net {
 struct Model0_nf_proto_net Model0_pn;
 unsigned int Model0_timeout;
};

struct Model0_nf_ip_net {
 struct Model0_nf_generic_net Model0_generic;
 struct Model0_nf_tcp_net Model0_tcp;
 struct Model0_nf_udp_net Model0_udp;
 struct Model0_nf_icmp_net Model0_icmp;
 struct Model0_nf_icmp_net Model0_icmpv6;

 struct Model0_ctl_table_header *Model0_ctl_table_header;
 struct Model0_ctl_table *Model0_ctl_table;

};

struct Model0_ct_pcpu {
 Model0_spinlock_t Model0_lock;
 struct Model0_hlist_nulls_head Model0_unconfirmed;
 struct Model0_hlist_nulls_head Model0_dying;
};

struct Model0_netns_ct {
 Model0_atomic_t Model0_count;
 unsigned int Model0_expect_count;





 struct Model0_ctl_table_header *Model0_sysctl_header;
 struct Model0_ctl_table_header *Model0_acct_sysctl_header;
 struct Model0_ctl_table_header *Model0_tstamp_sysctl_header;
 struct Model0_ctl_table_header *Model0_event_sysctl_header;
 struct Model0_ctl_table_header *Model0_helper_sysctl_header;

 unsigned int Model0_sysctl_log_invalid; /* Log invalid packets */
 int Model0_sysctl_events;
 int Model0_sysctl_acct;
 int Model0_sysctl_auto_assign_helper;
 bool Model0_auto_assign_helper_warned;
 int Model0_sysctl_tstamp;
 int Model0_sysctl_checksum;

 struct Model0_ct_pcpu *Model0_pcpu_lists;
 struct Model0_ip_conntrack_stat *Model0_stat;
 struct Model0_nf_ct_event_notifier *Model0_nf_conntrack_event_cb;
 struct Model0_nf_exp_event_notifier *Model0_nf_expect_event_cb;
 struct Model0_nf_ip_net Model0_nf_ct_proto;




};






struct Model0_nft_af_info;

struct Model0_netns_nftables {
 struct Model0_list_head Model0_af_info;
 struct Model0_list_head Model0_commit_list;
 struct Model0_nft_af_info *Model0_ipv4;
 struct Model0_nft_af_info *Model0_ipv6;
 struct Model0_nft_af_info *Model0_inet;
 struct Model0_nft_af_info *Model0_arp;
 struct Model0_nft_af_info *Model0_bridge;
 struct Model0_nft_af_info *Model0_netdev;
 unsigned int Model0_base_seq;
 Model0_u8 Model0_gencursor;
};












/* All of the structures in this file may not change size as they are
 * passed into the kernel from userspace via netlink sockets.
 */

/* Structure to encapsulate addresses. I do not want to use
 * "standard" structure. My apologies.
 */
typedef union {
 Model0___be32 Model0_a4;
 Model0___be32 Model0_a6[4];
 struct Model0_in6_addr Model0_in6;
} Model0_xfrm_address_t;

/* Ident of a specific xfrm_state. It is used on input to lookup
 * the state by (spi,daddr,ah/esp) or to store information about
 * spi, protocol and tunnel address on output.
 */
struct Model0_xfrm_id {
 Model0_xfrm_address_t Model0_daddr;
 Model0___be32 Model0_spi;
 __u8 Model0_proto;
};

struct Model0_xfrm_sec_ctx {
 __u8 Model0_ctx_doi;
 __u8 Model0_ctx_alg;
 Model0___u16 Model0_ctx_len;
 __u32 Model0_ctx_sid;
 char Model0_ctx_str[0];
};

/* Security Context Domains of Interpretation */



/* Security Context Algorithms */



/* Selector, used as selector both on policy rules (SPD) and SAs. */

struct Model0_xfrm_selector {
 Model0_xfrm_address_t Model0_daddr;
 Model0_xfrm_address_t Model0_saddr;
 Model0___be16 Model0_dport;
 Model0___be16 Model0_dport_mask;
 Model0___be16 Model0_sport;
 Model0___be16 Model0_sport_mask;
 Model0___u16 Model0_family;
 __u8 Model0_prefixlen_d;
 __u8 Model0_prefixlen_s;
 __u8 Model0_proto;
 int Model0_ifindex;
 Model0___kernel_uid32_t Model0_user;
};



struct Model0_xfrm_lifetime_cfg {
 __u64 Model0_soft_byte_limit;
 __u64 Model0_hard_byte_limit;
 __u64 Model0_soft_packet_limit;
 __u64 Model0_hard_packet_limit;
 __u64 Model0_soft_add_expires_seconds;
 __u64 Model0_hard_add_expires_seconds;
 __u64 Model0_soft_use_expires_seconds;
 __u64 Model0_hard_use_expires_seconds;
};

struct Model0_xfrm_lifetime_cur {
 __u64 Model0_bytes;
 __u64 Model0_packets;
 __u64 Model0_add_time;
 __u64 Model0_use_time;
};

struct Model0_xfrm_replay_state {
 __u32 Model0_oseq;
 __u32 Model0_seq;
 __u32 Model0_bitmap;
};



struct Model0_xfrm_replay_state_esn {
 unsigned int Model0_bmp_len;
 __u32 Model0_oseq;
 __u32 Model0_seq;
 __u32 Model0_oseq_hi;
 __u32 Model0_seq_hi;
 __u32 Model0_replay_window;
 __u32 Model0_bmp[0];
};

struct Model0_xfrm_algo {
 char Model0_alg_name[64];
 unsigned int Model0_alg_key_len; /* in bits */
 char Model0_alg_key[0];
};

struct Model0_xfrm_algo_auth {
 char Model0_alg_name[64];
 unsigned int Model0_alg_key_len; /* in bits */
 unsigned int Model0_alg_trunc_len; /* in bits */
 char Model0_alg_key[0];
};

struct Model0_xfrm_algo_aead {
 char Model0_alg_name[64];
 unsigned int Model0_alg_key_len; /* in bits */
 unsigned int Model0_alg_icv_len; /* in bits */
 char Model0_alg_key[0];
};

struct Model0_xfrm_stats {
 __u32 Model0_replay_window;
 __u32 Model0_replay;
 __u32 Model0_integrity_failed;
};

enum {
 Model0_XFRM_POLICY_TYPE_MAIN = 0,
 Model0_XFRM_POLICY_TYPE_SUB = 1,
 Model0_XFRM_POLICY_TYPE_MAX = 2,
 Model0_XFRM_POLICY_TYPE_ANY = 255
};

enum {
 Model0_XFRM_POLICY_IN = 0,
 Model0_XFRM_POLICY_OUT = 1,
 Model0_XFRM_POLICY_FWD = 2,
 Model0_XFRM_POLICY_MASK = 3,
 Model0_XFRM_POLICY_MAX = 3
};

enum {
 Model0_XFRM_SHARE_ANY, /* No limitations */
 Model0_XFRM_SHARE_SESSION, /* For this session only */
 Model0_XFRM_SHARE_USER, /* For this user only */
 Model0_XFRM_SHARE_UNIQUE /* Use once */
};
/* Netlink configuration messages.  */
enum {
 Model0_XFRM_MSG_BASE = 0x10,

 Model0_XFRM_MSG_NEWSA = 0x10,

 Model0_XFRM_MSG_DELSA,

 Model0_XFRM_MSG_GETSA,


 Model0_XFRM_MSG_NEWPOLICY,

 Model0_XFRM_MSG_DELPOLICY,

 Model0_XFRM_MSG_GETPOLICY,


 Model0_XFRM_MSG_ALLOCSPI,

 Model0_XFRM_MSG_ACQUIRE,

 Model0_XFRM_MSG_EXPIRE,


 Model0_XFRM_MSG_UPDPOLICY,

 Model0_XFRM_MSG_UPDSA,


 Model0_XFRM_MSG_POLEXPIRE,


 Model0_XFRM_MSG_FLUSHSA,

 Model0_XFRM_MSG_FLUSHPOLICY,


 Model0_XFRM_MSG_NEWAE,

 Model0_XFRM_MSG_GETAE,


 Model0_XFRM_MSG_REPORT,


 Model0_XFRM_MSG_MIGRATE,


 Model0_XFRM_MSG_NEWSADINFO,

 Model0_XFRM_MSG_GETSADINFO,


 Model0_XFRM_MSG_NEWSPDINFO,

 Model0_XFRM_MSG_GETSPDINFO,


 Model0_XFRM_MSG_MAPPING,

 Model0___XFRM_MSG_MAX
};




/*
 * Generic LSM security context for comunicating to user space
 * NOTE: Same format as sadb_x_sec_ctx
 */
struct Model0_xfrm_user_sec_ctx {
 Model0___u16 Model0_len;
 Model0___u16 Model0_exttype;
 __u8 Model0_ctx_alg; /* LSMs: e.g., selinux == 1 */
 __u8 Model0_ctx_doi;
 Model0___u16 Model0_ctx_len;
};

struct Model0_xfrm_user_tmpl {
 struct Model0_xfrm_id Model0_id;
 Model0___u16 Model0_family;
 Model0_xfrm_address_t Model0_saddr;
 __u32 Model0_reqid;
 __u8 Model0_mode;
 __u8 Model0_share;
 __u8 Model0_optional;
 __u32 Model0_aalgos;
 __u32 Model0_ealgos;
 __u32 Model0_calgos;
};

struct Model0_xfrm_encap_tmpl {
 Model0___u16 Model0_encap_type;
 Model0___be16 Model0_encap_sport;
 Model0___be16 Model0_encap_dport;
 Model0_xfrm_address_t Model0_encap_oa;
};

/* AEVENT flags  */
enum Model0_xfrm_ae_ftype_t {
 Model0_XFRM_AE_UNSPEC,
 Model0_XFRM_AE_RTHR=1, /* replay threshold*/
 Model0_XFRM_AE_RVAL=2, /* replay value */
 Model0_XFRM_AE_LVAL=4, /* lifetime value */
 Model0_XFRM_AE_ETHR=8, /* expiry timer threshold */
 Model0_XFRM_AE_CR=16, /* Event cause is replay update */
 Model0_XFRM_AE_CE=32, /* Event cause is timer expiry */
 Model0_XFRM_AE_CU=64, /* Event cause is policy update */
 Model0___XFRM_AE_MAX


};

struct Model0_xfrm_userpolicy_type {
 __u8 Model0_type;
 Model0___u16 Model0_reserved1;
 __u8 Model0_reserved2;
};

/* Netlink message attributes.  */
enum Model0_xfrm_attr_type_t {
 Model0_XFRMA_UNSPEC,
 Model0_XFRMA_ALG_AUTH, /* struct xfrm_algo */
 Model0_XFRMA_ALG_CRYPT, /* struct xfrm_algo */
 Model0_XFRMA_ALG_COMP, /* struct xfrm_algo */
 Model0_XFRMA_ENCAP, /* struct xfrm_algo + struct xfrm_encap_tmpl */
 Model0_XFRMA_TMPL, /* 1 or more struct xfrm_user_tmpl */
 Model0_XFRMA_SA, /* struct xfrm_usersa_info  */
 Model0_XFRMA_POLICY, /*struct xfrm_userpolicy_info */
 Model0_XFRMA_SEC_CTX, /* struct xfrm_sec_ctx */
 Model0_XFRMA_LTIME_VAL,
 Model0_XFRMA_REPLAY_VAL,
 Model0_XFRMA_REPLAY_THRESH,
 Model0_XFRMA_ETIMER_THRESH,
 Model0_XFRMA_SRCADDR, /* xfrm_address_t */
 Model0_XFRMA_COADDR, /* xfrm_address_t */
 Model0_XFRMA_LASTUSED, /* unsigned long  */
 Model0_XFRMA_POLICY_TYPE, /* struct xfrm_userpolicy_type */
 Model0_XFRMA_MIGRATE,
 Model0_XFRMA_ALG_AEAD, /* struct xfrm_algo_aead */
 Model0_XFRMA_KMADDRESS, /* struct xfrm_user_kmaddress */
 Model0_XFRMA_ALG_AUTH_TRUNC, /* struct xfrm_algo_auth */
 Model0_XFRMA_MARK, /* struct xfrm_mark */
 Model0_XFRMA_TFCPAD, /* __u32 */
 Model0_XFRMA_REPLAY_ESN_VAL, /* struct xfrm_replay_esn */
 Model0_XFRMA_SA_EXTRA_FLAGS, /* __u32 */
 Model0_XFRMA_PROTO, /* __u8 */
 Model0_XFRMA_ADDRESS_FILTER, /* struct xfrm_address_filter */
 Model0_XFRMA_PAD,
 Model0___XFRMA_MAX


};

struct Model0_xfrm_mark {
 __u32 Model0_v; /* value */
 __u32 Model0_m; /* mask */
};

enum Model0_xfrm_sadattr_type_t {
 Model0_XFRMA_SAD_UNSPEC,
 Model0_XFRMA_SAD_CNT,
 Model0_XFRMA_SAD_HINFO,
 Model0___XFRMA_SAD_MAX


};

struct Model0_xfrmu_sadhinfo {
 __u32 Model0_sadhcnt; /* current hash bkts */
 __u32 Model0_sadhmcnt; /* max allowed hash bkts */
};

enum Model0_xfrm_spdattr_type_t {
 Model0_XFRMA_SPD_UNSPEC,
 Model0_XFRMA_SPD_INFO,
 Model0_XFRMA_SPD_HINFO,
 Model0_XFRMA_SPD_IPV4_HTHRESH,
 Model0_XFRMA_SPD_IPV6_HTHRESH,
 Model0___XFRMA_SPD_MAX


};

struct Model0_xfrmu_spdinfo {
 __u32 Model0_incnt;
 __u32 Model0_outcnt;
 __u32 Model0_fwdcnt;
 __u32 Model0_inscnt;
 __u32 Model0_outscnt;
 __u32 Model0_fwdscnt;
};

struct Model0_xfrmu_spdhinfo {
 __u32 Model0_spdhcnt;
 __u32 Model0_spdhmcnt;
};

struct Model0_xfrmu_spdhthresh {
 __u8 Model0_lbits;
 __u8 Model0_rbits;
};

struct Model0_xfrm_usersa_info {
 struct Model0_xfrm_selector Model0_sel;
 struct Model0_xfrm_id Model0_id;
 Model0_xfrm_address_t Model0_saddr;
 struct Model0_xfrm_lifetime_cfg Model0_lft;
 struct Model0_xfrm_lifetime_cur Model0_curlft;
 struct Model0_xfrm_stats Model0_stats;
 __u32 Model0_seq;
 __u32 Model0_reqid;
 Model0___u16 Model0_family;
 __u8 Model0_mode; /* XFRM_MODE_xxx */
 __u8 Model0_replay_window;
 __u8 Model0_flags;
};



struct Model0_xfrm_usersa_id {
 Model0_xfrm_address_t Model0_daddr;
 Model0___be32 Model0_spi;
 Model0___u16 Model0_family;
 __u8 Model0_proto;
};

struct Model0_xfrm_aevent_id {
 struct Model0_xfrm_usersa_id Model0_sa_id;
 Model0_xfrm_address_t Model0_saddr;
 __u32 Model0_flags;
 __u32 Model0_reqid;
};

struct Model0_xfrm_userspi_info {
 struct Model0_xfrm_usersa_info Model0_info;
 __u32 Model0_min;
 __u32 Model0_max;
};

struct Model0_xfrm_userpolicy_info {
 struct Model0_xfrm_selector Model0_sel;
 struct Model0_xfrm_lifetime_cfg Model0_lft;
 struct Model0_xfrm_lifetime_cur Model0_curlft;
 __u32 Model0_priority;
 __u32 Model0_index;
 __u8 Model0_dir;
 __u8 Model0_action;


 __u8 Model0_flags;

 /* Automatically expand selector to include matching ICMP payloads. */

 __u8 Model0_share;
};

struct Model0_xfrm_userpolicy_id {
 struct Model0_xfrm_selector Model0_sel;
 __u32 Model0_index;
 __u8 Model0_dir;
};

struct Model0_xfrm_user_acquire {
 struct Model0_xfrm_id Model0_id;
 Model0_xfrm_address_t Model0_saddr;
 struct Model0_xfrm_selector Model0_sel;
 struct Model0_xfrm_userpolicy_info Model0_policy;
 __u32 Model0_aalgos;
 __u32 Model0_ealgos;
 __u32 Model0_calgos;
 __u32 Model0_seq;
};

struct Model0_xfrm_user_expire {
 struct Model0_xfrm_usersa_info Model0_state;
 __u8 Model0_hard;
};

struct Model0_xfrm_user_polexpire {
 struct Model0_xfrm_userpolicy_info Model0_pol;
 __u8 Model0_hard;
};

struct Model0_xfrm_usersa_flush {
 __u8 Model0_proto;
};

struct Model0_xfrm_user_report {
 __u8 Model0_proto;
 struct Model0_xfrm_selector Model0_sel;
};

/* Used by MIGRATE to pass addresses IKE should use to perform
 * SA negotiation with the peer */
struct Model0_xfrm_user_kmaddress {
 Model0_xfrm_address_t Model0_local;
 Model0_xfrm_address_t Model0_remote;
 __u32 Model0_reserved;
 Model0___u16 Model0_family;
};

struct Model0_xfrm_user_migrate {
 Model0_xfrm_address_t Model0_old_daddr;
 Model0_xfrm_address_t Model0_old_saddr;
 Model0_xfrm_address_t Model0_new_daddr;
 Model0_xfrm_address_t Model0_new_saddr;
 __u8 Model0_proto;
 __u8 Model0_mode;
 Model0___u16 Model0_reserved;
 __u32 Model0_reqid;
 Model0___u16 Model0_old_family;
 Model0___u16 Model0_new_family;
};

struct Model0_xfrm_user_mapping {
 struct Model0_xfrm_usersa_id Model0_id;
 __u32 Model0_reqid;
 Model0_xfrm_address_t Model0_old_saddr;
 Model0_xfrm_address_t Model0_new_saddr;
 Model0___be16 Model0_old_sport;
 Model0___be16 Model0_new_sport;
};

struct Model0_xfrm_address_filter {
 Model0_xfrm_address_t Model0_saddr;
 Model0_xfrm_address_t Model0_daddr;
 Model0___u16 Model0_family;
 __u8 Model0_splen;
 __u8 Model0_dplen;
};
enum Model0_xfrm_nlgroups {
 Model0_XFRMNLGRP_NONE,

 Model0_XFRMNLGRP_ACQUIRE,

 Model0_XFRMNLGRP_EXPIRE,

 Model0_XFRMNLGRP_SA,

 Model0_XFRMNLGRP_POLICY,

 Model0_XFRMNLGRP_AEVENTS,

 Model0_XFRMNLGRP_REPORT,

 Model0_XFRMNLGRP_MIGRATE,

 Model0_XFRMNLGRP_MAPPING,

 Model0___XFRMNLGRP_MAX
};




/* interrupt.h */



/**
 * enum irqreturn
 * @IRQ_NONE		interrupt was not from this device or was not handled
 * @IRQ_HANDLED		interrupt was handled by this device
 * @IRQ_WAKE_THREAD	handler requests to wake the handler thread
 */
enum Model0_irqreturn {
 Model0_IRQ_NONE = (0 << 0),
 Model0_IRQ_HANDLED = (1 << 0),
 Model0_IRQ_WAKE_THREAD = (1 << 1),
};

typedef enum Model0_irqreturn Model0_irqreturn_t;














static inline __attribute__((no_instrument_function)) void Model0_ftrace_nmi_enter(void) { }
static inline __attribute__((no_instrument_function)) void Model0_ftrace_nmi_exit(void) { }








struct Model0_context_tracking {
 /*
	 * When active is false, probes are unset in order
	 * to minimize overhead: TIF flags are cleared
	 * and calls to user_enter/exit are ignored. This
	 * may be further optimized using static keys.
	 */
 bool Model0_active;
 int Model0_recursion;
 enum Model0_ctx_state {
  Model0_CONTEXT_DISABLED = -1, /* returned by ct_state() if unknown */
  Model0_CONTEXT_KERNEL = 0,
  Model0_CONTEXT_USER,
  Model0_CONTEXT_GUEST,
 } Model0_state;
};
static inline __attribute__((no_instrument_function)) bool Model0_context_tracking_in_user(void) { return false; }
static inline __attribute__((no_instrument_function)) bool Model0_context_tracking_active(void) { return false; }
static inline __attribute__((no_instrument_function)) bool Model0_context_tracking_is_enabled(void) { return false; }
static inline __attribute__((no_instrument_function)) bool Model0_context_tracking_cpu_is_enabled(void) { return false; }





struct Model0_task_struct;

/*
 * vtime_accounting_cpu_enabled() definitions/declarations
 */
static inline __attribute__((no_instrument_function)) bool Model0_vtime_accounting_cpu_enabled(void) { return false; }



/*
 * Common vtime APIs
 */
static inline __attribute__((no_instrument_function)) void Model0_vtime_task_switch(struct Model0_task_struct *Model0_prev) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_account_system(struct Model0_task_struct *Model0_tsk) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_account_user(struct Model0_task_struct *Model0_tsk) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_user_enter(struct Model0_task_struct *Model0_tsk) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_user_exit(struct Model0_task_struct *Model0_tsk) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_guest_enter(struct Model0_task_struct *Model0_tsk) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_guest_exit(struct Model0_task_struct *Model0_tsk) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_init_idle(struct Model0_task_struct *Model0_tsk, int Model0_cpu) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_account_irq_enter(struct Model0_task_struct *Model0_tsk) { }
static inline __attribute__((no_instrument_function)) void Model0_vtime_account_irq_exit(struct Model0_task_struct *Model0_tsk) { }






static inline __attribute__((no_instrument_function)) void Model0_irqtime_account_irq(struct Model0_task_struct *Model0_tsk) { }


static inline __attribute__((no_instrument_function)) void Model0_account_irq_enter_time(struct Model0_task_struct *Model0_tsk)
{
 Model0_vtime_account_irq_enter(Model0_tsk);
 Model0_irqtime_account_irq(Model0_tsk);
}

static inline __attribute__((no_instrument_function)) void Model0_account_irq_exit_time(struct Model0_task_struct *Model0_tsk)
{
 Model0_vtime_account_irq_exit(Model0_tsk);
 Model0_irqtime_account_irq(Model0_tsk);
}







/*
 * Please do not include this file in generic code.  There is currently
 * no requirement for any architecture to implement anything held
 * within this file.
 *
 * Thanks. --rmk
 */











/*
 * Interrupt flow handler typedefs are defined here to avoid circular
 * include dependencies.
 */

struct Model0_irq_desc;
struct Model0_irq_data;
typedef void (*Model0_irq_flow_handler_t)(struct Model0_irq_desc *Model0_desc);
typedef void (*Model0_irq_preflow_handler_t)(struct Model0_irq_data *Model0_data);





/*
 * Copyright 2006 PathScale, Inc.  All Rights Reserved.
 *
 * This file is free software; you can redistribute it and/or modify
 * it under the terms of version 2 of the GNU General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software Foundation,
 * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA.
 */
struct Model0_device;
struct Model0_resource;

          void Model0___iowrite32_copy(void *Model0_to, const void *Model0_from, Model0_size_t Model0_count);
void Model0___ioread32_copy(void *Model0_to, const void *Model0_from, Model0_size_t Model0_count);
void Model0___iowrite64_copy(void *Model0_to, const void *Model0_from, Model0_size_t Model0_count);


int Model0_ioremap_page_range(unsigned long Model0_addr, unsigned long Model0_end,
         Model0_phys_addr_t Model0_phys_addr, Model0_pgprot_t Model0_prot);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_ioremap_huge_init(void);
int Model0_arch_ioremap_pud_supported(void);
int Model0_arch_ioremap_pmd_supported(void);




/*
 * Managed iomap interface
 */

void * Model0_devm_ioport_map(struct Model0_device *Model0_dev, unsigned long Model0_port,
          unsigned int Model0_nr);
void Model0_devm_ioport_unmap(struct Model0_device *Model0_dev, void *Model0_addr);
void *Model0_devm_ioremap(struct Model0_device *Model0_dev, Model0_resource_size_t Model0_offset,
      Model0_resource_size_t Model0_size);
void *Model0_devm_ioremap_nocache(struct Model0_device *Model0_dev, Model0_resource_size_t Model0_offset,
       Model0_resource_size_t Model0_size);
void *Model0_devm_ioremap_wc(struct Model0_device *Model0_dev, Model0_resource_size_t Model0_offset,
       Model0_resource_size_t Model0_size);
void Model0_devm_iounmap(struct Model0_device *Model0_dev, void *Model0_addr);
int Model0_check_signature(const volatile void *Model0_io_addr,
   const unsigned char *Model0_signature, int Model0_length);
void Model0_devm_ioremap_release(struct Model0_device *Model0_dev, void *Model0_res);

void *Model0_devm_memremap(struct Model0_device *Model0_dev, Model0_resource_size_t Model0_offset,
  Model0_size_t Model0_size, unsigned long Model0_flags);
void Model0_devm_memunmap(struct Model0_device *Model0_dev, void *Model0_addr);

void *Model0___devm_memremap_pages(struct Model0_device *Model0_dev, struct Model0_resource *Model0_res);

/*
 * Some systems do not have legacy ISA devices.
 * /dev/port is not a valid interface on these systems.
 * So for those archs, <asm/io.h> should define the following symbol.
 */




/*
 * Some systems (x86 without PAT) have a somewhat reliable way to mark a
 * physical address range such that uncached mappings will actually
 * end up write-combining.  This facility should be used in conjunction
 * with pgprot_writecombine, ioremap-wc, or set_memory_wc, since it has
 * no effect if the per-page mechanisms are functional.
 * (On x86 without PAT, these functions manipulate MTRRs.)
 *
 * arch_phys_del_wc(0) or arch_phys_del_wc(any error code) is guaranteed
 * to have no effect.
 */
enum {
 /* See memremap() kernel-doc for usage description... */
 Model0_MEMREMAP_WB = 1 << 0,
 Model0_MEMREMAP_WT = 1 << 1,
 Model0_MEMREMAP_WC = 1 << 2,
};

void *Model0_memremap(Model0_resource_size_t Model0_offset, Model0_size_t Model0_size, unsigned long Model0_flags);
void Model0_memunmap(void *Model0_addr);



/*
 *	(C) 1992, 1993 Linus Torvalds, (C) 1997 Ingo Molnar
 *
 *	IRQ/IPI changes taken from work by Thomas Radke
 *	<tomsoft@informatik.tu-chemnitz.de>
 */




static inline __attribute__((no_instrument_function)) int Model0_irq_canonicalize(int Model0_irq)
{
 return ((Model0_irq == 2) ? 9 : Model0_irq);
}
struct Model0_irq_desc;



extern int Model0_check_irq_vectors_for_cpu_disable(void);
extern void Model0_fixup_irqs(void);
extern void Model0_irq_force_complete_move(struct Model0_irq_desc *Model0_desc);



extern void Model0_kvm_set_posted_intr_wakeup_handler(void (*Model0_handler)(void));


extern void (*Model0_x86_platform_ipi_callback)(void);
extern void Model0_native_init_IRQ(void);

extern bool Model0_handle_irq(struct Model0_irq_desc *Model0_desc, struct Model0_pt_regs *Model0_regs);

extern unsigned int Model0_do_IRQ(struct Model0_pt_regs *Model0_regs);

/* Interrupt vector management */
extern unsigned long Model0_used_vectors[(((256) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
extern int Model0_vector_used_by_percpu_irq(unsigned int Model0_vector);

extern void Model0_init_ISA_irqs(void);


void Model0_arch_trigger_all_cpu_backtrace(bool);

/*
 * Per-cpu current frame pointer - the location of the last exception frame on
 * the stack, stored in the per-cpu area.
 *
 * Jeremy Fitzhardinge <jeremy@goop.org>
 */







extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model0_pt_regs *) Model0_irq_regs;

static inline __attribute__((no_instrument_function)) struct Model0_pt_regs *Model0_get_irq_regs(void)
{
 return ({ typeof(Model0_irq_regs) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_irq_regs)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_irq_regs)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_irq_regs) Model0_pfo_ret__; switch (sizeof(Model0_irq_regs)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_irq_regs) Model0_pfo_ret__; switch (sizeof(Model0_irq_regs)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_irq_regs) Model0_pfo_ret__; switch (sizeof(Model0_irq_regs)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_irq_regs) Model0_pfo_ret__; switch (sizeof(Model0_irq_regs)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_irq_regs)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; });
}

static inline __attribute__((no_instrument_function)) struct Model0_pt_regs *Model0_set_irq_regs(struct Model0_pt_regs *Model0_new_regs)
{
 struct Model0_pt_regs *Model0_old_regs;

 Model0_old_regs = Model0_get_irq_regs();
 do { do { const void *Model0___vpp_verify = (typeof((&(Model0_irq_regs)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_irq_regs)) { case 1: do { typedef typeof((Model0_irq_regs)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_new_regs); (void)Model0_pto_tmp__; } switch (sizeof((Model0_irq_regs))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "qi" ((Model0_pto_T__)(Model0_new_regs))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "ri" ((Model0_pto_T__)(Model0_new_regs))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "ri" ((Model0_pto_T__)(Model0_new_regs))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "re" ((Model0_pto_T__)(Model0_new_regs))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model0_irq_regs)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_new_regs); (void)Model0_pto_tmp__; } switch (sizeof((Model0_irq_regs))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "qi" ((Model0_pto_T__)(Model0_new_regs))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "ri" ((Model0_pto_T__)(Model0_new_regs))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "ri" ((Model0_pto_T__)(Model0_new_regs))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "re" ((Model0_pto_T__)(Model0_new_regs))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model0_irq_regs)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_new_regs); (void)Model0_pto_tmp__; } switch (sizeof((Model0_irq_regs))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "qi" ((Model0_pto_T__)(Model0_new_regs))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "ri" ((Model0_pto_T__)(Model0_new_regs))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "ri" ((Model0_pto_T__)(Model0_new_regs))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "re" ((Model0_pto_T__)(Model0_new_regs))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model0_irq_regs)) Model0_pto_T__; if (0) { Model0_pto_T__ Model0_pto_tmp__; Model0_pto_tmp__ = (Model0_new_regs); (void)Model0_pto_tmp__; } switch (sizeof((Model0_irq_regs))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "qi" ((Model0_pto_T__)(Model0_new_regs))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "ri" ((Model0_pto_T__)(Model0_new_regs))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "ri" ((Model0_pto_T__)(Model0_new_regs))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model0_irq_regs)) : "re" ((Model0_pto_T__)(Model0_new_regs))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);

 return Model0_old_regs;
}

struct Model0_seq_file;
struct Model0_module;
struct Model0_msi_msg;
enum Model0_irqchip_irq_state;

/*
 * IRQ line status.
 *
 * Bits 0-7 are the same as the IRQF_* bits in linux/interrupt.h
 *
 * IRQ_TYPE_NONE		- default, unspecified type
 * IRQ_TYPE_EDGE_RISING		- rising edge triggered
 * IRQ_TYPE_EDGE_FALLING	- falling edge triggered
 * IRQ_TYPE_EDGE_BOTH		- rising and falling edge triggered
 * IRQ_TYPE_LEVEL_HIGH		- high level triggered
 * IRQ_TYPE_LEVEL_LOW		- low level triggered
 * IRQ_TYPE_LEVEL_MASK		- Mask to filter out the level bits
 * IRQ_TYPE_SENSE_MASK		- Mask for all the above bits
 * IRQ_TYPE_DEFAULT		- For use by some PICs to ask irq_set_type
 *				  to setup the HW to a sane default (used
 *                                by irqdomain map() callbacks to synchronize
 *                                the HW state and SW flags for a newly
 *                                allocated descriptor).
 *
 * IRQ_TYPE_PROBE		- Special flag for probing in progress
 *
 * Bits which can be modified via irq_set/clear/modify_status_flags()
 * IRQ_LEVEL			- Interrupt is level type. Will be also
 *				  updated in the code when the above trigger
 *				  bits are modified via irq_set_irq_type()
 * IRQ_PER_CPU			- Mark an interrupt PER_CPU. Will protect
 *				  it from affinity setting
 * IRQ_NOPROBE			- Interrupt cannot be probed by autoprobing
 * IRQ_NOREQUEST		- Interrupt cannot be requested via
 *				  request_irq()
 * IRQ_NOTHREAD			- Interrupt cannot be threaded
 * IRQ_NOAUTOEN			- Interrupt is not automatically enabled in
 *				  request/setup_irq()
 * IRQ_NO_BALANCING		- Interrupt cannot be balanced (affinity set)
 * IRQ_MOVE_PCNTXT		- Interrupt can be migrated from process context
 * IRQ_NESTED_THREAD		- Interrupt nests into another thread
 * IRQ_PER_CPU_DEVID		- Dev_id is a per-cpu variable
 * IRQ_IS_POLLED		- Always polled by another interrupt. Exclude
 *				  it from the spurious interrupt detection
 *				  mechanism and from core side polling.
 * IRQ_DISABLE_UNLAZY		- Disable lazy irq disable
 */
enum {
 Model0_IRQ_TYPE_NONE = 0x00000000,
 Model0_IRQ_TYPE_EDGE_RISING = 0x00000001,
 Model0_IRQ_TYPE_EDGE_FALLING = 0x00000002,
 Model0_IRQ_TYPE_EDGE_BOTH = (Model0_IRQ_TYPE_EDGE_FALLING | Model0_IRQ_TYPE_EDGE_RISING),
 Model0_IRQ_TYPE_LEVEL_HIGH = 0x00000004,
 Model0_IRQ_TYPE_LEVEL_LOW = 0x00000008,
 Model0_IRQ_TYPE_LEVEL_MASK = (Model0_IRQ_TYPE_LEVEL_LOW | Model0_IRQ_TYPE_LEVEL_HIGH),
 Model0_IRQ_TYPE_SENSE_MASK = 0x0000000f,
 Model0_IRQ_TYPE_DEFAULT = Model0_IRQ_TYPE_SENSE_MASK,

 Model0_IRQ_TYPE_PROBE = 0x00000010,

 Model0_IRQ_LEVEL = (1 << 8),
 Model0_IRQ_PER_CPU = (1 << 9),
 Model0_IRQ_NOPROBE = (1 << 10),
 Model0_IRQ_NOREQUEST = (1 << 11),
 Model0_IRQ_NOAUTOEN = (1 << 12),
 Model0_IRQ_NO_BALANCING = (1 << 13),
 Model0_IRQ_MOVE_PCNTXT = (1 << 14),
 Model0_IRQ_NESTED_THREAD = (1 << 15),
 Model0_IRQ_NOTHREAD = (1 << 16),
 Model0_IRQ_PER_CPU_DEVID = (1 << 17),
 Model0_IRQ_IS_POLLED = (1 << 18),
 Model0_IRQ_DISABLE_UNLAZY = (1 << 19),
};
/*
 * Return value for chip->irq_set_affinity()
 *
 * IRQ_SET_MASK_OK	- OK, core updates irq_common_data.affinity
 * IRQ_SET_MASK_NOCPY	- OK, chip did update irq_common_data.affinity
 * IRQ_SET_MASK_OK_DONE	- Same as IRQ_SET_MASK_OK for core. Special code to
 *			  support stacked irqchips, which indicates skipping
 *			  all descendent irqchips.
 */
enum {
 Model0_IRQ_SET_MASK_OK = 0,
 Model0_IRQ_SET_MASK_OK_NOCOPY,
 Model0_IRQ_SET_MASK_OK_DONE,
};

struct Model0_msi_desc;
struct Model0_irq_domain;

/**
 * struct irq_common_data - per irq data shared by all irqchips
 * @state_use_accessors: status information for irq chip functions.
 *			Use accessor functions to deal with it
 * @node:		node index useful for balancing
 * @handler_data:	per-IRQ data for the irq_chip methods
 * @affinity:		IRQ affinity on SMP. If this is an IPI
 *			related irq, then this is the mask of the
 *			CPUs to which an IPI can be sent.
 * @msi_desc:		MSI descriptor
 * @ipi_offset:		Offset of first IPI target cpu in @affinity. Optional.
 */
struct Model0_irq_common_data {
 unsigned int Model0_state_use_accessors;

 unsigned int Model0_node;

 void *Model0_handler_data;
 struct Model0_msi_desc *Model0_msi_desc;
 Model0_cpumask_var_t Model0_affinity;



};

/**
 * struct irq_data - per irq chip data passed down to chip functions
 * @mask:		precomputed bitmask for accessing the chip registers
 * @irq:		interrupt number
 * @hwirq:		hardware interrupt number, local to the interrupt domain
 * @common:		point to data shared by all irqchips
 * @chip:		low level interrupt hardware access
 * @domain:		Interrupt translation domain; responsible for mapping
 *			between hwirq number and linux irq number.
 * @parent_data:	pointer to parent struct irq_data to support hierarchy
 *			irq_domain
 * @chip_data:		platform-specific per-chip private data for the chip
 *			methods, to allow shared chip implementations
 */
struct Model0_irq_data {
 Model0_u32 Model0_mask;
 unsigned int Model0_irq;
 unsigned long Model0_hwirq;
 struct Model0_irq_common_data *Model0_common;
 struct Model0_irq_chip *Model0_chip;
 struct Model0_irq_domain *Model0_domain;

 struct Model0_irq_data *Model0_parent_data;

 void *Model0_chip_data;
};

/*
 * Bit masks for irq_common_data.state_use_accessors
 *
 * IRQD_TRIGGER_MASK		- Mask for the trigger type bits
 * IRQD_SETAFFINITY_PENDING	- Affinity setting is pending
 * IRQD_NO_BALANCING		- Balancing disabled for this IRQ
 * IRQD_PER_CPU			- Interrupt is per cpu
 * IRQD_AFFINITY_SET		- Interrupt affinity was set
 * IRQD_LEVEL			- Interrupt is level triggered
 * IRQD_WAKEUP_STATE		- Interrupt is configured for wakeup
 *				  from suspend
 * IRDQ_MOVE_PCNTXT		- Interrupt can be moved in process
 *				  context
 * IRQD_IRQ_DISABLED		- Disabled state of the interrupt
 * IRQD_IRQ_MASKED		- Masked state of the interrupt
 * IRQD_IRQ_INPROGRESS		- In progress state of the interrupt
 * IRQD_WAKEUP_ARMED		- Wakeup mode armed
 * IRQD_FORWARDED_TO_VCPU	- The interrupt is forwarded to a VCPU
 * IRQD_AFFINITY_MANAGED	- Affinity is auto-managed by the kernel
 */
enum {
 Model0_IRQD_TRIGGER_MASK = 0xf,
 Model0_IRQD_SETAFFINITY_PENDING = (1 << 8),
 Model0_IRQD_NO_BALANCING = (1 << 10),
 Model0_IRQD_PER_CPU = (1 << 11),
 Model0_IRQD_AFFINITY_SET = (1 << 12),
 Model0_IRQD_LEVEL = (1 << 13),
 Model0_IRQD_WAKEUP_STATE = (1 << 14),
 Model0_IRQD_MOVE_PCNTXT = (1 << 15),
 Model0_IRQD_IRQ_DISABLED = (1 << 16),
 Model0_IRQD_IRQ_MASKED = (1 << 17),
 Model0_IRQD_IRQ_INPROGRESS = (1 << 18),
 Model0_IRQD_WAKEUP_ARMED = (1 << 19),
 Model0_IRQD_FORWARDED_TO_VCPU = (1 << 20),
 Model0_IRQD_AFFINITY_MANAGED = (1 << 21),
};



static inline __attribute__((no_instrument_function)) bool Model0_irqd_is_setaffinity_pending(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_SETAFFINITY_PENDING;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_is_per_cpu(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_PER_CPU;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_can_balance(struct Model0_irq_data *Model0_d)
{
 return !((((Model0_d)->Model0_common)->Model0_state_use_accessors) & (Model0_IRQD_PER_CPU | Model0_IRQD_NO_BALANCING));
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_affinity_was_set(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_AFFINITY_SET;
}

static inline __attribute__((no_instrument_function)) void Model0_irqd_mark_affinity_was_set(struct Model0_irq_data *Model0_d)
{
 (((Model0_d)->Model0_common)->Model0_state_use_accessors) |= Model0_IRQD_AFFINITY_SET;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_irqd_get_trigger_type(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_TRIGGER_MASK;
}

/*
 * Must only be called inside irq_chip.irq_set_type() functions.
 */
static inline __attribute__((no_instrument_function)) void Model0_irqd_set_trigger_type(struct Model0_irq_data *Model0_d, Model0_u32 Model0_type)
{
 (((Model0_d)->Model0_common)->Model0_state_use_accessors) &= ~Model0_IRQD_TRIGGER_MASK;
 (((Model0_d)->Model0_common)->Model0_state_use_accessors) |= Model0_type & Model0_IRQD_TRIGGER_MASK;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_is_level_type(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_LEVEL;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_is_wakeup_set(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_WAKEUP_STATE;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_can_move_in_process_context(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_MOVE_PCNTXT;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_irq_disabled(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_IRQ_DISABLED;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_irq_masked(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_IRQ_MASKED;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_irq_inprogress(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_IRQ_INPROGRESS;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_is_wakeup_armed(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_WAKEUP_ARMED;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_is_forwarded_to_vcpu(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((no_instrument_function)) void Model0_irqd_set_forwarded_to_vcpu(struct Model0_irq_data *Model0_d)
{
 (((Model0_d)->Model0_common)->Model0_state_use_accessors) |= Model0_IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((no_instrument_function)) void Model0_irqd_clr_forwarded_to_vcpu(struct Model0_irq_data *Model0_d)
{
 (((Model0_d)->Model0_common)->Model0_state_use_accessors) &= ~Model0_IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((no_instrument_function)) bool Model0_irqd_affinity_is_managed(struct Model0_irq_data *Model0_d)
{
 return (((Model0_d)->Model0_common)->Model0_state_use_accessors) & Model0_IRQD_AFFINITY_MANAGED;
}



static inline __attribute__((no_instrument_function)) Model0_irq_hw_number_t Model0_irqd_to_hwirq(struct Model0_irq_data *Model0_d)
{
 return Model0_d->Model0_hwirq;
}

/**
 * struct irq_chip - hardware interrupt chip descriptor
 *
 * @parent_device:	pointer to parent device for irqchip
 * @name:		name for /proc/interrupts
 * @irq_startup:	start up the interrupt (defaults to ->enable if NULL)
 * @irq_shutdown:	shut down the interrupt (defaults to ->disable if NULL)
 * @irq_enable:		enable the interrupt (defaults to chip->unmask if NULL)
 * @irq_disable:	disable the interrupt
 * @irq_ack:		start of a new interrupt
 * @irq_mask:		mask an interrupt source
 * @irq_mask_ack:	ack and mask an interrupt source
 * @irq_unmask:		unmask an interrupt source
 * @irq_eoi:		end of interrupt
 * @irq_set_affinity:	set the CPU affinity on SMP machines
 * @irq_retrigger:	resend an IRQ to the CPU
 * @irq_set_type:	set the flow type (IRQ_TYPE_LEVEL/etc.) of an IRQ
 * @irq_set_wake:	enable/disable power-management wake-on of an IRQ
 * @irq_bus_lock:	function to lock access to slow bus (i2c) chips
 * @irq_bus_sync_unlock:function to sync and unlock slow bus (i2c) chips
 * @irq_cpu_online:	configure an interrupt source for a secondary CPU
 * @irq_cpu_offline:	un-configure an interrupt source for a secondary CPU
 * @irq_suspend:	function called from core code on suspend once per
 *			chip, when one or more interrupts are installed
 * @irq_resume:		function called from core code on resume once per chip,
 *			when one ore more interrupts are installed
 * @irq_pm_shutdown:	function called from core code on shutdown once per chip
 * @irq_calc_mask:	Optional function to set irq_data.mask for special cases
 * @irq_print_chip:	optional to print special chip info in show_interrupts
 * @irq_request_resources:	optional to request resources before calling
 *				any other callback related to this irq
 * @irq_release_resources:	optional to release resources acquired with
 *				irq_request_resources
 * @irq_compose_msi_msg:	optional to compose message content for MSI
 * @irq_write_msi_msg:	optional to write message content for MSI
 * @irq_get_irqchip_state:	return the internal state of an interrupt
 * @irq_set_irqchip_state:	set the internal state of a interrupt
 * @irq_set_vcpu_affinity:	optional to target a vCPU in a virtual machine
 * @ipi_send_single:	send a single IPI to destination cpus
 * @ipi_send_mask:	send an IPI to destination cpus in cpumask
 * @flags:		chip specific flags
 */
struct Model0_irq_chip {
 struct Model0_device *Model0_parent_device;
 const char *Model0_name;
 unsigned int (*Model0_irq_startup)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_shutdown)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_enable)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_disable)(struct Model0_irq_data *Model0_data);

 void (*Model0_irq_ack)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_mask)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_mask_ack)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_unmask)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_eoi)(struct Model0_irq_data *Model0_data);

 int (*Model0_irq_set_affinity)(struct Model0_irq_data *Model0_data, const struct Model0_cpumask *Model0_dest, bool Model0_force);
 int (*Model0_irq_retrigger)(struct Model0_irq_data *Model0_data);
 int (*Model0_irq_set_type)(struct Model0_irq_data *Model0_data, unsigned int Model0_flow_type);
 int (*Model0_irq_set_wake)(struct Model0_irq_data *Model0_data, unsigned int Model0_on);

 void (*Model0_irq_bus_lock)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_bus_sync_unlock)(struct Model0_irq_data *Model0_data);

 void (*Model0_irq_cpu_online)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_cpu_offline)(struct Model0_irq_data *Model0_data);

 void (*Model0_irq_suspend)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_resume)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_pm_shutdown)(struct Model0_irq_data *Model0_data);

 void (*Model0_irq_calc_mask)(struct Model0_irq_data *Model0_data);

 void (*Model0_irq_print_chip)(struct Model0_irq_data *Model0_data, struct Model0_seq_file *Model0_p);
 int (*Model0_irq_request_resources)(struct Model0_irq_data *Model0_data);
 void (*Model0_irq_release_resources)(struct Model0_irq_data *Model0_data);

 void (*Model0_irq_compose_msi_msg)(struct Model0_irq_data *Model0_data, struct Model0_msi_msg *Model0_msg);
 void (*Model0_irq_write_msi_msg)(struct Model0_irq_data *Model0_data, struct Model0_msi_msg *Model0_msg);

 int (*Model0_irq_get_irqchip_state)(struct Model0_irq_data *Model0_data, enum Model0_irqchip_irq_state Model0_which, bool *Model0_state);
 int (*Model0_irq_set_irqchip_state)(struct Model0_irq_data *Model0_data, enum Model0_irqchip_irq_state Model0_which, bool Model0_state);

 int (*Model0_irq_set_vcpu_affinity)(struct Model0_irq_data *Model0_data, void *Model0_vcpu_info);

 void (*Model0_ipi_send_single)(struct Model0_irq_data *Model0_data, unsigned int Model0_cpu);
 void (*Model0_ipi_send_mask)(struct Model0_irq_data *Model0_data, const struct Model0_cpumask *Model0_dest);

 unsigned long Model0_flags;
};

/*
 * irq_chip specific flags
 *
 * IRQCHIP_SET_TYPE_MASKED:	Mask before calling chip.irq_set_type()
 * IRQCHIP_EOI_IF_HANDLED:	Only issue irq_eoi() when irq was handled
 * IRQCHIP_MASK_ON_SUSPEND:	Mask non wake irqs in the suspend path
 * IRQCHIP_ONOFFLINE_ENABLED:	Only call irq_on/off_line callbacks
 *				when irq enabled
 * IRQCHIP_SKIP_SET_WAKE:	Skip chip.irq_set_wake(), for this irq chip
 * IRQCHIP_ONESHOT_SAFE:	One shot does not require mask/unmask
 * IRQCHIP_EOI_THREADED:	Chip requires eoi() on unmask in threaded mode
 */
enum {
 Model0_IRQCHIP_SET_TYPE_MASKED = (1 << 0),
 Model0_IRQCHIP_EOI_IF_HANDLED = (1 << 1),
 Model0_IRQCHIP_MASK_ON_SUSPEND = (1 << 2),
 Model0_IRQCHIP_ONOFFLINE_ENABLED = (1 << 3),
 Model0_IRQCHIP_SKIP_SET_WAKE = (1 << 4),
 Model0_IRQCHIP_ONESHOT_SAFE = (1 << 5),
 Model0_IRQCHIP_EOI_THREADED = (1 << 6),
};







/*
 * Core internal functions to deal with irq descriptors
 */

struct Model0_irq_affinity_notify;
struct Model0_proc_dir_entry;
struct Model0_module;
struct Model0_irq_desc;
struct Model0_irq_domain;
struct Model0_pt_regs;

/**
 * struct irq_desc - interrupt descriptor
 * @irq_common_data:	per irq and chip data passed down to chip functions
 * @kstat_irqs:		irq stats per cpu
 * @handle_irq:		highlevel irq-events handler
 * @preflow_handler:	handler called before the flow handler (currently used by sparc)
 * @action:		the irq action chain
 * @status:		status information
 * @core_internal_state__do_not_mess_with_it: core internal status information
 * @depth:		disable-depth, for nested irq_disable() calls
 * @wake_depth:		enable depth, for multiple irq_set_irq_wake() callers
 * @irq_count:		stats field to detect stalled irqs
 * @last_unhandled:	aging timer for unhandled count
 * @irqs_unhandled:	stats field for spurious unhandled interrupts
 * @threads_handled:	stats field for deferred spurious detection of threaded handlers
 * @threads_handled_last: comparator field for deferred spurious detection of theraded handlers
 * @lock:		locking for SMP
 * @affinity_hint:	hint to user space for preferred irq affinity
 * @affinity_notify:	context for notification of affinity changes
 * @pending_mask:	pending rebalanced interrupts
 * @threads_oneshot:	bitfield to handle shared oneshot threads
 * @threads_active:	number of irqaction threads currently running
 * @wait_for_threads:	wait queue for sync_irq to wait for threaded handlers
 * @nr_actions:		number of installed actions on this descriptor
 * @no_suspend_depth:	number of irqactions on a irq descriptor with
 *			IRQF_NO_SUSPEND set
 * @force_resume_depth:	number of irqactions on a irq descriptor with
 *			IRQF_FORCE_RESUME set
 * @rcu:		rcu head for delayed free
 * @dir:		/proc/irq/ procfs entry
 * @name:		flow handler name for /proc/interrupts output
 */
struct Model0_irq_desc {
 struct Model0_irq_common_data Model0_irq_common_data;
 struct Model0_irq_data Model0_irq_data;
 unsigned int *Model0_kstat_irqs;
 Model0_irq_flow_handler_t Model0_handle_irq;



 struct Model0_irqaction *Model0_action; /* IRQ action list */
 unsigned int Model0_status_use_accessors;
 unsigned int Model0_core_internal_state__do_not_mess_with_it;
 unsigned int Model0_depth; /* nested irq disables */
 unsigned int Model0_wake_depth; /* nested wake enables */
 unsigned int Model0_irq_count; /* For detecting broken IRQs */
 unsigned long Model0_last_unhandled; /* Aging timer for unhandled count */
 unsigned int Model0_irqs_unhandled;
 Model0_atomic_t Model0_threads_handled;
 int Model0_threads_handled_last;
 Model0_raw_spinlock_t Model0_lock;
 struct Model0_cpumask *Model0_percpu_enabled;
 const struct Model0_cpumask *Model0_percpu_affinity;

 const struct Model0_cpumask *Model0_affinity_hint;
 struct Model0_irq_affinity_notify *Model0_affinity_notify;

 Model0_cpumask_var_t Model0_pending_mask;


 unsigned long Model0_threads_oneshot;
 Model0_atomic_t Model0_threads_active;
 Model0_wait_queue_head_t Model0_wait_for_threads;

 unsigned int Model0_nr_actions;
 unsigned int Model0_no_suspend_depth;
 unsigned int Model0_cond_suspend_depth;
 unsigned int Model0_force_resume_depth;


 struct Model0_proc_dir_entry *Model0_dir;


 struct Model0_callback_head Model0_rcu;

 int Model0_parent_irq;
 struct Model0_module *Model0_owner;
 const char *Model0_name;
} __attribute__((__aligned__(1 << (6))));


extern void Model0_irq_lock_sparse(void);
extern void Model0_irq_unlock_sparse(void);






static inline __attribute__((no_instrument_function)) struct Model0_irq_desc *Model0_irq_data_to_desc(struct Model0_irq_data *Model0_data)
{
 return ({ const typeof( ((struct Model0_irq_desc *)0)->Model0_irq_common_data ) *Model0___mptr = (Model0_data->Model0_common); (struct Model0_irq_desc *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_irq_desc, Model0_irq_common_data) );});
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_irq_desc_get_irq(struct Model0_irq_desc *Model0_desc)
{
 return Model0_desc->Model0_irq_data.Model0_irq;
}

static inline __attribute__((no_instrument_function)) struct Model0_irq_data *Model0_irq_desc_get_irq_data(struct Model0_irq_desc *Model0_desc)
{
 return &Model0_desc->Model0_irq_data;
}

static inline __attribute__((no_instrument_function)) struct Model0_irq_chip *Model0_irq_desc_get_chip(struct Model0_irq_desc *Model0_desc)
{
 return Model0_desc->Model0_irq_data.Model0_chip;
}

static inline __attribute__((no_instrument_function)) void *Model0_irq_desc_get_chip_data(struct Model0_irq_desc *Model0_desc)
{
 return Model0_desc->Model0_irq_data.Model0_chip_data;
}

static inline __attribute__((no_instrument_function)) void *Model0_irq_desc_get_handler_data(struct Model0_irq_desc *Model0_desc)
{
 return Model0_desc->Model0_irq_common_data.Model0_handler_data;
}

static inline __attribute__((no_instrument_function)) struct Model0_msi_desc *Model0_irq_desc_get_msi_desc(struct Model0_irq_desc *Model0_desc)
{
 return Model0_desc->Model0_irq_common_data.Model0_msi_desc;
}

/*
 * Architectures call this to let the generic IRQ layer
 * handle an interrupt.
 */
static inline __attribute__((no_instrument_function)) void Model0_generic_handle_irq_desc(struct Model0_irq_desc *Model0_desc)
{
 Model0_desc->Model0_handle_irq(Model0_desc);
}

int Model0_generic_handle_irq(unsigned int Model0_irq);
/* Test to see if a driver has successfully requested an irq */
static inline __attribute__((no_instrument_function)) int Model0_irq_desc_has_action(struct Model0_irq_desc *Model0_desc)
{
 return Model0_desc->Model0_action != ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_irq_has_action(unsigned int Model0_irq)
{
 return Model0_irq_desc_has_action(Model0_irq_to_desc(Model0_irq));
}

/**
 * irq_set_handler_locked - Set irq handler from a locked region
 * @data:	Pointer to the irq_data structure which identifies the irq
 * @handler:	Flow control handler function for this interrupt
 *
 * Sets the handler in the irq descriptor associated to @data.
 *
 * Must be called with irq_desc locked and valid parameters. Typical
 * call site is the irq_set_type() callback.
 */
static inline __attribute__((no_instrument_function)) void Model0_irq_set_handler_locked(struct Model0_irq_data *Model0_data,
       Model0_irq_flow_handler_t Model0_handler)
{
 struct Model0_irq_desc *Model0_desc = Model0_irq_data_to_desc(Model0_data);

 Model0_desc->Model0_handle_irq = Model0_handler;
}

/**
 * irq_set_chip_handler_name_locked - Set chip, handler and name from a locked region
 * @data:	Pointer to the irq_data structure for which the chip is set
 * @chip:	Pointer to the new irq chip
 * @handler:	Flow control handler function for this interrupt
 * @name:	Name of the interrupt
 *
 * Replace the irq chip at the proper hierarchy level in @data and
 * sets the handler and name in the associated irq descriptor.
 *
 * Must be called with irq_desc locked and valid parameters.
 */
static inline __attribute__((no_instrument_function)) void
Model0_irq_set_chip_handler_name_locked(struct Model0_irq_data *Model0_data, struct Model0_irq_chip *Model0_chip,
     Model0_irq_flow_handler_t Model0_handler, const char *Model0_name)
{
 struct Model0_irq_desc *Model0_desc = Model0_irq_data_to_desc(Model0_data);

 Model0_desc->Model0_handle_irq = Model0_handler;
 Model0_desc->Model0_name = Model0_name;
 Model0_data->Model0_chip = Model0_chip;
}

static inline __attribute__((no_instrument_function)) int Model0_irq_balancing_disabled(unsigned int Model0_irq)
{
 struct Model0_irq_desc *Model0_desc;

 Model0_desc = Model0_irq_to_desc(Model0_irq);
 return Model0_desc->Model0_status_use_accessors & (Model0_IRQ_PER_CPU | Model0_IRQ_NO_BALANCING);
}

static inline __attribute__((no_instrument_function)) int Model0_irq_is_percpu(unsigned int Model0_irq)
{
 struct Model0_irq_desc *Model0_desc;

 Model0_desc = Model0_irq_to_desc(Model0_irq);
 return Model0_desc->Model0_status_use_accessors & Model0_IRQ_PER_CPU;
}

static inline __attribute__((no_instrument_function)) void
Model0_irq_set_lockdep_class(unsigned int Model0_irq, struct Model0_lock_class_key *Model0_class)
{
 struct Model0_irq_desc *Model0_desc = Model0_irq_to_desc(Model0_irq);

 if (Model0_desc)
  do { (void)(Model0_class); } while (0);
}

/*
 * Pick up the arch-dependent methods:
 */




/*
 * (C) 1992, 1993 Linus Torvalds, (C) 1997 Ingo Molnar
 *
 * moved some of the old arch/i386/kernel/irq.h to here. VY
 *
 * IRQ/IPI changes taken from work by Thomas Radke
 * <tomsoft@informatik.tu-chemnitz.de>
 *
 * hacked by Andi Kleen for x86-64.
 * unified by tglx
 */





















struct Model0_proc_dir_entry;
struct Model0_pt_regs;
struct Model0_notifier_block;


void Model0_create_prof_cpu_mask(void);
int Model0_create_proc_profile(void);
enum Model0_profile_type {
 Model0_PROFILE_TASK_EXIT,
 Model0_PROFILE_MUNMAP
};



extern int Model0_prof_on __attribute__((__section__(".data..read_mostly")));

/* init basic kernel profiler */
int Model0_profile_init(void);
int Model0_profile_setup(char *Model0_str);
void Model0_profile_tick(int Model0_type);
int Model0_setup_profiling_timer(unsigned int Model0_multiplier);

/*
 * Add multiple profiler hits to a given address:
 */
void Model0_profile_hits(int Model0_type, void *Model0_ip, unsigned int Model0_nr_hits);

/*
 * Single profiler hit:
 */
static inline __attribute__((no_instrument_function)) void Model0_profile_hit(int Model0_type, void *Model0_ip)
{
 /*
	 * Speedup for the common (no profiling enabled) case:
	 */
 if (__builtin_expect(!!(Model0_prof_on == Model0_type), 0))
  Model0_profile_hits(Model0_type, Model0_ip, 1);
}

struct Model0_task_struct;
struct Model0_mm_struct;

/* task is in do_exit() */
void Model0_profile_task_exit(struct Model0_task_struct * Model0_task);

/* task is dead, free task struct ? Returns 1 if
 * the task was taken, 0 if the task should be freed.
 */
int Model0_profile_handoff_task(struct Model0_task_struct * Model0_task);

/* sys_munmap */
void Model0_profile_munmap(unsigned long Model0_addr);

int Model0_task_handoff_register(struct Model0_notifier_block * Model0_n);
int Model0_task_handoff_unregister(struct Model0_notifier_block * Model0_n);

int Model0_profile_event_register(enum Model0_profile_type, struct Model0_notifier_block * Model0_n);
int Model0_profile_event_unregister(enum Model0_profile_type, struct Model0_notifier_block * Model0_n);

struct Model0_pt_regs;










/* References to section boundaries */




/*
 * Usage guidelines:
 * _text, _data: architecture specific, don't use them in arch-independent code
 * [_stext, _etext]: contains .text.* sections, may also contain .rodata.*
 *                   and/or .init.* sections
 * [_sdata, _edata]: contains .data.* sections, may also contain .rodata.*
 *                   and/or .init.* sections.
 * [__start_rodata, __end_rodata]: contains .rodata.* sections
 * [__init_begin, __init_end]: contains .init.* sections, but .init.text.*
 *                   may be out of this range on some architectures.
 * [_sinittext, _einittext]: contains .init.text.* sections
 * [__bss_start, __bss_stop]: contains BSS sections
 *
 * Following global variables are optional and may be unavailable on some
 * architectures and/or kernel configurations.
 *	_text, _data
 *	__kprobes_text_start, __kprobes_text_end
 *	__entry_text_start, __entry_text_end
 *	__ctors_start, __ctors_end
 */
extern char Model0__text[], Model0__stext[], Model0__etext[];
extern char Model0__data[], Model0__sdata[], Model0__edata[];
extern char Model0___bss_start[], Model0___bss_stop[];
extern char Model0___init_begin[], Model0___init_end[];
extern char Model0__sinittext[], Model0__einittext[];
extern char Model0__end[];
extern char Model0___per_cpu_load[], Model0___per_cpu_start[], Model0___per_cpu_end[];
extern char Model0___kprobes_text_start[], Model0___kprobes_text_end[];
extern char Model0___entry_text_start[], Model0___entry_text_end[];
extern char Model0___start_rodata[], Model0___end_rodata[];

/* Start and end of .ctors section - used for constructor calls. */
extern char Model0___ctors_start[], Model0___ctors_end[];

extern const void Model0___nosave_begin, Model0___nosave_end;

/* function descriptor handling (if any).  Override
 * in asm/sections.h */




/* random extra sections (if any).  Override
 * in asm/sections.h */

static inline __attribute__((no_instrument_function)) int Model0_arch_is_kernel_text(unsigned long Model0_addr)
{
 return 0;
}



static inline __attribute__((no_instrument_function)) int Model0_arch_is_kernel_data(unsigned long Model0_addr)
{
 return 0;
}


/**
 * memory_contains - checks if an object is contained within a memory region
 * @begin: virtual address of the beginning of the memory region
 * @end: virtual address of the end of the memory region
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if the object specified by @virt and @size is entirely
 * contained within the memory region defined by @begin and @end, false
 * otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_memory_contains(void *Model0_begin, void *Model0_end, void *Model0_virt,
       Model0_size_t Model0_size)
{
 return Model0_virt >= Model0_begin && Model0_virt + Model0_size <= Model0_end;
}

/**
 * memory_intersects - checks if the region occupied by an object intersects
 *                     with another memory region
 * @begin: virtual address of the beginning of the memory regien
 * @end: virtual address of the end of the memory region
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if an object's memory region, specified by @virt and @size,
 * intersects with the region specified by @begin and @end, false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_memory_intersects(void *Model0_begin, void *Model0_end, void *Model0_virt,
         Model0_size_t Model0_size)
{
 void *Model0_vend = Model0_virt + Model0_size;

 return (Model0_virt >= Model0_begin && Model0_virt < Model0_end) || (Model0_vend >= Model0_begin && Model0_vend < Model0_end);
}

/**
 * init_section_contains - checks if an object is contained within the init
 *                         section
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if the object specified by @virt and @size is entirely
 * contained within the init section, false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_init_section_contains(void *Model0_virt, Model0_size_t Model0_size)
{
 return Model0_memory_contains(Model0___init_begin, Model0___init_end, Model0_virt, Model0_size);
}

/**
 * init_section_intersects - checks if the region occupied by an object
 *                           intersects with the init section
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if an object's memory region, specified by @virt and @size,
 * intersects with the init section, false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_init_section_intersects(void *Model0_virt, Model0_size_t Model0_size)
{
 return Model0_memory_intersects(Model0___init_begin, Model0___init_end, Model0_virt, Model0_size);
}


extern char Model0___brk_base[], Model0___brk_limit[];
extern struct Model0_exception_table_entry Model0___stop___ex_table[];


extern char Model0___end_rodata_hpage_align[];

/* Interrupt handlers registered during init_IRQ */
extern void Model0_apic_timer_interrupt(void);
extern void Model0_x86_platform_ipi(void);
extern void Model0_kvm_posted_intr_ipi(void);
extern void Model0_kvm_posted_intr_wakeup_ipi(void);
extern void Model0_error_interrupt(void);
extern void Model0_irq_work_interrupt(void);

extern void Model0_spurious_interrupt(void);
extern void Model0_thermal_interrupt(void);
extern void Model0_reschedule_interrupt(void);

extern void Model0_irq_move_cleanup_interrupt(void);
extern void Model0_reboot_interrupt(void);
extern void Model0_threshold_interrupt(void);
extern void Model0_deferred_error_interrupt(void);

extern void Model0_call_function_interrupt(void);
extern void Model0_call_function_single_interrupt(void);


/* Interrupt handlers registered during init_IRQ */
extern void Model0_trace_apic_timer_interrupt(void);
extern void Model0_trace_x86_platform_ipi(void);
extern void Model0_trace_error_interrupt(void);
extern void Model0_trace_irq_work_interrupt(void);
extern void Model0_trace_spurious_interrupt(void);
extern void Model0_trace_thermal_interrupt(void);
extern void Model0_trace_reschedule_interrupt(void);
extern void Model0_trace_threshold_interrupt(void);
extern void Model0_trace_deferred_error_interrupt(void);
extern void Model0_trace_call_function_interrupt(void);
extern void Model0_trace_call_function_single_interrupt(void);







struct Model0_irq_data;
struct Model0_pci_dev;
struct Model0_msi_desc;

enum Model0_irq_alloc_type {
 Model0_X86_IRQ_ALLOC_TYPE_IOAPIC = 1,
 Model0_X86_IRQ_ALLOC_TYPE_HPET,
 Model0_X86_IRQ_ALLOC_TYPE_MSI,
 Model0_X86_IRQ_ALLOC_TYPE_MSIX,
 Model0_X86_IRQ_ALLOC_TYPE_DMAR,
 Model0_X86_IRQ_ALLOC_TYPE_UV,
};

struct Model0_irq_alloc_info {
 enum Model0_irq_alloc_type Model0_type;
 Model0_u32 Model0_flags;
 const struct Model0_cpumask *Model0_mask; /* CPU mask for vector allocation */
 union {
  int unused;

  struct {
   int Model0_hpet_id;
   int Model0_hpet_index;
   void *Model0_hpet_data;
  };


  struct {
   struct Model0_pci_dev *Model0_msi_dev;
   Model0_irq_hw_number_t Model0_msi_hwirq;
  };


  struct {
   int Model0_ioapic_id;
   int Model0_ioapic_pin;
   int Model0_ioapic_node;
   Model0_u32 Model0_ioapic_trigger : 1;
   Model0_u32 Model0_ioapic_polarity : 1;
   Model0_u32 Model0_ioapic_valid : 1;
   struct Model0_IO_APIC_route_entry *Model0_ioapic_entry;
  };


  struct {
   int Model0_dmar_id;
   void *Model0_dmar_data;
  };


  struct {
   int Model0_ht_pos;
   int Model0_ht_idx;
   struct Model0_pci_dev *Model0_ht_dev;
   void *Model0_ht_update;
  };
 };
};

struct Model0_irq_cfg {
 unsigned int Model0_dest_apicid;
 Model0_u8 Model0_vector;
 Model0_u8 Model0_old_vector;
};

extern struct Model0_irq_cfg *Model0_irq_cfg(unsigned int Model0_irq);
extern struct Model0_irq_cfg *Model0_irqd_cfg(struct Model0_irq_data *Model0_irq_data);
extern void Model0_lock_vector_lock(void);
extern void Model0_unlock_vector_lock(void);
extern void Model0_setup_vector_irq(int Model0_cpu);

extern void Model0_send_cleanup_vector(struct Model0_irq_cfg *);
extern void Model0_irq_complete_move(struct Model0_irq_cfg *Model0_cfg);





extern void Model0_apic_ack_edge(struct Model0_irq_data *Model0_data);





/* Statistics */
extern Model0_atomic_t Model0_irq_err_count;
extern Model0_atomic_t Model0_irq_mis_count;

extern void Model0_elcr_set_level_irq(unsigned int Model0_irq);

extern char Model0_irq_entries_start[];







typedef struct Model0_irq_desc* Model0_vector_irq_t[256];
extern __attribute__((section(".data..percpu" ""))) __typeof__(Model0_vector_irq_t) Model0_vector_irq;
struct Model0_irqaction;
extern int Model0_setup_irq(unsigned int Model0_irq, struct Model0_irqaction *Model0_new);
extern void Model0_remove_irq(unsigned int Model0_irq, struct Model0_irqaction *Model0_act);
extern int Model0_setup_percpu_irq(unsigned int Model0_irq, struct Model0_irqaction *Model0_new);
extern void Model0_remove_percpu_irq(unsigned int Model0_irq, struct Model0_irqaction *Model0_act);

extern void Model0_irq_cpu_online(void);
extern void Model0_irq_cpu_offline(void);
extern int Model0_irq_set_affinity_locked(struct Model0_irq_data *Model0_data,
       const struct Model0_cpumask *Model0_cpumask, bool Model0_force);
extern int Model0_irq_set_vcpu_affinity(unsigned int Model0_irq, void *Model0_vcpu_info);

extern void Model0_irq_migrate_all_off_this_cpu(void);


void Model0_irq_move_irq(struct Model0_irq_data *Model0_data);
void Model0_irq_move_masked_irq(struct Model0_irq_data *Model0_data);





extern int Model0_no_irq_affinity;




static inline __attribute__((no_instrument_function)) int Model0_irq_set_parent(int Model0_irq, int Model0_parent_irq)
{
 return 0;
}


/*
 * Built-in IRQ handlers for various IRQ types,
 * callable via desc->handle_irq()
 */
extern void Model0_handle_level_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_fasteoi_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_edge_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_edge_eoi_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_simple_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_untracked_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_percpu_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_percpu_devid_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_bad_irq(struct Model0_irq_desc *Model0_desc);
extern void Model0_handle_nested_irq(unsigned int Model0_irq);

extern int Model0_irq_chip_compose_msi_msg(struct Model0_irq_data *Model0_data, struct Model0_msi_msg *Model0_msg);
extern int Model0_irq_chip_pm_get(struct Model0_irq_data *Model0_data);
extern int Model0_irq_chip_pm_put(struct Model0_irq_data *Model0_data);

extern void Model0_irq_chip_enable_parent(struct Model0_irq_data *Model0_data);
extern void Model0_irq_chip_disable_parent(struct Model0_irq_data *Model0_data);
extern void Model0_irq_chip_ack_parent(struct Model0_irq_data *Model0_data);
extern int Model0_irq_chip_retrigger_hierarchy(struct Model0_irq_data *Model0_data);
extern void Model0_irq_chip_mask_parent(struct Model0_irq_data *Model0_data);
extern void Model0_irq_chip_unmask_parent(struct Model0_irq_data *Model0_data);
extern void Model0_irq_chip_eoi_parent(struct Model0_irq_data *Model0_data);
extern int Model0_irq_chip_set_affinity_parent(struct Model0_irq_data *Model0_data,
     const struct Model0_cpumask *Model0_dest,
     bool Model0_force);
extern int Model0_irq_chip_set_wake_parent(struct Model0_irq_data *Model0_data, unsigned int Model0_on);
extern int Model0_irq_chip_set_vcpu_affinity_parent(struct Model0_irq_data *Model0_data,
          void *Model0_vcpu_info);
extern int Model0_irq_chip_set_type_parent(struct Model0_irq_data *Model0_data, unsigned int Model0_type);


/* Handling of unhandled and spurious interrupts: */
extern void Model0_note_interrupt(struct Model0_irq_desc *Model0_desc, Model0_irqreturn_t Model0_action_ret);


/* Enable/disable irq debugging output: */
extern int Model0_noirqdebug_setup(char *Model0_str);

/* Checks whether the interrupt can be requested by request_irq(): */
extern int Model0_can_request_irq(unsigned int Model0_irq, unsigned long Model0_irqflags);

/* Dummy irq-chip implementations: */
extern struct Model0_irq_chip Model0_no_irq_chip;
extern struct Model0_irq_chip Model0_dummy_irq_chip;

extern void
Model0_irq_set_chip_and_handler_name(unsigned int Model0_irq, struct Model0_irq_chip *Model0_chip,
         Model0_irq_flow_handler_t Model0_handle, const char *Model0_name);

static inline __attribute__((no_instrument_function)) void Model0_irq_set_chip_and_handler(unsigned int Model0_irq, struct Model0_irq_chip *Model0_chip,
         Model0_irq_flow_handler_t Model0_handle)
{
 Model0_irq_set_chip_and_handler_name(Model0_irq, Model0_chip, Model0_handle, ((void *)0));
}

extern int Model0_irq_set_percpu_devid(unsigned int Model0_irq);
extern int Model0_irq_set_percpu_devid_partition(unsigned int Model0_irq,
       const struct Model0_cpumask *Model0_affinity);
extern int Model0_irq_get_percpu_devid_partition(unsigned int Model0_irq,
       struct Model0_cpumask *Model0_affinity);

extern void
Model0___irq_set_handler(unsigned int Model0_irq, Model0_irq_flow_handler_t Model0_handle, int Model0_is_chained,
    const char *Model0_name);

static inline __attribute__((no_instrument_function)) void
Model0_irq_set_handler(unsigned int Model0_irq, Model0_irq_flow_handler_t Model0_handle)
{
 Model0___irq_set_handler(Model0_irq, Model0_handle, 0, ((void *)0));
}

/*
 * Set a highlevel chained flow handler for a given IRQ.
 * (a chained handler is automatically enabled and set to
 *  IRQ_NOREQUEST, IRQ_NOPROBE, and IRQ_NOTHREAD)
 */
static inline __attribute__((no_instrument_function)) void
Model0_irq_set_chained_handler(unsigned int Model0_irq, Model0_irq_flow_handler_t Model0_handle)
{
 Model0___irq_set_handler(Model0_irq, Model0_handle, 1, ((void *)0));
}

/*
 * Set a highlevel chained flow handler and its data for a given IRQ.
 * (a chained handler is automatically enabled and set to
 *  IRQ_NOREQUEST, IRQ_NOPROBE, and IRQ_NOTHREAD)
 */
void
Model0_irq_set_chained_handler_and_data(unsigned int Model0_irq, Model0_irq_flow_handler_t Model0_handle,
     void *Model0_data);

void Model0_irq_modify_status(unsigned int Model0_irq, unsigned long Model0_clr, unsigned long Model0_set);

static inline __attribute__((no_instrument_function)) void Model0_irq_set_status_flags(unsigned int Model0_irq, unsigned long Model0_set)
{
 Model0_irq_modify_status(Model0_irq, 0, Model0_set);
}

static inline __attribute__((no_instrument_function)) void Model0_irq_clear_status_flags(unsigned int Model0_irq, unsigned long Model0_clr)
{
 Model0_irq_modify_status(Model0_irq, Model0_clr, 0);
}

static inline __attribute__((no_instrument_function)) void Model0_irq_set_noprobe(unsigned int Model0_irq)
{
 Model0_irq_modify_status(Model0_irq, 0, Model0_IRQ_NOPROBE);
}

static inline __attribute__((no_instrument_function)) void Model0_irq_set_probe(unsigned int Model0_irq)
{
 Model0_irq_modify_status(Model0_irq, Model0_IRQ_NOPROBE, 0);
}

static inline __attribute__((no_instrument_function)) void Model0_irq_set_nothread(unsigned int Model0_irq)
{
 Model0_irq_modify_status(Model0_irq, 0, Model0_IRQ_NOTHREAD);
}

static inline __attribute__((no_instrument_function)) void Model0_irq_set_thread(unsigned int Model0_irq)
{
 Model0_irq_modify_status(Model0_irq, Model0_IRQ_NOTHREAD, 0);
}

static inline __attribute__((no_instrument_function)) void Model0_irq_set_nested_thread(unsigned int Model0_irq, bool Model0_nest)
{
 if (Model0_nest)
  Model0_irq_set_status_flags(Model0_irq, Model0_IRQ_NESTED_THREAD);
 else
  Model0_irq_clear_status_flags(Model0_irq, Model0_IRQ_NESTED_THREAD);
}

static inline __attribute__((no_instrument_function)) void Model0_irq_set_percpu_devid_flags(unsigned int Model0_irq)
{
 Model0_irq_set_status_flags(Model0_irq,
        Model0_IRQ_NOAUTOEN | Model0_IRQ_PER_CPU | Model0_IRQ_NOTHREAD |
        Model0_IRQ_NOPROBE | Model0_IRQ_PER_CPU_DEVID);
}

/* Set/get chip/data for an IRQ: */
extern int Model0_irq_set_chip(unsigned int Model0_irq, struct Model0_irq_chip *Model0_chip);
extern int Model0_irq_set_handler_data(unsigned int Model0_irq, void *Model0_data);
extern int Model0_irq_set_chip_data(unsigned int Model0_irq, void *Model0_data);
extern int Model0_irq_set_irq_type(unsigned int Model0_irq, unsigned int Model0_type);
extern int Model0_irq_set_msi_desc(unsigned int Model0_irq, struct Model0_msi_desc *Model0_entry);
extern int Model0_irq_set_msi_desc_off(unsigned int Model0_irq_base, unsigned int Model0_irq_offset,
    struct Model0_msi_desc *Model0_entry);
extern struct Model0_irq_data *Model0_irq_get_irq_data(unsigned int Model0_irq);

static inline __attribute__((no_instrument_function)) struct Model0_irq_chip *Model0_irq_get_chip(unsigned int Model0_irq)
{
 struct Model0_irq_data *Model0_d = Model0_irq_get_irq_data(Model0_irq);
 return Model0_d ? Model0_d->Model0_chip : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_irq_chip *Model0_irq_data_get_irq_chip(struct Model0_irq_data *Model0_d)
{
 return Model0_d->Model0_chip;
}

static inline __attribute__((no_instrument_function)) void *Model0_irq_get_chip_data(unsigned int Model0_irq)
{
 struct Model0_irq_data *Model0_d = Model0_irq_get_irq_data(Model0_irq);
 return Model0_d ? Model0_d->Model0_chip_data : ((void *)0);
}

static inline __attribute__((no_instrument_function)) void *Model0_irq_data_get_irq_chip_data(struct Model0_irq_data *Model0_d)
{
 return Model0_d->Model0_chip_data;
}

static inline __attribute__((no_instrument_function)) void *Model0_irq_get_handler_data(unsigned int Model0_irq)
{
 struct Model0_irq_data *Model0_d = Model0_irq_get_irq_data(Model0_irq);
 return Model0_d ? Model0_d->Model0_common->Model0_handler_data : ((void *)0);
}

static inline __attribute__((no_instrument_function)) void *Model0_irq_data_get_irq_handler_data(struct Model0_irq_data *Model0_d)
{
 return Model0_d->Model0_common->Model0_handler_data;
}

static inline __attribute__((no_instrument_function)) struct Model0_msi_desc *Model0_irq_get_msi_desc(unsigned int Model0_irq)
{
 struct Model0_irq_data *Model0_d = Model0_irq_get_irq_data(Model0_irq);
 return Model0_d ? Model0_d->Model0_common->Model0_msi_desc : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_msi_desc *Model0_irq_data_get_msi_desc(struct Model0_irq_data *Model0_d)
{
 return Model0_d->Model0_common->Model0_msi_desc;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_irq_get_trigger_type(unsigned int Model0_irq)
{
 struct Model0_irq_data *Model0_d = Model0_irq_get_irq_data(Model0_irq);
 return Model0_d ? Model0_irqd_get_trigger_type(Model0_d) : 0;
}

static inline __attribute__((no_instrument_function)) int Model0_irq_common_data_get_node(struct Model0_irq_common_data *Model0_d)
{

 return Model0_d->Model0_node;



}

static inline __attribute__((no_instrument_function)) int Model0_irq_data_get_node(struct Model0_irq_data *Model0_d)
{
 return Model0_irq_common_data_get_node(Model0_d->Model0_common);
}

static inline __attribute__((no_instrument_function)) struct Model0_cpumask *Model0_irq_get_affinity_mask(int Model0_irq)
{
 struct Model0_irq_data *Model0_d = Model0_irq_get_irq_data(Model0_irq);

 return Model0_d ? Model0_d->Model0_common->Model0_affinity : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_cpumask *Model0_irq_data_get_affinity_mask(struct Model0_irq_data *Model0_d)
{
 return Model0_d->Model0_common->Model0_affinity;
}

unsigned int Model0_arch_dynirq_lower_bound(unsigned int Model0_from);

int Model0___irq_alloc_descs(int Model0_irq, unsigned int Model0_from, unsigned int Model0_cnt, int Model0_node,
        struct Model0_module *Model0_owner, const struct Model0_cpumask *Model0_affinity);

/* use macros to avoid needing export.h for THIS_MODULE */
void Model0_irq_free_descs(unsigned int Model0_irq, unsigned int Model0_cnt);
static inline __attribute__((no_instrument_function)) void Model0_irq_free_desc(unsigned int Model0_irq)
{
 Model0_irq_free_descs(Model0_irq, 1);
}
/**
 * struct irq_chip_regs - register offsets for struct irq_gci
 * @enable:	Enable register offset to reg_base
 * @disable:	Disable register offset to reg_base
 * @mask:	Mask register offset to reg_base
 * @ack:	Ack register offset to reg_base
 * @eoi:	Eoi register offset to reg_base
 * @type:	Type configuration register offset to reg_base
 * @polarity:	Polarity configuration register offset to reg_base
 */
struct Model0_irq_chip_regs {
 unsigned long Model0_enable;
 unsigned long Model0_disable;
 unsigned long Model0_mask;
 unsigned long Model0_ack;
 unsigned long Model0_eoi;
 unsigned long Model0_type;
 unsigned long Model0_polarity;
};

/**
 * struct irq_chip_type - Generic interrupt chip instance for a flow type
 * @chip:		The real interrupt chip which provides the callbacks
 * @regs:		Register offsets for this chip
 * @handler:		Flow handler associated with this chip
 * @type:		Chip can handle these flow types
 * @mask_cache_priv:	Cached mask register private to the chip type
 * @mask_cache:		Pointer to cached mask register
 *
 * A irq_generic_chip can have several instances of irq_chip_type when
 * it requires different functions and register offsets for different
 * flow types.
 */
struct Model0_irq_chip_type {
 struct Model0_irq_chip Model0_chip;
 struct Model0_irq_chip_regs Model0_regs;
 Model0_irq_flow_handler_t Model0_handler;
 Model0_u32 Model0_type;
 Model0_u32 Model0_mask_cache_priv;
 Model0_u32 *Model0_mask_cache;
};

/**
 * struct irq_chip_generic - Generic irq chip data structure
 * @lock:		Lock to protect register and cache data access
 * @reg_base:		Register base address (virtual)
 * @reg_readl:		Alternate I/O accessor (defaults to readl if NULL)
 * @reg_writel:		Alternate I/O accessor (defaults to writel if NULL)
 * @suspend:		Function called from core code on suspend once per
 *			chip; can be useful instead of irq_chip::suspend to
 *			handle chip details even when no interrupts are in use
 * @resume:		Function called from core code on resume once per chip;
 *			can be useful instead of irq_chip::suspend to handle
 *			chip details even when no interrupts are in use
 * @irq_base:		Interrupt base nr for this chip
 * @irq_cnt:		Number of interrupts handled by this chip
 * @mask_cache:		Cached mask register shared between all chip types
 * @type_cache:		Cached type register
 * @polarity_cache:	Cached polarity register
 * @wake_enabled:	Interrupt can wakeup from suspend
 * @wake_active:	Interrupt is marked as an wakeup from suspend source
 * @num_ct:		Number of available irq_chip_type instances (usually 1)
 * @private:		Private data for non generic chip callbacks
 * @installed:		bitfield to denote installed interrupts
 * @unused:		bitfield to denote unused interrupts
 * @domain:		irq domain pointer
 * @list:		List head for keeping track of instances
 * @chip_types:		Array of interrupt irq_chip_types
 *
 * Note, that irq_chip_generic can have multiple irq_chip_type
 * implementations which can be associated to a particular irq line of
 * an irq_chip_generic instance. That allows to share and protect
 * state in an irq_chip_generic instance when we need to implement
 * different flow mechanisms (level/edge) for it.
 */
struct Model0_irq_chip_generic {
 Model0_raw_spinlock_t Model0_lock;
 void *Model0_reg_base;
 Model0_u32 (*Model0_reg_readl)(void *Model0_addr);
 void (*Model0_reg_writel)(Model0_u32 Model0_val, void *Model0_addr);
 void (*Model0_suspend)(struct Model0_irq_chip_generic *Model0_gc);
 void (*Model0_resume)(struct Model0_irq_chip_generic *Model0_gc);
 unsigned int Model0_irq_base;
 unsigned int Model0_irq_cnt;
 Model0_u32 Model0_mask_cache;
 Model0_u32 Model0_type_cache;
 Model0_u32 Model0_polarity_cache;
 Model0_u32 Model0_wake_enabled;
 Model0_u32 Model0_wake_active;
 unsigned int Model0_num_ct;
 void *Model0_private;
 unsigned long Model0_installed;
 unsigned long unused;
 struct Model0_irq_domain *Model0_domain;
 struct Model0_list_head Model0_list;
 struct Model0_irq_chip_type Model0_chip_types[0];
};

/**
 * enum irq_gc_flags - Initialization flags for generic irq chips
 * @IRQ_GC_INIT_MASK_CACHE:	Initialize the mask_cache by reading mask reg
 * @IRQ_GC_INIT_NESTED_LOCK:	Set the lock class of the irqs to nested for
 *				irq chips which need to call irq_set_wake() on
 *				the parent irq. Usually GPIO implementations
 * @IRQ_GC_MASK_CACHE_PER_TYPE:	Mask cache is chip type private
 * @IRQ_GC_NO_MASK:		Do not calculate irq_data->mask
 * @IRQ_GC_BE_IO:		Use big-endian register accesses (default: LE)
 */
enum Model0_irq_gc_flags {
 Model0_IRQ_GC_INIT_MASK_CACHE = 1 << 0,
 Model0_IRQ_GC_INIT_NESTED_LOCK = 1 << 1,
 Model0_IRQ_GC_MASK_CACHE_PER_TYPE = 1 << 2,
 Model0_IRQ_GC_NO_MASK = 1 << 3,
 Model0_IRQ_GC_BE_IO = 1 << 4,
};

/*
 * struct irq_domain_chip_generic - Generic irq chip data structure for irq domains
 * @irqs_per_chip:	Number of interrupts per chip
 * @num_chips:		Number of chips
 * @irq_flags_to_set:	IRQ* flags to set on irq setup
 * @irq_flags_to_clear:	IRQ* flags to clear on irq setup
 * @gc_flags:		Generic chip specific setup flags
 * @gc:			Array of pointers to generic interrupt chips
 */
struct Model0_irq_domain_chip_generic {
 unsigned int Model0_irqs_per_chip;
 unsigned int Model0_num_chips;
 unsigned int Model0_irq_flags_to_clear;
 unsigned int Model0_irq_flags_to_set;
 enum Model0_irq_gc_flags Model0_gc_flags;
 struct Model0_irq_chip_generic *Model0_gc[0];
};

/* Generic chip callback functions */
void Model0_irq_gc_noop(struct Model0_irq_data *Model0_d);
void Model0_irq_gc_mask_disable_reg(struct Model0_irq_data *Model0_d);
void Model0_irq_gc_mask_set_bit(struct Model0_irq_data *Model0_d);
void Model0_irq_gc_mask_clr_bit(struct Model0_irq_data *Model0_d);
void Model0_irq_gc_unmask_enable_reg(struct Model0_irq_data *Model0_d);
void Model0_irq_gc_ack_set_bit(struct Model0_irq_data *Model0_d);
void Model0_irq_gc_ack_clr_bit(struct Model0_irq_data *Model0_d);
void Model0_irq_gc_mask_disable_reg_and_ack(struct Model0_irq_data *Model0_d);
void Model0_irq_gc_eoi(struct Model0_irq_data *Model0_d);
int Model0_irq_gc_set_wake(struct Model0_irq_data *Model0_d, unsigned int Model0_on);

/* Setup functions for irq_chip_generic */
int Model0_irq_map_generic_chip(struct Model0_irq_domain *Model0_d, unsigned int Model0_virq,
    Model0_irq_hw_number_t Model0_hw_irq);
struct Model0_irq_chip_generic *
Model0_irq_alloc_generic_chip(const char *Model0_name, int Model0_nr_ct, unsigned int Model0_irq_base,
         void *Model0_reg_base, Model0_irq_flow_handler_t Model0_handler);
void Model0_irq_setup_generic_chip(struct Model0_irq_chip_generic *Model0_gc, Model0_u32 Model0_msk,
       enum Model0_irq_gc_flags Model0_flags, unsigned int Model0_clr,
       unsigned int Model0_set);
int Model0_irq_setup_alt_chip(struct Model0_irq_data *Model0_d, unsigned int Model0_type);
void Model0_irq_remove_generic_chip(struct Model0_irq_chip_generic *Model0_gc, Model0_u32 Model0_msk,
        unsigned int Model0_clr, unsigned int Model0_set);

struct Model0_irq_chip_generic *Model0_irq_get_domain_generic_chip(struct Model0_irq_domain *Model0_d, unsigned int Model0_hw_irq);
int Model0_irq_alloc_domain_generic_chips(struct Model0_irq_domain *Model0_d, int Model0_irqs_per_chip,
       int Model0_num_ct, const char *Model0_name,
       Model0_irq_flow_handler_t Model0_handler,
       unsigned int Model0_clr, unsigned int Model0_set,
       enum Model0_irq_gc_flags Model0_flags);


static inline __attribute__((no_instrument_function)) struct Model0_irq_chip_type *Model0_irq_data_get_chip_type(struct Model0_irq_data *Model0_d)
{
 return ({ const typeof( ((struct Model0_irq_chip_type *)0)->Model0_chip ) *Model0___mptr = (Model0_d->Model0_chip); (struct Model0_irq_chip_type *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_irq_chip_type, Model0_chip) );});
}




static inline __attribute__((no_instrument_function)) void Model0_irq_gc_lock(struct Model0_irq_chip_generic *Model0_gc)
{
 Model0__raw_spin_lock(&Model0_gc->Model0_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_irq_gc_unlock(struct Model0_irq_chip_generic *Model0_gc)
{
 Model0___raw_spin_unlock(&Model0_gc->Model0_lock);
}





/*
 * The irqsave variants are for usage in non interrupt code. Do not use
 * them in irq_chip callbacks. Use irq_gc_lock() instead.
 */






static inline __attribute__((no_instrument_function)) void Model0_irq_reg_writel(struct Model0_irq_chip_generic *Model0_gc,
      Model0_u32 Model0_val, int Model0_reg_offset)
{
 if (Model0_gc->Model0_reg_writel)
  Model0_gc->Model0_reg_writel(Model0_val, Model0_gc->Model0_reg_base + Model0_reg_offset);
 else
  Model0_writel(Model0_val, Model0_gc->Model0_reg_base + Model0_reg_offset);
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_irq_reg_readl(struct Model0_irq_chip_generic *Model0_gc,
    int Model0_reg_offset)
{
 if (Model0_gc->Model0_reg_readl)
  return Model0_gc->Model0_reg_readl(Model0_gc->Model0_reg_base + Model0_reg_offset);
 else
  return Model0_readl(Model0_gc->Model0_reg_base + Model0_reg_offset);
}

/* Contrary to Linux irqs, for hardware irqs the irq number 0 is valid */

Model0_irq_hw_number_t Model0_ipi_get_hwirq(unsigned int Model0_irq, unsigned int Model0_cpu);
int Model0___ipi_send_single(struct Model0_irq_desc *Model0_desc, unsigned int Model0_cpu);
int Model0___ipi_send_mask(struct Model0_irq_desc *Model0_desc, const struct Model0_cpumask *Model0_dest);
int Model0_ipi_send_single(unsigned int Model0_virq, unsigned int Model0_cpu);
int Model0_ipi_send_mask(unsigned int Model0_virq, const struct Model0_cpumask *Model0_dest);

typedef struct {
 unsigned int Model0___softirq_pending;
 unsigned int Model0___nmi_count; /* arch dependent */

 unsigned int Model0_apic_timer_irqs; /* arch dependent */
 unsigned int Model0_irq_spurious_count;
 unsigned int Model0_icr_read_retry_count;


 unsigned int Model0_kvm_posted_intr_ipis;
 unsigned int Model0_kvm_posted_intr_wakeup_ipis;

 unsigned int Model0_x86_platform_ipis; /* arch dependent */
 unsigned int Model0_apic_perf_irqs;
 unsigned int Model0_apic_irq_work_irqs;

 unsigned int Model0_irq_resched_count;
 unsigned int Model0_irq_call_count;
 unsigned int Model0_irq_tlb_count;


 unsigned int Model0_irq_thermal_count;


 unsigned int Model0_irq_threshold_count;


 unsigned int Model0_irq_deferred_error_count;




} __attribute__((__aligned__((1 << (6))))) Model0_irq_cpustat_t;

extern __attribute__((section(".data..percpu" "..shared_aligned"))) __typeof__(Model0_irq_cpustat_t) Model0_irq_stat __attribute__((__aligned__((1 << (6)))));
extern void Model0_ack_bad_irq(unsigned int Model0_irq);

extern Model0_u64 Model0_arch_irq_stat_cpu(unsigned int Model0_cpu);


extern Model0_u64 Model0_arch_irq_stat(void);


extern void Model0_synchronize_irq(unsigned int Model0_irq);
extern bool Model0_synchronize_hardirq(unsigned int Model0_irq);
extern void Model0_rcu_nmi_enter(void);
extern void Model0_rcu_nmi_exit(void);


/*
 * It is safe to do non-atomic ops on ->hardirq_context,
 * because NMI handlers may not preempt and the ops are
 * always balanced, so the interrupted value of ->hardirq_context
 * will always be restored.
 */







/*
 * Enter irq context (on NO_HZ, update jiffies):
 */
extern void Model0_irq_enter(void);

/*
 * Exit irq context without processing softirqs:
 */







/*
 * Exit irq context and process softirqs if needed:
 */
extern void Model0_irq_exit(void);
/*
 * These correspond to the IORESOURCE_IRQ_* defines in
 * linux/ioport.h to select the interrupt line behaviour.  When
 * requesting an interrupt without specifying a IRQF_TRIGGER, the
 * setting should be assumed to be "as already configured", which
 * may be as per machine or firmware initialisation.
 */
/*
 * These flags used only by the kernel as part of the
 * irq handling routines.
 *
 * IRQF_SHARED - allow sharing the irq among several devices
 * IRQF_PROBE_SHARED - set by callers when they expect sharing mismatches to occur
 * IRQF_TIMER - Flag to mark this interrupt as timer interrupt
 * IRQF_PERCPU - Interrupt is per cpu
 * IRQF_NOBALANCING - Flag to exclude this interrupt from irq balancing
 * IRQF_IRQPOLL - Interrupt is used for polling (only the interrupt that is
 *                registered first in an shared interrupt is considered for
 *                performance reasons)
 * IRQF_ONESHOT - Interrupt is not reenabled after the hardirq handler finished.
 *                Used by threaded interrupts which need to keep the
 *                irq line disabled until the threaded handler has been run.
 * IRQF_NO_SUSPEND - Do not disable this IRQ during suspend.  Does not guarantee
 *                   that this interrupt will wake the system from a suspended
 *                   state.  See Documentation/power/suspend-and-interrupts.txt
 * IRQF_FORCE_RESUME - Force enable it on resume even if IRQF_NO_SUSPEND is set
 * IRQF_NO_THREAD - Interrupt cannot be threaded
 * IRQF_EARLY_RESUME - Resume IRQ early during syscore instead of at device
 *                resume time.
 * IRQF_COND_SUSPEND - If the IRQ is shared with a NO_SUSPEND user, execute this
 *                interrupt handler after suspending interrupts. For system
 *                wakeup devices users need to implement wakeup detection in
 *                their interrupt handlers.
 */
/*
 * These values can be returned by request_any_context_irq() and
 * describe the context the interrupt will be run in.
 *
 * IRQC_IS_HARDIRQ - interrupt runs in hardirq context
 * IRQC_IS_NESTED - interrupt runs in a nested threaded context
 */
enum {
 Model0_IRQC_IS_HARDIRQ = 0,
 Model0_IRQC_IS_NESTED,
};

typedef Model0_irqreturn_t (*Model0_irq_handler_t)(int, void *);

/**
 * struct irqaction - per interrupt action descriptor
 * @handler:	interrupt handler function
 * @name:	name of the device
 * @dev_id:	cookie to identify the device
 * @percpu_dev_id:	cookie to identify the device
 * @next:	pointer to the next irqaction for shared interrupts
 * @irq:	interrupt number
 * @flags:	flags (see IRQF_* above)
 * @thread_fn:	interrupt handler function for threaded interrupts
 * @thread:	thread pointer for threaded interrupts
 * @secondary:	pointer to secondary irqaction (force threading)
 * @thread_flags:	flags related to @thread
 * @thread_mask:	bitmask for keeping track of @thread activity
 * @dir:	pointer to the proc/irq/NN/name entry
 */
struct Model0_irqaction {
 Model0_irq_handler_t Model0_handler;
 void *Model0_dev_id;
 void *Model0_percpu_dev_id;
 struct Model0_irqaction *Model0_next;
 Model0_irq_handler_t Model0_thread_fn;
 struct Model0_task_struct *thread;
 struct Model0_irqaction *Model0_secondary;
 unsigned int Model0_irq;
 unsigned int Model0_flags;
 unsigned long Model0_thread_flags;
 unsigned long Model0_thread_mask;
 const char *Model0_name;
 struct Model0_proc_dir_entry *Model0_dir;
} __attribute__((__aligned__(1 << (6))));

extern Model0_irqreturn_t Model0_no_action(int Model0_cpl, void *Model0_dev_id);

/*
 * If a (PCI) device interrupt is not connected we set dev->irq to
 * IRQ_NOTCONNECTED. This causes request_irq() to fail with -ENOTCONN, so we
 * can distingiush that case from other error returns.
 *
 * 0x80000000 is guaranteed to be outside the available range of interrupts
 * and easy to distinguish from other possible incorrect values.
 */


extern int __attribute__((warn_unused_result))
Model0_request_threaded_irq(unsigned int Model0_irq, Model0_irq_handler_t Model0_handler,
       Model0_irq_handler_t Model0_thread_fn,
       unsigned long Model0_flags, const char *Model0_name, void *Model0_dev);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result))
Model0_request_irq(unsigned int Model0_irq, Model0_irq_handler_t Model0_handler, unsigned long Model0_flags,
     const char *Model0_name, void *Model0_dev)
{
 return Model0_request_threaded_irq(Model0_irq, Model0_handler, ((void *)0), Model0_flags, Model0_name, Model0_dev);
}

extern int __attribute__((warn_unused_result))
Model0_request_any_context_irq(unsigned int Model0_irq, Model0_irq_handler_t Model0_handler,
   unsigned long Model0_flags, const char *Model0_name, void *Model0_dev_id);

extern int __attribute__((warn_unused_result))
Model0_request_percpu_irq(unsigned int Model0_irq, Model0_irq_handler_t Model0_handler,
     const char *Model0_devname, void *Model0_percpu_dev_id);

extern void Model0_free_irq(unsigned int, void *);
extern void Model0_free_percpu_irq(unsigned int, void *);

struct Model0_device;

extern int __attribute__((warn_unused_result))
Model0_devm_request_threaded_irq(struct Model0_device *Model0_dev, unsigned int Model0_irq,
     Model0_irq_handler_t Model0_handler, Model0_irq_handler_t Model0_thread_fn,
     unsigned long Model0_irqflags, const char *Model0_devname,
     void *Model0_dev_id);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result))
Model0_devm_request_irq(struct Model0_device *Model0_dev, unsigned int Model0_irq, Model0_irq_handler_t Model0_handler,
   unsigned long Model0_irqflags, const char *Model0_devname, void *Model0_dev_id)
{
 return Model0_devm_request_threaded_irq(Model0_dev, Model0_irq, Model0_handler, ((void *)0), Model0_irqflags,
      Model0_devname, Model0_dev_id);
}

extern int __attribute__((warn_unused_result))
Model0_devm_request_any_context_irq(struct Model0_device *Model0_dev, unsigned int Model0_irq,
   Model0_irq_handler_t Model0_handler, unsigned long Model0_irqflags,
   const char *Model0_devname, void *Model0_dev_id);

extern void Model0_devm_free_irq(struct Model0_device *Model0_dev, unsigned int Model0_irq, void *Model0_dev_id);

/*
 * On lockdep we dont want to enable hardirqs in hardirq
 * context. Use local_irq_enable_in_hardirq() to annotate
 * kernel code that has to do this nevertheless (pretty much
 * the only valid case is for old/broken hardware that is
 * insanely slow).
 *
 * NOTE: in theory this might break fragile code that relies
 * on hardirq delivery - in practice we dont seem to have such
 * places left. So the only effect should be slightly increased
 * irqs-off latencies.
 */






extern void Model0_disable_irq_nosync(unsigned int Model0_irq);
extern bool Model0_disable_hardirq(unsigned int Model0_irq);
extern void Model0_disable_irq(unsigned int Model0_irq);
extern void Model0_disable_percpu_irq(unsigned int Model0_irq);
extern void Model0_enable_irq(unsigned int Model0_irq);
extern void Model0_enable_percpu_irq(unsigned int Model0_irq, unsigned int Model0_type);
extern bool Model0_irq_percpu_is_enabled(unsigned int Model0_irq);
extern void Model0_irq_wake_thread(unsigned int Model0_irq, void *Model0_dev_id);

/* The following three functions are for the core kernel use only. */
extern void Model0_suspend_device_irqs(void);
extern void Model0_resume_device_irqs(void);

/**
 * struct irq_affinity_notify - context for notification of IRQ affinity changes
 * @irq:		Interrupt to which notification applies
 * @kref:		Reference count, for internal use
 * @work:		Work item, for internal use
 * @notify:		Function to be called on change.  This will be
 *			called in process context.
 * @release:		Function to be called on release.  This will be
 *			called in process context.  Once registered, the
 *			structure must only be freed when this function is
 *			called or later.
 */
struct Model0_irq_affinity_notify {
 unsigned int Model0_irq;
 struct Model0_kref Model0_kref;
 struct Model0_work_struct Model0_work;
 void (*Model0_notify)(struct Model0_irq_affinity_notify *, const Model0_cpumask_t *Model0_mask);
 void (*Model0_release)(struct Model0_kref *Model0_ref);
};



extern Model0_cpumask_var_t Model0_irq_default_affinity;

/* Internal implementation. Use the helpers below */
extern int Model0___irq_set_affinity(unsigned int Model0_irq, const struct Model0_cpumask *Model0_cpumask,
         bool Model0_force);

/**
 * irq_set_affinity - Set the irq affinity of a given irq
 * @irq:	Interrupt to set affinity
 * @cpumask:	cpumask
 *
 * Fails if cpumask does not contain an online CPU
 */
static inline __attribute__((no_instrument_function)) int
Model0_irq_set_affinity(unsigned int Model0_irq, const struct Model0_cpumask *Model0_cpumask)
{
 return Model0___irq_set_affinity(Model0_irq, Model0_cpumask, false);
}

/**
 * irq_force_affinity - Force the irq affinity of a given irq
 * @irq:	Interrupt to set affinity
 * @cpumask:	cpumask
 *
 * Same as irq_set_affinity, but without checking the mask against
 * online cpus.
 *
 * Solely for low level cpu hotplug code, where we need to make per
 * cpu interrupts affine before the cpu becomes online.
 */
static inline __attribute__((no_instrument_function)) int
Model0_irq_force_affinity(unsigned int Model0_irq, const struct Model0_cpumask *Model0_cpumask)
{
 return Model0___irq_set_affinity(Model0_irq, Model0_cpumask, true);
}

extern int Model0_irq_can_set_affinity(unsigned int Model0_irq);
extern int Model0_irq_select_affinity(unsigned int Model0_irq);

extern int Model0_irq_set_affinity_hint(unsigned int Model0_irq, const struct Model0_cpumask *Model0_m);

extern int
Model0_irq_set_affinity_notifier(unsigned int Model0_irq, struct Model0_irq_affinity_notify *Model0_notify);

struct Model0_cpumask *Model0_irq_create_affinity_mask(unsigned int *Model0_nr_vecs);
/*
 * Special lockdep variants of irq disabling/enabling.
 * These should be used for locking constructs that
 * know that a particular irq context which is disabled,
 * and which is the only irq-context user of a lock,
 * that it's safe to take the lock in the irq-disabled
 * section without disabling hardirqs.
 *
 * On !CONFIG_LOCKDEP they are equivalent to the normal
 * irq disable/enable methods.
 */
static inline __attribute__((no_instrument_function)) void Model0_disable_irq_nosync_lockdep(unsigned int Model0_irq)
{
 Model0_disable_irq_nosync(Model0_irq);



}

static inline __attribute__((no_instrument_function)) void Model0_disable_irq_nosync_lockdep_irqsave(unsigned int Model0_irq, unsigned long *Model0_flags)
{
 Model0_disable_irq_nosync(Model0_irq);



}

static inline __attribute__((no_instrument_function)) void Model0_disable_irq_lockdep(unsigned int Model0_irq)
{
 Model0_disable_irq(Model0_irq);



}

static inline __attribute__((no_instrument_function)) void Model0_enable_irq_lockdep(unsigned int Model0_irq)
{



 Model0_enable_irq(Model0_irq);
}

static inline __attribute__((no_instrument_function)) void Model0_enable_irq_lockdep_irqrestore(unsigned int Model0_irq, unsigned long *Model0_flags)
{



 Model0_enable_irq(Model0_irq);
}

/* IRQ wakeup (PM) control: */
extern int Model0_irq_set_irq_wake(unsigned int Model0_irq, unsigned int Model0_on);

static inline __attribute__((no_instrument_function)) int Model0_enable_irq_wake(unsigned int Model0_irq)
{
 return Model0_irq_set_irq_wake(Model0_irq, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_disable_irq_wake(unsigned int Model0_irq)
{
 return Model0_irq_set_irq_wake(Model0_irq, 0);
}

/*
 * irq_get_irqchip_state/irq_set_irqchip_state specific flags
 */
enum Model0_irqchip_irq_state {
 Model0_IRQCHIP_STATE_PENDING, /* Is interrupt pending? */
 Model0_IRQCHIP_STATE_ACTIVE, /* Is interrupt in progress? */
 Model0_IRQCHIP_STATE_MASKED, /* Is interrupt masked? */
 Model0_IRQCHIP_STATE_LINE_LEVEL, /* Is IRQ line high? */
};

extern int Model0_irq_get_irqchip_state(unsigned int Model0_irq, enum Model0_irqchip_irq_state Model0_which,
     bool *Model0_state);
extern int Model0_irq_set_irqchip_state(unsigned int Model0_irq, enum Model0_irqchip_irq_state Model0_which,
     bool Model0_state);


extern bool Model0_force_irqthreads;
/* Some architectures might implement lazy enabling/disabling of
 * interrupts. In some cases, such as stop_machine, we might want
 * to ensure that after a local_irq_disable(), interrupts have
 * really been disabled in hardware. Such architectures need to
 * implement the following hook.
 */




/* PLEASE, avoid to allocate new softirqs, if you need not _really_ high
   frequency threaded job scheduling. For almost all the purposes
   tasklets are more than enough. F.e. all serial device BHs et
   al. should be converted to tasklets, not to softirqs.
 */

enum
{
 Model0_HI_SOFTIRQ=0,
 Model0_TIMER_SOFTIRQ,
 Model0_NET_TX_SOFTIRQ,
 Model0_NET_RX_SOFTIRQ,
 Model0_BLOCK_SOFTIRQ,
 Model0_IRQ_POLL_SOFTIRQ,
 Model0_TASKLET_SOFTIRQ,
 Model0_SCHED_SOFTIRQ,
 Model0_HRTIMER_SOFTIRQ, /* Unused, but kept as tools rely on the
			    numbering. Sigh! */
 Model0_RCU_SOFTIRQ, /* Preferable RCU should always be the last softirq */

 Model0_NR_SOFTIRQS
};



/* map softirq index to softirq name. update 'softirq_to_name' in
 * kernel/softirq.c when adding a new softirq.
 */
extern const char * const Model0_softirq_to_name[Model0_NR_SOFTIRQS];

/* softirq mask and active fields moved to irq_cpustat_t in
 * asm/hardirq.h to get better cache usage.  KAO
 */

struct Model0_softirq_action
{
 void (*Model0_action)(struct Model0_softirq_action *);
};

           void Model0_do_softirq(void);
           void Model0___do_softirq(void);


void Model0_do_softirq_own_stack(void);







extern void Model0_open_softirq(int Model0_nr, void (*Model0_action)(struct Model0_softirq_action *));
extern void Model0_softirq_init(void);
extern void Model0___raise_softirq_irqoff(unsigned int Model0_nr);

extern void Model0_raise_softirq_irqoff(unsigned int Model0_nr);
extern void Model0_raise_softirq(unsigned int Model0_nr);

extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model0_task_struct *) Model0_ksoftirqd;

static inline __attribute__((no_instrument_function)) struct Model0_task_struct *Model0_this_cpu_ksoftirqd(void)
{
 return ({ typeof(Model0_ksoftirqd) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_ksoftirqd)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_ksoftirqd)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_ksoftirqd) Model0_pfo_ret__; switch (sizeof(Model0_ksoftirqd)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_ksoftirqd) Model0_pfo_ret__; switch (sizeof(Model0_ksoftirqd)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_ksoftirqd) Model0_pfo_ret__; switch (sizeof(Model0_ksoftirqd)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_ksoftirqd) Model0_pfo_ret__; switch (sizeof(Model0_ksoftirqd)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_ksoftirqd)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; });
}

/* Tasklets --- multithreaded analogue of BHs.

   Main feature differing them of generic softirqs: tasklet
   is running only on one CPU simultaneously.

   Main feature differing them of BHs: different tasklets
   may be run simultaneously on different CPUs.

   Properties:
   * If tasklet_schedule() is called, then tasklet is guaranteed
     to be executed on some cpu at least once after this.
   * If the tasklet is already scheduled, but its execution is still not
     started, it will be executed only once.
   * If this tasklet is already running on another CPU (or schedule is called
     from tasklet itself), it is rescheduled for later.
   * Tasklet is strictly serialized wrt itself, but not
     wrt another tasklets. If client needs some intertask synchronization,
     he makes it with spinlocks.
 */

struct Model0_tasklet_struct
{
 struct Model0_tasklet_struct *Model0_next;
 unsigned long Model0_state;
 Model0_atomic_t Model0_count;
 void (*func)(unsigned long);
 unsigned long Model0_data;
};
enum
{
 Model0_TASKLET_STATE_SCHED, /* Tasklet is scheduled for execution */
 Model0_TASKLET_STATE_RUN /* Tasklet is running (SMP only) */
};


static inline __attribute__((no_instrument_function)) int Model0_tasklet_trylock(struct Model0_tasklet_struct *Model0_t)
{
 return !Model0_test_and_set_bit(Model0_TASKLET_STATE_RUN, &(Model0_t)->Model0_state);
}

static inline __attribute__((no_instrument_function)) void Model0_tasklet_unlock(struct Model0_tasklet_struct *Model0_t)
{
 __asm__ __volatile__("": : :"memory");
 Model0_clear_bit(Model0_TASKLET_STATE_RUN, &(Model0_t)->Model0_state);
}

static inline __attribute__((no_instrument_function)) void Model0_tasklet_unlock_wait(struct Model0_tasklet_struct *Model0_t)
{
 while ((__builtin_constant_p((Model0_TASKLET_STATE_RUN)) ? Model0_constant_test_bit((Model0_TASKLET_STATE_RUN), (&(Model0_t)->Model0_state)) : Model0_variable_test_bit((Model0_TASKLET_STATE_RUN), (&(Model0_t)->Model0_state)))) { __asm__ __volatile__("": : :"memory"); }
}






extern void Model0___tasklet_schedule(struct Model0_tasklet_struct *Model0_t);

static inline __attribute__((no_instrument_function)) void Model0_tasklet_schedule(struct Model0_tasklet_struct *Model0_t)
{
 if (!Model0_test_and_set_bit(Model0_TASKLET_STATE_SCHED, &Model0_t->Model0_state))
  Model0___tasklet_schedule(Model0_t);
}

extern void Model0___tasklet_hi_schedule(struct Model0_tasklet_struct *Model0_t);

static inline __attribute__((no_instrument_function)) void Model0_tasklet_hi_schedule(struct Model0_tasklet_struct *Model0_t)
{
 if (!Model0_test_and_set_bit(Model0_TASKLET_STATE_SCHED, &Model0_t->Model0_state))
  Model0___tasklet_hi_schedule(Model0_t);
}

extern void Model0___tasklet_hi_schedule_first(struct Model0_tasklet_struct *Model0_t);

/*
 * This version avoids touching any other tasklets. Needed for kmemcheck
 * in order not to take any page faults while enqueueing this tasklet;
 * consider VERY carefully whether you really need this or
 * tasklet_hi_schedule()...
 */
static inline __attribute__((no_instrument_function)) void Model0_tasklet_hi_schedule_first(struct Model0_tasklet_struct *Model0_t)
{
 if (!Model0_test_and_set_bit(Model0_TASKLET_STATE_SCHED, &Model0_t->Model0_state))
  Model0___tasklet_hi_schedule_first(Model0_t);
}


static inline __attribute__((no_instrument_function)) void Model0_tasklet_disable_nosync(struct Model0_tasklet_struct *Model0_t)
{
 Model0_atomic_inc(&Model0_t->Model0_count);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0_tasklet_disable(struct Model0_tasklet_struct *Model0_t)
{
 Model0_tasklet_disable_nosync(Model0_t);
 Model0_tasklet_unlock_wait(Model0_t);
 asm volatile("mfence":::"memory");
}

static inline __attribute__((no_instrument_function)) void Model0_tasklet_enable(struct Model0_tasklet_struct *Model0_t)
{
 __asm__ __volatile__("": : :"memory");
 Model0_atomic_dec(&Model0_t->Model0_count);
}

extern void Model0_tasklet_kill(struct Model0_tasklet_struct *Model0_t);
extern void Model0_tasklet_kill_immediate(struct Model0_tasklet_struct *Model0_t, unsigned int Model0_cpu);
extern void Model0_tasklet_init(struct Model0_tasklet_struct *Model0_t,
    void (*func)(unsigned long), unsigned long Model0_data);

struct Model0_tasklet_hrtimer {
 struct Model0_hrtimer Model0_timer;
 struct Model0_tasklet_struct Model0_tasklet;
 enum Model0_hrtimer_restart (*Model0_function)(struct Model0_hrtimer *);
};

extern void
Model0_tasklet_hrtimer_init(struct Model0_tasklet_hrtimer *Model0_ttimer,
       enum Model0_hrtimer_restart (*Model0_function)(struct Model0_hrtimer *),
       Model0_clockid_t Model0_which_clock, enum Model0_hrtimer_mode Model0_mode);

static inline __attribute__((no_instrument_function))
void Model0_tasklet_hrtimer_start(struct Model0_tasklet_hrtimer *Model0_ttimer, Model0_ktime_t Model0_time,
      const enum Model0_hrtimer_mode Model0_mode)
{
 Model0_hrtimer_start(&Model0_ttimer->Model0_timer, Model0_time, Model0_mode);
}

static inline __attribute__((no_instrument_function))
void Model0_tasklet_hrtimer_cancel(struct Model0_tasklet_hrtimer *Model0_ttimer)
{
 Model0_hrtimer_cancel(&Model0_ttimer->Model0_timer);
 Model0_tasklet_kill(&Model0_ttimer->Model0_tasklet);
}

/*
 * Autoprobing for irqs:
 *
 * probe_irq_on() and probe_irq_off() provide robust primitives
 * for accurate IRQ probing during kernel initialization.  They are
 * reasonably simple to use, are not "fooled" by spurious interrupts,
 * and, unlike other attempts at IRQ probing, they do not get hung on
 * stuck interrupts (such as unused PS2 mouse interfaces on ASUS boards).
 *
 * For reasonably foolproof probing, use them as follows:
 *
 * 1. clear and/or mask the device's internal interrupt.
 * 2. sti();
 * 3. irqs = probe_irq_on();      // "take over" all unassigned idle IRQs
 * 4. enable the device and cause it to trigger an interrupt.
 * 5. wait for the device to interrupt, using non-intrusive polling or a delay.
 * 6. irq = probe_irq_off(irqs);  // get IRQ number, 0=none, negative=multiple
 * 7. service the device to clear its pending interrupt.
 * 8. loop again if paranoia is required.
 *
 * probe_irq_on() returns a mask of allocated irq's.
 *
 * probe_irq_off() takes the mask as a parameter,
 * and returns the irq number which occurred,
 * or zero if none occurred, or a negative irq number
 * if more than one irq occurred.
 */
extern unsigned long Model0_probe_irq_on(void); /* returns 0 on failure */
extern int Model0_probe_irq_off(unsigned long); /* returns 0 or negative on failure */
extern unsigned int Model0_probe_irq_mask(unsigned long); /* returns mask of ISA interrupts */



/* Initialize /proc/irq/ */
extern void Model0_init_irq_proc(void);






struct Model0_seq_file;
int Model0_show_interrupts(struct Model0_seq_file *Model0_p, void *Model0_v);
int Model0_arch_show_interrupts(struct Model0_seq_file *Model0_p, int Model0_prec);

extern int Model0_early_irq_init(void);
extern int Model0_arch_probe_nr_irqs(void);
extern int Model0_arch_early_irq_init(void);




struct Model0_flow_cache_percpu {
 struct Model0_hlist_head *Model0_hash_table;
 int Model0_hash_count;
 Model0_u32 Model0_hash_rnd;
 int Model0_hash_rnd_recalc;
 struct Model0_tasklet_struct Model0_flush_tasklet;
};

struct Model0_flow_cache {
 Model0_u32 Model0_hash_shift;
 struct Model0_flow_cache_percpu *Model0_percpu;
 struct Model0_notifier_block Model0_hotcpu_notifier;
 int Model0_low_watermark;
 int Model0_high_watermark;
 struct Model0_timer_list Model0_rnd_timer;
};

struct Model0_ctl_table_header;

struct Model0_xfrm_policy_hash {
 struct Model0_hlist_head *Model0_table;
 unsigned int Model0_hmask;
 Model0_u8 Model0_dbits4;
 Model0_u8 Model0_sbits4;
 Model0_u8 Model0_dbits6;
 Model0_u8 Model0_sbits6;
};

struct Model0_xfrm_policy_hthresh {
 struct Model0_work_struct Model0_work;
 Model0_seqlock_t Model0_lock;
 Model0_u8 Model0_lbits4;
 Model0_u8 Model0_rbits4;
 Model0_u8 Model0_lbits6;
 Model0_u8 Model0_rbits6;
};

struct Model0_netns_xfrm {
 struct Model0_list_head Model0_state_all;
 /*
	 * Hash table to find appropriate SA towards given target (endpoint of
	 * tunnel or destination of transport mode) allowed by selector.
	 *
	 * Main use is finding SA after policy selected tunnel or transport
	 * mode. Also, it can be used by ah/esp icmp error handler to find
	 * offending SA.
	 */
 struct Model0_hlist_head *Model0_state_bydst;
 struct Model0_hlist_head *Model0_state_bysrc;
 struct Model0_hlist_head *Model0_state_byspi;
 unsigned int Model0_state_hmask;
 unsigned int Model0_state_num;
 struct Model0_work_struct Model0_state_hash_work;
 struct Model0_hlist_head Model0_state_gc_list;
 struct Model0_work_struct Model0_state_gc_work;

 struct Model0_list_head Model0_policy_all;
 struct Model0_hlist_head *Model0_policy_byidx;
 unsigned int Model0_policy_idx_hmask;
 struct Model0_hlist_head Model0_policy_inexact[Model0_XFRM_POLICY_MAX];
 struct Model0_xfrm_policy_hash Model0_policy_bydst[Model0_XFRM_POLICY_MAX];
 unsigned int Model0_policy_count[Model0_XFRM_POLICY_MAX * 2];
 struct Model0_work_struct Model0_policy_hash_work;
 struct Model0_xfrm_policy_hthresh Model0_policy_hthresh;


 struct Model0_sock *Model0_nlsk;
 struct Model0_sock *Model0_nlsk_stash;

 Model0_u32 Model0_sysctl_aevent_etime;
 Model0_u32 Model0_sysctl_aevent_rseqth;
 int Model0_sysctl_larval_drop;
 Model0_u32 Model0_sysctl_acq_expires;

 struct Model0_ctl_table_header *Model0_sysctl_hdr;


 struct Model0_dst_ops Model0_xfrm4_dst_ops;

 struct Model0_dst_ops Model0_xfrm6_dst_ops;

 Model0_spinlock_t Model0_xfrm_state_lock;
 Model0_rwlock_t Model0_xfrm_policy_lock;
 struct Model0_mutex Model0_xfrm_cfg_mutex;

 /* flow cache part */
 struct Model0_flow_cache Model0_flow_cache_global;
 Model0_atomic_t Model0_flow_cache_genid;
 struct Model0_list_head Model0_flow_cache_gc_list;
 Model0_atomic_t Model0_flow_cache_gc_count;
 Model0_spinlock_t Model0_flow_cache_gc_lock;
 struct Model0_work_struct Model0_flow_cache_gc_work;
 struct Model0_work_struct Model0_flow_cache_flush_work;
 struct Model0_mutex Model0_flow_flush_sem;
};
/*
 * mpls in net namespaces
 */




struct Model0_mpls_route;
struct Model0_ctl_table_header;

struct Model0_netns_mpls {
 Model0_size_t Model0_platform_labels;
 struct Model0_mpls_route * *Model0_platform_label;
 struct Model0_ctl_table_header *Model0_ctl;
};



struct Model0_proc_ns_operations;

struct Model0_ns_common {
 Model0_atomic_long_t Model0_stashed;
 const struct Model0_proc_ns_operations *Model0_ops;
 unsigned int Model0_inum;
};



struct Model0_user_namespace;
struct Model0_proc_dir_entry;
struct Model0_net_device;
struct Model0_sock;
struct Model0_ctl_table_header;
struct Model0_net_generic;
struct Model0_sock;
struct Model0_netns_ipvs;





struct Model0_net {
 Model0_atomic_t Model0_passive; /* To decided when the network
						 * namespace should be freed.
						 */
 Model0_atomic_t Model0_count; /* To decided when the network
						 *  namespace should be shut down.
						 */
 Model0_spinlock_t Model0_rules_mod_lock;

 Model0_atomic64_t Model0_cookie_gen;

 struct Model0_list_head Model0_list; /* list of network namespaces */
 struct Model0_list_head Model0_cleanup_list; /* namespaces on death row */
 struct Model0_list_head Model0_exit_list; /* Use only net_mutex */

 struct Model0_user_namespace *Model0_user_ns; /* Owning user namespace */
 Model0_spinlock_t Model0_nsid_lock;
 struct Model0_idr Model0_netns_ids;

 struct Model0_ns_common Model0_ns;

 struct Model0_proc_dir_entry *Model0_proc_net;
 struct Model0_proc_dir_entry *Model0_proc_net_stat;


 struct Model0_ctl_table_set Model0_sysctls;


 struct Model0_sock *Model0_rtnl; /* rtnetlink socket */
 struct Model0_sock *Model0_genl_sock;

 struct Model0_list_head Model0_dev_base_head;
 struct Model0_hlist_head *Model0_dev_name_head;
 struct Model0_hlist_head *Model0_dev_index_head;
 unsigned int Model0_dev_base_seq; /* protected by rtnl_mutex */
 int Model0_ifindex;
 unsigned int Model0_dev_unreg_count;

 /* core fib_rules */
 struct Model0_list_head Model0_rules_ops;


 struct Model0_net_device *Model0_loopback_dev; /* The loopback */
 struct Model0_netns_core Model0_core;
 struct Model0_netns_mib Model0_mib;
 struct Model0_netns_packet Model0_packet;
 struct Model0_netns_unix Model0_unx;
 struct Model0_netns_ipv4 Model0_ipv4;

 struct Model0_netns_ipv6 Model0_ipv6;
 struct Model0_netns_nf Model0_nf;
 struct Model0_netns_xt Model0_xt;

 struct Model0_netns_ct Model0_ct;





 struct Model0_netns_nf_frag Model0_nf_frag;

 struct Model0_sock *Model0_nfnl;
 struct Model0_sock *Model0_nfnl_stash;
 struct Model0_net_generic *Model0_gen;

 /* Note : following structs are cache line aligned */

 struct Model0_netns_xfrm Model0_xfrm;







 struct Model0_sock *Model0_diag_nlsk;
 Model0_atomic_t Model0_fnhe_genid;
};





struct Model0_seq_operations;

struct Model0_seq_file {
 char *Model0_buf;
 Model0_size_t Model0_size;
 Model0_size_t Model0_from;
 Model0_size_t Model0_count;
 Model0_size_t Model0_pad_until;
 Model0_loff_t Model0_index;
 Model0_loff_t Model0_read_pos;
 Model0_u64 Model0_version;
 struct Model0_mutex Model0_lock;
 const struct Model0_seq_operations *Model0_op;
 int Model0_poll_event;
 const struct Model0_file *Model0_file;
 void *Model0_private;
};

struct Model0_seq_operations {
 void * (*Model0_start) (struct Model0_seq_file *Model0_m, Model0_loff_t *Model0_pos);
 void (*Model0_stop) (struct Model0_seq_file *Model0_m, void *Model0_v);
 void * (*Model0_next) (struct Model0_seq_file *Model0_m, void *Model0_v, Model0_loff_t *Model0_pos);
 int (*Model0_show) (struct Model0_seq_file *Model0_m, void *Model0_v);
};



/**
 * seq_has_overflowed - check if the buffer has overflowed
 * @m: the seq_file handle
 *
 * seq_files have a buffer which may overflow. When this happens a larger
 * buffer is reallocated and all the data will be printed again.
 * The overflow state is true when m->count == m->size.
 *
 * Returns true if the buffer received more than it can hold.
 */
static inline __attribute__((no_instrument_function)) bool Model0_seq_has_overflowed(struct Model0_seq_file *Model0_m)
{
 return Model0_m->Model0_count == Model0_m->Model0_size;
}

/**
 * seq_get_buf - get buffer to write arbitrary data to
 * @m: the seq_file handle
 * @bufp: the beginning of the buffer is stored here
 *
 * Return the number of bytes available in the buffer, or zero if
 * there's no space.
 */
static inline __attribute__((no_instrument_function)) Model0_size_t Model0_seq_get_buf(struct Model0_seq_file *Model0_m, char **Model0_bufp)
{
 do { if (__builtin_expect(!!(Model0_m->Model0_count > Model0_m->Model0_size), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/seq_file.h"), "i" (65), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model0_m->Model0_count < Model0_m->Model0_size)
  *Model0_bufp = Model0_m->Model0_buf + Model0_m->Model0_count;
 else
  *Model0_bufp = ((void *)0);

 return Model0_m->Model0_size - Model0_m->Model0_count;
}

/**
 * seq_commit - commit data to the buffer
 * @m: the seq_file handle
 * @num: the number of bytes to commit
 *
 * Commit @num bytes of data written to a buffer previously acquired
 * by seq_buf_get.  To signal an error condition, or that the data
 * didn't fit in the available space, pass a negative @num value.
 */
static inline __attribute__((no_instrument_function)) void Model0_seq_commit(struct Model0_seq_file *Model0_m, int Model0_num)
{
 if (Model0_num < 0) {
  Model0_m->Model0_count = Model0_m->Model0_size;
 } else {
  do { if (__builtin_expect(!!(Model0_m->Model0_count + Model0_num > Model0_m->Model0_size), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/seq_file.h"), "i" (88), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
  Model0_m->Model0_count += Model0_num;
 }
}

/**
 * seq_setwidth - set padding width
 * @m: the seq_file handle
 * @size: the max number of bytes to pad.
 *
 * Call seq_setwidth() for setting max width, then call seq_printf() etc. and
 * finally call seq_pad() to pad the remaining bytes.
 */
static inline __attribute__((no_instrument_function)) void Model0_seq_setwidth(struct Model0_seq_file *Model0_m, Model0_size_t Model0_size)
{
 Model0_m->Model0_pad_until = Model0_m->Model0_count + Model0_size;
}
void Model0_seq_pad(struct Model0_seq_file *Model0_m, char Model0_c);

char *Model0_mangle_path(char *Model0_s, const char *Model0_p, const char *Model0_esc);
int Model0_seq_open(struct Model0_file *, const struct Model0_seq_operations *);
Model0_ssize_t Model0_seq_read(struct Model0_file *, char *, Model0_size_t, Model0_loff_t *);
Model0_loff_t Model0_seq_lseek(struct Model0_file *, Model0_loff_t, int);
int Model0_seq_release(struct Model0_inode *, struct Model0_file *);
int Model0_seq_write(struct Model0_seq_file *Model0_seq, const void *Model0_data, Model0_size_t Model0_len);

__attribute__((format(printf, 2, 0)))
void Model0_seq_vprintf(struct Model0_seq_file *Model0_m, const char *Model0_fmt, Model0_va_list Model0_args);
__attribute__((format(printf, 2, 3)))
void Model0_seq_printf(struct Model0_seq_file *Model0_m, const char *Model0_fmt, ...);
void Model0_seq_putc(struct Model0_seq_file *Model0_m, char Model0_c);
void Model0_seq_puts(struct Model0_seq_file *Model0_m, const char *Model0_s);
void Model0_seq_put_decimal_ull(struct Model0_seq_file *Model0_m, char Model0_delimiter,
    unsigned long long Model0_num);
void Model0_seq_put_decimal_ll(struct Model0_seq_file *Model0_m, char Model0_delimiter, long long Model0_num);
void Model0_seq_escape(struct Model0_seq_file *Model0_m, const char *Model0_s, const char *Model0_esc);

void Model0_seq_hex_dump(struct Model0_seq_file *Model0_m, const char *Model0_prefix_str, int Model0_prefix_type,
    int Model0_rowsize, int Model0_groupsize, const void *Model0_buf, Model0_size_t Model0_len,
    bool Model0_ascii);

int Model0_seq_path(struct Model0_seq_file *, const struct Model0_path *, const char *);
int Model0_seq_file_path(struct Model0_seq_file *, struct Model0_file *, const char *);
int Model0_seq_dentry(struct Model0_seq_file *, struct Model0_dentry *, const char *);
int Model0_seq_path_root(struct Model0_seq_file *Model0_m, const struct Model0_path *Model0_path,
    const struct Model0_path *Model0_root, const char *Model0_esc);

int Model0_single_open(struct Model0_file *, int (*)(struct Model0_seq_file *, void *), void *);
int Model0_single_open_size(struct Model0_file *, int (*)(struct Model0_seq_file *, void *), void *, Model0_size_t);
int Model0_single_release(struct Model0_inode *, struct Model0_file *);
void *Model0___seq_open_private(struct Model0_file *, const struct Model0_seq_operations *, int);
int Model0_seq_open_private(struct Model0_file *, const struct Model0_seq_operations *, int);
int Model0_seq_release_private(struct Model0_inode *, struct Model0_file *);

static inline __attribute__((no_instrument_function)) struct Model0_user_namespace *Model0_seq_user_ns(struct Model0_seq_file *Model0_seq)
{



 extern struct Model0_user_namespace Model0_init_user_ns;
 return &Model0_init_user_ns;

}

/**
 * seq_show_options - display mount options with appropriate escapes.
 * @m: the seq_file handle
 * @name: the mount option name
 * @value: the mount option name's value, can be NULL
 */
static inline __attribute__((no_instrument_function)) void Model0_seq_show_option(struct Model0_seq_file *Model0_m, const char *Model0_name,
       const char *Model0_value)
{
 Model0_seq_putc(Model0_m, ',');
 Model0_seq_escape(Model0_m, Model0_name, ",= \t\n\\");
 if (Model0_value) {
  Model0_seq_putc(Model0_m, '=');
  Model0_seq_escape(Model0_m, Model0_value, ", \t\n\\");
 }
}

/**
 * seq_show_option_n - display mount options with appropriate escapes
 *		       where @value must be a specific length.
 * @m: the seq_file handle
 * @name: the mount option name
 * @value: the mount option name's value, cannot be NULL
 * @length: the length of @value to display
 *
 * This is a macro since this uses "length" to define the size of the
 * stack buffer.
 */
/*
 * Helpers for iteration over list_head-s in seq_files
 */

extern struct Model0_list_head *Model0_seq_list_start(struct Model0_list_head *Model0_head,
  Model0_loff_t Model0_pos);
extern struct Model0_list_head *Model0_seq_list_start_head(struct Model0_list_head *Model0_head,
  Model0_loff_t Model0_pos);
extern struct Model0_list_head *Model0_seq_list_next(void *Model0_v, struct Model0_list_head *Model0_head,
  Model0_loff_t *Model0_ppos);

/*
 * Helpers for iteration over hlist_head-s in seq_files
 */

extern struct Model0_hlist_node *Model0_seq_hlist_start(struct Model0_hlist_head *Model0_head,
       Model0_loff_t Model0_pos);
extern struct Model0_hlist_node *Model0_seq_hlist_start_head(struct Model0_hlist_head *Model0_head,
            Model0_loff_t Model0_pos);
extern struct Model0_hlist_node *Model0_seq_hlist_next(void *Model0_v, struct Model0_hlist_head *Model0_head,
      Model0_loff_t *Model0_ppos);

extern struct Model0_hlist_node *Model0_seq_hlist_start_rcu(struct Model0_hlist_head *Model0_head,
           Model0_loff_t Model0_pos);
extern struct Model0_hlist_node *Model0_seq_hlist_start_head_rcu(struct Model0_hlist_head *Model0_head,
         Model0_loff_t Model0_pos);
extern struct Model0_hlist_node *Model0_seq_hlist_next_rcu(void *Model0_v,
         struct Model0_hlist_head *Model0_head,
         Model0_loff_t *Model0_ppos);

/* Helpers for iterating over per-cpu hlist_head-s in seq_files */
extern struct Model0_hlist_node *Model0_seq_hlist_start_percpu(struct Model0_hlist_head *Model0_head, int *Model0_cpu, Model0_loff_t Model0_pos);

extern struct Model0_hlist_node *Model0_seq_hlist_next_percpu(void *Model0_v, struct Model0_hlist_head *Model0_head, int *Model0_cpu, Model0_loff_t *Model0_pos);

struct Model0_net;
extern struct Model0_net Model0_init_net;

struct Model0_seq_net_private {

 struct Model0_net *Model0_net;

};

int Model0_seq_open_net(struct Model0_inode *, struct Model0_file *,
   const struct Model0_seq_operations *, int);
int Model0_single_open_net(struct Model0_inode *, struct Model0_file *Model0_file,
  int (*Model0_show)(struct Model0_seq_file *, void *));
int Model0_seq_release_net(struct Model0_inode *, struct Model0_file *);
int Model0_single_release_net(struct Model0_inode *, struct Model0_file *);
static inline __attribute__((no_instrument_function)) struct Model0_net *Model0_seq_file_net(struct Model0_seq_file *Model0_seq)
{

 return ((struct Model0_seq_net_private *)Model0_seq->Model0_private)->Model0_net;



}

/* Init's network namespace */
extern struct Model0_net Model0_init_net;


struct Model0_net *Model0_copy_net_ns(unsigned long Model0_flags, struct Model0_user_namespace *Model0_user_ns,
   struct Model0_net *Model0_old_net);
extern struct Model0_list_head Model0_net_namespace_list;

struct Model0_net *Model0_get_net_ns_by_pid(Model0_pid_t Model0_pid);
struct Model0_net *Model0_get_net_ns_by_fd(int Model0_pid);


void Model0_ipx_register_sysctl(void);
void Model0_ipx_unregister_sysctl(void);






void Model0___put_net(struct Model0_net *Model0_net);

static inline __attribute__((no_instrument_function)) struct Model0_net *Model0_get_net(struct Model0_net *Model0_net)
{
 Model0_atomic_inc(&Model0_net->Model0_count);
 return Model0_net;
}

static inline __attribute__((no_instrument_function)) struct Model0_net *Model0_maybe_get_net(struct Model0_net *Model0_net)
{
 /* Used when we know struct net exists but we
	 * aren't guaranteed a previous reference count
	 * exists.  If the reference count is zero this
	 * function fails and returns NULL.
	 */
 if (!Model0_atomic_add_unless((&Model0_net->Model0_count), 1, 0))
  Model0_net = ((void *)0);
 return Model0_net;
}

static inline __attribute__((no_instrument_function)) void Model0_put_net(struct Model0_net *Model0_net)
{
 if (Model0_atomic_dec_and_test(&Model0_net->Model0_count))
  Model0___put_net(Model0_net);
}

static inline __attribute__((no_instrument_function))
int Model0_net_eq(const struct Model0_net *Model0_net1, const struct Model0_net *Model0_net2)
{
 return Model0_net1 == Model0_net2;
}

void Model0_net_drop_ns(void *);
typedef struct {

 struct Model0_net *Model0_net;

} Model0_possible_net_t;

static inline __attribute__((no_instrument_function)) void Model0_write_pnet(Model0_possible_net_t *Model0_pnet, struct Model0_net *Model0_net)
{

 Model0_pnet->Model0_net = Model0_net;

}

static inline __attribute__((no_instrument_function)) struct Model0_net *Model0_read_pnet(const Model0_possible_net_t *Model0_pnet)
{
 return Model0_pnet->Model0_net;

}
int Model0_peernet2id_alloc(struct Model0_net *Model0_net, struct Model0_net *Model0_peer);
int Model0_peernet2id(struct Model0_net *Model0_net, struct Model0_net *Model0_peer);
bool Model0_peernet_has_id(struct Model0_net *Model0_net, struct Model0_net *Model0_peer);
struct Model0_net *Model0_get_net_ns_by_id(struct Model0_net *Model0_net, int Model0_id);

struct Model0_pernet_operations {
 struct Model0_list_head Model0_list;
 int (*Model0_init)(struct Model0_net *Model0_net);
 void (*Model0_exit)(struct Model0_net *Model0_net);
 void (*Model0_exit_batch)(struct Model0_list_head *Model0_net_exit_list);
 int *Model0_id;
 Model0_size_t Model0_size;
};

/*
 * Use these carefully.  If you implement a network device and it
 * needs per network namespace operations use device pernet operations,
 * otherwise use pernet subsys operations.
 *
 * Network interfaces need to be removed from a dying netns _before_
 * subsys notifiers can be called, as most of the network code cleanup
 * (which is done from subsys notifiers) runs with the assumption that
 * dev_remove_pack has been called so no new packets will arrive during
 * and after the cleanup functions have been called.  dev_remove_pack
 * is not per namespace so instead the guarantee of no more packets
 * arriving in a network namespace is provided by ensuring that all
 * network devices and all sockets have left the network namespace
 * before the cleanup methods are called.
 *
 * For the longest time the ipv4 icmp code was registered as a pernet
 * device which caused kernel oops, and panics during network
 * namespace cleanup.   So please don't get this wrong.
 */
int Model0_register_pernet_subsys(struct Model0_pernet_operations *);
void Model0_unregister_pernet_subsys(struct Model0_pernet_operations *);
int Model0_register_pernet_device(struct Model0_pernet_operations *);
void Model0_unregister_pernet_device(struct Model0_pernet_operations *);

struct Model0_ctl_table;
struct Model0_ctl_table_header;


int Model0_net_sysctl_init(void);
struct Model0_ctl_table_header *Model0_register_net_sysctl(struct Model0_net *Model0_net, const char *Model0_path,
          struct Model0_ctl_table *Model0_table);
void Model0_unregister_net_sysctl_table(struct Model0_ctl_table_header *Model0_header);
static inline __attribute__((no_instrument_function)) int Model0_rt_genid_ipv4(struct Model0_net *Model0_net)
{
 return Model0_atomic_read(&Model0_net->Model0_ipv4.Model0_rt_genid);
}

static inline __attribute__((no_instrument_function)) void Model0_rt_genid_bump_ipv4(struct Model0_net *Model0_net)
{
 Model0_atomic_inc(&Model0_net->Model0_ipv4.Model0_rt_genid);
}

extern void (*Model0___fib6_flush_trees)(struct Model0_net *Model0_net);
static inline __attribute__((no_instrument_function)) void Model0_rt_genid_bump_ipv6(struct Model0_net *Model0_net)
{
 if (Model0___fib6_flush_trees)
  Model0___fib6_flush_trees(Model0_net);
}
/* For callers who don't really care about whether it's IPv4 or IPv6 */
static inline __attribute__((no_instrument_function)) void Model0_rt_genid_bump_all(struct Model0_net *Model0_net)
{
 Model0_rt_genid_bump_ipv4(Model0_net);
 Model0_rt_genid_bump_ipv6(Model0_net);
}

static inline __attribute__((no_instrument_function)) int Model0_fnhe_genid(struct Model0_net *Model0_net)
{
 return Model0_atomic_read(&Model0_net->Model0_fnhe_genid);
}

static inline __attribute__((no_instrument_function)) void Model0_fnhe_genid_bump(struct Model0_net *Model0_net)
{
 Model0_atomic_inc(&Model0_net->Model0_fnhe_genid);
}
/*
 * include/net/dsa.h - Driver for Distributed Switch Architecture switch chips
 * Copyright (c) 2008-2009 Marvell Semiconductor
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 */


/*
 * Definitions for talking to the Open Firmware PROM on
 * Power Macintosh and other computers.
 *
 * Copyright (C) 1996-2005 Paul Mackerras.
 *
 * Updates for PPC64 by Peter Bergner & David Engebretsen, IBM Corp.
 * Updates for SPARC64 by David S. Miller
 * Derived from PowerPC and Sparc prom.h files by Stephen Rothwell, IBM Corp.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 */





/*
 * Device tables which are exported to userspace via
 * scripts/mod/file2alias.c.  You must keep that file in sync with this
 * header.
 */







/*
 * UUID/GUID definition
 *
 * Copyright (C) 2010, 2016 Intel Corp.
 *	Huang Ying <ying.huang@intel.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License version
 * 2 as published by the Free Software Foundation;
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 */




/*
 * UUID/GUID definition
 *
 * Copyright (C) 2010, Intel Corp.
 *	Huang Ying <ying.huang@intel.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License version
 * 2 as published by the Free Software Foundation;
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 */







typedef struct {
 __u8 Model0_b[16];
} Model0_uuid_le;

typedef struct {
 __u8 Model0_b[16];
} Model0_uuid_be;

/*
 * The length of a UUID string ("aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee")
 * not including trailing NUL.
 */


static inline __attribute__((no_instrument_function)) int Model0_uuid_le_cmp(const Model0_uuid_le Model0_u1, const Model0_uuid_le Model0_u2)
{
 return Model0_memcmp(&Model0_u1, &Model0_u2, sizeof(Model0_uuid_le));
}

static inline __attribute__((no_instrument_function)) int Model0_uuid_be_cmp(const Model0_uuid_be Model0_u1, const Model0_uuid_be Model0_u2)
{
 return Model0_memcmp(&Model0_u1, &Model0_u2, sizeof(Model0_uuid_be));
}

void Model0_generate_random_uuid(unsigned char Model0_uuid[16]);

extern void Model0_uuid_le_gen(Model0_uuid_le *Model0_u);
extern void Model0_uuid_be_gen(Model0_uuid_be *Model0_u);

bool __attribute__((warn_unused_result)) Model0_uuid_is_valid(const char *Model0_uuid);

extern const Model0_u8 Model0_uuid_le_index[16];
extern const Model0_u8 Model0_uuid_be_index[16];

int Model0_uuid_le_to_bin(const char *Model0_uuid, Model0_uuid_le *Model0_u);
int Model0_uuid_be_to_bin(const char *Model0_uuid, Model0_uuid_be *Model0_u);
typedef unsigned long Model0_kernel_ulong_t;




struct Model0_pci_device_id {
 __u32 Model0_vendor, Model0_device; /* Vendor and device ID or PCI_ANY_ID*/
 __u32 Model0_subvendor, Model0_subdevice; /* Subsystem ID's or PCI_ANY_ID */
 __u32 Model0_class, Model0_class_mask; /* (class,subclass,prog-if) triplet */
 Model0_kernel_ulong_t Model0_driver_data; /* Data private to the driver */
};







struct Model0_ieee1394_device_id {
 __u32 Model0_match_flags;
 __u32 Model0_vendor_id;
 __u32 Model0_model_id;
 __u32 Model0_specifier_id;
 __u32 Model0_version;
 Model0_kernel_ulong_t Model0_driver_data;
};


/*
 * Device table entry for "new style" table-driven USB drivers.
 * User mode code can read these tables to choose which modules to load.
 * Declare the table as a MODULE_DEVICE_TABLE.
 *
 * A probe() parameter will point to a matching entry from this table.
 * Use the driver_info field for each match to hold information tied
 * to that match:  device quirks, etc.
 *
 * Terminate the driver's table with an all-zeroes entry.
 * Use the flag values to control which fields are compared.
 */

/**
 * struct usb_device_id - identifies USB devices for probing and hotplugging
 * @match_flags: Bit mask controlling which of the other fields are used to
 *	match against new devices. Any field except for driver_info may be
 *	used, although some only make sense in conjunction with other fields.
 *	This is usually set by a USB_DEVICE_*() macro, which sets all
 *	other fields in this structure except for driver_info.
 * @idVendor: USB vendor ID for a device; numbers are assigned
 *	by the USB forum to its members.
 * @idProduct: Vendor-assigned product ID.
 * @bcdDevice_lo: Low end of range of vendor-assigned product version numbers.
 *	This is also used to identify individual product versions, for
 *	a range consisting of a single device.
 * @bcdDevice_hi: High end of version number range.  The range of product
 *	versions is inclusive.
 * @bDeviceClass: Class of device; numbers are assigned
 *	by the USB forum.  Products may choose to implement classes,
 *	or be vendor-specific.  Device classes specify behavior of all
 *	the interfaces on a device.
 * @bDeviceSubClass: Subclass of device; associated with bDeviceClass.
 * @bDeviceProtocol: Protocol of device; associated with bDeviceClass.
 * @bInterfaceClass: Class of interface; numbers are assigned
 *	by the USB forum.  Products may choose to implement classes,
 *	or be vendor-specific.  Interface classes specify behavior only
 *	of a given interface; other interfaces may support other classes.
 * @bInterfaceSubClass: Subclass of interface; associated with bInterfaceClass.
 * @bInterfaceProtocol: Protocol of interface; associated with bInterfaceClass.
 * @bInterfaceNumber: Number of interface; composite devices may use
 *	fixed interface numbers to differentiate between vendor-specific
 *	interfaces.
 * @driver_info: Holds information used by the driver.  Usually it holds
 *	a pointer to a descriptor understood by the driver, or perhaps
 *	device flags.
 *
 * In most cases, drivers will create a table of device IDs by using
 * USB_DEVICE(), or similar macros designed for that purpose.
 * They will then export it to userspace using MODULE_DEVICE_TABLE(),
 * and provide it to the USB core through their usb_driver structure.
 *
 * See the usb_match_id() function for information about how matches are
 * performed.  Briefly, you will normally use one of several macros to help
 * construct these entries.  Each entry you provide will either identify
 * one or more specific products, or will identify a class of products
 * which have agreed to behave the same.  You should put the more specific
 * matches towards the beginning of your table, so that driver_info can
 * record quirks of specific products.
 */
struct Model0_usb_device_id {
 /* which fields to match against? */
 Model0___u16 Model0_match_flags;

 /* Used for product specific matches; range is inclusive */
 Model0___u16 Model0_idVendor;
 Model0___u16 Model0_idProduct;
 Model0___u16 Model0_bcdDevice_lo;
 Model0___u16 Model0_bcdDevice_hi;

 /* Used for device class matches */
 __u8 Model0_bDeviceClass;
 __u8 Model0_bDeviceSubClass;
 __u8 Model0_bDeviceProtocol;

 /* Used for interface class matches */
 __u8 Model0_bInterfaceClass;
 __u8 Model0_bInterfaceSubClass;
 __u8 Model0_bInterfaceProtocol;

 /* Used for vendor-specific interface matches */
 __u8 Model0_bInterfaceNumber;

 /* not matched against */
 Model0_kernel_ulong_t Model0_driver_info
  __attribute__((aligned(sizeof(Model0_kernel_ulong_t))));
};

/* Some useful macros to use to create struct usb_device_id */
struct Model0_hid_device_id {
 Model0___u16 Model0_bus;
 Model0___u16 Model0_group;
 __u32 Model0_vendor;
 __u32 Model0_product;
 Model0_kernel_ulong_t Model0_driver_data;
};

/* s390 CCW devices */
struct Model0_ccw_device_id {
 Model0___u16 Model0_match_flags; /* which fields to match against */

 Model0___u16 Model0_cu_type; /* control unit type     */
 Model0___u16 Model0_dev_type; /* device type           */
 __u8 Model0_cu_model; /* control unit model    */
 __u8 Model0_dev_model; /* device model          */

 Model0_kernel_ulong_t Model0_driver_info;
};






/* s390 AP bus devices */
struct Model0_ap_device_id {
 Model0___u16 Model0_match_flags; /* which fields to match against */
 __u8 Model0_dev_type; /* device type */
 Model0_kernel_ulong_t Model0_driver_info;
};



/* s390 css bus devices (subchannels) */
struct Model0_css_device_id {
 __u8 Model0_match_flags;
 __u8 Model0_type; /* subchannel type */
 Model0_kernel_ulong_t Model0_driver_data;
};



struct Model0_acpi_device_id {
 __u8 Model0_id[9];
 Model0_kernel_ulong_t Model0_driver_data;
 __u32 Model0_cls;
 __u32 Model0_cls_msk;
};




struct Model0_pnp_device_id {
 __u8 Model0_id[8];
 Model0_kernel_ulong_t Model0_driver_data;
};

struct Model0_pnp_card_device_id {
 __u8 Model0_id[8];
 Model0_kernel_ulong_t Model0_driver_data;
 struct {
  __u8 Model0_id[8];
 } Model0_devs[8];
};




struct Model0_serio_device_id {
 __u8 Model0_type;
 __u8 Model0_extra;
 __u8 Model0_id;
 __u8 Model0_proto;
};

struct Model0_hda_device_id {
 __u32 Model0_vendor_id;
 __u32 Model0_rev_id;
 __u8 Model0_api_version;
 const char *Model0_name;
 unsigned long Model0_driver_data;
};

/*
 * Struct used for matching a device
 */
struct Model0_of_device_id {
 char Model0_name[32];
 char Model0_type[32];
 char Model0_compatible[128];
 const void *Model0_data;
};

/* VIO */
struct Model0_vio_device_id {
 char Model0_type[32];
 char Model0_compat[32];
};

/* PCMCIA */

struct Model0_pcmcia_device_id {
 Model0___u16 Model0_match_flags;

 Model0___u16 Model0_manf_id;
 Model0___u16 Model0_card_id;

 __u8 Model0_func_id;

 /* for real multi-function devices */
 __u8 Model0_function;

 /* for pseudo multi-function devices */
 __u8 Model0_device_no;

 __u32 Model0_prod_id_hash[4];

 /* not matched against in kernelspace */
 const char * Model0_prod_id[4];

 /* not matched against */
 Model0_kernel_ulong_t Model0_driver_info;
 char * Model0_cisfile;
};
/* Input */
struct Model0_input_device_id {

 Model0_kernel_ulong_t Model0_flags;

 Model0___u16 Model0_bustype;
 Model0___u16 Model0_vendor;
 Model0___u16 Model0_product;
 Model0___u16 Model0_version;

 Model0_kernel_ulong_t Model0_evbit[0x1f / 64 + 1];
 Model0_kernel_ulong_t Model0_keybit[0x2ff / 64 + 1];
 Model0_kernel_ulong_t Model0_relbit[0x0f / 64 + 1];
 Model0_kernel_ulong_t Model0_absbit[0x3f / 64 + 1];
 Model0_kernel_ulong_t Model0_mscbit[0x07 / 64 + 1];
 Model0_kernel_ulong_t Model0_ledbit[0x0f / 64 + 1];
 Model0_kernel_ulong_t Model0_sndbit[0x07 / 64 + 1];
 Model0_kernel_ulong_t Model0_ffbit[0x7f / 64 + 1];
 Model0_kernel_ulong_t Model0_swbit[0x0f / 64 + 1];

 Model0_kernel_ulong_t Model0_driver_info;
};

/* EISA */



/* The EISA signature, in ASCII form, null terminated */
struct Model0_eisa_device_id {
 char Model0_sig[8];
 Model0_kernel_ulong_t Model0_driver_data;
};



struct Model0_parisc_device_id {
 __u8 Model0_hw_type; /* 5 bits used */
 __u8 Model0_hversion_rev; /* 4 bits */
 Model0___u16 Model0_hversion; /* 12 bits */
 __u32 Model0_sversion; /* 20 bits */
};






/* SDIO */



struct Model0_sdio_device_id {
 __u8 Model0_class; /* Standard interface or SDIO_ANY_ID */
 Model0___u16 Model0_vendor; /* Vendor or SDIO_ANY_ID */
 Model0___u16 Model0_device; /* Device ID or SDIO_ANY_ID */
 Model0_kernel_ulong_t Model0_driver_data; /* Data private to the driver */
};

/* SSB core, see drivers/ssb/ */
struct Model0_ssb_device_id {
 Model0___u16 Model0_vendor;
 Model0___u16 Model0_coreid;
 __u8 Model0_revision;
 __u8 Model0___pad;
} __attribute__((packed, aligned(2)));







/* Broadcom's specific AMBA core, see drivers/bcma/ */
struct Model0_bcma_device_id {
 Model0___u16 Model0_manuf;
 Model0___u16 Model0_id;
 __u8 Model0_rev;
 __u8 Model0_class;
} __attribute__((packed,aligned(2)));
struct Model0_virtio_device_id {
 __u32 Model0_device;
 __u32 Model0_vendor;
};


/*
 * For Hyper-V devices we use the device guid as the id.
 */
struct Model0_hv_vmbus_device_id {
 Model0_uuid_le Model0_guid;
 Model0_kernel_ulong_t Model0_driver_data; /* Data private to the driver */
};

/* rpmsg */




struct Model0_rpmsg_device_id {
 char Model0_name[32];
};

/* i2c */




struct Model0_i2c_device_id {
 char Model0_name[20];
 Model0_kernel_ulong_t Model0_driver_data; /* Data private to the driver */
};

/* spi */




struct Model0_spi_device_id {
 char Model0_name[32];
 Model0_kernel_ulong_t Model0_driver_data; /* Data private to the driver */
};




struct Model0_spmi_device_id {
 char Model0_name[32];
 Model0_kernel_ulong_t Model0_driver_data; /* Data private to the driver */
};

/* dmi */
enum Model0_dmi_field {
 Model0_DMI_NONE,
 Model0_DMI_BIOS_VENDOR,
 Model0_DMI_BIOS_VERSION,
 Model0_DMI_BIOS_DATE,
 Model0_DMI_SYS_VENDOR,
 Model0_DMI_PRODUCT_NAME,
 Model0_DMI_PRODUCT_VERSION,
 Model0_DMI_PRODUCT_SERIAL,
 Model0_DMI_PRODUCT_UUID,
 Model0_DMI_BOARD_VENDOR,
 Model0_DMI_BOARD_NAME,
 Model0_DMI_BOARD_VERSION,
 Model0_DMI_BOARD_SERIAL,
 Model0_DMI_BOARD_ASSET_TAG,
 Model0_DMI_CHASSIS_VENDOR,
 Model0_DMI_CHASSIS_TYPE,
 Model0_DMI_CHASSIS_VERSION,
 Model0_DMI_CHASSIS_SERIAL,
 Model0_DMI_CHASSIS_ASSET_TAG,
 Model0_DMI_STRING_MAX,
};

struct Model0_dmi_strmatch {
 unsigned char Model0_slot:7;
 unsigned char Model0_exact_match:1;
 char Model0_substr[79];
};

struct Model0_dmi_system_id {
 int (*Model0_callback)(const struct Model0_dmi_system_id *);
 const char *Model0_ident;
 struct Model0_dmi_strmatch Model0_matches[4];
 void *Model0_driver_data;
};
/*
 * struct dmi_device_id appears during expansion of
 * "MODULE_DEVICE_TABLE(dmi, x)". Compiler doesn't look inside it
 * but this is enough for gcc 3.4.6 to error out:
 *	error: storage size of '__mod_dmi_device_table' isn't known
 */
struct Model0_platform_device_id {
 char Model0_name[20];
 Model0_kernel_ulong_t Model0_driver_data;
};
/**
 * struct mdio_device_id - identifies PHY devices on an MDIO/MII bus
 * @phy_id: The result of
 *     (mdio_read(&MII_PHYSID1) << 16 | mdio_read(&PHYSID2)) & @phy_id_mask
 *     for this PHY type
 * @phy_id_mask: Defines the significant bits of @phy_id.  A value of 0
 *     is used to terminate an array of struct mdio_device_id.
 */
struct Model0_mdio_device_id {
 __u32 Model0_phy_id;
 __u32 Model0_phy_id_mask;
};

struct Model0_zorro_device_id {
 __u32 Model0_id; /* Device ID or ZORRO_WILDCARD */
 Model0_kernel_ulong_t Model0_driver_data; /* Data private to the driver */
};






struct Model0_isapnp_device_id {
 unsigned short Model0_card_vendor, Model0_card_device;
 unsigned short Model0_vendor, Model0_function;
 Model0_kernel_ulong_t Model0_driver_data; /* data private to the driver */
};

/**
 * struct amba_id - identifies a device on an AMBA bus
 * @id: The significant bits if the hardware device ID
 * @mask: Bitmask specifying which bits of the id field are significant when
 *	matching.  A driver binds to a device when ((hardware device ID) & mask)
 *	== id.
 * @data: Private data used by the driver.
 */
struct Model0_amba_id {
 unsigned int Model0_id;
 unsigned int Model0_mask;
 void *Model0_data;
};

/**
 * struct mips_cdmm_device_id - identifies devices in MIPS CDMM bus
 * @type:	Device type identifier.
 */
struct Model0_mips_cdmm_device_id {
 __u8 Model0_type;
};

/*
 * Match x86 CPUs for CPU specific drivers.
 * See documentation of "x86_match_cpu" for details.
 */

/*
 * MODULE_DEVICE_TABLE expects this struct to be called x86cpu_device_id.
 * Although gcc seems to ignore this error, clang fails without this define.
 */

struct Model0_x86_cpu_id {
 Model0___u16 Model0_vendor;
 Model0___u16 Model0_family;
 Model0___u16 Model0_model;
 Model0___u16 Model0_feature; /* bit index */
 Model0_kernel_ulong_t Model0_driver_data;
};
/*
 * Generic table type for matching CPU features.
 * @feature:	the bit number of the feature (0 - 65535)
 */

struct Model0_cpu_feature {
 Model0___u16 Model0_feature;
};



struct Model0_ipack_device_id {
 __u8 format; /* Format version or IPACK_ANY_ID */
 __u32 Model0_vendor; /* Vendor ID or IPACK_ANY_ID */
 __u32 Model0_device; /* Device ID or IPACK_ANY_ID */
};





/**
 * struct mei_cl_device_id - MEI client device identifier
 * @name: helper name
 * @uuid: client uuid
 * @version: client protocol version
 * @driver_info: information used by the driver.
 *
 * identifies mei client device by uuid and name
 */
struct Model0_mei_cl_device_id {
 char Model0_name[32];
 Model0_uuid_le Model0_uuid;
 __u8 Model0_version;
 Model0_kernel_ulong_t Model0_driver_info;
};

/* RapidIO */



/**
 * struct rio_device_id - RIO device identifier
 * @did: RapidIO device ID
 * @vid: RapidIO vendor ID
 * @asm_did: RapidIO assembly device ID
 * @asm_vid: RapidIO assembly vendor ID
 *
 * Identifies a RapidIO device based on both the device/vendor IDs and
 * the assembly device/vendor IDs.
 */
struct Model0_rio_device_id {
 Model0___u16 Model0_did, Model0_vid;
 Model0___u16 Model0_asm_did, Model0_asm_vid;
};

struct Model0_mcb_device_id {
 Model0___u16 Model0_device;
 Model0_kernel_ulong_t Model0_driver_data;
};

struct Model0_ulpi_device_id {
 Model0___u16 Model0_vendor;
 Model0___u16 Model0_product;
 Model0_kernel_ulong_t Model0_driver_data;
};

/**
 * struct fsl_mc_device_id - MC object device identifier
 * @vendor: vendor ID
 * @obj_type: MC object type
 * @ver_major: MC object version major number
 * @ver_minor: MC object version minor number
 *
 * Type of entries in the "device Id" table for MC object devices supported by
 * a MC object device driver. The last entry of the table has vendor set to 0x0
 */
struct Model0_fsl_mc_device_id {
 Model0___u16 Model0_vendor;
 const char Model0_obj_type[16];
};



/*
 * property.h - Unified device property interface.
 *
 * Copyright (C) 2014, Intel Corporation
 * Authors: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
 *          Mika Westerberg <mika.westerberg@linux.intel.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */





/*
 * fwnode.h - Firmware device node object handle type definition.
 *
 * Copyright (C) 2015, Intel Corporation
 * Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */




enum Model0_fwnode_type {
 Model0_FWNODE_INVALID = 0,
 Model0_FWNODE_OF,
 Model0_FWNODE_ACPI,
 Model0_FWNODE_ACPI_DATA,
 Model0_FWNODE_PDATA,
 Model0_FWNODE_IRQCHIP,
};

struct Model0_fwnode_handle {
 enum Model0_fwnode_type Model0_type;
 struct Model0_fwnode_handle *Model0_secondary;
};


struct Model0_device;

enum Model0_dev_prop_type {
 Model0_DEV_PROP_U8,
 Model0_DEV_PROP_U16,
 Model0_DEV_PROP_U32,
 Model0_DEV_PROP_U64,
 Model0_DEV_PROP_STRING,
 Model0_DEV_PROP_MAX,
};

enum Model0_dev_dma_attr {
 Model0_DEV_DMA_NOT_SUPPORTED,
 Model0_DEV_DMA_NON_COHERENT,
 Model0_DEV_DMA_COHERENT,
};

bool Model0_device_property_present(struct Model0_device *Model0_dev, const char *Model0_propname);
int Model0_device_property_read_u8_array(struct Model0_device *Model0_dev, const char *Model0_propname,
      Model0_u8 *Model0_val, Model0_size_t Model0_nval);
int Model0_device_property_read_u16_array(struct Model0_device *Model0_dev, const char *Model0_propname,
       Model0_u16 *Model0_val, Model0_size_t Model0_nval);
int Model0_device_property_read_u32_array(struct Model0_device *Model0_dev, const char *Model0_propname,
       Model0_u32 *Model0_val, Model0_size_t Model0_nval);
int Model0_device_property_read_u64_array(struct Model0_device *Model0_dev, const char *Model0_propname,
       Model0_u64 *Model0_val, Model0_size_t Model0_nval);
int Model0_device_property_read_string_array(struct Model0_device *Model0_dev, const char *Model0_propname,
          const char **Model0_val, Model0_size_t Model0_nval);
int Model0_device_property_read_string(struct Model0_device *Model0_dev, const char *Model0_propname,
    const char **Model0_val);
int Model0_device_property_match_string(struct Model0_device *Model0_dev,
     const char *Model0_propname, const char *Model0_string);

bool Model0_fwnode_property_present(struct Model0_fwnode_handle *Model0_fwnode, const char *Model0_propname);
int Model0_fwnode_property_read_u8_array(struct Model0_fwnode_handle *Model0_fwnode,
      const char *Model0_propname, Model0_u8 *Model0_val,
      Model0_size_t Model0_nval);
int Model0_fwnode_property_read_u16_array(struct Model0_fwnode_handle *Model0_fwnode,
       const char *Model0_propname, Model0_u16 *Model0_val,
       Model0_size_t Model0_nval);
int Model0_fwnode_property_read_u32_array(struct Model0_fwnode_handle *Model0_fwnode,
       const char *Model0_propname, Model0_u32 *Model0_val,
       Model0_size_t Model0_nval);
int Model0_fwnode_property_read_u64_array(struct Model0_fwnode_handle *Model0_fwnode,
       const char *Model0_propname, Model0_u64 *Model0_val,
       Model0_size_t Model0_nval);
int Model0_fwnode_property_read_string_array(struct Model0_fwnode_handle *Model0_fwnode,
          const char *Model0_propname, const char **Model0_val,
          Model0_size_t Model0_nval);
int Model0_fwnode_property_read_string(struct Model0_fwnode_handle *Model0_fwnode,
    const char *Model0_propname, const char **Model0_val);
int Model0_fwnode_property_match_string(struct Model0_fwnode_handle *Model0_fwnode,
     const char *Model0_propname, const char *Model0_string);

struct Model0_fwnode_handle *Model0_device_get_next_child_node(struct Model0_device *Model0_dev,
       struct Model0_fwnode_handle *Model0_child);





struct Model0_fwnode_handle *Model0_device_get_named_child_node(struct Model0_device *Model0_dev,
        const char *Model0_childname);

void Model0_fwnode_handle_put(struct Model0_fwnode_handle *Model0_fwnode);

unsigned int Model0_device_get_child_node_count(struct Model0_device *Model0_dev);

static inline __attribute__((no_instrument_function)) bool Model0_device_property_read_bool(struct Model0_device *Model0_dev,
          const char *Model0_propname)
{
 return Model0_device_property_present(Model0_dev, Model0_propname);
}

static inline __attribute__((no_instrument_function)) int Model0_device_property_read_u8(struct Model0_device *Model0_dev,
       const char *Model0_propname, Model0_u8 *Model0_val)
{
 return Model0_device_property_read_u8_array(Model0_dev, Model0_propname, Model0_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_device_property_read_u16(struct Model0_device *Model0_dev,
        const char *Model0_propname, Model0_u16 *Model0_val)
{
 return Model0_device_property_read_u16_array(Model0_dev, Model0_propname, Model0_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_device_property_read_u32(struct Model0_device *Model0_dev,
        const char *Model0_propname, Model0_u32 *Model0_val)
{
 return Model0_device_property_read_u32_array(Model0_dev, Model0_propname, Model0_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_device_property_read_u64(struct Model0_device *Model0_dev,
        const char *Model0_propname, Model0_u64 *Model0_val)
{
 return Model0_device_property_read_u64_array(Model0_dev, Model0_propname, Model0_val, 1);
}

static inline __attribute__((no_instrument_function)) bool Model0_fwnode_property_read_bool(struct Model0_fwnode_handle *Model0_fwnode,
          const char *Model0_propname)
{
 return Model0_fwnode_property_present(Model0_fwnode, Model0_propname);
}

static inline __attribute__((no_instrument_function)) int Model0_fwnode_property_read_u8(struct Model0_fwnode_handle *Model0_fwnode,
       const char *Model0_propname, Model0_u8 *Model0_val)
{
 return Model0_fwnode_property_read_u8_array(Model0_fwnode, Model0_propname, Model0_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_fwnode_property_read_u16(struct Model0_fwnode_handle *Model0_fwnode,
        const char *Model0_propname, Model0_u16 *Model0_val)
{
 return Model0_fwnode_property_read_u16_array(Model0_fwnode, Model0_propname, Model0_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_fwnode_property_read_u32(struct Model0_fwnode_handle *Model0_fwnode,
        const char *Model0_propname, Model0_u32 *Model0_val)
{
 return Model0_fwnode_property_read_u32_array(Model0_fwnode, Model0_propname, Model0_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_fwnode_property_read_u64(struct Model0_fwnode_handle *Model0_fwnode,
        const char *Model0_propname, Model0_u64 *Model0_val)
{
 return Model0_fwnode_property_read_u64_array(Model0_fwnode, Model0_propname, Model0_val, 1);
}

/**
 * struct property_entry - "Built-in" device property representation.
 * @name: Name of the property.
 * @length: Length of data making up the value.
 * @is_array: True when the property is an array.
 * @is_string: True when property is a string.
 * @pointer: Pointer to the property (an array of items of the given type).
 * @value: Value of the property (when it is a single item of the given type).
 */
struct Model0_property_entry {
 const char *Model0_name;
 Model0_size_t Model0_length;
 bool Model0_is_array;
 bool Model0_is_string;
 union {
  union {
   void *Model0_raw_data;
   Model0_u8 *Model0_u8_data;
   Model0_u16 *Model0_u16_data;
   Model0_u32 *Model0_u32_data;
   Model0_u64 *Model0_u64_data;
   const char **Model0_str;
  } Model0_pointer;
  union {
   unsigned long long Model0_raw_data;
   Model0_u8 Model0_u8_data;
   Model0_u16 Model0_u16_data;
   Model0_u32 Model0_u32_data;
   Model0_u64 Model0_u64_data;
   const char *Model0_str;
  } Model0_value;
 };
};

/*
 * Note: the below four initializers for the anonymous union are carefully
 * crafted to avoid gcc-4.4.4's problems with initialization of anon unions
 * and structs.
 */
int Model0_device_add_properties(struct Model0_device *Model0_dev,
     struct Model0_property_entry *Model0_properties);
void Model0_device_remove_properties(struct Model0_device *Model0_dev);

bool Model0_device_dma_supported(struct Model0_device *Model0_dev);

enum Model0_dev_dma_attr Model0_device_get_dma_attr(struct Model0_device *Model0_dev);

int Model0_device_get_phy_mode(struct Model0_device *Model0_dev);

void *Model0_device_get_mac_address(struct Model0_device *Model0_dev, char *Model0_addr, int Model0_alen);




typedef Model0_u32 Model0_phandle;
typedef Model0_u32 Model0_ihandle;

struct Model0_property {
 char *Model0_name;
 int Model0_length;
 void *Model0_value;
 struct Model0_property *Model0_next;
 unsigned long Model0__flags;
 unsigned int Model0_unique_id;
 struct Model0_bin_attribute Model0_attr;
};





struct Model0_device_node {
 const char *Model0_name;
 const char *Model0_type;
 Model0_phandle Model0_phandle;
 const char *Model0_full_name;
 struct Model0_fwnode_handle Model0_fwnode;

 struct Model0_property *Model0_properties;
 struct Model0_property *Model0_deadprops; /* removed properties */
 struct Model0_device_node *Model0_parent;
 struct Model0_device_node *Model0_child;
 struct Model0_device_node *Model0_sibling;
 struct Model0_kobject Model0_kobj;
 unsigned long Model0__flags;
 void *Model0_data;





};


struct Model0_of_phandle_args {
 struct Model0_device_node *Model0_np;
 int Model0_args_count;
 Model0_uint32_t Model0_args[16];
};

struct Model0_of_phandle_iterator {
 /* Common iterator information */
 const char *Model0_cells_name;
 int Model0_cell_count;
 const struct Model0_device_node *Model0_parent;

 /* List size information */
 const Model0___be32 *Model0_list_end;
 const Model0___be32 *Model0_phandle_end;

 /* Current position state */
 const Model0___be32 *Model0_cur;
 Model0_uint32_t Model0_cur_count;
 Model0_phandle Model0_phandle;
 struct Model0_device_node *Model0_node;
};

struct Model0_of_reconfig_data {
 struct Model0_device_node *Model0_dn;
 struct Model0_property *Model0_prop;
 struct Model0_property *Model0_old_prop;
};

/* initialize a node */
extern struct Model0_kobj_type Model0_of_node_ktype;
static inline __attribute__((no_instrument_function)) void Model0_of_node_init(struct Model0_device_node *Model0_node)
{
 Model0_kobject_init(&Model0_node->Model0_kobj, &Model0_of_node_ktype);
 Model0_node->Model0_fwnode.Model0_type = Model0_FWNODE_OF;
}

/* true when node is initialized */
static inline __attribute__((no_instrument_function)) int Model0_of_node_is_initialized(struct Model0_device_node *Model0_node)
{
 return Model0_node && Model0_node->Model0_kobj.Model0_state_initialized;
}

/* true when node is attached (i.e. present on sysfs) */
static inline __attribute__((no_instrument_function)) int Model0_of_node_is_attached(struct Model0_device_node *Model0_node)
{
 return Model0_node && Model0_node->Model0_kobj.Model0_state_in_sysfs;
}





/* Dummy ref counting routines - to be implemented later */
static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_node_get(struct Model0_device_node *Model0_node)
{
 return Model0_node;
}
static inline __attribute__((no_instrument_function)) void Model0_of_node_put(struct Model0_device_node *Model0_node) { }


/* Pointer for first entry in chain of all nodes. */
extern struct Model0_device_node *Model0_of_root;
extern struct Model0_device_node *Model0_of_chosen;
extern struct Model0_device_node *Model0_of_aliases;
extern struct Model0_device_node *Model0_of_stdout;
extern Model0_raw_spinlock_t Model0_devtree_lock;

/* flag descriptions (need to be visible even when !CONFIG_OF) */
static inline __attribute__((no_instrument_function)) void Model0_of_core_init(void)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_is_of_node(struct Model0_fwnode_handle *Model0_fwnode)
{
 return false;
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_to_of_node(struct Model0_fwnode_handle *Model0_fwnode)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) const char* Model0_of_node_full_name(const struct Model0_device_node *Model0_np)
{
 return "<no-node>";
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_node_by_name(struct Model0_device_node *Model0_from,
 const char *Model0_name)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_node_by_type(struct Model0_device_node *Model0_from,
 const char *Model0_type)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_matching_node_and_match(
 struct Model0_device_node *Model0_from,
 const struct Model0_of_device_id *Model0_matches,
 const struct Model0_of_device_id **Model0_match)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_node_by_path(const char *Model0_path)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_node_opts_by_path(const char *Model0_path,
 const char **Model0_opts)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_node_by_phandle(Model0_phandle Model0_handle)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_get_parent(const struct Model0_device_node *Model0_node)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_get_next_child(
 const struct Model0_device_node *Model0_node, struct Model0_device_node *Model0_prev)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_get_next_available_child(
 const struct Model0_device_node *Model0_node, struct Model0_device_node *Model0_prev)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_node_with_property(
 struct Model0_device_node *Model0_from, const char *Model0_prop_name)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model0_of_have_populated_dt(void)
{
 return false;
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_get_child_by_name(
     const struct Model0_device_node *Model0_node,
     const char *Model0_name)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_of_device_is_compatible(const struct Model0_device_node *Model0_device,
       const char *Model0_name)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) bool Model0_of_device_is_available(const struct Model0_device_node *Model0_device)
{
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model0_of_device_is_big_endian(const struct Model0_device_node *Model0_device)
{
 return false;
}

static inline __attribute__((no_instrument_function)) struct Model0_property *Model0_of_find_property(const struct Model0_device_node *Model0_np,
      const char *Model0_name,
      int *Model0_lenp)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_compatible_node(
      struct Model0_device_node *Model0_from,
      const char *Model0_type,
      const char *Model0_compat)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_count_elems_of_size(const struct Model0_device_node *Model0_np,
   const char *Model0_propname, int Model0_elem_size)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u32_index(const struct Model0_device_node *Model0_np,
   const char *Model0_propname, Model0_u32 Model0_index, Model0_u32 *Model0_out_value)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u8_array(const struct Model0_device_node *Model0_np,
   const char *Model0_propname, Model0_u8 *Model0_out_values, Model0_size_t Model0_sz)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u16_array(const struct Model0_device_node *Model0_np,
   const char *Model0_propname, Model0_u16 *Model0_out_values, Model0_size_t Model0_sz)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u32_array(const struct Model0_device_node *Model0_np,
          const char *Model0_propname,
          Model0_u32 *Model0_out_values, Model0_size_t Model0_sz)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u64_array(const struct Model0_device_node *Model0_np,
          const char *Model0_propname,
          Model0_u64 *Model0_out_values, Model0_size_t Model0_sz)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_string(const struct Model0_device_node *Model0_np,
       const char *Model0_propname,
       const char **Model0_out_string)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_string_helper(const struct Model0_device_node *Model0_np,
       const char *Model0_propname,
       const char **Model0_out_strs, Model0_size_t Model0_sz, int Model0_index)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) const void *Model0_of_get_property(const struct Model0_device_node *Model0_node,
    const char *Model0_name,
    int *Model0_lenp)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_get_cpu_node(int Model0_cpu,
     unsigned int *thread)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u64(const struct Model0_device_node *Model0_np,
           const char *Model0_propname, Model0_u64 *Model0_out_value)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_match_string(const struct Model0_device_node *Model0_np,
        const char *Model0_propname,
        const char *Model0_string)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_parse_phandle(const struct Model0_device_node *Model0_np,
         const char *Model0_phandle_name,
         int Model0_index)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_of_parse_phandle_with_args(const struct Model0_device_node *Model0_np,
          const char *Model0_list_name,
          const char *Model0_cells_name,
          int Model0_index,
          struct Model0_of_phandle_args *Model0_out_args)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_parse_phandle_with_fixed_args(const struct Model0_device_node *Model0_np,
 const char *Model0_list_name, int Model0_cells_count, int Model0_index,
 struct Model0_of_phandle_args *Model0_out_args)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_count_phandle_with_args(struct Model0_device_node *Model0_np,
          const char *Model0_list_name,
          const char *Model0_cells_name)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_phandle_iterator_init(struct Model0_of_phandle_iterator *Model0_it,
        const struct Model0_device_node *Model0_np,
        const char *Model0_list_name,
        const char *Model0_cells_name,
        int Model0_cell_count)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_phandle_iterator_next(struct Model0_of_phandle_iterator *Model0_it)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_phandle_iterator_args(struct Model0_of_phandle_iterator *Model0_it,
        Model0_uint32_t *Model0_args,
        int Model0_size)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_of_alias_get_id(struct Model0_device_node *Model0_np, const char *Model0_stem)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_alias_get_highest_id(const char *Model0_stem)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model0_of_machine_is_compatible(const char *Model0_compat)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) bool Model0_of_console_check(const struct Model0_device_node *Model0_dn, const char *Model0_name, int Model0_index)
{
 return false;
}

static inline __attribute__((no_instrument_function)) const Model0___be32 *Model0_of_prop_next_u32(struct Model0_property *Model0_prop,
  const Model0___be32 *Model0_cur, Model0_u32 *Model0_pu)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) const char *Model0_of_prop_next_string(struct Model0_property *Model0_prop,
  const char *Model0_cur)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_of_node_check_flag(struct Model0_device_node *Model0_n, unsigned long Model0_flag)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_of_node_test_and_set_flag(struct Model0_device_node *Model0_n,
         unsigned long Model0_flag)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_of_node_set_flag(struct Model0_device_node *Model0_n, unsigned long Model0_flag)
{
}

static inline __attribute__((no_instrument_function)) void Model0_of_node_clear_flag(struct Model0_device_node *Model0_n, unsigned long Model0_flag)
{
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_check_flag(struct Model0_property *Model0_p, unsigned long Model0_flag)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_of_property_set_flag(struct Model0_property *Model0_p, unsigned long Model0_flag)
{
}

static inline __attribute__((no_instrument_function)) void Model0_of_property_clear_flag(struct Model0_property *Model0_p, unsigned long Model0_flag)
{
}





/* Default string compare functions, Allow arch asm/prom.h to override */
static inline __attribute__((no_instrument_function)) int Model0_of_node_to_nid(struct Model0_device_node *Model0_device)
{
 return (-1);
}





static inline __attribute__((no_instrument_function)) int Model0_of_numa_init(void)
{
 return -38;
}


static inline __attribute__((no_instrument_function)) struct Model0_device_node *Model0_of_find_matching_node(
 struct Model0_device_node *Model0_from,
 const struct Model0_of_device_id *Model0_matches)
{
 return Model0_of_find_matching_node_and_match(Model0_from, Model0_matches, ((void *)0));
}

/**
 * of_property_count_u8_elems - Count the number of u8 elements in a property
 *
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node and count the number of u8 elements
 * in it. Returns number of elements on sucess, -EINVAL if the property does
 * not exist or its length does not match a multiple of u8 and -ENODATA if the
 * property does not have a value.
 */
static inline __attribute__((no_instrument_function)) int Model0_of_property_count_u8_elems(const struct Model0_device_node *Model0_np,
    const char *Model0_propname)
{
 return Model0_of_property_count_elems_of_size(Model0_np, Model0_propname, sizeof(Model0_u8));
}

/**
 * of_property_count_u16_elems - Count the number of u16 elements in a property
 *
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node and count the number of u16 elements
 * in it. Returns number of elements on sucess, -EINVAL if the property does
 * not exist or its length does not match a multiple of u16 and -ENODATA if the
 * property does not have a value.
 */
static inline __attribute__((no_instrument_function)) int Model0_of_property_count_u16_elems(const struct Model0_device_node *Model0_np,
    const char *Model0_propname)
{
 return Model0_of_property_count_elems_of_size(Model0_np, Model0_propname, sizeof(Model0_u16));
}

/**
 * of_property_count_u32_elems - Count the number of u32 elements in a property
 *
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node and count the number of u32 elements
 * in it. Returns number of elements on sucess, -EINVAL if the property does
 * not exist or its length does not match a multiple of u32 and -ENODATA if the
 * property does not have a value.
 */
static inline __attribute__((no_instrument_function)) int Model0_of_property_count_u32_elems(const struct Model0_device_node *Model0_np,
    const char *Model0_propname)
{
 return Model0_of_property_count_elems_of_size(Model0_np, Model0_propname, sizeof(Model0_u32));
}

/**
 * of_property_count_u64_elems - Count the number of u64 elements in a property
 *
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node and count the number of u64 elements
 * in it. Returns number of elements on sucess, -EINVAL if the property does
 * not exist or its length does not match a multiple of u64 and -ENODATA if the
 * property does not have a value.
 */
static inline __attribute__((no_instrument_function)) int Model0_of_property_count_u64_elems(const struct Model0_device_node *Model0_np,
    const char *Model0_propname)
{
 return Model0_of_property_count_elems_of_size(Model0_np, Model0_propname, sizeof(Model0_u64));
}

/**
 * of_property_read_string_array() - Read an array of strings from a multiple
 * strings property.
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 * @out_strs:	output array of string pointers.
 * @sz:		number of array elements to read.
 *
 * Search for a property in a device tree node and retrieve a list of
 * terminated string values (pointer to data, not a copy) in that property.
 *
 * If @out_strs is NULL, the number of strings in the property is returned.
 */
static inline __attribute__((no_instrument_function)) int Model0_of_property_read_string_array(const struct Model0_device_node *Model0_np,
      const char *Model0_propname, const char **Model0_out_strs,
      Model0_size_t Model0_sz)
{
 return Model0_of_property_read_string_helper(Model0_np, Model0_propname, Model0_out_strs, Model0_sz, 0);
}

/**
 * of_property_count_strings() - Find and return the number of strings from a
 * multiple strings property.
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device tree node and retrieve the number of null
 * terminated string contain in it. Returns the number of strings on
 * success, -EINVAL if the property does not exist, -ENODATA if property
 * does not have a value, and -EILSEQ if the string is not null-terminated
 * within the length of the property data.
 */
static inline __attribute__((no_instrument_function)) int Model0_of_property_count_strings(const struct Model0_device_node *Model0_np,
         const char *Model0_propname)
{
 return Model0_of_property_read_string_helper(Model0_np, Model0_propname, ((void *)0), 0, 0);
}

/**
 * of_property_read_string_index() - Find and read a string from a multiple
 * strings property.
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 * @index:	index of the string in the list of strings
 * @out_string:	pointer to null terminated return string, modified only if
 *		return value is 0.
 *
 * Search for a property in a device tree node and retrieve a null
 * terminated string value (pointer to data, not a copy) in the list of strings
 * contained in that property.
 * Returns 0 on success, -EINVAL if the property does not exist, -ENODATA if
 * property does not have a value, and -EILSEQ if the string is not
 * null-terminated within the length of the property data.
 *
 * The out_string pointer is modified only if a valid string can be decoded.
 */
static inline __attribute__((no_instrument_function)) int Model0_of_property_read_string_index(const struct Model0_device_node *Model0_np,
      const char *Model0_propname,
      int Model0_index, const char **Model0_output)
{
 int Model0_rc = Model0_of_property_read_string_helper(Model0_np, Model0_propname, Model0_output, 1, Model0_index);
 return Model0_rc < 0 ? Model0_rc : 0;
}

/**
 * of_property_read_bool - Findfrom a property
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node.
 * Returns true if the property exists false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_of_property_read_bool(const struct Model0_device_node *Model0_np,
      const char *Model0_propname)
{
 struct Model0_property *Model0_prop = Model0_of_find_property(Model0_np, Model0_propname, ((void *)0));

 return Model0_prop ? true : false;
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u8(const struct Model0_device_node *Model0_np,
           const char *Model0_propname,
           Model0_u8 *Model0_out_value)
{
 return Model0_of_property_read_u8_array(Model0_np, Model0_propname, Model0_out_value, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u16(const struct Model0_device_node *Model0_np,
           const char *Model0_propname,
           Model0_u16 *Model0_out_value)
{
 return Model0_of_property_read_u16_array(Model0_np, Model0_propname, Model0_out_value, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_u32(const struct Model0_device_node *Model0_np,
           const char *Model0_propname,
           Model0_u32 *Model0_out_value)
{
 return Model0_of_property_read_u32_array(Model0_np, Model0_propname, Model0_out_value, 1);
}

static inline __attribute__((no_instrument_function)) int Model0_of_property_read_s32(const struct Model0_device_node *Model0_np,
           const char *Model0_propname,
           Model0_s32 *Model0_out_value)
{
 return Model0_of_property_read_u32(Model0_np, Model0_propname, (Model0_u32*) Model0_out_value);
}
static inline __attribute__((no_instrument_function)) int Model0_of_get_child_count(const struct Model0_device_node *Model0_np)
{
 struct Model0_device_node *Model0_child;
 int Model0_num = 0;

 for (Model0_child = Model0_of_get_next_child(Model0_np, ((void *)0)); Model0_child != ((void *)0); Model0_child = Model0_of_get_next_child(Model0_np, Model0_child))
  Model0_num++;

 return Model0_num;
}

static inline __attribute__((no_instrument_function)) int Model0_of_get_available_child_count(const struct Model0_device_node *Model0_np)
{
 struct Model0_device_node *Model0_child;
 int Model0_num = 0;

 for (Model0_child = Model0_of_get_next_available_child(Model0_np, ((void *)0)); Model0_child != ((void *)0); Model0_child = Model0_of_get_next_available_child(Model0_np, Model0_child))
  Model0_num++;

 return Model0_num;
}
typedef int (*Model0_of_init_fn_2)(struct Model0_device_node *, struct Model0_device_node *);
typedef int (*Model0_of_init_fn_1_ret)(struct Model0_device_node *);
typedef void (*Model0_of_init_fn_1)(struct Model0_device_node *);
/**
 * struct of_changeset_entry	- Holds a changeset entry
 *
 * @node:	list_head for the log list
 * @action:	notifier action
 * @np:		pointer to the device node affected
 * @prop:	pointer to the property affected
 * @old_prop:	hold a pointer to the original property
 *
 * Every modification of the device tree during a changeset
 * is held in a list of of_changeset_entry structures.
 * That way we can recover from a partial application, or we can
 * revert the changeset
 */
struct Model0_of_changeset_entry {
 struct Model0_list_head Model0_node;
 unsigned long Model0_action;
 struct Model0_device_node *Model0_np;
 struct Model0_property *Model0_prop;
 struct Model0_property *Model0_old_prop;
};

/**
 * struct of_changeset - changeset tracker structure
 *
 * @entries:	list_head for the changeset entries
 *
 * changesets are a convenient way to apply bulk changes to the
 * live tree. In case of an error, changes are rolled-back.
 * changesets live on after initial application, and if not
 * destroyed after use, they can be reverted in one single call.
 */
struct Model0_of_changeset {
 struct Model0_list_head Model0_entries;
};

enum Model0_of_reconfig_change {
 Model0_OF_RECONFIG_NO_CHANGE = 0,
 Model0_OF_RECONFIG_CHANGE_ADD,
 Model0_OF_RECONFIG_CHANGE_REMOVE,
};
static inline __attribute__((no_instrument_function)) int Model0_of_reconfig_notifier_register(struct Model0_notifier_block *Model0_nb)
{
 return -22;
}
static inline __attribute__((no_instrument_function)) int Model0_of_reconfig_notifier_unregister(struct Model0_notifier_block *Model0_nb)
{
 return -22;
}
static inline __attribute__((no_instrument_function)) int Model0_of_reconfig_notify(unsigned long Model0_action,
         struct Model0_of_reconfig_data *Model0_arg)
{
 return -22;
}
static inline __attribute__((no_instrument_function)) int Model0_of_reconfig_get_state_change(unsigned long Model0_action,
      struct Model0_of_reconfig_data *Model0_arg)
{
 return -22;
}


/* CONFIG_OF_RESOLVE api */
extern int Model0_of_resolve_phandles(struct Model0_device_node *Model0_tree);

/**
 * of_device_is_system_power_controller - Tells if system-power-controller is found for device_node
 * @np: Pointer to the given device_node
 *
 * return true if present false otherwise
 */
static inline __attribute__((no_instrument_function)) bool Model0_of_device_is_system_power_controller(const struct Model0_device_node *Model0_np)
{
 return Model0_of_property_read_bool(Model0_np, "system-power-controller");
}

/**
 * Overlay support
 */
static inline __attribute__((no_instrument_function)) int Model0_of_overlay_create(struct Model0_device_node *Model0_tree)
{
 return -524;
}

static inline __attribute__((no_instrument_function)) int Model0_of_overlay_destroy(int Model0_id)
{
 return -524;
}

static inline __attribute__((no_instrument_function)) int Model0_of_overlay_destroy_all(void)
{
 return -524;
}
/*
 * Framework and drivers for configuring and reading different PHYs
 * Based on code in sungem_phy.c and gianfar_phy.c
 *
 * Author: Andy Fleming
 *
 * Copyright (c) 2004 Freescale Semiconductor, Inc.
 *
 * This program is free software; you can redistribute  it and/or modify it
 * under  the terms of  the GNU General  Public License as published by the
 * Free Software Foundation;  either version 2 of the  License, or (at your
 * option) any later version.
 *
 */








/*
 * linux/mdio.h: definitions for MDIO (clause 45) transceivers
 * Copyright 2006-2009 Solarflare Communications Inc.
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 as published
 * by the Free Software Foundation, incorporated herein by reference.
 */




/*
 * linux/mdio.h: definitions for MDIO (clause 45) transceivers
 * Copyright 2006-2009 Solarflare Communications Inc.
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 as published
 * by the Free Software Foundation, incorporated herein by reference.
 */






/*
 * linux/mii.h: definitions for MII-compatible transceivers
 * Originally drivers/net/sunhme.h.
 *
 * Copyright (C) 1996, 1999, 2001 David S. Miller (davem@redhat.com)
 */






/*
 * linux/mii.h: definitions for MII-compatible transceivers
 * Originally drivers/net/sunhme.h.
 *
 * Copyright (C) 1996, 1999, 2001 David S. Miller (davem@redhat.com)
 */







/* Generic MII registers. */
/* Basic mode control register. */
/* Basic mode status register. */
/* Advertisement control register. */
/* Link partner ability register. */
/* Expansion register for auto-negotiation. */
/* N-way test register. */




/* 1000BASE-T Control register */





/* 1000BASE-T Status register */





/* Flow control flags */



/* MMD Access Control register fields */






/* This structure is used in all SIOCxMIIxxx ioctl calls */
struct Model0_mii_ioctl_data {
 Model0___u16 Model0_phy_id;
 Model0___u16 Model0_reg_num;
 Model0___u16 Model0_val_in;
 Model0___u16 Model0_val_out;
};

struct Model0_ethtool_cmd;

struct Model0_mii_if_info {
 int Model0_phy_id;
 int Model0_advertising;
 int Model0_phy_id_mask;
 int Model0_reg_num_mask;

 unsigned int Model0_full_duplex : 1; /* is full duplex? */
 unsigned int Model0_force_media : 1; /* is autoneg. disabled? */
 unsigned int Model0_supports_gmii : 1; /* are GMII registers supported? */

 struct Model0_net_device *Model0_dev;
 int (*Model0_mdio_read) (struct Model0_net_device *Model0_dev, int Model0_phy_id, int Model0_location);
 void (*Model0_mdio_write) (struct Model0_net_device *Model0_dev, int Model0_phy_id, int Model0_location, int Model0_val);
};

extern int Model0_mii_link_ok (struct Model0_mii_if_info *Model0_mii);
extern int Model0_mii_nway_restart (struct Model0_mii_if_info *Model0_mii);
extern int Model0_mii_ethtool_gset(struct Model0_mii_if_info *Model0_mii, struct Model0_ethtool_cmd *Model0_ecmd);
extern int Model0_mii_ethtool_sset(struct Model0_mii_if_info *Model0_mii, struct Model0_ethtool_cmd *Model0_ecmd);
extern int Model0_mii_check_gmii_support(struct Model0_mii_if_info *Model0_mii);
extern void Model0_mii_check_link (struct Model0_mii_if_info *Model0_mii);
extern unsigned int Model0_mii_check_media (struct Model0_mii_if_info *Model0_mii,
         unsigned int Model0_ok_to_print,
         unsigned int Model0_init_media);
extern int Model0_generic_mii_ioctl(struct Model0_mii_if_info *Model0_mii_if,
        struct Model0_mii_ioctl_data *Model0_mii_data, int Model0_cmd,
        unsigned int *Model0_duplex_changed);


static inline __attribute__((no_instrument_function)) struct Model0_mii_ioctl_data *Model0_if_mii(struct Model0_ifreq *Model0_rq)
{
 return (struct Model0_mii_ioctl_data *) &Model0_rq->Model0_ifr_ifru;
}

/**
 * mii_nway_result
 * @negotiated: value of MII ANAR and'd with ANLPAR
 *
 * Given a set of MII abilities, check each bit and returns the
 * currently supported media, in the priority order defined by
 * IEEE 802.3u.  We use LPA_xxx constants but note this is not the
 * value of LPA solely, as described above.
 *
 * The one exception to IEEE 802.3u is that 100baseT4 is placed
 * between 100T-full and 100T-half.  If your phy does not support
 * 100T4 this is fine.  If your phy places 100T4 elsewhere in the
 * priority order, you will need to roll your own function.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_mii_nway_result (unsigned int Model0_negotiated)
{
 unsigned int Model0_ret;

 if (Model0_negotiated & 0x0100)
  Model0_ret = 0x0100;
 else if (Model0_negotiated & 0x0200)
  Model0_ret = 0x0200;
 else if (Model0_negotiated & 0x0080)
  Model0_ret = 0x0080;
 else if (Model0_negotiated & 0x0040)
  Model0_ret = 0x0040;
 else
  Model0_ret = 0x0020;

 return Model0_ret;
}

/**
 * mii_duplex
 * @duplex_lock: Non-zero if duplex is locked at full
 * @negotiated: value of MII ANAR and'd with ANLPAR
 *
 * A small helper function for a common case.  Returns one
 * if the media is operating or locked at full duplex, and
 * returns zero otherwise.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_mii_duplex (unsigned int Model0_duplex_lock,
           unsigned int Model0_negotiated)
{
 if (Model0_duplex_lock)
  return 1;
 if (Model0_mii_nway_result(Model0_negotiated) & (0x0040 | 0x0100))
  return 1;
 return 0;
}

/**
 * ethtool_adv_to_mii_adv_t
 * @ethadv: the ethtool advertisement settings
 *
 * A small helper function that translates ethtool advertisement
 * settings to phy autonegotiation advertisements for the
 * MII_ADVERTISE register.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_ethtool_adv_to_mii_adv_t(Model0_u32 Model0_ethadv)
{
 Model0_u32 Model0_result = 0;

 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_10baseT_Half_BIT)))
  Model0_result |= 0x0020;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_10baseT_Full_BIT)))
  Model0_result |= 0x0040;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_100baseT_Half_BIT)))
  Model0_result |= 0x0080;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_100baseT_Full_BIT)))
  Model0_result |= 0x0100;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_Pause_BIT)))
  Model0_result |= 0x0400;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_Asym_Pause_BIT)))
  Model0_result |= 0x0800;

 return Model0_result;
}

/**
 * mii_adv_to_ethtool_adv_t
 * @adv: value of the MII_ADVERTISE register
 *
 * A small helper function that translates MII_ADVERTISE bits
 * to ethtool advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_mii_adv_to_ethtool_adv_t(Model0_u32 Model0_adv)
{
 Model0_u32 Model0_result = 0;

 if (Model0_adv & 0x0020)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_10baseT_Half_BIT));
 if (Model0_adv & 0x0040)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_10baseT_Full_BIT));
 if (Model0_adv & 0x0080)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_100baseT_Half_BIT));
 if (Model0_adv & 0x0100)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_100baseT_Full_BIT));
 if (Model0_adv & 0x0400)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_Pause_BIT));
 if (Model0_adv & 0x0800)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_Asym_Pause_BIT));

 return Model0_result;
}

/**
 * ethtool_adv_to_mii_ctrl1000_t
 * @ethadv: the ethtool advertisement settings
 *
 * A small helper function that translates ethtool advertisement
 * settings to phy autonegotiation advertisements for the
 * MII_CTRL1000 register when in 1000T mode.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_ethtool_adv_to_mii_ctrl1000_t(Model0_u32 Model0_ethadv)
{
 Model0_u32 Model0_result = 0;

 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Half_BIT)))
  Model0_result |= 0x0100;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT)))
  Model0_result |= 0x0200;

 return Model0_result;
}

/**
 * mii_ctrl1000_to_ethtool_adv_t
 * @adv: value of the MII_CTRL1000 register
 *
 * A small helper function that translates MII_CTRL1000
 * bits, when in 1000Base-T mode, to ethtool
 * advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_mii_ctrl1000_to_ethtool_adv_t(Model0_u32 Model0_adv)
{
 Model0_u32 Model0_result = 0;

 if (Model0_adv & 0x0100)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Half_BIT));
 if (Model0_adv & 0x0200)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));

 return Model0_result;
}

/**
 * mii_lpa_to_ethtool_lpa_t
 * @adv: value of the MII_LPA register
 *
 * A small helper function that translates MII_LPA
 * bits, when in 1000Base-T mode, to ethtool
 * LP advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_mii_lpa_to_ethtool_lpa_t(Model0_u32 Model0_lpa)
{
 Model0_u32 Model0_result = 0;

 if (Model0_lpa & 0x4000)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_Autoneg_BIT));

 return Model0_result | Model0_mii_adv_to_ethtool_adv_t(Model0_lpa);
}

/**
 * mii_stat1000_to_ethtool_lpa_t
 * @adv: value of the MII_STAT1000 register
 *
 * A small helper function that translates MII_STAT1000
 * bits, when in 1000Base-T mode, to ethtool
 * advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_mii_stat1000_to_ethtool_lpa_t(Model0_u32 Model0_lpa)
{
 Model0_u32 Model0_result = 0;

 if (Model0_lpa & 0x0400)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Half_BIT));
 if (Model0_lpa & 0x0800)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));

 return Model0_result;
}

/**
 * ethtool_adv_to_mii_adv_x
 * @ethadv: the ethtool advertisement settings
 *
 * A small helper function that translates ethtool advertisement
 * settings to phy autonegotiation advertisements for the
 * MII_CTRL1000 register when in 1000Base-X mode.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_ethtool_adv_to_mii_adv_x(Model0_u32 Model0_ethadv)
{
 Model0_u32 Model0_result = 0;

 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Half_BIT)))
  Model0_result |= 0x0040;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT)))
  Model0_result |= 0x0020;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_Pause_BIT)))
  Model0_result |= 0x0080;
 if (Model0_ethadv & (1UL << (Model0_ETHTOOL_LINK_MODE_Asym_Pause_BIT)))
  Model0_result |= 0x0100;

 return Model0_result;
}

/**
 * mii_adv_to_ethtool_adv_x
 * @adv: value of the MII_CTRL1000 register
 *
 * A small helper function that translates MII_CTRL1000
 * bits, when in 1000Base-X mode, to ethtool
 * advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_mii_adv_to_ethtool_adv_x(Model0_u32 Model0_adv)
{
 Model0_u32 Model0_result = 0;

 if (Model0_adv & 0x0040)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Half_BIT));
 if (Model0_adv & 0x0020)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));
 if (Model0_adv & 0x0080)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_Pause_BIT));
 if (Model0_adv & 0x0100)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_Asym_Pause_BIT));

 return Model0_result;
}

/**
 * mii_lpa_to_ethtool_lpa_x
 * @adv: value of the MII_LPA register
 *
 * A small helper function that translates MII_LPA
 * bits, when in 1000Base-X mode, to ethtool
 * LP advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_mii_lpa_to_ethtool_lpa_x(Model0_u32 Model0_lpa)
{
 Model0_u32 Model0_result = 0;

 if (Model0_lpa & 0x4000)
  Model0_result |= (1UL << (Model0_ETHTOOL_LINK_MODE_Autoneg_BIT));

 return Model0_result | Model0_mii_adv_to_ethtool_adv_x(Model0_lpa);
}

/**
 * mii_advertise_flowctrl - get flow control advertisement flags
 * @cap: Flow control capabilities (FLOW_CTRL_RX, FLOW_CTRL_TX or both)
 */
static inline __attribute__((no_instrument_function)) Model0_u16 Model0_mii_advertise_flowctrl(int Model0_cap)
{
 Model0_u16 Model0_adv = 0;

 if (Model0_cap & 0x02)
  Model0_adv = 0x0400 | 0x0800;
 if (Model0_cap & 0x01)
  Model0_adv ^= 0x0800;

 return Model0_adv;
}

/**
 * mii_resolve_flowctrl_fdx
 * @lcladv: value of MII ADVERTISE register
 * @rmtadv: value of MII LPA register
 *
 * Resolve full duplex flow control as per IEEE 802.3-2005 table 28B-3
 */
static inline __attribute__((no_instrument_function)) Model0_u8 Model0_mii_resolve_flowctrl_fdx(Model0_u16 Model0_lcladv, Model0_u16 Model0_rmtadv)
{
 Model0_u8 Model0_cap = 0;

 if (Model0_lcladv & Model0_rmtadv & 0x0400) {
  Model0_cap = 0x01 | 0x02;
 } else if (Model0_lcladv & Model0_rmtadv & 0x0800) {
  if (Model0_lcladv & 0x0400)
   Model0_cap = 0x02;
  else if (Model0_rmtadv & 0x0400)
   Model0_cap = 0x01;
 }

 return Model0_cap;
}

/* MDIO Manageable Devices (MMDs). */
/* Generic MDIO registers. */
/* Media-dependent registers. */
/* LASI (Link Alarm Status Interrupt) registers, defined by XENPAK MSA. */







/* Control register 1. */
/* Enable extended speed selection */

/* All speed selection bits */
/* 10 Gb/s */

/* 10PASS-TS/2BASE-TL */


/* Status register 1. */
/* Speed register. */
/* Device present registers. */
/* Control register 2. */
/* Status register 2. */
/* Transmit disable register. */






/* Receive signal detect register. */






/* Extended abilities register. */
/* PHY XGXS lane state register. */






/* PMA 10GBASE-T pair swap & polarity */







/* PMA 10GBASE-T TX power register. */


/* PMA 10GBASE-T SNR registers. */
/* Value is SNR margin in dB, clamped to range [-127, 127], plus 0x8000. */



/* PMA 10GBASE-R FEC ability register. */



/* PCS 10GBASE-R/-T status register 1. */


/* PCS 10GBASE-R/-T status register 2. */



/* AN 10GBASE-T control register. */


/* AN 10GBASE-T status register. */
/* EEE Supported/Advertisement/LP Advertisement registers.
 *
 * EEE capability Register (3.20), Advertisement (7.60) and
 * Link partner ability (7.61) registers have and can use the same identical
 * bit masks.
 */


/* Note: the two defines above can be potentially used by the user-land
 * and cannot remove them now.
 * So, we define the new generic MDIO_EEE_100TX and MDIO_EEE_1000T macros
 * using the previous ones (that can be considered obsolete).
 */







/* LASI RX_ALARM control/status registers. */






/* LASI TX_ALARM control/status registers. */







/* LASI control/status registers. */




/* Mapping between MDIO PRTAD/DEVAD and mii_ioctl_data::phy_id */







static inline __attribute__((no_instrument_function)) Model0___u16 Model0_mdio_phy_id_c45(int Model0_prtad, int Model0_devad)
{
 return 0x8000 | (Model0_prtad << 5) | Model0_devad;
}

struct Model0_mii_bus;

/* Multiple levels of nesting are possible. However typically this is
 * limited to nested DSA like layer, a MUX layer, and the normal
 * user. Instead of trying to handle the general case, just define
 * these cases.
 */
enum Model0_mdio_mutex_lock_class {
 Model0_MDIO_MUTEX_NORMAL,
 Model0_MDIO_MUTEX_MUX,
 Model0_MDIO_MUTEX_NESTED,
};

struct Model0_mdio_device {
 struct Model0_device Model0_dev;

 const struct Model0_dev_pm_ops *Model0_pm_ops;
 struct Model0_mii_bus *Model0_bus;

 int (*Model0_bus_match)(struct Model0_device *Model0_dev, struct Model0_device_driver *Model0_drv);
 void (*Model0_device_free)(struct Model0_mdio_device *Model0_mdiodev);
 void (*Model0_device_remove)(struct Model0_mdio_device *Model0_mdiodev);

 /* Bus address of the MDIO device (0-31) */
 int Model0_addr;
 int Model0_flags;
};


/* struct mdio_driver_common: Common to all MDIO drivers */
struct Model0_mdio_driver_common {
 struct Model0_device_driver Model0_driver;
 int Model0_flags;
};




/* struct mdio_driver: Generic MDIO driver */
struct Model0_mdio_driver {
 struct Model0_mdio_driver_common Model0_mdiodrv;

 /*
	 * Called during discovery.  Used to set
	 * up device-specific structures, if any
	 */
 int (*Model0_probe)(struct Model0_mdio_device *Model0_mdiodev);

 /* Clears up any memory if needed */
 void (*Model0_remove)(struct Model0_mdio_device *Model0_mdiodev);
};



void Model0_mdio_device_free(struct Model0_mdio_device *Model0_mdiodev);
struct Model0_mdio_device *Model0_mdio_device_create(struct Model0_mii_bus *Model0_bus, int Model0_addr);
int Model0_mdio_device_register(struct Model0_mdio_device *Model0_mdiodev);
void Model0_mdio_device_remove(struct Model0_mdio_device *Model0_mdiodev);
int Model0_mdio_driver_register(struct Model0_mdio_driver *Model0_drv);
void Model0_mdio_driver_unregister(struct Model0_mdio_driver *Model0_drv);

static inline __attribute__((no_instrument_function)) bool Model0_mdio_phy_id_is_c45(int Model0_phy_id)
{
 return (Model0_phy_id & 0x8000) && !(Model0_phy_id & ~(0x8000 | 0x03e0 | 0x001f));
}

static inline __attribute__((no_instrument_function)) Model0___u16 Model0_mdio_phy_id_prtad(int Model0_phy_id)
{
 return (Model0_phy_id & 0x03e0) >> 5;
}

static inline __attribute__((no_instrument_function)) Model0___u16 Model0_mdio_phy_id_devad(int Model0_phy_id)
{
 return Model0_phy_id & 0x001f;
}

/**
 * struct mdio_if_info - Ethernet controller MDIO interface
 * @prtad: PRTAD of the PHY (%MDIO_PRTAD_NONE if not present/unknown)
 * @mmds: Mask of MMDs expected to be present in the PHY.  This must be
 *	non-zero unless @prtad = %MDIO_PRTAD_NONE.
 * @mode_support: MDIO modes supported.  If %MDIO_SUPPORTS_C22 is set then
 *	MII register access will be passed through with @devad =
 *	%MDIO_DEVAD_NONE.  If %MDIO_EMULATE_C22 is set then access to
 *	commonly used clause 22 registers will be translated into
 *	clause 45 registers.
 * @dev: Net device structure
 * @mdio_read: Register read function; returns value or negative error code
 * @mdio_write: Register write function; returns 0 or negative error code
 */
struct Model0_mdio_if_info {
 int Model0_prtad;
 Model0_u32 Model0_mmds;
 unsigned Model0_mode_support;

 struct Model0_net_device *Model0_dev;
 int (*Model0_mdio_read)(struct Model0_net_device *Model0_dev, int Model0_prtad, int Model0_devad,
    Model0_u16 Model0_addr);
 int (*Model0_mdio_write)(struct Model0_net_device *Model0_dev, int Model0_prtad, int Model0_devad,
     Model0_u16 Model0_addr, Model0_u16 Model0_val);
};







struct Model0_ethtool_cmd;
struct Model0_ethtool_pauseparam;
extern int Model0_mdio45_probe(struct Model0_mdio_if_info *Model0_mdio, int Model0_prtad);
extern int Model0_mdio_set_flag(const struct Model0_mdio_if_info *Model0_mdio,
    int Model0_prtad, int Model0_devad, Model0_u16 Model0_addr, int Model0_mask,
    bool Model0_sense);
extern int Model0_mdio45_links_ok(const struct Model0_mdio_if_info *Model0_mdio, Model0_u32 Model0_mmds);
extern int Model0_mdio45_nway_restart(const struct Model0_mdio_if_info *Model0_mdio);
extern void Model0_mdio45_ethtool_gset_npage(const struct Model0_mdio_if_info *Model0_mdio,
          struct Model0_ethtool_cmd *Model0_ecmd,
          Model0_u32 Model0_npage_adv, Model0_u32 Model0_npage_lpa);

/**
 * mdio45_ethtool_gset - get settings for ETHTOOL_GSET
 * @mdio: MDIO interface
 * @ecmd: Ethtool request structure
 *
 * Since the CSRs for auto-negotiation using next pages are not fully
 * standardised, this function does not attempt to decode them.  Use
 * mdio45_ethtool_gset_npage() to specify advertisement bits from next
 * pages.
 */
static inline __attribute__((no_instrument_function)) void Model0_mdio45_ethtool_gset(const struct Model0_mdio_if_info *Model0_mdio,
           struct Model0_ethtool_cmd *Model0_ecmd)
{
 Model0_mdio45_ethtool_gset_npage(Model0_mdio, Model0_ecmd, 0, 0);
}

extern int Model0_mdio_mii_ioctl(const struct Model0_mdio_if_info *Model0_mdio,
     struct Model0_mii_ioctl_data *Model0_mii_data, int Model0_cmd);

/**
 * mmd_eee_cap_to_ethtool_sup_t
 * @eee_cap: value of the MMD EEE Capability register
 *
 * A small helper function that translates MMD EEE Capability (3.20) bits
 * to ethtool supported settings.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_mmd_eee_cap_to_ethtool_sup_t(Model0_u16 Model0_eee_cap)
{
 Model0_u32 Model0_supported = 0;

 if (Model0_eee_cap & 0x0002)
  Model0_supported |= (1UL << (Model0_ETHTOOL_LINK_MODE_100baseT_Full_BIT));
 if (Model0_eee_cap & 0x0004)
  Model0_supported |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));
 if (Model0_eee_cap & 0x0008)
  Model0_supported |= (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseT_Full_BIT));
 if (Model0_eee_cap & 0x0010)
  Model0_supported |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseKX_Full_BIT));
 if (Model0_eee_cap & 0x0020)
  Model0_supported |= (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseKX4_Full_BIT));
 if (Model0_eee_cap & 0x0040)
  Model0_supported |= (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseKR_Full_BIT));

 return Model0_supported;
}

/**
 * mmd_eee_adv_to_ethtool_adv_t
 * @eee_adv: value of the MMD EEE Advertisement/Link Partner Ability registers
 *
 * A small helper function that translates the MMD EEE Advertisment (7.60)
 * and MMD EEE Link Partner Ability (7.61) bits to ethtool advertisement
 * settings.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_mmd_eee_adv_to_ethtool_adv_t(Model0_u16 Model0_eee_adv)
{
 Model0_u32 Model0_adv = 0;

 if (Model0_eee_adv & 0x0002)
  Model0_adv |= (1UL << (Model0_ETHTOOL_LINK_MODE_100baseT_Full_BIT));
 if (Model0_eee_adv & 0x0004)
  Model0_adv |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));
 if (Model0_eee_adv & 0x0008)
  Model0_adv |= (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseT_Full_BIT));
 if (Model0_eee_adv & 0x0010)
  Model0_adv |= (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseKX_Full_BIT));
 if (Model0_eee_adv & 0x0020)
  Model0_adv |= (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseKX4_Full_BIT));
 if (Model0_eee_adv & 0x0040)
  Model0_adv |= (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseKR_Full_BIT));

 return Model0_adv;
}

/**
 * ethtool_adv_to_mmd_eee_adv_t
 * @adv: the ethtool advertisement settings
 *
 * A small helper function that translates ethtool advertisement settings
 * to EEE advertisements for the MMD EEE Advertisement (7.60) and
 * MMD EEE Link Partner Ability (7.61) registers.
 */
static inline __attribute__((no_instrument_function)) Model0_u16 Model0_ethtool_adv_to_mmd_eee_adv_t(Model0_u32 Model0_adv)
{
 Model0_u16 Model0_reg = 0;

 if (Model0_adv & (1UL << (Model0_ETHTOOL_LINK_MODE_100baseT_Full_BIT)))
  Model0_reg |= 0x0002;
 if (Model0_adv & (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseT_Full_BIT)))
  Model0_reg |= 0x0004;
 if (Model0_adv & (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseT_Full_BIT)))
  Model0_reg |= 0x0008;
 if (Model0_adv & (1UL << (Model0_ETHTOOL_LINK_MODE_1000baseKX_Full_BIT)))
  Model0_reg |= 0x0010;
 if (Model0_adv & (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseKX4_Full_BIT)))
  Model0_reg |= 0x0020;
 if (Model0_adv & (1UL << (Model0_ETHTOOL_LINK_MODE_10000baseKR_Full_BIT)))
  Model0_reg |= 0x0040;

 return Model0_reg;
}

int Model0_mdiobus_read(struct Model0_mii_bus *Model0_bus, int Model0_addr, Model0_u32 Model0_regnum);
int Model0_mdiobus_read_nested(struct Model0_mii_bus *Model0_bus, int Model0_addr, Model0_u32 Model0_regnum);
int Model0_mdiobus_write(struct Model0_mii_bus *Model0_bus, int Model0_addr, Model0_u32 Model0_regnum, Model0_u16 Model0_val);
int Model0_mdiobus_write_nested(struct Model0_mii_bus *Model0_bus, int Model0_addr, Model0_u32 Model0_regnum, Model0_u16 Model0_val);

int Model0_mdiobus_register_device(struct Model0_mdio_device *Model0_mdiodev);
int Model0_mdiobus_unregister_device(struct Model0_mdio_device *Model0_mdiodev);
bool Model0_mdiobus_is_registered_device(struct Model0_mii_bus *Model0_bus, int Model0_addr);
struct Model0_phy_device *Model0_mdiobus_get_phy(struct Model0_mii_bus *Model0_bus, int Model0_addr);

/**
 * module_mdio_driver() - Helper macro for registering mdio drivers
 *
 * Helper macro for MDIO drivers which do not do anything special in module
 * init/exit. Each module may only use this macro once, and calling it
 * replaces module_init() and module_exit().
 */
/*
 * Set phydev->irq to PHY_POLL if interrupts are not supported,
 * or not desired for this PHY.  Set to PHY_IGNORE_INTERRUPT if
 * the attached driver handles the interrupt
 */
/* Interface Mode definitions */
typedef enum {
 Model0_PHY_INTERFACE_MODE_NA,
 Model0_PHY_INTERFACE_MODE_MII,
 Model0_PHY_INTERFACE_MODE_GMII,
 Model0_PHY_INTERFACE_MODE_SGMII,
 Model0_PHY_INTERFACE_MODE_TBI,
 Model0_PHY_INTERFACE_MODE_REVMII,
 Model0_PHY_INTERFACE_MODE_RMII,
 Model0_PHY_INTERFACE_MODE_RGMII,
 Model0_PHY_INTERFACE_MODE_RGMII_ID,
 Model0_PHY_INTERFACE_MODE_RGMII_RXID,
 Model0_PHY_INTERFACE_MODE_RGMII_TXID,
 Model0_PHY_INTERFACE_MODE_RTBI,
 Model0_PHY_INTERFACE_MODE_SMII,
 Model0_PHY_INTERFACE_MODE_XGMII,
 Model0_PHY_INTERFACE_MODE_MOCA,
 Model0_PHY_INTERFACE_MODE_QSGMII,
 Model0_PHY_INTERFACE_MODE_MAX,
} Model0_phy_interface_t;

/**
 * It maps 'enum phy_interface_t' found in include/linux/phy.h
 * into the device tree binding of 'phy-mode', so that Ethernet
 * device driver can get phy interface from device tree.
 */
static inline __attribute__((no_instrument_function)) const char *Model0_phy_modes(Model0_phy_interface_t Model0_interface)
{
 switch (Model0_interface) {
 case Model0_PHY_INTERFACE_MODE_NA:
  return "";
 case Model0_PHY_INTERFACE_MODE_MII:
  return "mii";
 case Model0_PHY_INTERFACE_MODE_GMII:
  return "gmii";
 case Model0_PHY_INTERFACE_MODE_SGMII:
  return "sgmii";
 case Model0_PHY_INTERFACE_MODE_TBI:
  return "tbi";
 case Model0_PHY_INTERFACE_MODE_REVMII:
  return "rev-mii";
 case Model0_PHY_INTERFACE_MODE_RMII:
  return "rmii";
 case Model0_PHY_INTERFACE_MODE_RGMII:
  return "rgmii";
 case Model0_PHY_INTERFACE_MODE_RGMII_ID:
  return "rgmii-id";
 case Model0_PHY_INTERFACE_MODE_RGMII_RXID:
  return "rgmii-rxid";
 case Model0_PHY_INTERFACE_MODE_RGMII_TXID:
  return "rgmii-txid";
 case Model0_PHY_INTERFACE_MODE_RTBI:
  return "rtbi";
 case Model0_PHY_INTERFACE_MODE_SMII:
  return "smii";
 case Model0_PHY_INTERFACE_MODE_XGMII:
  return "xgmii";
 case Model0_PHY_INTERFACE_MODE_MOCA:
  return "moca";
 case Model0_PHY_INTERFACE_MODE_QSGMII:
  return "qsgmii";
 default:
  return "unknown";
 }
}
/* Used when trying to connect to a specific phy (mii bus id:phy device id) */


/*
 * Need to be a little smaller than phydev->dev.bus_id to leave room
 * for the ":%02x"
 */


/* Or MII_ADDR_C45 into regnum for read/write on mii_bus to enable the 21 bit
   IEEE 802.3ae clause 45 addressing mode used by 10GIGE phy chips. */


struct Model0_device;
struct Model0_sk_buff;

/*
 * The Bus class for PHYs.  Devices which provide access to
 * PHYs should register using this structure
 */
struct Model0_mii_bus {
 struct Model0_module *Model0_owner;
 const char *Model0_name;
 char Model0_id[(20 - 3)];
 void *Model0_priv;
 int (*Model0_read)(struct Model0_mii_bus *Model0_bus, int Model0_addr, int Model0_regnum);
 int (*Model0_write)(struct Model0_mii_bus *Model0_bus, int Model0_addr, int Model0_regnum, Model0_u16 Model0_val);
 int (*Model0_reset)(struct Model0_mii_bus *Model0_bus);

 /*
	 * A lock to ensure that only one thing can read/write
	 * the MDIO bus at a time
	 */
 struct Model0_mutex Model0_mdio_lock;

 struct Model0_device *Model0_parent;
 enum {
  Model0_MDIOBUS_ALLOCATED = 1,
  Model0_MDIOBUS_REGISTERED,
  Model0_MDIOBUS_UNREGISTERED,
  Model0_MDIOBUS_RELEASED,
 } Model0_state;
 struct Model0_device Model0_dev;

 /* list of all PHYs on bus */
 struct Model0_mdio_device *Model0_mdio_map[32];

 /* PHY addresses to be ignored when probing */
 Model0_u32 Model0_phy_mask;

 /* PHY addresses to ignore the TA/read failure */
 Model0_u32 Model0_phy_ignore_ta_mask;

 /*
	 * An array of interrupts, each PHY's interrupt at the index
	 * matching its address
	 */
 int Model0_irq[32];
};


struct Model0_mii_bus *Model0_mdiobus_alloc_size(Model0_size_t);
static inline __attribute__((no_instrument_function)) struct Model0_mii_bus *Model0_mdiobus_alloc(void)
{
 return Model0_mdiobus_alloc_size(0);
}

int Model0___mdiobus_register(struct Model0_mii_bus *Model0_bus, struct Model0_module *Model0_owner);

void Model0_mdiobus_unregister(struct Model0_mii_bus *Model0_bus);
void Model0_mdiobus_free(struct Model0_mii_bus *Model0_bus);
struct Model0_mii_bus *Model0_devm_mdiobus_alloc_size(struct Model0_device *Model0_dev, int Model0_sizeof_priv);
static inline __attribute__((no_instrument_function)) struct Model0_mii_bus *Model0_devm_mdiobus_alloc(struct Model0_device *Model0_dev)
{
 return Model0_devm_mdiobus_alloc_size(Model0_dev, 0);
}

void Model0_devm_mdiobus_free(struct Model0_device *Model0_dev, struct Model0_mii_bus *Model0_bus);
struct Model0_phy_device *Model0_mdiobus_scan(struct Model0_mii_bus *Model0_bus, int Model0_addr);




/* PHY state machine states:
 *
 * DOWN: PHY device and driver are not ready for anything.  probe
 * should be called if and only if the PHY is in this state,
 * given that the PHY device exists.
 * - PHY driver probe function will, depending on the PHY, set
 * the state to STARTING or READY
 *
 * STARTING:  PHY device is coming up, and the ethernet driver is
 * not ready.  PHY drivers may set this in the probe function.
 * If they do, they are responsible for making sure the state is
 * eventually set to indicate whether the PHY is UP or READY,
 * depending on the state when the PHY is done starting up.
 * - PHY driver will set the state to READY
 * - start will set the state to PENDING
 *
 * READY: PHY is ready to send and receive packets, but the
 * controller is not.  By default, PHYs which do not implement
 * probe will be set to this state by phy_probe().  If the PHY
 * driver knows the PHY is ready, and the PHY state is STARTING,
 * then it sets this STATE.
 * - start will set the state to UP
 *
 * PENDING: PHY device is coming up, but the ethernet driver is
 * ready.  phy_start will set this state if the PHY state is
 * STARTING.
 * - PHY driver will set the state to UP when the PHY is ready
 *
 * UP: The PHY and attached device are ready to do work.
 * Interrupts should be started here.
 * - timer moves to AN
 *
 * AN: The PHY is currently negotiating the link state.  Link is
 * therefore down for now.  phy_timer will set this state when it
 * detects the state is UP.  config_aneg will set this state
 * whenever called with phydev->autoneg set to AUTONEG_ENABLE.
 * - If autonegotiation finishes, but there's no link, it sets
 *   the state to NOLINK.
 * - If aneg finishes with link, it sets the state to RUNNING,
 *   and calls adjust_link
 * - If autonegotiation did not finish after an arbitrary amount
 *   of time, autonegotiation should be tried again if the PHY
 *   supports "magic" autonegotiation (back to AN)
 * - If it didn't finish, and no magic_aneg, move to FORCING.
 *
 * NOLINK: PHY is up, but not currently plugged in.
 * - If the timer notes that the link comes back, we move to RUNNING
 * - config_aneg moves to AN
 * - phy_stop moves to HALTED
 *
 * FORCING: PHY is being configured with forced settings
 * - if link is up, move to RUNNING
 * - If link is down, we drop to the next highest setting, and
 *   retry (FORCING) after a timeout
 * - phy_stop moves to HALTED
 *
 * RUNNING: PHY is currently up, running, and possibly sending
 * and/or receiving packets
 * - timer will set CHANGELINK if we're polling (this ensures the
 *   link state is polled every other cycle of this state machine,
 *   which makes it every other second)
 * - irq will set CHANGELINK
 * - config_aneg will set AN
 * - phy_stop moves to HALTED
 *
 * CHANGELINK: PHY experienced a change in link state
 * - timer moves to RUNNING if link
 * - timer moves to NOLINK if the link is down
 * - phy_stop moves to HALTED
 *
 * HALTED: PHY is up, but no polling or interrupts are done. Or
 * PHY is in an error state.
 *
 * - phy_start moves to RESUMING
 *
 * RESUMING: PHY was halted, but now wants to run again.
 * - If we are forcing, or aneg is done, timer moves to RUNNING
 * - If aneg is not done, timer moves to AN
 * - phy_stop moves to HALTED
 */
enum Model0_phy_state {
 Model0_PHY_DOWN = 0,
 Model0_PHY_STARTING,
 Model0_PHY_READY,
 Model0_PHY_PENDING,
 Model0_PHY_UP,
 Model0_PHY_AN,
 Model0_PHY_RUNNING,
 Model0_PHY_NOLINK,
 Model0_PHY_FORCING,
 Model0_PHY_CHANGELINK,
 Model0_PHY_HALTED,
 Model0_PHY_RESUMING
};

/**
 * struct phy_c45_device_ids - 802.3-c45 Device Identifiers
 * @devices_in_package: Bit vector of devices present.
 * @device_ids: The device identifer for each present device.
 */
struct Model0_phy_c45_device_ids {
 Model0_u32 Model0_devices_in_package;
 Model0_u32 Model0_device_ids[8];
};

/* phy_device: An instance of a PHY
 *
 * drv: Pointer to the driver for this PHY instance
 * phy_id: UID for this device found during discovery
 * c45_ids: 802.3-c45 Device Identifers if is_c45.
 * is_c45:  Set to true if this phy uses clause 45 addressing.
 * is_internal: Set to true if this phy is internal to a MAC.
 * is_pseudo_fixed_link: Set to true if this phy is an Ethernet switch, etc.
 * has_fixups: Set to true if this phy has fixups/quirks.
 * suspended: Set to true if this phy has been suspended successfully.
 * state: state of the PHY for management purposes
 * dev_flags: Device-specific flags used by the PHY driver.
 * link_timeout: The number of timer firings to wait before the
 * giving up on the current attempt at acquiring a link
 * irq: IRQ number of the PHY's interrupt (-1 if none)
 * phy_timer: The timer for handling the state machine
 * phy_queue: A work_queue for the interrupt
 * attached_dev: The attached enet driver's device instance ptr
 * adjust_link: Callback for the enet controller to respond to
 * changes in the link state.
 *
 * speed, duplex, pause, supported, advertising, lp_advertising,
 * and autoneg are used like in mii_if_info
 *
 * interrupts currently only supports enabled or disabled,
 * but could be changed in the future to support enabling
 * and disabling specific interrupts
 *
 * Contains some infrastructure for polling and interrupt
 * handling, as well as handling shifts in PHY hardware state
 */
struct Model0_phy_device {
 struct Model0_mdio_device Model0_mdio;

 /* Information about the PHY type */
 /* And management functions */
 struct Model0_phy_driver *Model0_drv;

 Model0_u32 Model0_phy_id;

 struct Model0_phy_c45_device_ids Model0_c45_ids;
 bool Model0_is_c45;
 bool Model0_is_internal;
 bool Model0_is_pseudo_fixed_link;
 bool Model0_has_fixups;
 bool Model0_suspended;

 enum Model0_phy_state Model0_state;

 Model0_u32 Model0_dev_flags;

 Model0_phy_interface_t Model0_interface;

 /*
	 * forced speed & duplex (no autoneg)
	 * partner speed & duplex & pause (autoneg)
	 */
 int Model0_speed;
 int Model0_duplex;
 int Model0_pause;
 int Model0_asym_pause;

 /* The most recently read link state */
 int Model0_link;

 /* Enabled Interrupts */
 Model0_u32 Model0_interrupts;

 /* Union of PHY and Attached devices' supported modes */
 /* See mii.h for more info */
 Model0_u32 Model0_supported;
 Model0_u32 Model0_advertising;
 Model0_u32 Model0_lp_advertising;

 int Model0_autoneg;

 int Model0_link_timeout;

 /*
	 * Interrupt number for this PHY
	 * -1 means no interrupt
	 */
 int Model0_irq;

 /* private data pointer */
 /* For use by PHYs to maintain extra state */
 void *Model0_priv;

 /* Interrupt and Polling infrastructure */
 struct Model0_work_struct Model0_phy_queue;
 struct Model0_delayed_work Model0_state_queue;
 Model0_atomic_t Model0_irq_disable;

 struct Model0_mutex Model0_lock;

 struct Model0_net_device *Model0_attached_dev;

 Model0_u8 Model0_mdix;

 void (*Model0_adjust_link)(struct Model0_net_device *Model0_dev);
};



/* struct phy_driver: Driver structure for a particular PHY type
 *
 * driver_data: static driver data
 * phy_id: The result of reading the UID registers of this PHY
 *   type, and ANDing them with the phy_id_mask.  This driver
 *   only works for PHYs with IDs which match this field
 * name: The friendly name of this PHY type
 * phy_id_mask: Defines the important bits of the phy_id
 * features: A list of features (speed, duplex, etc) supported
 *   by this PHY
 * flags: A bitfield defining certain other features this PHY
 *   supports (like interrupts)
 *
 * The drivers must implement config_aneg and read_status.  All
 * other functions are optional. Note that none of these
 * functions should be called from interrupt time.  The goal is
 * for the bus read/write functions to be able to block when the
 * bus transaction is happening, and be freed up by an interrupt
 * (The MPC85xx has this ability, though it is not currently
 * supported in the driver).
 */
struct Model0_phy_driver {
 struct Model0_mdio_driver_common Model0_mdiodrv;
 Model0_u32 Model0_phy_id;
 char *Model0_name;
 unsigned int Model0_phy_id_mask;
 Model0_u32 Model0_features;
 Model0_u32 Model0_flags;
 const void *Model0_driver_data;

 /*
	 * Called to issue a PHY software reset
	 */
 int (*Model0_soft_reset)(struct Model0_phy_device *Model0_phydev);

 /*
	 * Called to initialize the PHY,
	 * including after a reset
	 */
 int (*Model0_config_init)(struct Model0_phy_device *Model0_phydev);

 /*
	 * Called during discovery.  Used to set
	 * up device-specific structures, if any
	 */
 int (*Model0_probe)(struct Model0_phy_device *Model0_phydev);

 /* PHY Power Management */
 int (*Model0_suspend)(struct Model0_phy_device *Model0_phydev);
 int (*Model0_resume)(struct Model0_phy_device *Model0_phydev);

 /*
	 * Configures the advertisement and resets
	 * autonegotiation if phydev->autoneg is on,
	 * forces the speed to the current settings in phydev
	 * if phydev->autoneg is off
	 */
 int (*Model0_config_aneg)(struct Model0_phy_device *Model0_phydev);

 /* Determines the auto negotiation result */
 int (*Model0_aneg_done)(struct Model0_phy_device *Model0_phydev);

 /* Determines the negotiated speed and duplex */
 int (*Model0_read_status)(struct Model0_phy_device *Model0_phydev);

 /* Clears any pending interrupts */
 int (*Model0_ack_interrupt)(struct Model0_phy_device *Model0_phydev);

 /* Enables or disables interrupts */
 int (*Model0_config_intr)(struct Model0_phy_device *Model0_phydev);

 /*
	 * Checks if the PHY generated an interrupt.
	 * For multi-PHY devices with shared PHY interrupt pin
	 */
 int (*Model0_did_interrupt)(struct Model0_phy_device *Model0_phydev);

 /* Clears up any memory if needed */
 void (*Model0_remove)(struct Model0_phy_device *Model0_phydev);

 /* Returns true if this is a suitable driver for the given
	 * phydev.  If NULL, matching is based on phy_id and
	 * phy_id_mask.
	 */
 int (*Model0_match_phy_device)(struct Model0_phy_device *Model0_phydev);

 /* Handles ethtool queries for hardware time stamping. */
 int (*Model0_ts_info)(struct Model0_phy_device *Model0_phydev, struct Model0_ethtool_ts_info *Model0_ti);

 /* Handles SIOCSHWTSTAMP ioctl for hardware time stamping. */
 int (*Model0_hwtstamp)(struct Model0_phy_device *Model0_phydev, struct Model0_ifreq *Model0_ifr);

 /*
	 * Requests a Rx timestamp for 'skb'. If the skb is accepted,
	 * the phy driver promises to deliver it using netif_rx() as
	 * soon as a timestamp becomes available. One of the
	 * PTP_CLASS_ values is passed in 'type'. The function must
	 * return true if the skb is accepted for delivery.
	 */
 bool (*Model0_rxtstamp)(struct Model0_phy_device *Model0_dev, struct Model0_sk_buff *Model0_skb, int Model0_type);

 /*
	 * Requests a Tx timestamp for 'skb'. The phy driver promises
	 * to deliver it using skb_complete_tx_timestamp() as soon as a
	 * timestamp becomes available. One of the PTP_CLASS_ values
	 * is passed in 'type'.
	 */
 void (*Model0_txtstamp)(struct Model0_phy_device *Model0_dev, struct Model0_sk_buff *Model0_skb, int Model0_type);

 /* Some devices (e.g. qnap TS-119P II) require PHY register changes to
	 * enable Wake on LAN, so set_wol is provided to be called in the
	 * ethernet driver's set_wol function. */
 int (*Model0_set_wol)(struct Model0_phy_device *Model0_dev, struct Model0_ethtool_wolinfo *Model0_wol);

 /* See set_wol, but for checking whether Wake on LAN is enabled. */
 void (*Model0_get_wol)(struct Model0_phy_device *Model0_dev, struct Model0_ethtool_wolinfo *Model0_wol);

 /*
	 * Called to inform a PHY device driver when the core is about to
	 * change the link state. This callback is supposed to be used as
	 * fixup hook for drivers that need to take action when the link
	 * state changes. Drivers are by no means allowed to mess with the
	 * PHY device structure in their implementations.
	 */
 void (*Model0_link_change_notify)(struct Model0_phy_device *Model0_dev);

 /* A function provided by a phy specific driver to override the
	 * the PHY driver framework support for reading a MMD register
	 * from the PHY. If not supported, return -1. This function is
	 * optional for PHY specific drivers, if not provided then the
	 * default MMD read function is used by the PHY framework.
	 */
 int (*Model0_read_mmd_indirect)(struct Model0_phy_device *Model0_dev, int Model0_ptrad,
     int Model0_devnum, int Model0_regnum);

 /* A function provided by a phy specific driver to override the
	 * the PHY driver framework support for writing a MMD register
	 * from the PHY. This function is optional for PHY specific drivers,
	 * if not provided then the default MMD read function is used by
	 * the PHY framework.
	 */
 void (*Model0_write_mmd_indirect)(struct Model0_phy_device *Model0_dev, int Model0_ptrad,
       int Model0_devnum, int Model0_regnum, Model0_u32 Model0_val);

 /* Get the size and type of the eeprom contained within a plug-in
	 * module */
 int (*Model0_module_info)(struct Model0_phy_device *Model0_dev,
      struct Model0_ethtool_modinfo *Model0_modinfo);

 /* Get the eeprom information from the plug-in module */
 int (*Model0_module_eeprom)(struct Model0_phy_device *Model0_dev,
        struct Model0_ethtool_eeprom *Model0_ee, Model0_u8 *Model0_data);

 /* Get statistics from the phy using ethtool */
 int (*Model0_get_sset_count)(struct Model0_phy_device *Model0_dev);
 void (*Model0_get_strings)(struct Model0_phy_device *Model0_dev, Model0_u8 *Model0_data);
 void (*Model0_get_stats)(struct Model0_phy_device *Model0_dev,
     struct Model0_ethtool_stats *Model0_stats, Model0_u64 *Model0_data);
};






/* A Structure for boards to register fixups with the PHY Lib */
struct Model0_phy_fixup {
 struct Model0_list_head Model0_list;
 char Model0_bus_id[20];
 Model0_u32 Model0_phy_uid;
 Model0_u32 Model0_phy_uid_mask;
 int (*Model0_run)(struct Model0_phy_device *Model0_phydev);
};

/**
 * phy_read_mmd - Convenience function for reading a register
 * from an MMD on a given PHY.
 * @phydev: The phy_device struct
 * @devad: The MMD to read from
 * @regnum: The register on the MMD to read
 *
 * Same rules as for phy_read();
 */
static inline __attribute__((no_instrument_function)) int Model0_phy_read_mmd(struct Model0_phy_device *Model0_phydev, int Model0_devad, Model0_u32 Model0_regnum)
{
 if (!Model0_phydev->Model0_is_c45)
  return -95;

 return Model0_mdiobus_read(Model0_phydev->Model0_mdio.Model0_bus, Model0_phydev->Model0_mdio.Model0_addr,
       (1<<30) | (Model0_devad << 16) | (Model0_regnum & 0xffff));
}

/**
 * phy_read_mmd_indirect - reads data from the MMD registers
 * @phydev: The PHY device bus
 * @prtad: MMD Address
 * @addr: PHY address on the MII bus
 *
 * Description: it reads data from the MMD registers (clause 22 to access to
 * clause 45) of the specified phy address.
 */
int Model0_phy_read_mmd_indirect(struct Model0_phy_device *Model0_phydev, int Model0_prtad, int Model0_devad);

/**
 * phy_read - Convenience function for reading a given PHY register
 * @phydev: the phy_device struct
 * @regnum: register number to read
 *
 * NOTE: MUST NOT be called from interrupt context,
 * because the bus read/write functions may wait for an interrupt
 * to conclude the operation.
 */
static inline __attribute__((no_instrument_function)) int Model0_phy_read(struct Model0_phy_device *Model0_phydev, Model0_u32 Model0_regnum)
{
 return Model0_mdiobus_read(Model0_phydev->Model0_mdio.Model0_bus, Model0_phydev->Model0_mdio.Model0_addr, Model0_regnum);
}

/**
 * phy_write - Convenience function for writing a given PHY register
 * @phydev: the phy_device struct
 * @regnum: register number to write
 * @val: value to write to @regnum
 *
 * NOTE: MUST NOT be called from interrupt context,
 * because the bus read/write functions may wait for an interrupt
 * to conclude the operation.
 */
static inline __attribute__((no_instrument_function)) int Model0_phy_write(struct Model0_phy_device *Model0_phydev, Model0_u32 Model0_regnum, Model0_u16 Model0_val)
{
 return Model0_mdiobus_write(Model0_phydev->Model0_mdio.Model0_bus, Model0_phydev->Model0_mdio.Model0_addr, Model0_regnum, Model0_val);
}

/**
 * phy_interrupt_is_valid - Convenience function for testing a given PHY irq
 * @phydev: the phy_device struct
 *
 * NOTE: must be kept in sync with addition/removal of PHY_POLL and
 * PHY_IGNORE_INTERRUPT
 */
static inline __attribute__((no_instrument_function)) bool Model0_phy_interrupt_is_valid(struct Model0_phy_device *Model0_phydev)
{
 return Model0_phydev->Model0_irq != -1 && Model0_phydev->Model0_irq != -2;
}

/**
 * phy_is_internal - Convenience function for testing if a PHY is internal
 * @phydev: the phy_device struct
 */
static inline __attribute__((no_instrument_function)) bool Model0_phy_is_internal(struct Model0_phy_device *Model0_phydev)
{
 return Model0_phydev->Model0_is_internal;
}

/**
 * phy_interface_is_rgmii - Convenience function for testing if a PHY interface
 * is RGMII (all variants)
 * @phydev: the phy_device struct
 */
static inline __attribute__((no_instrument_function)) bool Model0_phy_interface_is_rgmii(struct Model0_phy_device *Model0_phydev)
{
 return Model0_phydev->Model0_interface >= Model0_PHY_INTERFACE_MODE_RGMII &&
  Model0_phydev->Model0_interface <= Model0_PHY_INTERFACE_MODE_RGMII_TXID;
};

/*
 * phy_is_pseudo_fixed_link - Convenience function for testing if this
 * PHY is the CPU port facing side of an Ethernet switch, or similar.
 * @phydev: the phy_device struct
 */
static inline __attribute__((no_instrument_function)) bool Model0_phy_is_pseudo_fixed_link(struct Model0_phy_device *Model0_phydev)
{
 return Model0_phydev->Model0_is_pseudo_fixed_link;
}

/**
 * phy_write_mmd - Convenience function for writing a register
 * on an MMD on a given PHY.
 * @phydev: The phy_device struct
 * @devad: The MMD to read from
 * @regnum: The register on the MMD to read
 * @val: value to write to @regnum
 *
 * Same rules as for phy_write();
 */
static inline __attribute__((no_instrument_function)) int Model0_phy_write_mmd(struct Model0_phy_device *Model0_phydev, int Model0_devad,
    Model0_u32 Model0_regnum, Model0_u16 Model0_val)
{
 if (!Model0_phydev->Model0_is_c45)
  return -95;

 Model0_regnum = (1<<30) | ((Model0_devad & 0x1f) << 16) | (Model0_regnum & 0xffff);

 return Model0_mdiobus_write(Model0_phydev->Model0_mdio.Model0_bus, Model0_phydev->Model0_mdio.Model0_addr, Model0_regnum, Model0_val);
}

/**
 * phy_write_mmd_indirect - writes data to the MMD registers
 * @phydev: The PHY device
 * @prtad: MMD Address
 * @devad: MMD DEVAD
 * @data: data to write in the MMD register
 *
 * Description: Write data from the MMD registers of the specified
 * phy address.
 */
void Model0_phy_write_mmd_indirect(struct Model0_phy_device *Model0_phydev, int Model0_prtad,
       int Model0_devad, Model0_u32 Model0_data);

struct Model0_phy_device *Model0_phy_device_create(struct Model0_mii_bus *Model0_bus, int Model0_addr, int Model0_phy_id,
         bool Model0_is_c45,
         struct Model0_phy_c45_device_ids *Model0_c45_ids);
struct Model0_phy_device *Model0_get_phy_device(struct Model0_mii_bus *Model0_bus, int Model0_addr, bool Model0_is_c45);
int Model0_phy_device_register(struct Model0_phy_device *Model0_phy);
void Model0_phy_device_remove(struct Model0_phy_device *Model0_phydev);
int Model0_phy_init_hw(struct Model0_phy_device *Model0_phydev);
int Model0_phy_suspend(struct Model0_phy_device *Model0_phydev);
int Model0_phy_resume(struct Model0_phy_device *Model0_phydev);
struct Model0_phy_device *Model0_phy_attach(struct Model0_net_device *Model0_dev, const char *Model0_bus_id,
         Model0_phy_interface_t Model0_interface);
struct Model0_phy_device *Model0_phy_find_first(struct Model0_mii_bus *Model0_bus);
int Model0_phy_attach_direct(struct Model0_net_device *Model0_dev, struct Model0_phy_device *Model0_phydev,
        Model0_u32 Model0_flags, Model0_phy_interface_t Model0_interface);
int Model0_phy_connect_direct(struct Model0_net_device *Model0_dev, struct Model0_phy_device *Model0_phydev,
         void (*Model0_handler)(struct Model0_net_device *),
         Model0_phy_interface_t Model0_interface);
struct Model0_phy_device *Model0_phy_connect(struct Model0_net_device *Model0_dev, const char *Model0_bus_id,
          void (*Model0_handler)(struct Model0_net_device *),
          Model0_phy_interface_t Model0_interface);
void Model0_phy_disconnect(struct Model0_phy_device *Model0_phydev);
void Model0_phy_detach(struct Model0_phy_device *Model0_phydev);
void Model0_phy_start(struct Model0_phy_device *Model0_phydev);
void Model0_phy_stop(struct Model0_phy_device *Model0_phydev);
int Model0_phy_start_aneg(struct Model0_phy_device *Model0_phydev);

int Model0_phy_stop_interrupts(struct Model0_phy_device *Model0_phydev);

static inline __attribute__((no_instrument_function)) int Model0_phy_read_status(struct Model0_phy_device *Model0_phydev)
{
 return Model0_phydev->Model0_drv->Model0_read_status(Model0_phydev);
}







static inline __attribute__((no_instrument_function)) const char *Model0_phydev_name(const struct Model0_phy_device *Model0_phydev)
{
 return Model0_dev_name(&Model0_phydev->Model0_mdio.Model0_dev);
}

void Model0_phy_attached_print(struct Model0_phy_device *Model0_phydev, const char *Model0_fmt, ...)
 __attribute__((format(printf, 2, 3)));
void Model0_phy_attached_info(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_config_init(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_setup_forced(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_restart_aneg(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_config_aneg(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_aneg_done(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_update_link(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_read_status(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_suspend(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_resume(struct Model0_phy_device *Model0_phydev);
int Model0_genphy_soft_reset(struct Model0_phy_device *Model0_phydev);
void Model0_phy_driver_unregister(struct Model0_phy_driver *Model0_drv);
void Model0_phy_drivers_unregister(struct Model0_phy_driver *Model0_drv, int Model0_n);
int Model0_phy_driver_register(struct Model0_phy_driver *Model0_new_driver, struct Model0_module *Model0_owner);
int Model0_phy_drivers_register(struct Model0_phy_driver *Model0_new_driver, int Model0_n,
    struct Model0_module *Model0_owner);
void Model0_phy_state_machine(struct Model0_work_struct *Model0_work);
void Model0_phy_change(struct Model0_work_struct *Model0_work);
void Model0_phy_mac_interrupt(struct Model0_phy_device *Model0_phydev, int Model0_new_link);
void Model0_phy_start_machine(struct Model0_phy_device *Model0_phydev);
void Model0_phy_stop_machine(struct Model0_phy_device *Model0_phydev);
int Model0_phy_ethtool_sset(struct Model0_phy_device *Model0_phydev, struct Model0_ethtool_cmd *Model0_cmd);
int Model0_phy_ethtool_gset(struct Model0_phy_device *Model0_phydev, struct Model0_ethtool_cmd *Model0_cmd);
int Model0_phy_ethtool_ksettings_get(struct Model0_phy_device *Model0_phydev,
         struct Model0_ethtool_link_ksettings *Model0_cmd);
int Model0_phy_ethtool_ksettings_set(struct Model0_phy_device *Model0_phydev,
         const struct Model0_ethtool_link_ksettings *Model0_cmd);
int Model0_phy_mii_ioctl(struct Model0_phy_device *Model0_phydev, struct Model0_ifreq *Model0_ifr, int Model0_cmd);
int Model0_phy_start_interrupts(struct Model0_phy_device *Model0_phydev);
void Model0_phy_print_status(struct Model0_phy_device *Model0_phydev);
void Model0_phy_device_free(struct Model0_phy_device *Model0_phydev);
int Model0_phy_set_max_speed(struct Model0_phy_device *Model0_phydev, Model0_u32 Model0_max_speed);

int Model0_phy_register_fixup(const char *Model0_bus_id, Model0_u32 Model0_phy_uid, Model0_u32 Model0_phy_uid_mask,
         int (*Model0_run)(struct Model0_phy_device *));
int Model0_phy_register_fixup_for_id(const char *Model0_bus_id,
         int (*Model0_run)(struct Model0_phy_device *));
int Model0_phy_register_fixup_for_uid(Model0_u32 Model0_phy_uid, Model0_u32 Model0_phy_uid_mask,
          int (*Model0_run)(struct Model0_phy_device *));

int Model0_phy_init_eee(struct Model0_phy_device *Model0_phydev, bool Model0_clk_stop_enable);
int Model0_phy_get_eee_err(struct Model0_phy_device *Model0_phydev);
int Model0_phy_ethtool_set_eee(struct Model0_phy_device *Model0_phydev, struct Model0_ethtool_eee *Model0_data);
int Model0_phy_ethtool_get_eee(struct Model0_phy_device *Model0_phydev, struct Model0_ethtool_eee *Model0_data);
int Model0_phy_ethtool_set_wol(struct Model0_phy_device *Model0_phydev, struct Model0_ethtool_wolinfo *Model0_wol);
void Model0_phy_ethtool_get_wol(struct Model0_phy_device *Model0_phydev,
    struct Model0_ethtool_wolinfo *Model0_wol);
int Model0_phy_ethtool_get_link_ksettings(struct Model0_net_device *Model0_ndev,
       struct Model0_ethtool_link_ksettings *Model0_cmd);
int Model0_phy_ethtool_set_link_ksettings(struct Model0_net_device *Model0_ndev,
       const struct Model0_ethtool_link_ksettings *Model0_cmd);

int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_mdio_bus_init(void);
void Model0_mdio_bus_exit(void);

extern struct Model0_bus_type Model0_mdio_bus_type;

/**
 * module_phy_driver() - Helper macro for registering PHY drivers
 * @__phy_drivers: array of PHY drivers to register
 *
 * Helper macro for PHY drivers which do not do anything special in module
 * init/exit. Each module may only use this macro once, and calling it
 * replaces module_init() and module_exit().
 */



struct Model0_fixed_phy_status {
 int Model0_link;
 int Model0_speed;
 int Model0_duplex;
 int Model0_pause;
 int Model0_asym_pause;
};

struct Model0_device_node;
static inline __attribute__((no_instrument_function)) int Model0_fixed_phy_add(unsigned int Model0_irq, int Model0_phy_id,
    struct Model0_fixed_phy_status *Model0_status,
    int Model0_link_gpio)
{
 return -19;
}
static inline __attribute__((no_instrument_function)) struct Model0_phy_device *Model0_fixed_phy_register(unsigned int Model0_irq,
      struct Model0_fixed_phy_status *Model0_status,
      int Model0_gpio_link,
      struct Model0_device_node *Model0_np)
{
 return Model0_ERR_PTR(-19);
}
static inline __attribute__((no_instrument_function)) void Model0_fixed_phy_unregister(struct Model0_phy_device *Model0_phydev)
{
}
static inline __attribute__((no_instrument_function)) int Model0_fixed_phy_set_link_update(struct Model0_phy_device *Model0_phydev,
   int (*Model0_link_update)(struct Model0_net_device *,
        struct Model0_fixed_phy_status *))
{
 return -19;
}
static inline __attribute__((no_instrument_function)) int Model0_fixed_phy_update_state(struct Model0_phy_device *Model0_phydev,
      const struct Model0_fixed_phy_status *Model0_status,
      const struct Model0_fixed_phy_status *Model0_changed)
{
 return -19;
}


enum Model0_dsa_tag_protocol {
 Model0_DSA_TAG_PROTO_NONE = 0,
 Model0_DSA_TAG_PROTO_DSA,
 Model0_DSA_TAG_PROTO_TRAILER,
 Model0_DSA_TAG_PROTO_EDSA,
 Model0_DSA_TAG_PROTO_BRCM,
 Model0_DSA_TAG_LAST, /* MUST BE LAST */
};






struct Model0_dsa_chip_data {
 /*
	 * How to access the switch configuration registers.
	 */
 struct Model0_device *Model0_host_dev;
 int Model0_sw_addr;

 /* set to size of eeprom if supported by the switch */
 int Model0_eeprom_len;

 /* Device tree node pointer for this specific switch chip
	 * used during switch setup in case additional properties
	 * and resources needs to be used
	 */
 struct Model0_device_node *Model0_of_node;

 /*
	 * The names of the switch's ports.  Use "cpu" to
	 * designate the switch port that the cpu is connected to,
	 * "dsa" to indicate that this port is a DSA link to
	 * another switch, NULL to indicate the port is unused,
	 * or any other string to indicate this is a physical port.
	 */
 char *Model0_port_names[12];
 struct Model0_device_node *Model0_port_dn[12];

 /*
	 * An array of which element [a] indicates which port on this
	 * switch should be used to send packets to that are destined
	 * for switch a. Can be NULL if there is only one switch chip.
	 */
 Model0_s8 Model0_rtable[4];
};

struct Model0_dsa_platform_data {
 /*
	 * Reference to a Linux network interface that connects
	 * to the root switch chip of the tree.
	 */
 struct Model0_device *Model0_netdev;
 struct Model0_net_device *Model0_of_netdev;

 /*
	 * Info structs describing each of the switch chips
	 * connected via this network interface.
	 */
 int Model0_nr_chips;
 struct Model0_dsa_chip_data *Model0_chip;
};

struct Model0_packet_type;

struct Model0_dsa_switch_tree {
 struct Model0_list_head Model0_list;

 /* Tree identifier */
 Model0_u32 Model0_tree;

 /* Number of switches attached to this tree */
 struct Model0_kref Model0_refcount;

 /* Has this tree been applied to the hardware? */
 bool Model0_applied;

 /*
	 * Configuration data for the platform device that owns
	 * this dsa switch tree instance.
	 */
 struct Model0_dsa_platform_data *Model0_pd;

 /*
	 * Reference to network device to use, and which tagging
	 * protocol to use.
	 */
 struct Model0_net_device *Model0_master_netdev;
 int (*Model0_rcv)(struct Model0_sk_buff *Model0_skb,
           struct Model0_net_device *Model0_dev,
           struct Model0_packet_type *Model0_pt,
           struct Model0_net_device *Model0_orig_dev);

 /*
	 * Original copy of the master netdev ethtool_ops
	 */
 struct Model0_ethtool_ops Model0_master_ethtool_ops;
 const struct Model0_ethtool_ops *Model0_master_orig_ethtool_ops;

 /*
	 * The switch and port to which the CPU is attached.
	 */
 Model0_s8 Model0_cpu_switch;
 Model0_s8 Model0_cpu_port;

 /*
	 * Data for the individual switch chips.
	 */
 struct Model0_dsa_switch *Model0_ds[4];

 /*
	 * Tagging protocol operations for adding and removing an
	 * encapsulation tag.
	 */
 const struct Model0_dsa_device_ops *Model0_tag_ops;
};

struct Model0_dsa_port {
 struct Model0_net_device *Model0_netdev;
 struct Model0_device_node *Model0_dn;
 unsigned int Model0_ageing_time;
};

struct Model0_dsa_switch {
 struct Model0_device *Model0_dev;

 /*
	 * Parent switch tree, and switch index.
	 */
 struct Model0_dsa_switch_tree *Model0_dst;
 int Model0_index;

 /*
	 * Give the switch driver somewhere to hang its private data
	 * structure.
	 */
 void *Model0_priv;

 /*
	 * Configuration data for this switch.
	 */
 struct Model0_dsa_chip_data *Model0_cd;

 /*
	 * The used switch driver.
	 */
 struct Model0_dsa_switch_driver *Model0_drv;

 /*
	 * An array of which element [a] indicates which port on this
	 * switch should be used to send packets to that are destined
	 * for switch a. Can be NULL if there is only one switch chip.
	 */
 Model0_s8 Model0_rtable[4];
 /*
	 * The lower device this switch uses to talk to the host
	 */
 struct Model0_net_device *Model0_master_netdev;

 /*
	 * Slave mii_bus and devices for the individual ports.
	 */
 Model0_u32 Model0_dsa_port_mask;
 Model0_u32 Model0_cpu_port_mask;
 Model0_u32 Model0_enabled_port_mask;
 Model0_u32 Model0_phys_mii_mask;
 struct Model0_dsa_port Model0_ports[12];
 struct Model0_mii_bus *Model0_slave_mii_bus;
};

static inline __attribute__((no_instrument_function)) bool Model0_dsa_is_cpu_port(struct Model0_dsa_switch *Model0_ds, int Model0_p)
{
 return !!(Model0_ds->Model0_index == Model0_ds->Model0_dst->Model0_cpu_switch && Model0_p == Model0_ds->Model0_dst->Model0_cpu_port);
}

static inline __attribute__((no_instrument_function)) bool Model0_dsa_is_dsa_port(struct Model0_dsa_switch *Model0_ds, int Model0_p)
{
 return !!((Model0_ds->Model0_dsa_port_mask) & (1 << Model0_p));
}

static inline __attribute__((no_instrument_function)) bool Model0_dsa_is_port_initialized(struct Model0_dsa_switch *Model0_ds, int Model0_p)
{
 return Model0_ds->Model0_enabled_port_mask & (1 << Model0_p) && Model0_ds->Model0_ports[Model0_p].Model0_netdev;
}

static inline __attribute__((no_instrument_function)) Model0_u8 Model0_dsa_upstream_port(struct Model0_dsa_switch *Model0_ds)
{
 struct Model0_dsa_switch_tree *Model0_dst = Model0_ds->Model0_dst;

 /*
	 * If this is the root switch (i.e. the switch that connects
	 * to the CPU), return the cpu port number on this switch.
	 * Else return the (DSA) port number that connects to the
	 * switch that is one hop closer to the cpu.
	 */
 if (Model0_dst->Model0_cpu_switch == Model0_ds->Model0_index)
  return Model0_dst->Model0_cpu_port;
 else
  return Model0_ds->Model0_rtable[Model0_dst->Model0_cpu_switch];
}

struct Model0_switchdev_trans;
struct Model0_switchdev_obj;
struct Model0_switchdev_obj_port_fdb;
struct Model0_switchdev_obj_port_vlan;

struct Model0_dsa_switch_driver {
 struct Model0_list_head Model0_list;

 enum Model0_dsa_tag_protocol Model0_tag_protocol;

 /*
	 * Probing and setup.
	 */
 const char *(*Model0_probe)(struct Model0_device *Model0_dsa_dev,
      struct Model0_device *Model0_host_dev, int Model0_sw_addr,
      void **Model0_priv);
 int (*Model0_setup)(struct Model0_dsa_switch *Model0_ds);
 int (*Model0_set_addr)(struct Model0_dsa_switch *Model0_ds, Model0_u8 *Model0_addr);
 Model0_u32 (*Model0_get_phy_flags)(struct Model0_dsa_switch *Model0_ds, int Model0_port);

 /*
	 * Access to the switch's PHY registers.
	 */
 int (*Model0_phy_read)(struct Model0_dsa_switch *Model0_ds, int Model0_port, int Model0_regnum);
 int (*Model0_phy_write)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
        int Model0_regnum, Model0_u16 Model0_val);

 /*
	 * Link state adjustment (called from libphy)
	 */
 void (*Model0_adjust_link)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
    struct Model0_phy_device *Model0_phydev);
 void (*Model0_fixed_link_update)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
    struct Model0_fixed_phy_status *Model0_st);

 /*
	 * ethtool hardware statistics.
	 */
 void (*Model0_get_strings)(struct Model0_dsa_switch *Model0_ds, int Model0_port, Model0_uint8_t *Model0_data);
 void (*Model0_get_ethtool_stats)(struct Model0_dsa_switch *Model0_ds,
         int Model0_port, Model0_uint64_t *Model0_data);
 int (*Model0_get_sset_count)(struct Model0_dsa_switch *Model0_ds);

 /*
	 * ethtool Wake-on-LAN
	 */
 void (*Model0_get_wol)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
      struct Model0_ethtool_wolinfo *Model0_w);
 int (*Model0_set_wol)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
      struct Model0_ethtool_wolinfo *Model0_w);

 /*
	 * Suspend and resume
	 */
 int (*Model0_suspend)(struct Model0_dsa_switch *Model0_ds);
 int (*Model0_resume)(struct Model0_dsa_switch *Model0_ds);

 /*
	 * Port enable/disable
	 */
 int (*Model0_port_enable)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
          struct Model0_phy_device *Model0_phy);
 void (*Model0_port_disable)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
    struct Model0_phy_device *Model0_phy);

 /*
	 * EEE setttings
	 */
 int (*Model0_set_eee)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
      struct Model0_phy_device *Model0_phydev,
      struct Model0_ethtool_eee *Model0_e);
 int (*Model0_get_eee)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
      struct Model0_ethtool_eee *Model0_e);
 /* EEPROM access */
 int (*Model0_get_eeprom_len)(struct Model0_dsa_switch *Model0_ds);
 int (*Model0_get_eeprom)(struct Model0_dsa_switch *Model0_ds,
         struct Model0_ethtool_eeprom *Model0_eeprom, Model0_u8 *Model0_data);
 int (*Model0_set_eeprom)(struct Model0_dsa_switch *Model0_ds,
         struct Model0_ethtool_eeprom *Model0_eeprom, Model0_u8 *Model0_data);

 /*
	 * Register access.
	 */
 int (*Model0_get_regs_len)(struct Model0_dsa_switch *Model0_ds, int Model0_port);
 void (*Model0_get_regs)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
       struct Model0_ethtool_regs *Model0_regs, void *Model0_p);

 /*
	 * Bridge integration
	 */
 int (*Model0_set_ageing_time)(struct Model0_dsa_switch *Model0_ds, unsigned int Model0_msecs);
 int (*Model0_port_bridge_join)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
        struct Model0_net_device *Model0_bridge);
 void (*Model0_port_bridge_leave)(struct Model0_dsa_switch *Model0_ds, int Model0_port);
 void (*Model0_port_stp_state_set)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
          Model0_u8 Model0_state);

 /*
	 * VLAN support
	 */
 int (*Model0_port_vlan_filtering)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
           bool Model0_vlan_filtering);
 int (*Model0_port_vlan_prepare)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
         const struct Model0_switchdev_obj_port_vlan *Model0_vlan,
         struct Model0_switchdev_trans *Model0_trans);
 void (*Model0_port_vlan_add)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
     const struct Model0_switchdev_obj_port_vlan *Model0_vlan,
     struct Model0_switchdev_trans *Model0_trans);
 int (*Model0_port_vlan_del)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
     const struct Model0_switchdev_obj_port_vlan *Model0_vlan);
 int (*Model0_port_vlan_dump)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
      struct Model0_switchdev_obj_port_vlan *Model0_vlan,
      int (*Model0_cb)(struct Model0_switchdev_obj *Model0_obj));

 /*
	 * Forwarding database
	 */
 int (*Model0_port_fdb_prepare)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
        const struct Model0_switchdev_obj_port_fdb *Model0_fdb,
        struct Model0_switchdev_trans *Model0_trans);
 void (*Model0_port_fdb_add)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
    const struct Model0_switchdev_obj_port_fdb *Model0_fdb,
    struct Model0_switchdev_trans *Model0_trans);
 int (*Model0_port_fdb_del)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
    const struct Model0_switchdev_obj_port_fdb *Model0_fdb);
 int (*Model0_port_fdb_dump)(struct Model0_dsa_switch *Model0_ds, int Model0_port,
     struct Model0_switchdev_obj_port_fdb *Model0_fdb,
     int (*Model0_cb)(struct Model0_switchdev_obj *Model0_obj));
};

void Model0_register_switch_driver(struct Model0_dsa_switch_driver *Model0_type);
void Model0_unregister_switch_driver(struct Model0_dsa_switch_driver *Model0_type);
struct Model0_mii_bus *Model0_dsa_host_dev_to_mii_bus(struct Model0_device *Model0_dev);

static inline __attribute__((no_instrument_function)) void *Model0_ds_to_priv(struct Model0_dsa_switch *Model0_ds)
{
 return Model0_ds->Model0_priv;
}

static inline __attribute__((no_instrument_function)) bool Model0_dsa_uses_tagged_protocol(struct Model0_dsa_switch_tree *Model0_dst)
{
 return Model0_dst->Model0_rcv != ((void *)0);
}

void Model0_dsa_unregister_switch(struct Model0_dsa_switch *Model0_ds);
int Model0_dsa_register_switch(struct Model0_dsa_switch *Model0_ds, struct Model0_device_node *Model0_np);



/*
 * netprio_cgroup.h			Control Group Priority set
 *
 *
 * Authors:	Neil Horman <nhorman@tuxdriver.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 * Software Foundation; either version 2 of the License, or (at your option)
 * any later version.
 *
 */







/*
 *  cgroup interface
 *
 *  Copyright (C) 2003 BULL SA
 *  Copyright (C) 2004-2006 Silicon Graphics, Inc.
 *
 */






/* cgroupstats.h - exporting per-cgroup statistics
 *
 * Copyright IBM Corporation, 2007
 * Author Balbir Singh <balbir@linux.vnet.ibm.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of version 2.1 of the GNU Lesser General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it would be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 */






/* taskstats.h - exporting per-task statistics
 *
 * Copyright (C) Shailabh Nagar, IBM Corp. 2006
 *           (C) Balbir Singh,   IBM Corp. 2006
 *           (C) Jay Lan,        SGI, 2006
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of version 2.1 of the GNU Lesser General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it would be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 */






/* Format for per-task data returned to userland when
 *	- a task exits
 *	- listener requests stats for a task
 *
 * The struct is versioned. Newer versions should only add fields to
 * the bottom of the struct to maintain backward compatibility.
 *
 *
 * To add new fields
 *	a) bump up TASKSTATS_VERSION
 *	b) add comment indicating new version number at end of struct
 *	c) add new fields after version comment; maintain 64-bit alignment
 */






struct Model0_taskstats {

 /* The version number of this struct. This field is always set to
	 * TAKSTATS_VERSION, which is defined in <linux/taskstats.h>.
	 * Each time the struct is changed, the value should be incremented.
	 */
 Model0___u16 Model0_version;
 __u32 Model0_ac_exitcode; /* Exit status */

 /* The accounting flags of a task as defined in <linux/acct.h>
	 * Defined values are AFORK, ASU, ACOMPAT, ACORE, and AXSIG.
	 */
 __u8 Model0_ac_flag; /* Record flags */
 __u8 Model0_ac_nice; /* task_nice */

 /* Delay accounting fields start
	 *
	 * All values, until comment "Delay accounting fields end" are
	 * available only if delay accounting is enabled, even though the last
	 * few fields are not delays
	 *
	 * xxx_count is the number of delay values recorded
	 * xxx_delay_total is the corresponding cumulative delay in nanoseconds
	 *
	 * xxx_delay_total wraps around to zero on overflow
	 * xxx_count incremented regardless of overflow
	 */

 /* Delay waiting for cpu, while runnable
	 * count, delay_total NOT updated atomically
	 */
 __u64 Model0_cpu_count __attribute__((aligned(8)));
 __u64 Model0_cpu_delay_total;

 /* Following four fields atomically updated using task->delays->lock */

 /* Delay waiting for synchronous block I/O to complete
	 * does not account for delays in I/O submission
	 */
 __u64 Model0_blkio_count;
 __u64 Model0_blkio_delay_total;

 /* Delay waiting for page fault I/O (swap in only) */
 __u64 Model0_swapin_count;
 __u64 Model0_swapin_delay_total;

 /* cpu "wall-clock" running time
	 * On some architectures, value will adjust for cpu time stolen
	 * from the kernel in involuntary waits due to virtualization.
	 * Value is cumulative, in nanoseconds, without a corresponding count
	 * and wraps around to zero silently on overflow
	 */
 __u64 Model0_cpu_run_real_total;

 /* cpu "virtual" running time
	 * Uses time intervals seen by the kernel i.e. no adjustment
	 * for kernel's involuntary waits due to virtualization.
	 * Value is cumulative, in nanoseconds, without a corresponding count
	 * and wraps around to zero silently on overflow
	 */
 __u64 Model0_cpu_run_virtual_total;
 /* Delay accounting fields end */
 /* version 1 ends here */

 /* Basic Accounting Fields start */
 char Model0_ac_comm[32]; /* Command name */
 __u8 Model0_ac_sched __attribute__((aligned(8)));
     /* Scheduling discipline */
 __u8 Model0_ac_pad[3];
 __u32 Model0_ac_uid __attribute__((aligned(8)));
     /* User ID */
 __u32 Model0_ac_gid; /* Group ID */
 __u32 Model0_ac_pid; /* Process ID */
 __u32 Model0_ac_ppid; /* Parent process ID */
 __u32 Model0_ac_btime; /* Begin time [sec since 1970] */
 __u64 Model0_ac_etime __attribute__((aligned(8)));
     /* Elapsed time [usec] */
 __u64 Model0_ac_utime; /* User CPU time [usec] */
 __u64 Model0_ac_stime; /* SYstem CPU time [usec] */
 __u64 Model0_ac_minflt; /* Minor Page Fault Count */
 __u64 Model0_ac_majflt; /* Major Page Fault Count */
 /* Basic Accounting Fields end */

 /* Extended accounting fields start */
 /* Accumulated RSS usage in duration of a task, in MBytes-usecs.
	 * The current rss usage is added to this counter every time
	 * a tick is charged to a task's system time. So, at the end we
	 * will have memory usage multiplied by system time. Thus an
	 * average usage per system time unit can be calculated.
	 */
 __u64 Model0_coremem; /* accumulated RSS usage in MB-usec */
 /* Accumulated virtual memory usage in duration of a task.
	 * Same as acct_rss_mem1 above except that we keep track of VM usage.
	 */
 __u64 Model0_virtmem; /* accumulated VM  usage in MB-usec */

 /* High watermark of RSS and virtual memory usage in duration of
	 * a task, in KBytes.
	 */
 __u64 Model0_hiwater_rss; /* High-watermark of RSS usage, in KB */
 __u64 Model0_hiwater_vm; /* High-water VM usage, in KB */

 /* The following four fields are I/O statistics of a task. */
 __u64 Model0_read_char; /* bytes read */
 __u64 Model0_write_char; /* bytes written */
 __u64 Model0_read_syscalls; /* read syscalls */
 __u64 Model0_write_syscalls; /* write syscalls */
 /* Extended accounting fields end */


 /* Per-task storage I/O accounting starts */
 __u64 Model0_read_bytes; /* bytes of read I/O */
 __u64 Model0_write_bytes; /* bytes of write I/O */
 __u64 Model0_cancelled_write_bytes; /* bytes of cancelled write I/O */

 __u64 Model0_nvcsw; /* voluntary_ctxt_switches */
 __u64 Model0_nivcsw; /* nonvoluntary_ctxt_switches */

 /* time accounting for SMT machines */
 __u64 Model0_ac_utimescaled; /* utime scaled on frequency etc */
 __u64 Model0_ac_stimescaled; /* stime scaled on frequency etc */
 __u64 Model0_cpu_scaled_run_real_total; /* scaled cpu_run_real_total */

 /* Delay waiting for memory reclaim */
 __u64 Model0_freepages_count;
 __u64 Model0_freepages_delay_total;
};


/*
 * Commands sent from userspace
 * Not versioned. New commands should only be inserted at the enum's end
 * prior to __TASKSTATS_CMD_MAX
 */

enum {
 Model0_TASKSTATS_CMD_UNSPEC = 0, /* Reserved */
 Model0_TASKSTATS_CMD_GET, /* user->kernel request/get-response */
 Model0_TASKSTATS_CMD_NEW, /* kernel->user event */
 Model0___TASKSTATS_CMD_MAX,
};



enum {
 Model0_TASKSTATS_TYPE_UNSPEC = 0, /* Reserved */
 Model0_TASKSTATS_TYPE_PID, /* Process id */
 Model0_TASKSTATS_TYPE_TGID, /* Thread group id */
 Model0_TASKSTATS_TYPE_STATS, /* taskstats structure */
 Model0_TASKSTATS_TYPE_AGGR_PID, /* contains pid + stats */
 Model0_TASKSTATS_TYPE_AGGR_TGID, /* contains tgid + stats */
 Model0_TASKSTATS_TYPE_NULL, /* contains nothing */
 Model0___TASKSTATS_TYPE_MAX,
};



enum {
 Model0_TASKSTATS_CMD_ATTR_UNSPEC = 0,
 Model0_TASKSTATS_CMD_ATTR_PID,
 Model0_TASKSTATS_CMD_ATTR_TGID,
 Model0_TASKSTATS_CMD_ATTR_REGISTER_CPUMASK,
 Model0_TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK,
 Model0___TASKSTATS_CMD_ATTR_MAX,
};



/* NETLINK_GENERIC related info */

/*
 * Data shared between user space and kernel space on a per cgroup
 * basis. This data is shared using taskstats.
 *
 * Most of these states are derived by looking at the task->state value
 * For the nr_io_wait state, a flag in the delay accounting structure
 * indicates that the task is waiting on IO
 *
 * Each member is aligned to a 8 byte boundary.
 */
struct Model0_cgroupstats {
 __u64 Model0_nr_sleeping; /* Number of tasks sleeping */
 __u64 Model0_nr_running; /* Number of tasks running */
 __u64 Model0_nr_stopped; /* Number of tasks in stopped state */
 __u64 Model0_nr_uninterruptible; /* Number of tasks in uninterruptible */
     /* state */
 __u64 Model0_nr_io_wait; /* Number of tasks waiting on IO */
};

/*
 * Commands sent from userspace
 * Not versioned. New commands should only be inserted at the enum's end
 * prior to __CGROUPSTATS_CMD_MAX
 */

enum {
 Model0_CGROUPSTATS_CMD_UNSPEC = Model0___TASKSTATS_CMD_MAX, /* Reserved */
 Model0_CGROUPSTATS_CMD_GET, /* user->kernel request/get-response */
 Model0_CGROUPSTATS_CMD_NEW, /* kernel->user event */
 Model0___CGROUPSTATS_CMD_MAX,
};



enum {
 Model0_CGROUPSTATS_TYPE_UNSPEC = 0, /* Reserved */
 Model0_CGROUPSTATS_TYPE_CGROUP_STATS, /* contains name + stats */
 Model0___CGROUPSTATS_TYPE_MAX,
};



enum {
 Model0_CGROUPSTATS_CMD_ATTR_UNSPEC = 0,
 Model0_CGROUPSTATS_CMD_ATTR_FD,
 Model0___CGROUPSTATS_CMD_ATTR_MAX,
};










struct Model0_mnt_namespace;
struct Model0_uts_namespace;
struct Model0_ipc_namespace;
struct Model0_pid_namespace;
struct Model0_cgroup_namespace;
struct Model0_fs_struct;

/*
 * A structure to contain pointers to all per-process
 * namespaces - fs (mount), uts, network, sysvipc, etc.
 *
 * The pid namespace is an exception -- it's accessed using
 * task_active_pid_ns.  The pid namespace here is the
 * namespace that children will use.
 *
 * 'count' is the number of tasks holding a reference.
 * The count for each namespace, then, will be the number
 * of nsproxies pointing to it, not the number of tasks.
 *
 * The nsproxy is shared by tasks which share all namespaces.
 * As soon as a single namespace is cloned or unshared, the
 * nsproxy is copied.
 */
struct Model0_nsproxy {
 Model0_atomic_t Model0_count;
 struct Model0_uts_namespace *Model0_uts_ns;
 struct Model0_ipc_namespace *Model0_ipc_ns;
 struct Model0_mnt_namespace *Model0_mnt_ns;
 struct Model0_pid_namespace *Model0_pid_ns_for_children;
 struct Model0_net *Model0_net_ns;
 struct Model0_cgroup_namespace *Model0_cgroup_ns;
};
extern struct Model0_nsproxy Model0_init_nsproxy;

/*
 * the namespaces access rules are:
 *
 *  1. only current task is allowed to change tsk->nsproxy pointer or
 *     any pointer on the nsproxy itself.  Current must hold the task_lock
 *     when changing tsk->nsproxy.
 *
 *  2. when accessing (i.e. reading) current task's namespaces - no
 *     precautions should be taken - just dereference the pointers
 *
 *  3. the access to other task namespaces is performed like this
 *     task_lock(task);
 *     nsproxy = task->nsproxy;
 *     if (nsproxy != NULL) {
 *             / *
 *               * work with the namespaces here
 *               * e.g. get the reference on one of them
 *               * /
 *     } / *
 *         * NULL task->nsproxy means that this task is
 *         * almost dead (zombie)
 *         * /
 *     task_unlock(task);
 *
 */

int Model0_copy_namespaces(unsigned long Model0_flags, struct Model0_task_struct *Model0_tsk);
void Model0_exit_task_namespaces(struct Model0_task_struct *Model0_tsk);
void Model0_switch_task_namespaces(struct Model0_task_struct *Model0_tsk, struct Model0_nsproxy *Model0_new);
void Model0_free_nsproxy(struct Model0_nsproxy *Model0_ns);
int Model0_unshare_nsproxy_namespaces(unsigned long, struct Model0_nsproxy **,
 struct Model0_cred *, struct Model0_fs_struct *);
int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_nsproxy_cache_init(void);

static inline __attribute__((no_instrument_function)) void Model0_put_nsproxy(struct Model0_nsproxy *Model0_ns)
{
 if (Model0_atomic_dec_and_test(&Model0_ns->Model0_count)) {
  Model0_free_nsproxy(Model0_ns);
 }
}

static inline __attribute__((no_instrument_function)) void Model0_get_nsproxy(struct Model0_nsproxy *Model0_ns)
{
 Model0_atomic_inc(&Model0_ns->Model0_count);
}



struct Model0_uid_gid_map { /* 64 bytes -- 1 cache line */
 Model0_u32 Model0_nr_extents;
 struct Model0_uid_gid_extent {
  Model0_u32 Model0_first;
  Model0_u32 Model0_lower_first;
  Model0_u32 Model0_count;
 } Model0_extent[5];
};





struct Model0_user_namespace {
 struct Model0_uid_gid_map Model0_uid_map;
 struct Model0_uid_gid_map Model0_gid_map;
 struct Model0_uid_gid_map Model0_projid_map;
 Model0_atomic_t Model0_count;
 struct Model0_user_namespace *Model0_parent;
 int Model0_level;
 Model0_kuid_t Model0_owner;
 Model0_kgid_t Model0_group;
 struct Model0_ns_common Model0_ns;
 unsigned long Model0_flags;

 /* Register of per-UID persistent keyrings for this namespace */




};

extern struct Model0_user_namespace Model0_init_user_ns;
static inline __attribute__((no_instrument_function)) struct Model0_user_namespace *Model0_get_user_ns(struct Model0_user_namespace *Model0_ns)
{
 return &Model0_init_user_ns;
}

static inline __attribute__((no_instrument_function)) int Model0_create_user_ns(struct Model0_cred *Model0_new)
{
 return -22;
}

static inline __attribute__((no_instrument_function)) int Model0_unshare_userns(unsigned long Model0_unshare_flags,
     struct Model0_cred **Model0_new_cred)
{
 if (Model0_unshare_flags & 0x10000000)
  return -22;
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_put_user_ns(struct Model0_user_namespace *Model0_ns)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_userns_may_setgroups(const struct Model0_user_namespace *Model0_ns)
{
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_current_in_userns(const struct Model0_user_namespace *Model0_target_ns)
{
 return true;
}





/*
 * All weight knobs on the default hierarhcy should use the following min,
 * default and max values.  The default value is the logarithmic center of
 * MIN and MAX and allows 100x to be expressed in both directions.
 */




/* a css_task_iter should be treated as an opaque object */
struct Model0_css_task_iter {
 struct Model0_cgroup_subsys *Model0_ss;

 struct Model0_list_head *Model0_cset_pos;
 struct Model0_list_head *Model0_cset_head;

 struct Model0_list_head *Model0_task_pos;
 struct Model0_list_head *Model0_tasks_head;
 struct Model0_list_head *Model0_mg_tasks_head;

 struct Model0_css_set *Model0_cur_cset;
 struct Model0_task_struct *Model0_cur_task;
 struct Model0_list_head Model0_iters_node; /* css_set->task_iters */
};

extern struct Model0_cgroup_root Model0_cgrp_dfl_root;
extern struct Model0_css_set Model0_init_css_set;



/*
 * List of cgroup subsystems.
 *
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */

/*
 * This file *must* be included with SUBSYS() defined.
 */


extern struct Model0_cgroup_subsys Model0_cpuset_cgrp_subsys;



extern struct Model0_cgroup_subsys Model0_cpu_cgrp_subsys;



extern struct Model0_cgroup_subsys Model0_cpuacct_cgrp_subsys;
extern struct Model0_cgroup_subsys Model0_freezer_cgrp_subsys;
/*
 * The following subsystems are not supported on the default hierarchy.
 */




/*
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */





/*
 * List of cgroup subsystems.
 *
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */

/*
 * This file *must* be included with SUBSYS() defined.
 */


extern struct Model0_static_key_true Model0_cpuset_cgrp_subsys_enabled_key; extern struct Model0_static_key_true Model0_cpuset_cgrp_subsys_on_dfl_key;



extern struct Model0_static_key_true Model0_cpu_cgrp_subsys_enabled_key; extern struct Model0_static_key_true Model0_cpu_cgrp_subsys_on_dfl_key;



extern struct Model0_static_key_true Model0_cpuacct_cgrp_subsys_enabled_key; extern struct Model0_static_key_true Model0_cpuacct_cgrp_subsys_on_dfl_key;
extern struct Model0_static_key_true Model0_freezer_cgrp_subsys_enabled_key; extern struct Model0_static_key_true Model0_freezer_cgrp_subsys_on_dfl_key;
/*
 * The following subsystems are not supported on the default hierarchy.
 */




/*
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */


/**
 * cgroup_subsys_enabled - fast test on whether a subsys is enabled
 * @ss: subsystem in question
 */



/**
 * cgroup_subsys_on_dfl - fast test on whether a subsys is on default hierarchy
 * @ss: subsystem in question
 */



bool Model0_css_has_online_children(struct Model0_cgroup_subsys_state *Model0_css);
struct Model0_cgroup_subsys_state *Model0_css_from_id(int Model0_id, struct Model0_cgroup_subsys *Model0_ss);
struct Model0_cgroup_subsys_state *Model0_cgroup_get_e_css(struct Model0_cgroup *Model0_cgroup,
          struct Model0_cgroup_subsys *Model0_ss);
struct Model0_cgroup_subsys_state *Model0_css_tryget_online_from_dir(struct Model0_dentry *Model0_dentry,
             struct Model0_cgroup_subsys *Model0_ss);

struct Model0_cgroup *Model0_cgroup_get_from_path(const char *Model0_path);
struct Model0_cgroup *Model0_cgroup_get_from_fd(int Model0_fd);

int Model0_cgroup_attach_task_all(struct Model0_task_struct *Model0_from, struct Model0_task_struct *);
int Model0_cgroup_transfer_tasks(struct Model0_cgroup *Model0_to, struct Model0_cgroup *Model0_from);

int Model0_cgroup_add_dfl_cftypes(struct Model0_cgroup_subsys *Model0_ss, struct Model0_cftype *Model0_cfts);
int Model0_cgroup_add_legacy_cftypes(struct Model0_cgroup_subsys *Model0_ss, struct Model0_cftype *Model0_cfts);
int Model0_cgroup_rm_cftypes(struct Model0_cftype *Model0_cfts);
void Model0_cgroup_file_notify(struct Model0_cgroup_file *Model0_cfile);

char *Model0_task_cgroup_path(struct Model0_task_struct *Model0_task, char *Model0_buf, Model0_size_t Model0_buflen);
int Model0_cgroupstats_build(struct Model0_cgroupstats *Model0_stats, struct Model0_dentry *Model0_dentry);
int Model0_proc_cgroup_show(struct Model0_seq_file *Model0_m, struct Model0_pid_namespace *Model0_ns,
       struct Model0_pid *Model0_pid, struct Model0_task_struct *Model0_tsk);

void Model0_cgroup_fork(struct Model0_task_struct *Model0_p);
extern int Model0_cgroup_can_fork(struct Model0_task_struct *Model0_p);
extern void Model0_cgroup_cancel_fork(struct Model0_task_struct *Model0_p);
extern void Model0_cgroup_post_fork(struct Model0_task_struct *Model0_p);
void Model0_cgroup_exit(struct Model0_task_struct *Model0_p);
void Model0_cgroup_free(struct Model0_task_struct *Model0_p);

int Model0_cgroup_init_early(void);
int Model0_cgroup_init(void);

/*
 * Iteration helpers and macros.
 */

struct Model0_cgroup_subsys_state *Model0_css_next_child(struct Model0_cgroup_subsys_state *Model0_pos,
        struct Model0_cgroup_subsys_state *Model0_parent);
struct Model0_cgroup_subsys_state *Model0_css_next_descendant_pre(struct Model0_cgroup_subsys_state *Model0_pos,
          struct Model0_cgroup_subsys_state *Model0_css);
struct Model0_cgroup_subsys_state *Model0_css_rightmost_descendant(struct Model0_cgroup_subsys_state *Model0_pos);
struct Model0_cgroup_subsys_state *Model0_css_next_descendant_post(struct Model0_cgroup_subsys_state *Model0_pos,
           struct Model0_cgroup_subsys_state *Model0_css);

struct Model0_task_struct *Model0_cgroup_taskset_first(struct Model0_cgroup_taskset *Model0_tset,
      struct Model0_cgroup_subsys_state **Model0_dst_cssp);
struct Model0_task_struct *Model0_cgroup_taskset_next(struct Model0_cgroup_taskset *Model0_tset,
     struct Model0_cgroup_subsys_state **Model0_dst_cssp);

void Model0_css_task_iter_start(struct Model0_cgroup_subsys_state *Model0_css,
    struct Model0_css_task_iter *Model0_it);
struct Model0_task_struct *Model0_css_task_iter_next(struct Model0_css_task_iter *Model0_it);
void Model0_css_task_iter_end(struct Model0_css_task_iter *Model0_it);

/**
 * css_for_each_child - iterate through children of a css
 * @pos: the css * to use as the loop cursor
 * @parent: css whose children to walk
 *
 * Walk @parent's children.  Must be called under rcu_read_lock().
 *
 * If a subsystem synchronizes ->css_online() and the start of iteration, a
 * css which finished ->css_online() is guaranteed to be visible in the
 * future iterations and will stay visible until the last reference is put.
 * A css which hasn't finished ->css_online() or already finished
 * ->css_offline() may show up during traversal.  It's each subsystem's
 * responsibility to synchronize against on/offlining.
 *
 * It is allowed to temporarily drop RCU read lock during iteration.  The
 * caller is responsible for ensuring that @pos remains accessible until
 * the start of the next iteration by, for example, bumping the css refcnt.
 */




/**
 * css_for_each_descendant_pre - pre-order walk of a css's descendants
 * @pos: the css * to use as the loop cursor
 * @root: css whose descendants to walk
 *
 * Walk @root's descendants.  @root is included in the iteration and the
 * first node to be visited.  Must be called under rcu_read_lock().
 *
 * If a subsystem synchronizes ->css_online() and the start of iteration, a
 * css which finished ->css_online() is guaranteed to be visible in the
 * future iterations and will stay visible until the last reference is put.
 * A css which hasn't finished ->css_online() or already finished
 * ->css_offline() may show up during traversal.  It's each subsystem's
 * responsibility to synchronize against on/offlining.
 *
 * For example, the following guarantees that a descendant can't escape
 * state updates of its ancestors.
 *
 * my_online(@css)
 * {
 *	Lock @css's parent and @css;
 *	Inherit state from the parent;
 *	Unlock both.
 * }
 *
 * my_update_state(@css)
 * {
 *	css_for_each_descendant_pre(@pos, @css) {
 *		Lock @pos;
 *		if (@pos == @css)
 *			Update @css's state;
 *		else
 *			Verify @pos is alive and inherit state from its parent;
 *		Unlock @pos;
 *	}
 * }
 *
 * As long as the inheriting step, including checking the parent state, is
 * enclosed inside @pos locking, double-locking the parent isn't necessary
 * while inheriting.  The state update to the parent is guaranteed to be
 * visible by walking order and, as long as inheriting operations to the
 * same @pos are atomic to each other, multiple updates racing each other
 * still result in the correct state.  It's guaranateed that at least one
 * inheritance happens for any css after the latest update to its parent.
 *
 * If checking parent's state requires locking the parent, each inheriting
 * iteration should lock and unlock both @pos->parent and @pos.
 *
 * Alternatively, a subsystem may choose to use a single global lock to
 * synchronize ->css_online() and ->css_offline() against tree-walking
 * operations.
 *
 * It is allowed to temporarily drop RCU read lock during iteration.  The
 * caller is responsible for ensuring that @pos remains accessible until
 * the start of the next iteration by, for example, bumping the css refcnt.
 */




/**
 * css_for_each_descendant_post - post-order walk of a css's descendants
 * @pos: the css * to use as the loop cursor
 * @css: css whose descendants to walk
 *
 * Similar to css_for_each_descendant_pre() but performs post-order
 * traversal instead.  @root is included in the iteration and the last
 * node to be visited.
 *
 * If a subsystem synchronizes ->css_online() and the start of iteration, a
 * css which finished ->css_online() is guaranteed to be visible in the
 * future iterations and will stay visible until the last reference is put.
 * A css which hasn't finished ->css_online() or already finished
 * ->css_offline() may show up during traversal.  It's each subsystem's
 * responsibility to synchronize against on/offlining.
 *
 * Note that the walk visibility guarantee example described in pre-order
 * walk doesn't apply the same to post-order walks.
 */




/**
 * cgroup_taskset_for_each - iterate cgroup_taskset
 * @task: the loop cursor
 * @dst_css: the destination css
 * @tset: taskset to iterate
 *
 * @tset may contain multiple tasks and they may belong to multiple
 * processes.
 *
 * On the v2 hierarchy, there may be tasks from multiple processes and they
 * may not share the source or destination csses.
 *
 * On traditional hierarchies, when there are multiple tasks in @tset, if a
 * task of a process is in @tset, all tasks of the process are in @tset.
 * Also, all are guaranteed to share the same source and destination csses.
 *
 * Iteration is not in any specific order.
 */





/**
 * cgroup_taskset_for_each_leader - iterate group leaders in a cgroup_taskset
 * @leader: the loop cursor
 * @dst_css: the destination css
 * @tset: takset to iterate
 *
 * Iterate threadgroup leaders of @tset.  For single-task migrations, @tset
 * may not contain any.
 */
/*
 * Inline functions.
 */

/**
 * css_get - obtain a reference on the specified css
 * @css: target css
 *
 * The caller must already have a reference.
 */
static inline __attribute__((no_instrument_function)) void Model0_css_get(struct Model0_cgroup_subsys_state *Model0_css)
{
 if (!(Model0_css->Model0_flags & Model0_CSS_NO_REF))
  Model0_percpu_ref_get(&Model0_css->Model0_refcnt);
}

/**
 * css_get_many - obtain references on the specified css
 * @css: target css
 * @n: number of references to get
 *
 * The caller must already have a reference.
 */
static inline __attribute__((no_instrument_function)) void Model0_css_get_many(struct Model0_cgroup_subsys_state *Model0_css, unsigned int Model0_n)
{
 if (!(Model0_css->Model0_flags & Model0_CSS_NO_REF))
  Model0_percpu_ref_get_many(&Model0_css->Model0_refcnt, Model0_n);
}

/**
 * css_tryget - try to obtain a reference on the specified css
 * @css: target css
 *
 * Obtain a reference on @css unless it already has reached zero and is
 * being released.  This function doesn't care whether @css is on or
 * offline.  The caller naturally needs to ensure that @css is accessible
 * but doesn't have to be holding a reference on it - IOW, RCU protected
 * access is good enough for this function.  Returns %true if a reference
 * count was successfully obtained; %false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_css_tryget(struct Model0_cgroup_subsys_state *Model0_css)
{
 if (!(Model0_css->Model0_flags & Model0_CSS_NO_REF))
  return Model0_percpu_ref_tryget(&Model0_css->Model0_refcnt);
 return true;
}

/**
 * css_tryget_online - try to obtain a reference on the specified css if online
 * @css: target css
 *
 * Obtain a reference on @css if it's online.  The caller naturally needs
 * to ensure that @css is accessible but doesn't have to be holding a
 * reference on it - IOW, RCU protected access is good enough for this
 * function.  Returns %true if a reference count was successfully obtained;
 * %false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model0_css_tryget_online(struct Model0_cgroup_subsys_state *Model0_css)
{
 if (!(Model0_css->Model0_flags & Model0_CSS_NO_REF))
  return Model0_percpu_ref_tryget_live(&Model0_css->Model0_refcnt);
 return true;
}

/**
 * css_put - put a css reference
 * @css: target css
 *
 * Put a reference obtained via css_get() and css_tryget_online().
 */
static inline __attribute__((no_instrument_function)) void Model0_css_put(struct Model0_cgroup_subsys_state *Model0_css)
{
 if (!(Model0_css->Model0_flags & Model0_CSS_NO_REF))
  Model0_percpu_ref_put(&Model0_css->Model0_refcnt);
}

/**
 * css_put_many - put css references
 * @css: target css
 * @n: number of references to put
 *
 * Put references obtained via css_get() and css_tryget_online().
 */
static inline __attribute__((no_instrument_function)) void Model0_css_put_many(struct Model0_cgroup_subsys_state *Model0_css, unsigned int Model0_n)
{
 if (!(Model0_css->Model0_flags & Model0_CSS_NO_REF))
  Model0_percpu_ref_put_many(&Model0_css->Model0_refcnt, Model0_n);
}

static inline __attribute__((no_instrument_function)) void Model0_cgroup_put(struct Model0_cgroup *Model0_cgrp)
{
 Model0_css_put(&Model0_cgrp->Model0_self);
}

/**
 * task_css_set_check - obtain a task's css_set with extra access conditions
 * @task: the task to obtain css_set for
 * @__c: extra condition expression to be passed to rcu_dereference_check()
 *
 * A task's css_set is RCU protected, initialized and exited while holding
 * task_lock(), and can only be modified while holding both cgroup_mutex
 * and task_lock() while the task is alive.  This macro verifies that the
 * caller is inside proper critical section and returns @task's css_set.
 *
 * The caller can also specify additional allowed conditions via @__c, such
 * as locks used during the cgroup_subsys::attach() methods.
 */
/**
 * task_css_check - obtain css for (task, subsys) w/ extra access conds
 * @task: the target task
 * @subsys_id: the target subsystem ID
 * @__c: extra condition expression to be passed to rcu_dereference_check()
 *
 * Return the cgroup_subsys_state for the (@task, @subsys_id) pair.  The
 * synchronization rules are the same as task_css_set_check().
 */



/**
 * task_css_set - obtain a task's css_set
 * @task: the task to obtain css_set for
 *
 * See task_css_set_check().
 */
static inline __attribute__((no_instrument_function)) struct Model0_css_set *Model0_task_css_set(struct Model0_task_struct *Model0_task)
{
 return ({ typeof(*((Model0_task)->Model0_cgroups)) *Model0_________p1 = (typeof(*((Model0_task)->Model0_cgroups)) *)({ typeof(((Model0_task)->Model0_cgroups)) Model0__________p1 = ({ union { typeof(((Model0_task)->Model0_cgroups)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(((Model0_task)->Model0_cgroups)), Model0___u.Model0___c, sizeof(((Model0_task)->Model0_cgroups))); else Model0___read_once_size_nocheck(&(((Model0_task)->Model0_cgroups)), Model0___u.Model0___c, sizeof(((Model0_task)->Model0_cgroups))); Model0___u.Model0___val; }); typeof(*(((Model0_task)->Model0_cgroups))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*((Model0_task)->Model0_cgroups)) *)(Model0_________p1)); });
}

/**
 * task_css - obtain css for (task, subsys)
 * @task: the target task
 * @subsys_id: the target subsystem ID
 *
 * See task_css_check().
 */
static inline __attribute__((no_instrument_function)) struct Model0_cgroup_subsys_state *Model0_task_css(struct Model0_task_struct *Model0_task,
         int Model0_subsys_id)
{
 return ({ typeof(*(((Model0_task))->Model0_cgroups)) *Model0_________p1 = (typeof(*(((Model0_task))->Model0_cgroups)) *)({ typeof((((Model0_task))->Model0_cgroups)) Model0__________p1 = ({ union { typeof((((Model0_task))->Model0_cgroups)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((((Model0_task))->Model0_cgroups)), Model0___u.Model0___c, sizeof((((Model0_task))->Model0_cgroups))); else Model0___read_once_size_nocheck(&((((Model0_task))->Model0_cgroups)), Model0___u.Model0___c, sizeof((((Model0_task))->Model0_cgroups))); Model0___u.Model0___val; }); typeof(*((((Model0_task))->Model0_cgroups))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(((Model0_task))->Model0_cgroups)) *)(Model0_________p1)); })->Model0_subsys[(Model0_subsys_id)];
}

/**
 * task_get_css - find and get the css for (task, subsys)
 * @task: the target task
 * @subsys_id: the target subsystem ID
 *
 * Find the css for the (@task, @subsys_id) combination, increment a
 * reference on and return it.  This function is guaranteed to return a
 * valid css.
 */
static inline __attribute__((no_instrument_function)) struct Model0_cgroup_subsys_state *
Model0_task_get_css(struct Model0_task_struct *Model0_task, int Model0_subsys_id)
{
 struct Model0_cgroup_subsys_state *Model0_css;

 Model0_rcu_read_lock();
 while (true) {
  Model0_css = Model0_task_css(Model0_task, Model0_subsys_id);
  if (__builtin_expect(!!(Model0_css_tryget_online(Model0_css)), 1))
   break;
  Model0_cpu_relax();
 }
 Model0_rcu_read_unlock();
 return Model0_css;
}

/**
 * task_css_is_root - test whether a task belongs to the root css
 * @task: the target task
 * @subsys_id: the target subsystem ID
 *
 * Test whether @task belongs to the root css on the specified subsystem.
 * May be invoked in any context.
 */
static inline __attribute__((no_instrument_function)) bool Model0_task_css_is_root(struct Model0_task_struct *Model0_task, int Model0_subsys_id)
{
 return ({ typeof(*(((Model0_task))->Model0_cgroups)) *Model0_________p1 = (typeof(*(((Model0_task))->Model0_cgroups)) *)({ typeof((((Model0_task))->Model0_cgroups)) Model0__________p1 = ({ union { typeof((((Model0_task))->Model0_cgroups)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((((Model0_task))->Model0_cgroups)), Model0___u.Model0___c, sizeof((((Model0_task))->Model0_cgroups))); else Model0___read_once_size_nocheck(&((((Model0_task))->Model0_cgroups)), Model0___u.Model0___c, sizeof((((Model0_task))->Model0_cgroups))); Model0___u.Model0___val; }); typeof(*((((Model0_task))->Model0_cgroups))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(((Model0_task))->Model0_cgroups)) *)(Model0_________p1)); })->Model0_subsys[(Model0_subsys_id)] ==
  Model0_init_css_set.Model0_subsys[Model0_subsys_id];
}

static inline __attribute__((no_instrument_function)) struct Model0_cgroup *Model0_task_cgroup(struct Model0_task_struct *Model0_task,
      int Model0_subsys_id)
{
 return Model0_task_css(Model0_task, Model0_subsys_id)->Model0_cgroup;
}

/**
 * cgroup_is_descendant - test ancestry
 * @cgrp: the cgroup to be tested
 * @ancestor: possible ancestor of @cgrp
 *
 * Test whether @cgrp is a descendant of @ancestor.  It also returns %true
 * if @cgrp == @ancestor.  This function is safe to call as long as @cgrp
 * and @ancestor are accessible.
 */
static inline __attribute__((no_instrument_function)) bool Model0_cgroup_is_descendant(struct Model0_cgroup *Model0_cgrp,
     struct Model0_cgroup *Model0_ancestor)
{
 if (Model0_cgrp->Model0_root != Model0_ancestor->Model0_root || Model0_cgrp->Model0_level < Model0_ancestor->Model0_level)
  return false;
 return Model0_cgrp->Model0_ancestor_ids[Model0_ancestor->Model0_level] == Model0_ancestor->Model0_id;
}

/* no synchronization, the result can only be used as a hint */
static inline __attribute__((no_instrument_function)) bool Model0_cgroup_is_populated(struct Model0_cgroup *Model0_cgrp)
{
 return Model0_cgrp->Model0_populated_cnt;
}

/* returns ino associated with a cgroup */
static inline __attribute__((no_instrument_function)) Model0_ino_t Model0_cgroup_ino(struct Model0_cgroup *Model0_cgrp)
{
 return Model0_cgrp->Model0_kn->Model0_ino;
}

/* cft/css accessors for cftype->write() operation */
static inline __attribute__((no_instrument_function)) struct Model0_cftype *Model0_of_cft(struct Model0_kernfs_open_file *Model0_of)
{
 return Model0_of->Model0_kn->Model0_priv;
}

struct Model0_cgroup_subsys_state *Model0_of_css(struct Model0_kernfs_open_file *Model0_of);

/* cft/css accessors for cftype->seq_*() operations */
static inline __attribute__((no_instrument_function)) struct Model0_cftype *Model0_seq_cft(struct Model0_seq_file *Model0_seq)
{
 return Model0_of_cft(Model0_seq->Model0_private);
}

static inline __attribute__((no_instrument_function)) struct Model0_cgroup_subsys_state *Model0_seq_css(struct Model0_seq_file *Model0_seq)
{
 return Model0_of_css(Model0_seq->Model0_private);
}

/*
 * Name / path handling functions.  All are thin wrappers around the kernfs
 * counterparts and can be called under any context.
 */

static inline __attribute__((no_instrument_function)) int Model0_cgroup_name(struct Model0_cgroup *Model0_cgrp, char *Model0_buf, Model0_size_t Model0_buflen)
{
 return Model0_kernfs_name(Model0_cgrp->Model0_kn, Model0_buf, Model0_buflen);
}

static inline __attribute__((no_instrument_function)) char * __attribute__((warn_unused_result)) Model0_cgroup_path(struct Model0_cgroup *Model0_cgrp, char *Model0_buf,
           Model0_size_t Model0_buflen)
{
 return Model0_kernfs_path(Model0_cgrp->Model0_kn, Model0_buf, Model0_buflen);
}

static inline __attribute__((no_instrument_function)) void Model0_pr_cont_cgroup_name(struct Model0_cgroup *Model0_cgrp)
{
 Model0_pr_cont_kernfs_name(Model0_cgrp->Model0_kn);
}

static inline __attribute__((no_instrument_function)) void Model0_pr_cont_cgroup_path(struct Model0_cgroup *Model0_cgrp)
{
 Model0_pr_cont_kernfs_path(Model0_cgrp->Model0_kn);
}
/*
 * sock->sk_cgrp_data handling.  For more info, see sock_cgroup_data
 * definition in cgroup-defs.h.
 */
static inline __attribute__((no_instrument_function)) void Model0_cgroup_sk_alloc(struct Model0_sock_cgroup_data *Model0_skcd) {}
static inline __attribute__((no_instrument_function)) void Model0_cgroup_sk_free(struct Model0_sock_cgroup_data *Model0_skcd) {}



struct Model0_cgroup_namespace {
 Model0_atomic_t Model0_count;
 struct Model0_ns_common Model0_ns;
 struct Model0_user_namespace *Model0_user_ns;
 struct Model0_css_set *Model0_root_cset;
};

extern struct Model0_cgroup_namespace Model0_init_cgroup_ns;



void Model0_free_cgroup_ns(struct Model0_cgroup_namespace *Model0_ns);

struct Model0_cgroup_namespace *Model0_copy_cgroup_ns(unsigned long Model0_flags,
     struct Model0_user_namespace *Model0_user_ns,
     struct Model0_cgroup_namespace *Model0_old_ns);

char *Model0_cgroup_path_ns(struct Model0_cgroup *Model0_cgrp, char *Model0_buf, Model0_size_t Model0_buflen,
       struct Model0_cgroup_namespace *Model0_ns);
static inline __attribute__((no_instrument_function)) void Model0_get_cgroup_ns(struct Model0_cgroup_namespace *Model0_ns)
{
 if (Model0_ns)
  Model0_atomic_inc(&Model0_ns->Model0_count);
}

static inline __attribute__((no_instrument_function)) void Model0_put_cgroup_ns(struct Model0_cgroup_namespace *Model0_ns)
{
 if (Model0_ns && Model0_atomic_dec_and_test(&Model0_ns->Model0_count))
  Model0_free_cgroup_ns(Model0_ns);
}
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_task_netprioidx(struct Model0_task_struct *Model0_p)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_sock_update_netprioidx(struct Model0_sock_cgroup_data *Model0_skcd)
{
}


















/*
 * Linux Security plug
 *
 * Copyright (C) 2001 WireX Communications, Inc <chris@wirex.com>
 * Copyright (C) 2001 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (C) 2001 Networks Associates Technology, Inc <ssmalley@nai.com>
 * Copyright (C) 2001 James Morris <jmorris@intercode.com.au>
 * Copyright (C) 2001 Silicon Graphics, Inc. (Trust Technology Group)
 *
 *	This program is free software; you can redistribute it and/or modify
 *	it under the terms of the GNU General Public License as published by
 *	the Free Software Foundation; either version 2 of the License, or
 *	(at your option) any later version.
 *
 *	Due to this file being licensed under the GPL there is controversy over
 *	whether this permits you to write a module that #includes this file
 *	without placing your module under the GPL.  Please consult a lawyer for
 *	advice before doing this.
 *
 */
struct Model0_linux_binprm;
struct Model0_cred;
struct Model0_rlimit;
struct Model0_siginfo;
struct Model0_sem_array;
struct Model0_sembuf;
struct Model0_kern_ipc_perm;
struct Model0_audit_context;
struct Model0_super_block;
struct Model0_inode;
struct Model0_dentry;
struct Model0_file;
struct Model0_vfsmount;
struct Model0_path;
struct Model0_qstr;
struct Model0_iattr;
struct Model0_fown_struct;
struct Model0_file_operations;
struct Model0_shmid_kernel;
struct Model0_msg_msg;
struct Model0_msg_queue;
struct Model0_xattr;
struct Model0_xfrm_sec_ctx;
struct Model0_mm_struct;

/* If capable should audit the security request */



/* LSM Agnostic defines for sb_set_mnt_opts */


struct Model0_ctl_table;
struct Model0_audit_krule;
struct Model0_user_namespace;
struct Model0_timezone;

/* These functions are in security/commoncap.c */
extern int Model0_cap_capable(const struct Model0_cred *Model0_cred, struct Model0_user_namespace *Model0_ns,
         int Model0_cap, int Model0_audit);
extern int Model0_cap_settime(const struct Model0_timespec *Model0_ts, const struct Model0_timezone *Model0_tz);
extern int Model0_cap_ptrace_access_check(struct Model0_task_struct *Model0_child, unsigned int Model0_mode);
extern int Model0_cap_ptrace_traceme(struct Model0_task_struct *Model0_parent);
extern int Model0_cap_capget(struct Model0_task_struct *Model0_target, Model0_kernel_cap_t *Model0_effective, Model0_kernel_cap_t *Model0_inheritable, Model0_kernel_cap_t *Model0_permitted);
extern int Model0_cap_capset(struct Model0_cred *Model0_new, const struct Model0_cred *old,
        const Model0_kernel_cap_t *Model0_effective,
        const Model0_kernel_cap_t *Model0_inheritable,
        const Model0_kernel_cap_t *Model0_permitted);
extern int Model0_cap_bprm_set_creds(struct Model0_linux_binprm *Model0_bprm);
extern int Model0_cap_bprm_secureexec(struct Model0_linux_binprm *Model0_bprm);
extern int Model0_cap_inode_setxattr(struct Model0_dentry *Model0_dentry, const char *Model0_name,
         const void *Model0_value, Model0_size_t Model0_size, int Model0_flags);
extern int Model0_cap_inode_removexattr(struct Model0_dentry *Model0_dentry, const char *Model0_name);
extern int Model0_cap_inode_need_killpriv(struct Model0_dentry *Model0_dentry);
extern int Model0_cap_inode_killpriv(struct Model0_dentry *Model0_dentry);
extern int Model0_cap_mmap_addr(unsigned long Model0_addr);
extern int Model0_cap_mmap_file(struct Model0_file *Model0_file, unsigned long Model0_reqprot,
    unsigned long Model0_prot, unsigned long Model0_flags);
extern int Model0_cap_task_fix_setuid(struct Model0_cred *Model0_new, const struct Model0_cred *old, int Model0_flags);
extern int Model0_cap_task_prctl(int Model0_option, unsigned long Model0_arg2, unsigned long Model0_arg3,
     unsigned long Model0_arg4, unsigned long Model0_arg5);
extern int Model0_cap_task_setscheduler(struct Model0_task_struct *Model0_p);
extern int Model0_cap_task_setioprio(struct Model0_task_struct *Model0_p, int Model0_ioprio);
extern int Model0_cap_task_setnice(struct Model0_task_struct *Model0_p, int Model0_nice);
extern int Model0_cap_vm_enough_memory(struct Model0_mm_struct *Model0_mm, long Model0_pages);

struct Model0_msghdr;
struct Model0_sk_buff;
struct Model0_sock;
struct Model0_sockaddr;
struct Model0_socket;
struct Model0_flowi;
struct Model0_dst_entry;
struct Model0_xfrm_selector;
struct Model0_xfrm_policy;
struct Model0_xfrm_state;
struct Model0_xfrm_user_sec_ctx;
struct Model0_seq_file;


extern unsigned long Model0_mmap_min_addr;
extern unsigned long Model0_dac_mmap_min_addr;





/*
 * Values used in the task_security_ops calls
 */
/* setuid or setgid, id0 == uid or gid */


/* setreuid or setregid, id0 == real, id1 == eff */


/* setresuid or setresgid, id0 == real, id1 == eff, uid2 == saved */


/* setfsuid or setfsgid, id0 == fsuid or fsgid */


/* forward declares to avoid warnings */
struct Model0_sched_param;
struct Model0_request_sock;

/* bprm->unsafe reasons */






extern int Model0_mmap_min_addr_handler(struct Model0_ctl_table *Model0_table, int Model0_write,
     void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);


/* security_inode_init_security callback function to write xattrs */
typedef int (*Model0_initxattrs) (struct Model0_inode *Model0_inode,
      const struct Model0_xattr *Model0_xattr_array, void *Model0_fs_data);



struct Model0_security_mnt_opts {
 char **Model0_mnt_opts;
 int *Model0_mnt_opts_flags;
 int Model0_num_mnt_opts;
};

static inline __attribute__((no_instrument_function)) void Model0_security_init_mnt_opts(struct Model0_security_mnt_opts *Model0_opts)
{
 Model0_opts->Model0_mnt_opts = ((void *)0);
 Model0_opts->Model0_mnt_opts_flags = ((void *)0);
 Model0_opts->Model0_num_mnt_opts = 0;
}

static inline __attribute__((no_instrument_function)) void Model0_security_free_mnt_opts(struct Model0_security_mnt_opts *Model0_opts)
{
 int Model0_i;
 if (Model0_opts->Model0_mnt_opts)
  for (Model0_i = 0; Model0_i < Model0_opts->Model0_num_mnt_opts; Model0_i++)
   Model0_kfree(Model0_opts->Model0_mnt_opts[Model0_i]);
 Model0_kfree(Model0_opts->Model0_mnt_opts);
 Model0_opts->Model0_mnt_opts = ((void *)0);
 Model0_kfree(Model0_opts->Model0_mnt_opts_flags);
 Model0_opts->Model0_mnt_opts_flags = ((void *)0);
 Model0_opts->Model0_num_mnt_opts = 0;
}

/* prototypes */
extern int Model0_security_init(void);

/* Security operations */
int Model0_security_binder_set_context_mgr(struct Model0_task_struct *Model0_mgr);
int Model0_security_binder_transaction(struct Model0_task_struct *Model0_from,
    struct Model0_task_struct *Model0_to);
int Model0_security_binder_transfer_binder(struct Model0_task_struct *Model0_from,
        struct Model0_task_struct *Model0_to);
int Model0_security_binder_transfer_file(struct Model0_task_struct *Model0_from,
      struct Model0_task_struct *Model0_to, struct Model0_file *Model0_file);
int Model0_security_ptrace_access_check(struct Model0_task_struct *Model0_child, unsigned int Model0_mode);
int Model0_security_ptrace_traceme(struct Model0_task_struct *Model0_parent);
int Model0_security_capget(struct Model0_task_struct *Model0_target,
      Model0_kernel_cap_t *Model0_effective,
      Model0_kernel_cap_t *Model0_inheritable,
      Model0_kernel_cap_t *Model0_permitted);
int Model0_security_capset(struct Model0_cred *Model0_new, const struct Model0_cred *old,
      const Model0_kernel_cap_t *Model0_effective,
      const Model0_kernel_cap_t *Model0_inheritable,
      const Model0_kernel_cap_t *Model0_permitted);
int Model0_security_capable(const struct Model0_cred *Model0_cred, struct Model0_user_namespace *Model0_ns,
   int Model0_cap);
int Model0_security_capable_noaudit(const struct Model0_cred *Model0_cred, struct Model0_user_namespace *Model0_ns,
        int Model0_cap);
int Model0_security_quotactl(int Model0_cmds, int Model0_type, int Model0_id, struct Model0_super_block *Model0_sb);
int Model0_security_quota_on(struct Model0_dentry *Model0_dentry);
int Model0_security_syslog(int Model0_type);
int Model0_security_settime64(const struct Model0_timespec *Model0_ts, const struct Model0_timezone *Model0_tz);
static inline __attribute__((no_instrument_function)) int Model0_security_settime(const struct Model0_timespec *Model0_ts, const struct Model0_timezone *Model0_tz)
{
 struct Model0_timespec Model0_ts64 = Model0_timespec_to_timespec64(*Model0_ts);

 return Model0_security_settime64(&Model0_ts64, Model0_tz);
}
int Model0_security_vm_enough_memory_mm(struct Model0_mm_struct *Model0_mm, long Model0_pages);
int Model0_security_bprm_set_creds(struct Model0_linux_binprm *Model0_bprm);
int Model0_security_bprm_check(struct Model0_linux_binprm *Model0_bprm);
void Model0_security_bprm_committing_creds(struct Model0_linux_binprm *Model0_bprm);
void Model0_security_bprm_committed_creds(struct Model0_linux_binprm *Model0_bprm);
int Model0_security_bprm_secureexec(struct Model0_linux_binprm *Model0_bprm);
int Model0_security_sb_alloc(struct Model0_super_block *Model0_sb);
void Model0_security_sb_free(struct Model0_super_block *Model0_sb);
int Model0_security_sb_copy_data(char *Model0_orig, char *Model0_copy);
int Model0_security_sb_remount(struct Model0_super_block *Model0_sb, void *Model0_data);
int Model0_security_sb_kern_mount(struct Model0_super_block *Model0_sb, int Model0_flags, void *Model0_data);
int Model0_security_sb_show_options(struct Model0_seq_file *Model0_m, struct Model0_super_block *Model0_sb);
int Model0_security_sb_statfs(struct Model0_dentry *Model0_dentry);
int Model0_security_sb_mount(const char *Model0_dev_name, const struct Model0_path *Model0_path,
        const char *Model0_type, unsigned long Model0_flags, void *Model0_data);
int Model0_security_sb_umount(struct Model0_vfsmount *Model0_mnt, int Model0_flags);
int Model0_security_sb_pivotroot(const struct Model0_path *Model0_old_path, const struct Model0_path *Model0_new_path);
int Model0_security_sb_set_mnt_opts(struct Model0_super_block *Model0_sb,
    struct Model0_security_mnt_opts *Model0_opts,
    unsigned long Model0_kern_flags,
    unsigned long *Model0_set_kern_flags);
int Model0_security_sb_clone_mnt_opts(const struct Model0_super_block *Model0_oldsb,
    struct Model0_super_block *Model0_newsb);
int Model0_security_sb_parse_opts_str(char *Model0_options, struct Model0_security_mnt_opts *Model0_opts);
int Model0_security_dentry_init_security(struct Model0_dentry *Model0_dentry, int Model0_mode,
     const struct Model0_qstr *Model0_name, void **Model0_ctx,
     Model0_u32 *Model0_ctxlen);

int Model0_security_inode_alloc(struct Model0_inode *Model0_inode);
void Model0_security_inode_free(struct Model0_inode *Model0_inode);
int Model0_security_inode_init_security(struct Model0_inode *Model0_inode, struct Model0_inode *Model0_dir,
     const struct Model0_qstr *Model0_qstr,
     Model0_initxattrs Model0_initxattrs, void *Model0_fs_data);
int Model0_security_old_inode_init_security(struct Model0_inode *Model0_inode, struct Model0_inode *Model0_dir,
         const struct Model0_qstr *Model0_qstr, const char **Model0_name,
         void **Model0_value, Model0_size_t *Model0_len);
int Model0_security_inode_create(struct Model0_inode *Model0_dir, struct Model0_dentry *Model0_dentry, Model0_umode_t Model0_mode);
int Model0_security_inode_link(struct Model0_dentry *Model0_old_dentry, struct Model0_inode *Model0_dir,
    struct Model0_dentry *Model0_new_dentry);
int Model0_security_inode_unlink(struct Model0_inode *Model0_dir, struct Model0_dentry *Model0_dentry);
int Model0_security_inode_symlink(struct Model0_inode *Model0_dir, struct Model0_dentry *Model0_dentry,
      const char *Model0_old_name);
int Model0_security_inode_mkdir(struct Model0_inode *Model0_dir, struct Model0_dentry *Model0_dentry, Model0_umode_t Model0_mode);
int Model0_security_inode_rmdir(struct Model0_inode *Model0_dir, struct Model0_dentry *Model0_dentry);
int Model0_security_inode_mknod(struct Model0_inode *Model0_dir, struct Model0_dentry *Model0_dentry, Model0_umode_t Model0_mode, Model0_dev_t Model0_dev);
int Model0_security_inode_rename(struct Model0_inode *Model0_old_dir, struct Model0_dentry *Model0_old_dentry,
     struct Model0_inode *Model0_new_dir, struct Model0_dentry *Model0_new_dentry,
     unsigned int Model0_flags);
int Model0_security_inode_readlink(struct Model0_dentry *Model0_dentry);
int Model0_security_inode_follow_link(struct Model0_dentry *Model0_dentry, struct Model0_inode *Model0_inode,
          bool Model0_rcu);
int Model0_security_inode_permission(struct Model0_inode *Model0_inode, int Model0_mask);
int Model0_security_inode_setattr(struct Model0_dentry *Model0_dentry, struct Model0_iattr *Model0_attr);
int Model0_security_inode_getattr(const struct Model0_path *Model0_path);
int Model0_security_inode_setxattr(struct Model0_dentry *Model0_dentry, const char *Model0_name,
       const void *Model0_value, Model0_size_t Model0_size, int Model0_flags);
void Model0_security_inode_post_setxattr(struct Model0_dentry *Model0_dentry, const char *Model0_name,
      const void *Model0_value, Model0_size_t Model0_size, int Model0_flags);
int Model0_security_inode_getxattr(struct Model0_dentry *Model0_dentry, const char *Model0_name);
int Model0_security_inode_listxattr(struct Model0_dentry *Model0_dentry);
int Model0_security_inode_removexattr(struct Model0_dentry *Model0_dentry, const char *Model0_name);
int Model0_security_inode_need_killpriv(struct Model0_dentry *Model0_dentry);
int Model0_security_inode_killpriv(struct Model0_dentry *Model0_dentry);
int Model0_security_inode_getsecurity(struct Model0_inode *Model0_inode, const char *Model0_name, void **Model0_buffer, bool Model0_alloc);
int Model0_security_inode_setsecurity(struct Model0_inode *Model0_inode, const char *Model0_name, const void *Model0_value, Model0_size_t Model0_size, int Model0_flags);
int Model0_security_inode_listsecurity(struct Model0_inode *Model0_inode, char *Model0_buffer, Model0_size_t Model0_buffer_size);
void Model0_security_inode_getsecid(struct Model0_inode *Model0_inode, Model0_u32 *Model0_secid);
int Model0_security_file_permission(struct Model0_file *Model0_file, int Model0_mask);
int Model0_security_file_alloc(struct Model0_file *Model0_file);
void Model0_security_file_free(struct Model0_file *Model0_file);
int Model0_security_file_ioctl(struct Model0_file *Model0_file, unsigned int Model0_cmd, unsigned long Model0_arg);
int Model0_security_mmap_file(struct Model0_file *Model0_file, unsigned long Model0_prot,
   unsigned long Model0_flags);
int Model0_security_mmap_addr(unsigned long Model0_addr);
int Model0_security_file_mprotect(struct Model0_vm_area_struct *Model0_vma, unsigned long Model0_reqprot,
      unsigned long Model0_prot);
int Model0_security_file_lock(struct Model0_file *Model0_file, unsigned int Model0_cmd);
int Model0_security_file_fcntl(struct Model0_file *Model0_file, unsigned int Model0_cmd, unsigned long Model0_arg);
void Model0_security_file_set_fowner(struct Model0_file *Model0_file);
int Model0_security_file_send_sigiotask(struct Model0_task_struct *Model0_tsk,
     struct Model0_fown_struct *Model0_fown, int Model0_sig);
int Model0_security_file_receive(struct Model0_file *Model0_file);
int Model0_security_file_open(struct Model0_file *Model0_file, const struct Model0_cred *Model0_cred);
int Model0_security_task_create(unsigned long Model0_clone_flags);
void Model0_security_task_free(struct Model0_task_struct *Model0_task);
int Model0_security_cred_alloc_blank(struct Model0_cred *Model0_cred, Model0_gfp_t Model0_gfp);
void Model0_security_cred_free(struct Model0_cred *Model0_cred);
int Model0_security_prepare_creds(struct Model0_cred *Model0_new, const struct Model0_cred *old, Model0_gfp_t Model0_gfp);
void Model0_security_transfer_creds(struct Model0_cred *Model0_new, const struct Model0_cred *old);
int Model0_security_kernel_act_as(struct Model0_cred *Model0_new, Model0_u32 Model0_secid);
int Model0_security_kernel_create_files_as(struct Model0_cred *Model0_new, struct Model0_inode *Model0_inode);
int Model0_security_kernel_module_request(char *Model0_kmod_name);
int Model0_security_kernel_module_from_file(struct Model0_file *Model0_file);
int Model0_security_kernel_read_file(struct Model0_file *Model0_file, enum Model0_kernel_read_file_id Model0_id);
int Model0_security_kernel_post_read_file(struct Model0_file *Model0_file, char *Model0_buf, Model0_loff_t Model0_size,
       enum Model0_kernel_read_file_id Model0_id);
int Model0_security_task_fix_setuid(struct Model0_cred *Model0_new, const struct Model0_cred *old,
        int Model0_flags);
int Model0_security_task_setpgid(struct Model0_task_struct *Model0_p, Model0_pid_t Model0_pgid);
int Model0_security_task_getpgid(struct Model0_task_struct *Model0_p);
int Model0_security_task_getsid(struct Model0_task_struct *Model0_p);
void Model0_security_task_getsecid(struct Model0_task_struct *Model0_p, Model0_u32 *Model0_secid);
int Model0_security_task_setnice(struct Model0_task_struct *Model0_p, int Model0_nice);
int Model0_security_task_setioprio(struct Model0_task_struct *Model0_p, int Model0_ioprio);
int Model0_security_task_getioprio(struct Model0_task_struct *Model0_p);
int Model0_security_task_setrlimit(struct Model0_task_struct *Model0_p, unsigned int Model0_resource,
  struct Model0_rlimit *Model0_new_rlim);
int Model0_security_task_setscheduler(struct Model0_task_struct *Model0_p);
int Model0_security_task_getscheduler(struct Model0_task_struct *Model0_p);
int Model0_security_task_movememory(struct Model0_task_struct *Model0_p);
int Model0_security_task_kill(struct Model0_task_struct *Model0_p, struct Model0_siginfo *Model0_info,
   int Model0_sig, Model0_u32 Model0_secid);
int Model0_security_task_wait(struct Model0_task_struct *Model0_p);
int Model0_security_task_prctl(int Model0_option, unsigned long Model0_arg2, unsigned long Model0_arg3,
   unsigned long Model0_arg4, unsigned long Model0_arg5);
void Model0_security_task_to_inode(struct Model0_task_struct *Model0_p, struct Model0_inode *Model0_inode);
int Model0_security_ipc_permission(struct Model0_kern_ipc_perm *Model0_ipcp, short Model0_flag);
void Model0_security_ipc_getsecid(struct Model0_kern_ipc_perm *Model0_ipcp, Model0_u32 *Model0_secid);
int Model0_security_msg_msg_alloc(struct Model0_msg_msg *Model0_msg);
void Model0_security_msg_msg_free(struct Model0_msg_msg *Model0_msg);
int Model0_security_msg_queue_alloc(struct Model0_msg_queue *Model0_msq);
void Model0_security_msg_queue_free(struct Model0_msg_queue *Model0_msq);
int Model0_security_msg_queue_associate(struct Model0_msg_queue *Model0_msq, int Model0_msqflg);
int Model0_security_msg_queue_msgctl(struct Model0_msg_queue *Model0_msq, int Model0_cmd);
int Model0_security_msg_queue_msgsnd(struct Model0_msg_queue *Model0_msq,
         struct Model0_msg_msg *Model0_msg, int Model0_msqflg);
int Model0_security_msg_queue_msgrcv(struct Model0_msg_queue *Model0_msq, struct Model0_msg_msg *Model0_msg,
         struct Model0_task_struct *Model0_target, long Model0_type, int Model0_mode);
int Model0_security_shm_alloc(struct Model0_shmid_kernel *Model0_shp);
void Model0_security_shm_free(struct Model0_shmid_kernel *Model0_shp);
int Model0_security_shm_associate(struct Model0_shmid_kernel *Model0_shp, int Model0_shmflg);
int Model0_security_shm_shmctl(struct Model0_shmid_kernel *Model0_shp, int Model0_cmd);
int Model0_security_shm_shmat(struct Model0_shmid_kernel *Model0_shp, char *Model0_shmaddr, int Model0_shmflg);
int Model0_security_sem_alloc(struct Model0_sem_array *Model0_sma);
void Model0_security_sem_free(struct Model0_sem_array *Model0_sma);
int Model0_security_sem_associate(struct Model0_sem_array *Model0_sma, int Model0_semflg);
int Model0_security_sem_semctl(struct Model0_sem_array *Model0_sma, int Model0_cmd);
int Model0_security_sem_semop(struct Model0_sem_array *Model0_sma, struct Model0_sembuf *Model0_sops,
   unsigned Model0_nsops, int Model0_alter);
void Model0_security_d_instantiate(struct Model0_dentry *Model0_dentry, struct Model0_inode *Model0_inode);
int Model0_security_getprocattr(struct Model0_task_struct *Model0_p, char *Model0_name, char **Model0_value);
int Model0_security_setprocattr(struct Model0_task_struct *Model0_p, char *Model0_name, void *Model0_value, Model0_size_t Model0_size);
int Model0_security_netlink_send(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_security_ismaclabel(const char *Model0_name);
int Model0_security_secid_to_secctx(Model0_u32 Model0_secid, char **Model0_secdata, Model0_u32 *Model0_seclen);
int Model0_security_secctx_to_secid(const char *Model0_secdata, Model0_u32 Model0_seclen, Model0_u32 *Model0_secid);
void Model0_security_release_secctx(char *Model0_secdata, Model0_u32 Model0_seclen);

void Model0_security_inode_invalidate_secctx(struct Model0_inode *Model0_inode);
int Model0_security_inode_notifysecctx(struct Model0_inode *Model0_inode, void *Model0_ctx, Model0_u32 Model0_ctxlen);
int Model0_security_inode_setsecctx(struct Model0_dentry *Model0_dentry, void *Model0_ctx, Model0_u32 Model0_ctxlen);
int Model0_security_inode_getsecctx(struct Model0_inode *Model0_inode, void **Model0_ctx, Model0_u32 *Model0_ctxlen);
int Model0_security_unix_stream_connect(struct Model0_sock *Model0_sock, struct Model0_sock *Model0_other, struct Model0_sock *Model0_newsk);
int Model0_security_unix_may_send(struct Model0_socket *Model0_sock, struct Model0_socket *Model0_other);
int Model0_security_socket_create(int Model0_family, int Model0_type, int Model0_protocol, int Model0_kern);
int Model0_security_socket_post_create(struct Model0_socket *Model0_sock, int Model0_family,
    int Model0_type, int Model0_protocol, int Model0_kern);
int Model0_security_socket_bind(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_address, int Model0_addrlen);
int Model0_security_socket_connect(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_address, int Model0_addrlen);
int Model0_security_socket_listen(struct Model0_socket *Model0_sock, int Model0_backlog);
int Model0_security_socket_accept(struct Model0_socket *Model0_sock, struct Model0_socket *Model0_newsock);
int Model0_security_socket_sendmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, int Model0_size);
int Model0_security_socket_recvmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg,
       int Model0_size, int Model0_flags);
int Model0_security_socket_getsockname(struct Model0_socket *Model0_sock);
int Model0_security_socket_getpeername(struct Model0_socket *Model0_sock);
int Model0_security_socket_getsockopt(struct Model0_socket *Model0_sock, int Model0_level, int Model0_optname);
int Model0_security_socket_setsockopt(struct Model0_socket *Model0_sock, int Model0_level, int Model0_optname);
int Model0_security_socket_shutdown(struct Model0_socket *Model0_sock, int Model0_how);
int Model0_security_sock_rcv_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_security_socket_getpeersec_stream(struct Model0_socket *Model0_sock, char *Model0_optval,
          int *Model0_optlen, unsigned Model0_len);
int Model0_security_socket_getpeersec_dgram(struct Model0_socket *Model0_sock, struct Model0_sk_buff *Model0_skb, Model0_u32 *Model0_secid);
int Model0_security_sk_alloc(struct Model0_sock *Model0_sk, int Model0_family, Model0_gfp_t Model0_priority);
void Model0_security_sk_free(struct Model0_sock *Model0_sk);
void Model0_security_sk_clone(const struct Model0_sock *Model0_sk, struct Model0_sock *Model0_newsk);
void Model0_security_sk_classify_flow(struct Model0_sock *Model0_sk, struct Model0_flowi *Model0_fl);
void Model0_security_req_classify_flow(const struct Model0_request_sock *Model0_req, struct Model0_flowi *Model0_fl);
void Model0_security_sock_graft(struct Model0_sock*Model0_sk, struct Model0_socket *Model0_parent);
int Model0_security_inet_conn_request(struct Model0_sock *Model0_sk,
   struct Model0_sk_buff *Model0_skb, struct Model0_request_sock *Model0_req);
static void Model0_security_inet_csk_clone(struct Model0_sock *Model0_newsk,
   const struct Model0_request_sock *Model0_req);
void Model0_security_inet_conn_established(struct Model0_sock *Model0_sk,
   struct Model0_sk_buff *Model0_skb);
int Model0_security_secmark_relabel_packet(Model0_u32 Model0_secid);
void Model0_security_secmark_refcount_inc(void);
void Model0_security_secmark_refcount_dec(void);
int Model0_security_tun_dev_alloc_security(void **Model0_security);
void Model0_security_tun_dev_free_security(void *Model0_security);
int Model0_security_tun_dev_create(void);
int Model0_security_tun_dev_attach_queue(void *Model0_security);
int Model0_security_tun_dev_attach(struct Model0_sock *Model0_sk, void *Model0_security);
int Model0_security_tun_dev_open(void *Model0_security);
static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_policy_alloc(struct Model0_xfrm_sec_ctx **Model0_ctxp,
          struct Model0_xfrm_user_sec_ctx *Model0_sec_ctx,
          Model0_gfp_t Model0_gfp)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_policy_clone(struct Model0_xfrm_sec_ctx *old, struct Model0_xfrm_sec_ctx **Model0_new_ctxp)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_security_xfrm_policy_free(struct Model0_xfrm_sec_ctx *Model0_ctx)
{
}

static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_policy_delete(struct Model0_xfrm_sec_ctx *Model0_ctx)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_state_alloc(struct Model0_xfrm_state *Model0_x,
     struct Model0_xfrm_user_sec_ctx *Model0_sec_ctx)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_state_alloc_acquire(struct Model0_xfrm_state *Model0_x,
     struct Model0_xfrm_sec_ctx *Model0_polsec, Model0_u32 Model0_secid)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_security_xfrm_state_free(struct Model0_xfrm_state *Model0_x)
{
}

static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_state_delete(struct Model0_xfrm_state *Model0_x)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_policy_lookup(struct Model0_xfrm_sec_ctx *Model0_ctx, Model0_u32 Model0_fl_secid, Model0_u8 Model0_dir)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_state_pol_flow_match(struct Model0_xfrm_state *Model0_x,
   struct Model0_xfrm_policy *Model0_xp, const struct Model0_flowi *Model0_fl)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) int Model0_security_xfrm_decode_session(struct Model0_sk_buff *Model0_skb, Model0_u32 *Model0_secid)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_security_skb_classify_flow(struct Model0_sk_buff *Model0_skb, struct Model0_flowi *Model0_fl)
{
}
static inline __attribute__((no_instrument_function)) int Model0_security_path_unlink(const struct Model0_path *Model0_dir, struct Model0_dentry *Model0_dentry)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_mkdir(const struct Model0_path *Model0_dir, struct Model0_dentry *Model0_dentry,
          Model0_umode_t Model0_mode)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_rmdir(const struct Model0_path *Model0_dir, struct Model0_dentry *Model0_dentry)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_mknod(const struct Model0_path *Model0_dir, struct Model0_dentry *Model0_dentry,
          Model0_umode_t Model0_mode, unsigned int Model0_dev)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_truncate(const struct Model0_path *Model0_path)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_symlink(const struct Model0_path *Model0_dir, struct Model0_dentry *Model0_dentry,
     const char *Model0_old_name)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_link(struct Model0_dentry *Model0_old_dentry,
         const struct Model0_path *Model0_new_dir,
         struct Model0_dentry *Model0_new_dentry)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_rename(const struct Model0_path *Model0_old_dir,
           struct Model0_dentry *Model0_old_dentry,
           const struct Model0_path *Model0_new_dir,
           struct Model0_dentry *Model0_new_dentry,
           unsigned int Model0_flags)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_chmod(const struct Model0_path *Model0_path, Model0_umode_t Model0_mode)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_chown(const struct Model0_path *Model0_path, Model0_kuid_t Model0_uid, Model0_kgid_t Model0_gid)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_security_path_chroot(const struct Model0_path *Model0_path)
{
 return 0;
}





int Model0_security_key_alloc(struct Model0_key *Model0_key, const struct Model0_cred *Model0_cred, unsigned long Model0_flags);
void Model0_security_key_free(struct Model0_key *Model0_key);
int Model0_security_key_permission(Model0_key_ref_t Model0_key_ref,
       const struct Model0_cred *Model0_cred, unsigned Model0_perm);
int Model0_security_key_getsecurity(struct Model0_key *Model0_key, char **Model0__buffer);
int Model0_security_audit_rule_init(Model0_u32 Model0_field, Model0_u32 Model0_op, char *Model0_rulestr, void **Model0_lsmrule);
int Model0_security_audit_rule_known(struct Model0_audit_krule *Model0_krule);
int Model0_security_audit_rule_match(Model0_u32 Model0_secid, Model0_u32 Model0_field, Model0_u32 Model0_op, void *Model0_lsmrule,
         struct Model0_audit_context *Model0_actx);
void Model0_security_audit_rule_free(void *Model0_lsmrule);
static inline __attribute__((no_instrument_function)) struct Model0_dentry *Model0_securityfs_create_dir(const char *Model0_name,
         struct Model0_dentry *Model0_parent)
{
 return Model0_ERR_PTR(-19);
}

static inline __attribute__((no_instrument_function)) struct Model0_dentry *Model0_securityfs_create_file(const char *Model0_name,
          Model0_umode_t Model0_mode,
          struct Model0_dentry *Model0_parent,
          void *Model0_data,
          const struct Model0_file_operations *Model0_fops)
{
 return Model0_ERR_PTR(-19);
}

static inline __attribute__((no_instrument_function)) void Model0_securityfs_remove(struct Model0_dentry *Model0_dentry)
{}





static inline __attribute__((no_instrument_function)) char *Model0_alloc_secdata(void)
{
 return (char *)Model0_get_zeroed_page(((( Model0_gfp_t)(0x400000u|0x2000000u)) | (( Model0_gfp_t)0x40u) | (( Model0_gfp_t)0x80u)));
}

static inline __attribute__((no_instrument_function)) void Model0_free_secdata(void *Model0_secdata)
{
 Model0_free_pages(((unsigned long)Model0_secdata), 0);
}



/* Well, we should have at least one descriptor open
 * to accept passed FDs 8)
 */


struct Model0_scm_creds {
 Model0_u32 Model0_pid;
 Model0_kuid_t Model0_uid;
 Model0_kgid_t Model0_gid;
};

struct Model0_scm_fp_list {
 short Model0_count;
 short Model0_max;
 struct Model0_user_struct *Model0_user;
 struct Model0_file *Model0_fp[253];
};

struct Model0_scm_cookie {
 struct Model0_pid *Model0_pid; /* Skb credentials */
 struct Model0_scm_fp_list *Model0_fp; /* Passed files		*/
 struct Model0_scm_creds Model0_creds; /* Skb credentials	*/

 Model0_u32 Model0_secid; /* Passed security ID 	*/

};

void Model0_scm_detach_fds(struct Model0_msghdr *Model0_msg, struct Model0_scm_cookie *Model0_scm);
void Model0_scm_detach_fds_compat(struct Model0_msghdr *Model0_msg, struct Model0_scm_cookie *Model0_scm);
int Model0___scm_send(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, struct Model0_scm_cookie *Model0_scm);
void Model0___scm_destroy(struct Model0_scm_cookie *Model0_scm);
struct Model0_scm_fp_list *Model0_scm_fp_dup(struct Model0_scm_fp_list *Model0_fpl);


static __inline__ __attribute__((no_instrument_function)) void Model0_unix_get_peersec_dgram(struct Model0_socket *Model0_sock, struct Model0_scm_cookie *Model0_scm)
{
 Model0_security_socket_getpeersec_dgram(Model0_sock, ((void *)0), &Model0_scm->Model0_secid);
}





static __inline__ __attribute__((no_instrument_function)) void Model0_scm_set_cred(struct Model0_scm_cookie *Model0_scm,
        struct Model0_pid *Model0_pid, Model0_kuid_t Model0_uid, Model0_kgid_t Model0_gid)
{
 Model0_scm->Model0_pid = Model0_get_pid(Model0_pid);
 Model0_scm->Model0_creds.Model0_pid = Model0_pid_vnr(Model0_pid);
 Model0_scm->Model0_creds.Model0_uid = Model0_uid;
 Model0_scm->Model0_creds.Model0_gid = Model0_gid;
}

static __inline__ __attribute__((no_instrument_function)) void Model0_scm_destroy_cred(struct Model0_scm_cookie *Model0_scm)
{
 Model0_put_pid(Model0_scm->Model0_pid);
 Model0_scm->Model0_pid = ((void *)0);
}

static __inline__ __attribute__((no_instrument_function)) void Model0_scm_destroy(struct Model0_scm_cookie *Model0_scm)
{
 Model0_scm_destroy_cred(Model0_scm);
 if (Model0_scm->Model0_fp)
  Model0___scm_destroy(Model0_scm);
}

static __inline__ __attribute__((no_instrument_function)) int Model0_scm_send(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg,
          struct Model0_scm_cookie *Model0_scm, bool Model0_forcecreds)
{
 memset(Model0_scm, 0, sizeof(*Model0_scm));
 Model0_scm->Model0_creds.Model0_uid = (Model0_kuid_t){ -1 };
 Model0_scm->Model0_creds.Model0_gid = (Model0_kgid_t){ -1 };
 if (Model0_forcecreds)
  Model0_scm_set_cred(Model0_scm, Model0_task_tgid(Model0_get_current()), (({ ({ do { } while (0); ; ((typeof(*(Model0_get_current()->Model0_cred)) *)((Model0_get_current()->Model0_cred))); })->Model0_uid; })), (({ ({ do { } while (0); ; ((typeof(*(Model0_get_current()->Model0_cred)) *)((Model0_get_current()->Model0_cred))); })->Model0_gid; })));
 Model0_unix_get_peersec_dgram(Model0_sock, Model0_scm);
 if (Model0_msg->Model0_msg_controllen <= 0)
  return 0;
 return Model0___scm_send(Model0_sock, Model0_msg, Model0_scm);
}


static inline __attribute__((no_instrument_function)) void Model0_scm_passec(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, struct Model0_scm_cookie *Model0_scm)
{
 char *Model0_secdata;
 Model0_u32 Model0_seclen;
 int err;

 if ((__builtin_constant_p((4)) ? Model0_constant_test_bit((4), (&Model0_sock->Model0_flags)) : Model0_variable_test_bit((4), (&Model0_sock->Model0_flags)))) {
  err = Model0_security_secid_to_secctx(Model0_scm->Model0_secid, &Model0_secdata, &Model0_seclen);

  if (!err) {
   Model0_put_cmsg(Model0_msg, 1, 0x03, Model0_seclen, Model0_secdata);
   Model0_security_release_secctx(Model0_secdata, Model0_seclen);
  }
 }
}





static __inline__ __attribute__((no_instrument_function)) void Model0_scm_recv(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg,
    struct Model0_scm_cookie *Model0_scm, int Model0_flags)
{
 if (!Model0_msg->Model0_msg_control) {
  if ((__builtin_constant_p((3)) ? Model0_constant_test_bit((3), (&Model0_sock->Model0_flags)) : Model0_variable_test_bit((3), (&Model0_sock->Model0_flags))) || Model0_scm->Model0_fp)
   Model0_msg->Model0_msg_flags |= 8;
  Model0_scm_destroy(Model0_scm);
  return;
 }

 if ((__builtin_constant_p((3)) ? Model0_constant_test_bit((3), (&Model0_sock->Model0_flags)) : Model0_variable_test_bit((3), (&Model0_sock->Model0_flags)))) {
  struct Model0_user_namespace *Model0_current_ns = Model0_current_user_ns();
  struct Model0_ucred Model0_ucreds = {
   .Model0_pid = Model0_scm->Model0_creds.Model0_pid,
   .Model0_uid = Model0_from_kuid_munged(Model0_current_ns, Model0_scm->Model0_creds.Model0_uid),
   .Model0_gid = Model0_from_kgid_munged(Model0_current_ns, Model0_scm->Model0_creds.Model0_gid),
  };
  Model0_put_cmsg(Model0_msg, 1, 0x02, sizeof(Model0_ucreds), &Model0_ucreds);
 }

 Model0_scm_destroy_cred(Model0_scm);

 Model0_scm_passec(Model0_sock, Model0_msg, Model0_scm);

 if (!Model0_scm->Model0_fp)
  return;

 Model0_scm_detach_fds(Model0_msg, Model0_scm);
}
/* leave room for NETLINK_DM (DM Events) */
struct Model0_sockaddr_nl {
 Model0___kernel_sa_family_t Model0_nl_family; /* AF_NETLINK	*/
 unsigned short Model0_nl_pad; /* zero		*/
 __u32 Model0_nl_pid; /* port ID	*/
        __u32 Model0_nl_groups; /* multicast groups mask */
};

struct Model0_nlmsghdr {
 __u32 Model0_nlmsg_len; /* Length of message including header */
 Model0___u16 Model0_nlmsg_type; /* Message content */
 Model0___u16 Model0_nlmsg_flags; /* Additional flags */
 __u32 Model0_nlmsg_seq; /* Sequence number */
 __u32 Model0_nlmsg_pid; /* Sending process port ID */
};

/* Flags values */
/* Modifiers to GET request */





/* Modifiers to NEW request */





/*
   4.4BSD ADD		NLM_F_CREATE|NLM_F_EXCL
   4.4BSD CHANGE	NLM_F_REPLACE

   True CHANGE		NLM_F_CREATE|NLM_F_REPLACE
   Append		NLM_F_CREATE
   Check		NLM_F_EXCL
 */
struct Model0_nlmsgerr {
 int error;
 struct Model0_nlmsghdr Model0_msg;
};
struct Model0_nl_pktinfo {
 __u32 Model0_group;
};

struct Model0_nl_mmap_req {
 unsigned int Model0_nm_block_size;
 unsigned int Model0_nm_block_nr;
 unsigned int Model0_nm_frame_size;
 unsigned int Model0_nm_frame_nr;
};

struct Model0_nl_mmap_hdr {
 unsigned int Model0_nm_status;
 unsigned int Model0_nm_len;
 __u32 Model0_nm_group;
 /* credentials */
 __u32 Model0_nm_pid;
 __u32 Model0_nm_uid;
 __u32 Model0_nm_gid;
};
enum {
 Model0_NETLINK_UNCONNECTED = 0,
 Model0_NETLINK_CONNECTED,
};

/*
 *  <------- NLA_HDRLEN ------> <-- NLA_ALIGN(payload)-->
 * +---------------------+- - -+- - - - - - - - - -+- - -+
 * |        Header       | Pad |     Payload       | Pad |
 * |   (struct nlattr)   | ing |                   | ing |
 * +---------------------+- - -+- - - - - - - - - -+- - -+
 *  <-------------- nlattr->nla_len -------------->
 */

struct Model0_nlattr {
 Model0___u16 Model0_nla_len;
 Model0___u16 Model0_nla_type;
};

/*
 * nla_type (16 bits)
 * +---+---+-------------------------------+
 * | N | O | Attribute Type                |
 * +---+---+-------------------------------+
 * N := Carries nested attributes
 * O := Payload stored in network byte order
 *
 * Note: The N and O flag are mutually exclusive.
 */

struct Model0_net;

static inline __attribute__((no_instrument_function)) struct Model0_nlmsghdr *Model0_nlmsg_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_nlmsghdr *)Model0_skb->Model0_data;
}

enum Model0_netlink_skb_flags {
 Model0_NETLINK_SKB_MMAPED = 0x1, /* Packet data is mmaped */
 Model0_NETLINK_SKB_TX = 0x2, /* Packet was sent by userspace */
 Model0_NETLINK_SKB_DELIVERED = 0x4, /* Packet was delivered */
 Model0_NETLINK_SKB_DST = 0x8, /* Dst set in sendto or sendmsg */
};

struct Model0_netlink_skb_parms {
 struct Model0_scm_creds Model0_creds; /* Skb credentials	*/
 __u32 Model0_portid;
 __u32 Model0_dst_group;
 __u32 Model0_flags;
 struct Model0_sock *Model0_sk;
 bool Model0_nsid_is_set;
 int Model0_nsid;
};





extern void Model0_netlink_table_grab(void);
extern void Model0_netlink_table_ungrab(void);




/* optional Netlink kernel configuration parameters */
struct Model0_netlink_kernel_cfg {
 unsigned int Model0_groups;
 unsigned int Model0_flags;
 void (*Model0_input)(struct Model0_sk_buff *Model0_skb);
 struct Model0_mutex *Model0_cb_mutex;
 int (*Model0_bind)(struct Model0_net *Model0_net, int Model0_group);
 void (*Model0_unbind)(struct Model0_net *Model0_net, int Model0_group);
 bool (*Model0_compare)(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk);
};

extern struct Model0_sock *Model0___netlink_kernel_create(struct Model0_net *Model0_net, int Model0_unit,
         struct Model0_module *Model0_module,
         struct Model0_netlink_kernel_cfg *Model0_cfg);
static inline __attribute__((no_instrument_function)) struct Model0_sock *
Model0_netlink_kernel_create(struct Model0_net *Model0_net, int Model0_unit, struct Model0_netlink_kernel_cfg *Model0_cfg)
{
 return Model0___netlink_kernel_create(Model0_net, Model0_unit, ((struct Model0_module *)0), Model0_cfg);
}

extern void Model0_netlink_kernel_release(struct Model0_sock *Model0_sk);
extern int Model0___netlink_change_ngroups(struct Model0_sock *Model0_sk, unsigned int Model0_groups);
extern int Model0_netlink_change_ngroups(struct Model0_sock *Model0_sk, unsigned int Model0_groups);
extern void Model0___netlink_clear_multicast_users(struct Model0_sock *Model0_sk, unsigned int Model0_group);
extern void Model0_netlink_ack(struct Model0_sk_buff *Model0_in_skb, struct Model0_nlmsghdr *Model0_nlh, int err);
extern int Model0_netlink_has_listeners(struct Model0_sock *Model0_sk, unsigned int Model0_group);

extern int Model0_netlink_unicast(struct Model0_sock *Model0_ssk, struct Model0_sk_buff *Model0_skb, __u32 Model0_portid, int Model0_nonblock);
extern int Model0_netlink_broadcast(struct Model0_sock *Model0_ssk, struct Model0_sk_buff *Model0_skb, __u32 Model0_portid,
        __u32 Model0_group, Model0_gfp_t Model0_allocation);
extern int Model0_netlink_broadcast_filtered(struct Model0_sock *Model0_ssk, struct Model0_sk_buff *Model0_skb,
 __u32 Model0_portid, __u32 Model0_group, Model0_gfp_t Model0_allocation,
 int (*Model0_filter)(struct Model0_sock *Model0_dsk, struct Model0_sk_buff *Model0_skb, void *Model0_data),
 void *Model0_filter_data);
extern int Model0_netlink_set_err(struct Model0_sock *Model0_ssk, __u32 Model0_portid, __u32 Model0_group, int Model0_code);
extern int Model0_netlink_register_notifier(struct Model0_notifier_block *Model0_nb);
extern int Model0_netlink_unregister_notifier(struct Model0_notifier_block *Model0_nb);

/* finegrained unicast helpers: */
struct Model0_sock *Model0_netlink_getsockbyfilp(struct Model0_file *Model0_filp);
int Model0_netlink_attachskb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
        long *Model0_timeo, struct Model0_sock *Model0_ssk);
void Model0_netlink_detachskb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_netlink_sendskb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *
Model0_netlink_skb_clone(struct Model0_sk_buff *Model0_skb, Model0_gfp_t Model0_gfp_mask)
{
 struct Model0_sk_buff *Model0_nskb;

 Model0_nskb = Model0_skb_clone(Model0_skb, Model0_gfp_mask);
 if (!Model0_nskb)
  return ((void *)0);

 /* This is a large skb, set destructor callback to release head */
 if (Model0_is_vmalloc_addr(Model0_skb->Model0_head))
  Model0_nskb->Model0_destructor = Model0_skb->Model0_destructor;

 return Model0_nskb;
}

/*
 *	skb should fit one page. This choice is good for headerless malloc.
 *	But we should limit to 8K so that userspace does not have to
 *	use enormous buffer sizes on recvmsg() calls just to avoid
 *	MSG_TRUNC when PAGE_SIZE is very large.
 */
struct Model0_netlink_callback {
 struct Model0_sk_buff *Model0_skb;
 const struct Model0_nlmsghdr *Model0_nlh;
 int (*Model0_start)(struct Model0_netlink_callback *);
 int (*Model0_dump)(struct Model0_sk_buff * Model0_skb,
     struct Model0_netlink_callback *Model0_cb);
 int (*Model0_done)(struct Model0_netlink_callback *Model0_cb);
 void *Model0_data;
 /* the module that dump function belong to */
 struct Model0_module *Model0_module;
 Model0_u16 Model0_family;
 Model0_u16 Model0_min_dump_alloc;
 unsigned int Model0_prev_seq, Model0_seq;
 long Model0_args[6];
};

struct Model0_netlink_notify {
 struct Model0_net *Model0_net;
 Model0_u32 Model0_portid;
 int Model0_protocol;
};

struct Model0_nlmsghdr *
Model0___nlmsg_put(struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_portid, Model0_u32 Model0_seq, int Model0_type, int Model0_len, int Model0_flags);

struct Model0_netlink_dump_control {
 int (*Model0_start)(struct Model0_netlink_callback *);
 int (*Model0_dump)(struct Model0_sk_buff *Model0_skb, struct Model0_netlink_callback *);
 int (*Model0_done)(struct Model0_netlink_callback *);
 void *Model0_data;
 struct Model0_module *Model0_module;
 Model0_u16 Model0_min_dump_alloc;
};

extern int Model0___netlink_dump_start(struct Model0_sock *Model0_ssk, struct Model0_sk_buff *Model0_skb,
    const struct Model0_nlmsghdr *Model0_nlh,
    struct Model0_netlink_dump_control *Model0_control);
static inline __attribute__((no_instrument_function)) int Model0_netlink_dump_start(struct Model0_sock *Model0_ssk, struct Model0_sk_buff *Model0_skb,
         const struct Model0_nlmsghdr *Model0_nlh,
         struct Model0_netlink_dump_control *Model0_control)
{
 if (!Model0_control->Model0_module)
  Model0_control->Model0_module = ((struct Model0_module *)0);

 return Model0___netlink_dump_start(Model0_ssk, Model0_skb, Model0_nlh, Model0_control);
}

struct Model0_netlink_tap {
 struct Model0_net_device *Model0_dev;
 struct Model0_module *Model0_module;
 struct Model0_list_head Model0_list;
};

extern int Model0_netlink_add_tap(struct Model0_netlink_tap *Model0_nt);
extern int Model0_netlink_remove_tap(struct Model0_netlink_tap *Model0_nt);

bool Model0___netlink_ns_capable(const struct Model0_netlink_skb_parms *Model0_nsp,
     struct Model0_user_namespace *Model0_ns, int Model0_cap);
bool Model0_netlink_ns_capable(const struct Model0_sk_buff *Model0_skb,
   struct Model0_user_namespace *Model0_ns, int Model0_cap);
bool Model0_netlink_capable(const struct Model0_sk_buff *Model0_skb, int Model0_cap);
bool Model0_netlink_net_capable(const struct Model0_sk_buff *Model0_skb, int Model0_cap);

struct Model0_ndmsg {
 __u8 Model0_ndm_family;
 __u8 Model0_ndm_pad1;
 Model0___u16 Model0_ndm_pad2;
 Model0___s32 Model0_ndm_ifindex;
 Model0___u16 Model0_ndm_state;
 __u8 Model0_ndm_flags;
 __u8 Model0_ndm_type;
};

enum {
 Model0_NDA_UNSPEC,
 Model0_NDA_DST,
 Model0_NDA_LLADDR,
 Model0_NDA_CACHEINFO,
 Model0_NDA_PROBES,
 Model0_NDA_VLAN,
 Model0_NDA_PORT,
 Model0_NDA_VNI,
 Model0_NDA_IFINDEX,
 Model0_NDA_MASTER,
 Model0_NDA_LINK_NETNSID,
 Model0___NDA_MAX
};



/*
 *	Neighbor Cache Entry Flags
 */
/*
 *	Neighbor Cache Entry States.
 */
/* Dummy states */




/* NUD_NOARP & NUD_PERMANENT are pseudostates, they never change
   and make no address resolution or NUD.
   NUD_PERMANENT also cannot be deleted by garbage collectors.
 */

struct Model0_nda_cacheinfo {
 __u32 Model0_ndm_confirmed;
 __u32 Model0_ndm_used;
 __u32 Model0_ndm_updated;
 __u32 Model0_ndm_refcnt;
};

/*****************************************************************
 *		Neighbour tables specific messages.
 *
 * To retrieve the neighbour tables send RTM_GETNEIGHTBL with the
 * NLM_F_DUMP flag set. Every neighbour table configuration is
 * spread over multiple messages to avoid running into message
 * size limits on systems with many interfaces. The first message
 * in the sequence transports all not device specific data such as
 * statistics, configuration, and the default parameter set.
 * This message is followed by 0..n messages carrying device
 * specific parameter sets.
 * Although the ordering should be sufficient, NDTA_NAME can be
 * used to identify sequences. The initial message can be identified
 * by checking for NDTA_CONFIG. The device specific messages do
 * not contain this TLV but have NDTPA_IFINDEX set to the
 * corresponding interface index.
 *
 * To change neighbour table attributes, send RTM_SETNEIGHTBL
 * with NDTA_NAME set. Changeable attribute include NDTA_THRESH[1-3],
 * NDTA_GC_INTERVAL, and all TLVs in NDTA_PARMS unless marked
 * otherwise. Device specific parameter sets can be changed by
 * setting NDTPA_IFINDEX to the interface index of the corresponding
 * device.
 ****/

struct Model0_ndt_stats {
 __u64 Model0_ndts_allocs;
 __u64 Model0_ndts_destroys;
 __u64 Model0_ndts_hash_grows;
 __u64 Model0_ndts_res_failed;
 __u64 Model0_ndts_lookups;
 __u64 Model0_ndts_hits;
 __u64 Model0_ndts_rcv_probes_mcast;
 __u64 Model0_ndts_rcv_probes_ucast;
 __u64 Model0_ndts_periodic_gc_runs;
 __u64 Model0_ndts_forced_gc_runs;
 __u64 Model0_ndts_table_fulls;
};

enum {
 Model0_NDTPA_UNSPEC,
 Model0_NDTPA_IFINDEX, /* u32, unchangeable */
 Model0_NDTPA_REFCNT, /* u32, read-only */
 Model0_NDTPA_REACHABLE_TIME, /* u64, read-only, msecs */
 Model0_NDTPA_BASE_REACHABLE_TIME, /* u64, msecs */
 Model0_NDTPA_RETRANS_TIME, /* u64, msecs */
 Model0_NDTPA_GC_STALETIME, /* u64, msecs */
 Model0_NDTPA_DELAY_PROBE_TIME, /* u64, msecs */
 Model0_NDTPA_QUEUE_LEN, /* u32 */
 Model0_NDTPA_APP_PROBES, /* u32 */
 Model0_NDTPA_UCAST_PROBES, /* u32 */
 Model0_NDTPA_MCAST_PROBES, /* u32 */
 Model0_NDTPA_ANYCAST_DELAY, /* u64, msecs */
 Model0_NDTPA_PROXY_DELAY, /* u64, msecs */
 Model0_NDTPA_PROXY_QLEN, /* u32 */
 Model0_NDTPA_LOCKTIME, /* u64, msecs */
 Model0_NDTPA_QUEUE_LENBYTES, /* u32 */
 Model0_NDTPA_MCAST_REPROBES, /* u32 */
 Model0_NDTPA_PAD,
 Model0___NDTPA_MAX
};


struct Model0_ndtmsg {
 __u8 Model0_ndtm_family;
 __u8 Model0_ndtm_pad1;
 Model0___u16 Model0_ndtm_pad2;
};

struct Model0_ndt_config {
 Model0___u16 Model0_ndtc_key_len;
 Model0___u16 Model0_ndtc_entry_size;
 __u32 Model0_ndtc_entries;
 __u32 Model0_ndtc_last_flush; /* delta to now in msecs */
 __u32 Model0_ndtc_last_rand; /* delta to now in msecs */
 __u32 Model0_ndtc_hash_rnd;
 __u32 Model0_ndtc_hash_mask;
 __u32 Model0_ndtc_hash_chain_gc;
 __u32 Model0_ndtc_proxy_qlen;
};

enum {
 Model0_NDTA_UNSPEC,
 Model0_NDTA_NAME, /* char *, unchangeable */
 Model0_NDTA_THRESH1, /* u32 */
 Model0_NDTA_THRESH2, /* u32 */
 Model0_NDTA_THRESH3, /* u32 */
 Model0_NDTA_CONFIG, /* struct ndt_config, read-only */
 Model0_NDTA_PARMS, /* nested TLV NDTPA_* */
 Model0_NDTA_STATS, /* struct ndt_stats, read-only */
 Model0_NDTA_GC_INTERVAL, /* u64, msecs */
 Model0_NDTA_PAD,
 Model0___NDTA_MAX
};
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the Interfaces handler.
 *
 * Version:	@(#)dev.h	1.0.10	08/12/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Corey Minyard <wf-rch!minyard@relay.EU.net>
 *		Donald J. Becker, <becker@cesdis.gsfc.nasa.gov>
 *		Alan Cox, <alan@lxorguk.ukuu.org.uk>
 *		Bjorn Ekwall. <bj0rn@blox.se>
 *              Pekka Riikonen <priikone@poseidon.pspt.fi>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 *
 *		Moved to /usr/include/linux for NET3
 */
















/* This struct should be in sync with struct rtnl_link_stats64 */
struct Model0_rtnl_link_stats {
 __u32 Model0_rx_packets; /* total packets received	*/
 __u32 Model0_tx_packets; /* total packets transmitted	*/
 __u32 Model0_rx_bytes; /* total bytes received 	*/
 __u32 Model0_tx_bytes; /* total bytes transmitted	*/
 __u32 Model0_rx_errors; /* bad packets received		*/
 __u32 Model0_tx_errors; /* packet transmit problems	*/
 __u32 Model0_rx_dropped; /* no space in linux buffers	*/
 __u32 Model0_tx_dropped; /* no space available in linux	*/
 __u32 Model0_multicast; /* multicast packets received	*/
 __u32 Model0_collisions;

 /* detailed rx_errors: */
 __u32 Model0_rx_length_errors;
 __u32 Model0_rx_over_errors; /* receiver ring buff overflow	*/
 __u32 Model0_rx_crc_errors; /* recved pkt with crc error	*/
 __u32 Model0_rx_frame_errors; /* recv'd frame alignment error */
 __u32 Model0_rx_fifo_errors; /* recv'r fifo overrun		*/
 __u32 Model0_rx_missed_errors; /* receiver missed packet	*/

 /* detailed tx_errors */
 __u32 Model0_tx_aborted_errors;
 __u32 Model0_tx_carrier_errors;
 __u32 Model0_tx_fifo_errors;
 __u32 Model0_tx_heartbeat_errors;
 __u32 Model0_tx_window_errors;

 /* for cslip etc */
 __u32 Model0_rx_compressed;
 __u32 Model0_tx_compressed;

 __u32 Model0_rx_nohandler; /* dropped, no handler found	*/
};

/* The main device statistics structure */
struct Model0_rtnl_link_stats64 {
 __u64 Model0_rx_packets; /* total packets received	*/
 __u64 Model0_tx_packets; /* total packets transmitted	*/
 __u64 Model0_rx_bytes; /* total bytes received 	*/
 __u64 Model0_tx_bytes; /* total bytes transmitted	*/
 __u64 Model0_rx_errors; /* bad packets received		*/
 __u64 Model0_tx_errors; /* packet transmit problems	*/
 __u64 Model0_rx_dropped; /* no space in linux buffers	*/
 __u64 Model0_tx_dropped; /* no space available in linux	*/
 __u64 Model0_multicast; /* multicast packets received	*/
 __u64 Model0_collisions;

 /* detailed rx_errors: */
 __u64 Model0_rx_length_errors;
 __u64 Model0_rx_over_errors; /* receiver ring buff overflow	*/
 __u64 Model0_rx_crc_errors; /* recved pkt with crc error	*/
 __u64 Model0_rx_frame_errors; /* recv'd frame alignment error */
 __u64 Model0_rx_fifo_errors; /* recv'r fifo overrun		*/
 __u64 Model0_rx_missed_errors; /* receiver missed packet	*/

 /* detailed tx_errors */
 __u64 Model0_tx_aborted_errors;
 __u64 Model0_tx_carrier_errors;
 __u64 Model0_tx_fifo_errors;
 __u64 Model0_tx_heartbeat_errors;
 __u64 Model0_tx_window_errors;

 /* for cslip etc */
 __u64 Model0_rx_compressed;
 __u64 Model0_tx_compressed;

 __u64 Model0_rx_nohandler; /* dropped, no handler found	*/
};

/* The struct should be in sync with struct ifmap */
struct Model0_rtnl_link_ifmap {
 __u64 Model0_mem_start;
 __u64 Model0_mem_end;
 __u64 Model0_base_addr;
 Model0___u16 Model0_irq;
 __u8 Model0_dma;
 __u8 Model0_port;
};

/*
 * IFLA_AF_SPEC
 *   Contains nested attributes for address family specific attributes.
 *   Each address family may create a attribute with the address family
 *   number as type and create its own attribute structure in it.
 *
 *   Example:
 *   [IFLA_AF_SPEC] = {
 *       [AF_INET] = {
 *           [IFLA_INET_CONF] = ...,
 *       },
 *       [AF_INET6] = {
 *           [IFLA_INET6_FLAGS] = ...,
 *           [IFLA_INET6_CONF] = ...,
 *       }
 *   }
 */

enum {
 Model0_IFLA_UNSPEC,
 Model0_IFLA_ADDRESS,
 Model0_IFLA_BROADCAST,
 Model0_IFLA_IFNAME,
 Model0_IFLA_MTU,
 Model0_IFLA_LINK,
 Model0_IFLA_QDISC,
 Model0_IFLA_STATS,
 Model0_IFLA_COST,

 Model0_IFLA_PRIORITY,

 Model0_IFLA_MASTER,

 Model0_IFLA_WIRELESS, /* Wireless Extension event - see wireless.h */

 Model0_IFLA_PROTINFO, /* Protocol specific information for a link */

 Model0_IFLA_TXQLEN,

 Model0_IFLA_MAP,

 Model0_IFLA_WEIGHT,

 Model0_IFLA_OPERSTATE,
 Model0_IFLA_LINKMODE,
 Model0_IFLA_LINKINFO,

 Model0_IFLA_NET_NS_PID,
 Model0_IFLA_IFALIAS,
 Model0_IFLA_NUM_VF, /* Number of VFs if device is SR-IOV PF */
 Model0_IFLA_VFINFO_LIST,
 Model0_IFLA_STATS64,
 Model0_IFLA_VF_PORTS,
 Model0_IFLA_PORT_SELF,
 Model0_IFLA_AF_SPEC,
 Model0_IFLA_GROUP, /* Group the device belongs to */
 Model0_IFLA_NET_NS_FD,
 Model0_IFLA_EXT_MASK, /* Extended info mask, VFs, etc */
 Model0_IFLA_PROMISCUITY, /* Promiscuity count: > 0 means acts PROMISC */

 Model0_IFLA_NUM_TX_QUEUES,
 Model0_IFLA_NUM_RX_QUEUES,
 Model0_IFLA_CARRIER,
 Model0_IFLA_PHYS_PORT_ID,
 Model0_IFLA_CARRIER_CHANGES,
 Model0_IFLA_PHYS_SWITCH_ID,
 Model0_IFLA_LINK_NETNSID,
 Model0_IFLA_PHYS_PORT_NAME,
 Model0_IFLA_PROTO_DOWN,
 Model0_IFLA_GSO_MAX_SEGS,
 Model0_IFLA_GSO_MAX_SIZE,
 Model0_IFLA_PAD,
 Model0_IFLA_XDP,
 Model0___IFLA_MAX
};




/* backwards compatibility for userspace */





enum {
 Model0_IFLA_INET_UNSPEC,
 Model0_IFLA_INET_CONF,
 Model0___IFLA_INET_MAX,
};



/* ifi_flags.

   IFF_* flags.

   The only change is:
   IFF_LOOPBACK, IFF_BROADCAST and IFF_POINTOPOINT are
   more not changeable by user. They describe link media
   characteristics and set by device driver.

   Comments:
   - Combination IFF_BROADCAST|IFF_POINTOPOINT is invalid
   - If neither of these three flags are set;
     the interface is NBMA.

   - IFF_MULTICAST does not mean anything special:
   multicasts can be used on all not-NBMA links.
   IFF_MULTICAST means that this media uses special encapsulation
   for multicast frames. Apparently, all IFF_POINTOPOINT and
   IFF_BROADCAST devices are able to use multicasts too.
 */

/* IFLA_LINK.
   For usual devices it is equal ifi_index.
   If it is a "virtual interface" (f.e. tunnel), ifi_link
   can point to real physical interface (f.e. for bandwidth calculations),
   or maybe 0, what means, that real media is unknown (usual
   for IPIP tunnels, when route to endpoint is allowed to change)
 */

/* Subtype attributes for IFLA_PROTINFO */
enum {
 Model0_IFLA_INET6_UNSPEC,
 Model0_IFLA_INET6_FLAGS, /* link flags			*/
 Model0_IFLA_INET6_CONF, /* sysctl parameters		*/
 Model0_IFLA_INET6_STATS, /* statistics			*/
 Model0_IFLA_INET6_MCAST, /* MC things. What of them?	*/
 Model0_IFLA_INET6_CACHEINFO, /* time values and max reasm size */
 Model0_IFLA_INET6_ICMP6STATS, /* statistics (icmpv6)		*/
 Model0_IFLA_INET6_TOKEN, /* device token			*/
 Model0_IFLA_INET6_ADDR_GEN_MODE, /* implicit address generator mode */
 Model0___IFLA_INET6_MAX
};



enum Model0_in6_addr_gen_mode {
 Model0_IN6_ADDR_GEN_MODE_EUI64,
 Model0_IN6_ADDR_GEN_MODE_NONE,
 Model0_IN6_ADDR_GEN_MODE_STABLE_PRIVACY,
 Model0_IN6_ADDR_GEN_MODE_RANDOM,
};

/* Bridge section */

enum {
 Model0_IFLA_BR_UNSPEC,
 Model0_IFLA_BR_FORWARD_DELAY,
 Model0_IFLA_BR_HELLO_TIME,
 Model0_IFLA_BR_MAX_AGE,
 Model0_IFLA_BR_AGEING_TIME,
 Model0_IFLA_BR_STP_STATE,
 Model0_IFLA_BR_PRIORITY,
 Model0_IFLA_BR_VLAN_FILTERING,
 Model0_IFLA_BR_VLAN_PROTOCOL,
 Model0_IFLA_BR_GROUP_FWD_MASK,
 Model0_IFLA_BR_ROOT_ID,
 Model0_IFLA_BR_BRIDGE_ID,
 Model0_IFLA_BR_ROOT_PORT,
 Model0_IFLA_BR_ROOT_PATH_COST,
 Model0_IFLA_BR_TOPOLOGY_CHANGE,
 Model0_IFLA_BR_TOPOLOGY_CHANGE_DETECTED,
 Model0_IFLA_BR_HELLO_TIMER,
 Model0_IFLA_BR_TCN_TIMER,
 Model0_IFLA_BR_TOPOLOGY_CHANGE_TIMER,
 Model0_IFLA_BR_GC_TIMER,
 Model0_IFLA_BR_GROUP_ADDR,
 Model0_IFLA_BR_FDB_FLUSH,
 Model0_IFLA_BR_MCAST_ROUTER,
 Model0_IFLA_BR_MCAST_SNOOPING,
 Model0_IFLA_BR_MCAST_QUERY_USE_IFADDR,
 Model0_IFLA_BR_MCAST_QUERIER,
 Model0_IFLA_BR_MCAST_HASH_ELASTICITY,
 Model0_IFLA_BR_MCAST_HASH_MAX,
 Model0_IFLA_BR_MCAST_LAST_MEMBER_CNT,
 Model0_IFLA_BR_MCAST_STARTUP_QUERY_CNT,
 Model0_IFLA_BR_MCAST_LAST_MEMBER_INTVL,
 Model0_IFLA_BR_MCAST_MEMBERSHIP_INTVL,
 Model0_IFLA_BR_MCAST_QUERIER_INTVL,
 Model0_IFLA_BR_MCAST_QUERY_INTVL,
 Model0_IFLA_BR_MCAST_QUERY_RESPONSE_INTVL,
 Model0_IFLA_BR_MCAST_STARTUP_QUERY_INTVL,
 Model0_IFLA_BR_NF_CALL_IPTABLES,
 Model0_IFLA_BR_NF_CALL_IP6TABLES,
 Model0_IFLA_BR_NF_CALL_ARPTABLES,
 Model0_IFLA_BR_VLAN_DEFAULT_PVID,
 Model0_IFLA_BR_PAD,
 Model0_IFLA_BR_VLAN_STATS_ENABLED,
 Model0_IFLA_BR_MCAST_STATS_ENABLED,
 Model0___IFLA_BR_MAX,
};



struct Model0_ifla_bridge_id {
 __u8 Model0_prio[2];
 __u8 Model0_addr[6]; /* ETH_ALEN */
};

enum {
 Model0_BRIDGE_MODE_UNSPEC,
 Model0_BRIDGE_MODE_HAIRPIN,
};

enum {
 Model0_IFLA_BRPORT_UNSPEC,
 Model0_IFLA_BRPORT_STATE, /* Spanning tree state     */
 Model0_IFLA_BRPORT_PRIORITY, /* "             priority  */
 Model0_IFLA_BRPORT_COST, /* "             cost      */
 Model0_IFLA_BRPORT_MODE, /* mode (hairpin)          */
 Model0_IFLA_BRPORT_GUARD, /* bpdu guard              */
 Model0_IFLA_BRPORT_PROTECT, /* root port protection    */
 Model0_IFLA_BRPORT_FAST_LEAVE, /* multicast fast leave    */
 Model0_IFLA_BRPORT_LEARNING, /* mac learning */
 Model0_IFLA_BRPORT_UNICAST_FLOOD, /* flood unicast traffic */
 Model0_IFLA_BRPORT_PROXYARP, /* proxy ARP */
 Model0_IFLA_BRPORT_LEARNING_SYNC, /* mac learning sync from device */
 Model0_IFLA_BRPORT_PROXYARP_WIFI, /* proxy ARP for Wi-Fi */
 Model0_IFLA_BRPORT_ROOT_ID, /* designated root */
 Model0_IFLA_BRPORT_BRIDGE_ID, /* designated bridge */
 Model0_IFLA_BRPORT_DESIGNATED_PORT,
 Model0_IFLA_BRPORT_DESIGNATED_COST,
 Model0_IFLA_BRPORT_ID,
 Model0_IFLA_BRPORT_NO,
 Model0_IFLA_BRPORT_TOPOLOGY_CHANGE_ACK,
 Model0_IFLA_BRPORT_CONFIG_PENDING,
 Model0_IFLA_BRPORT_MESSAGE_AGE_TIMER,
 Model0_IFLA_BRPORT_FORWARD_DELAY_TIMER,
 Model0_IFLA_BRPORT_HOLD_TIMER,
 Model0_IFLA_BRPORT_FLUSH,
 Model0_IFLA_BRPORT_MULTICAST_ROUTER,
 Model0_IFLA_BRPORT_PAD,
 Model0___IFLA_BRPORT_MAX
};


struct Model0_ifla_cacheinfo {
 __u32 Model0_max_reasm_len;
 __u32 Model0_tstamp; /* ipv6InterfaceTable updated timestamp */
 __u32 Model0_reachable_time;
 __u32 Model0_retrans_time;
};

enum {
 Model0_IFLA_INFO_UNSPEC,
 Model0_IFLA_INFO_KIND,
 Model0_IFLA_INFO_DATA,
 Model0_IFLA_INFO_XSTATS,
 Model0_IFLA_INFO_SLAVE_KIND,
 Model0_IFLA_INFO_SLAVE_DATA,
 Model0___IFLA_INFO_MAX,
};



/* VLAN section */

enum {
 Model0_IFLA_VLAN_UNSPEC,
 Model0_IFLA_VLAN_ID,
 Model0_IFLA_VLAN_FLAGS,
 Model0_IFLA_VLAN_EGRESS_QOS,
 Model0_IFLA_VLAN_INGRESS_QOS,
 Model0_IFLA_VLAN_PROTOCOL,
 Model0___IFLA_VLAN_MAX,
};



struct Model0_ifla_vlan_flags {
 __u32 Model0_flags;
 __u32 Model0_mask;
};

enum {
 Model0_IFLA_VLAN_QOS_UNSPEC,
 Model0_IFLA_VLAN_QOS_MAPPING,
 Model0___IFLA_VLAN_QOS_MAX
};



struct Model0_ifla_vlan_qos_mapping {
 __u32 Model0_from;
 __u32 Model0_to;
};

/* MACVLAN section */
enum {
 Model0_IFLA_MACVLAN_UNSPEC,
 Model0_IFLA_MACVLAN_MODE,
 Model0_IFLA_MACVLAN_FLAGS,
 Model0_IFLA_MACVLAN_MACADDR_MODE,
 Model0_IFLA_MACVLAN_MACADDR,
 Model0_IFLA_MACVLAN_MACADDR_DATA,
 Model0_IFLA_MACVLAN_MACADDR_COUNT,
 Model0___IFLA_MACVLAN_MAX,
};



enum Model0_macvlan_mode {
 Model0_MACVLAN_MODE_PRIVATE = 1, /* don't talk to other macvlans */
 Model0_MACVLAN_MODE_VEPA = 2, /* talk to other ports through ext bridge */
 Model0_MACVLAN_MODE_BRIDGE = 4, /* talk to bridge ports directly */
 Model0_MACVLAN_MODE_PASSTHRU = 8,/* take over the underlying device */
 Model0_MACVLAN_MODE_SOURCE = 16,/* use source MAC address list to assign */
};

enum Model0_macvlan_macaddr_mode {
 Model0_MACVLAN_MACADDR_ADD,
 Model0_MACVLAN_MACADDR_DEL,
 Model0_MACVLAN_MACADDR_FLUSH,
 Model0_MACVLAN_MACADDR_SET,
};



/* VRF section */
enum {
 Model0_IFLA_VRF_UNSPEC,
 Model0_IFLA_VRF_TABLE,
 Model0___IFLA_VRF_MAX
};



enum {
 Model0_IFLA_VRF_PORT_UNSPEC,
 Model0_IFLA_VRF_PORT_TABLE,
 Model0___IFLA_VRF_PORT_MAX
};



/* MACSEC section */
enum {
 Model0_IFLA_MACSEC_UNSPEC,
 Model0_IFLA_MACSEC_SCI,
 Model0_IFLA_MACSEC_PORT,
 Model0_IFLA_MACSEC_ICV_LEN,
 Model0_IFLA_MACSEC_CIPHER_SUITE,
 Model0_IFLA_MACSEC_WINDOW,
 Model0_IFLA_MACSEC_ENCODING_SA,
 Model0_IFLA_MACSEC_ENCRYPT,
 Model0_IFLA_MACSEC_PROTECT,
 Model0_IFLA_MACSEC_INC_SCI,
 Model0_IFLA_MACSEC_ES,
 Model0_IFLA_MACSEC_SCB,
 Model0_IFLA_MACSEC_REPLAY_PROTECT,
 Model0_IFLA_MACSEC_VALIDATION,
 Model0_IFLA_MACSEC_PAD,
 Model0___IFLA_MACSEC_MAX,
};



enum Model0_macsec_validation_type {
 Model0_MACSEC_VALIDATE_DISABLED = 0,
 Model0_MACSEC_VALIDATE_CHECK = 1,
 Model0_MACSEC_VALIDATE_STRICT = 2,
 Model0___MACSEC_VALIDATE_END,
 Model0_MACSEC_VALIDATE_MAX = Model0___MACSEC_VALIDATE_END - 1,
};

/* IPVLAN section */
enum {
 Model0_IFLA_IPVLAN_UNSPEC,
 Model0_IFLA_IPVLAN_MODE,
 Model0___IFLA_IPVLAN_MAX
};



enum Model0_ipvlan_mode {
 Model0_IPVLAN_MODE_L2 = 0,
 Model0_IPVLAN_MODE_L3,
 Model0_IPVLAN_MODE_MAX
};

/* VXLAN section */
enum {
 Model0_IFLA_VXLAN_UNSPEC,
 Model0_IFLA_VXLAN_ID,
 Model0_IFLA_VXLAN_GROUP, /* group or remote address */
 Model0_IFLA_VXLAN_LINK,
 Model0_IFLA_VXLAN_LOCAL,
 Model0_IFLA_VXLAN_TTL,
 Model0_IFLA_VXLAN_TOS,
 Model0_IFLA_VXLAN_LEARNING,
 Model0_IFLA_VXLAN_AGEING,
 Model0_IFLA_VXLAN_LIMIT,
 Model0_IFLA_VXLAN_PORT_RANGE, /* source port */
 Model0_IFLA_VXLAN_PROXY,
 Model0_IFLA_VXLAN_RSC,
 Model0_IFLA_VXLAN_L2MISS,
 Model0_IFLA_VXLAN_L3MISS,
 Model0_IFLA_VXLAN_PORT, /* destination port */
 Model0_IFLA_VXLAN_GROUP6,
 Model0_IFLA_VXLAN_LOCAL6,
 Model0_IFLA_VXLAN_UDP_CSUM,
 Model0_IFLA_VXLAN_UDP_ZERO_CSUM6_TX,
 Model0_IFLA_VXLAN_UDP_ZERO_CSUM6_RX,
 Model0_IFLA_VXLAN_REMCSUM_TX,
 Model0_IFLA_VXLAN_REMCSUM_RX,
 Model0_IFLA_VXLAN_GBP,
 Model0_IFLA_VXLAN_REMCSUM_NOPARTIAL,
 Model0_IFLA_VXLAN_COLLECT_METADATA,
 Model0_IFLA_VXLAN_LABEL,
 Model0_IFLA_VXLAN_GPE,
 Model0___IFLA_VXLAN_MAX
};


struct Model0_ifla_vxlan_port_range {
 Model0___be16 Model0_low;
 Model0___be16 Model0_high;
};

/* GENEVE section */
enum {
 Model0_IFLA_GENEVE_UNSPEC,
 Model0_IFLA_GENEVE_ID,
 Model0_IFLA_GENEVE_REMOTE,
 Model0_IFLA_GENEVE_TTL,
 Model0_IFLA_GENEVE_TOS,
 Model0_IFLA_GENEVE_PORT, /* destination port */
 Model0_IFLA_GENEVE_COLLECT_METADATA,
 Model0_IFLA_GENEVE_REMOTE6,
 Model0_IFLA_GENEVE_UDP_CSUM,
 Model0_IFLA_GENEVE_UDP_ZERO_CSUM6_TX,
 Model0_IFLA_GENEVE_UDP_ZERO_CSUM6_RX,
 Model0_IFLA_GENEVE_LABEL,
 Model0___IFLA_GENEVE_MAX
};


/* PPP section */
enum {
 Model0_IFLA_PPP_UNSPEC,
 Model0_IFLA_PPP_DEV_FD,
 Model0___IFLA_PPP_MAX
};


/* GTP section */
enum {
 Model0_IFLA_GTP_UNSPEC,
 Model0_IFLA_GTP_FD0,
 Model0_IFLA_GTP_FD1,
 Model0_IFLA_GTP_PDP_HASHSIZE,
 Model0___IFLA_GTP_MAX,
};


/* Bonding section */

enum {
 Model0_IFLA_BOND_UNSPEC,
 Model0_IFLA_BOND_MODE,
 Model0_IFLA_BOND_ACTIVE_SLAVE,
 Model0_IFLA_BOND_MIIMON,
 Model0_IFLA_BOND_UPDELAY,
 Model0_IFLA_BOND_DOWNDELAY,
 Model0_IFLA_BOND_USE_CARRIER,
 Model0_IFLA_BOND_ARP_INTERVAL,
 Model0_IFLA_BOND_ARP_IP_TARGET,
 Model0_IFLA_BOND_ARP_VALIDATE,
 Model0_IFLA_BOND_ARP_ALL_TARGETS,
 Model0_IFLA_BOND_PRIMARY,
 Model0_IFLA_BOND_PRIMARY_RESELECT,
 Model0_IFLA_BOND_FAIL_OVER_MAC,
 Model0_IFLA_BOND_XMIT_HASH_POLICY,
 Model0_IFLA_BOND_RESEND_IGMP,
 Model0_IFLA_BOND_NUM_PEER_NOTIF,
 Model0_IFLA_BOND_ALL_SLAVES_ACTIVE,
 Model0_IFLA_BOND_MIN_LINKS,
 Model0_IFLA_BOND_LP_INTERVAL,
 Model0_IFLA_BOND_PACKETS_PER_SLAVE,
 Model0_IFLA_BOND_AD_LACP_RATE,
 Model0_IFLA_BOND_AD_SELECT,
 Model0_IFLA_BOND_AD_INFO,
 Model0_IFLA_BOND_AD_ACTOR_SYS_PRIO,
 Model0_IFLA_BOND_AD_USER_PORT_KEY,
 Model0_IFLA_BOND_AD_ACTOR_SYSTEM,
 Model0_IFLA_BOND_TLB_DYNAMIC_LB,
 Model0___IFLA_BOND_MAX,
};



enum {
 Model0_IFLA_BOND_AD_INFO_UNSPEC,
 Model0_IFLA_BOND_AD_INFO_AGGREGATOR,
 Model0_IFLA_BOND_AD_INFO_NUM_PORTS,
 Model0_IFLA_BOND_AD_INFO_ACTOR_KEY,
 Model0_IFLA_BOND_AD_INFO_PARTNER_KEY,
 Model0_IFLA_BOND_AD_INFO_PARTNER_MAC,
 Model0___IFLA_BOND_AD_INFO_MAX,
};



enum {
 Model0_IFLA_BOND_SLAVE_UNSPEC,
 Model0_IFLA_BOND_SLAVE_STATE,
 Model0_IFLA_BOND_SLAVE_MII_STATUS,
 Model0_IFLA_BOND_SLAVE_LINK_FAILURE_COUNT,
 Model0_IFLA_BOND_SLAVE_PERM_HWADDR,
 Model0_IFLA_BOND_SLAVE_QUEUE_ID,
 Model0_IFLA_BOND_SLAVE_AD_AGGREGATOR_ID,
 Model0_IFLA_BOND_SLAVE_AD_ACTOR_OPER_PORT_STATE,
 Model0_IFLA_BOND_SLAVE_AD_PARTNER_OPER_PORT_STATE,
 Model0___IFLA_BOND_SLAVE_MAX,
};



/* SR-IOV virtual function management section */

enum {
 Model0_IFLA_VF_INFO_UNSPEC,
 Model0_IFLA_VF_INFO,
 Model0___IFLA_VF_INFO_MAX,
};



enum {
 Model0_IFLA_VF_UNSPEC,
 Model0_IFLA_VF_MAC, /* Hardware queue specific attributes */
 Model0_IFLA_VF_VLAN,
 Model0_IFLA_VF_TX_RATE, /* Max TX Bandwidth Allocation */
 Model0_IFLA_VF_SPOOFCHK, /* Spoof Checking on/off switch */
 Model0_IFLA_VF_LINK_STATE, /* link state enable/disable/auto switch */
 Model0_IFLA_VF_RATE, /* Min and Max TX Bandwidth Allocation */
 Model0_IFLA_VF_RSS_QUERY_EN, /* RSS Redirection Table and Hash Key query
				 * on/off switch
				 */
 Model0_IFLA_VF_STATS, /* network device statistics */
 Model0_IFLA_VF_TRUST, /* Trust VF */
 Model0_IFLA_VF_IB_NODE_GUID, /* VF Infiniband node GUID */
 Model0_IFLA_VF_IB_PORT_GUID, /* VF Infiniband port GUID */
 Model0___IFLA_VF_MAX,
};



struct Model0_ifla_vf_mac {
 __u32 Model0_vf;
 __u8 Model0_mac[32]; /* MAX_ADDR_LEN */
};

struct Model0_ifla_vf_vlan {
 __u32 Model0_vf;
 __u32 Model0_vlan; /* 0 - 4095, 0 disables VLAN filter */
 __u32 Model0_qos;
};

struct Model0_ifla_vf_tx_rate {
 __u32 Model0_vf;
 __u32 Model0_rate; /* Max TX bandwidth in Mbps, 0 disables throttling */
};

struct Model0_ifla_vf_rate {
 __u32 Model0_vf;
 __u32 Model0_min_tx_rate; /* Min Bandwidth in Mbps */
 __u32 Model0_max_tx_rate; /* Max Bandwidth in Mbps */
};

struct Model0_ifla_vf_spoofchk {
 __u32 Model0_vf;
 __u32 Model0_setting;
};

struct Model0_ifla_vf_guid {
 __u32 Model0_vf;
 __u64 Model0_guid;
};

enum {
 Model0_IFLA_VF_LINK_STATE_AUTO, /* link state of the uplink */
 Model0_IFLA_VF_LINK_STATE_ENABLE, /* link always up */
 Model0_IFLA_VF_LINK_STATE_DISABLE, /* link always down */
 Model0___IFLA_VF_LINK_STATE_MAX,
};

struct Model0_ifla_vf_link_state {
 __u32 Model0_vf;
 __u32 Model0_link_state;
};

struct Model0_ifla_vf_rss_query_en {
 __u32 Model0_vf;
 __u32 Model0_setting;
};

enum {
 Model0_IFLA_VF_STATS_RX_PACKETS,
 Model0_IFLA_VF_STATS_TX_PACKETS,
 Model0_IFLA_VF_STATS_RX_BYTES,
 Model0_IFLA_VF_STATS_TX_BYTES,
 Model0_IFLA_VF_STATS_BROADCAST,
 Model0_IFLA_VF_STATS_MULTICAST,
 Model0_IFLA_VF_STATS_PAD,
 Model0___IFLA_VF_STATS_MAX,
};



struct Model0_ifla_vf_trust {
 __u32 Model0_vf;
 __u32 Model0_setting;
};

/* VF ports management section
 *
 *	Nested layout of set/get msg is:
 *
 *		[IFLA_NUM_VF]
 *		[IFLA_VF_PORTS]
 *			[IFLA_VF_PORT]
 *				[IFLA_PORT_*], ...
 *			[IFLA_VF_PORT]
 *				[IFLA_PORT_*], ...
 *			...
 *		[IFLA_PORT_SELF]
 *			[IFLA_PORT_*], ...
 */

enum {
 Model0_IFLA_VF_PORT_UNSPEC,
 Model0_IFLA_VF_PORT, /* nest */
 Model0___IFLA_VF_PORT_MAX,
};



enum {
 Model0_IFLA_PORT_UNSPEC,
 Model0_IFLA_PORT_VF, /* __u32 */
 Model0_IFLA_PORT_PROFILE, /* string */
 Model0_IFLA_PORT_VSI_TYPE, /* 802.1Qbg (pre-)standard VDP */
 Model0_IFLA_PORT_INSTANCE_UUID, /* binary UUID */
 Model0_IFLA_PORT_HOST_UUID, /* binary UUID */
 Model0_IFLA_PORT_REQUEST, /* __u8 */
 Model0_IFLA_PORT_RESPONSE, /* __u16, output only */
 Model0___IFLA_PORT_MAX,
};







enum {
 Model0_PORT_REQUEST_PREASSOCIATE = 0,
 Model0_PORT_REQUEST_PREASSOCIATE_RR,
 Model0_PORT_REQUEST_ASSOCIATE,
 Model0_PORT_REQUEST_DISASSOCIATE,
};

enum {
 Model0_PORT_VDP_RESPONSE_SUCCESS = 0,
 Model0_PORT_VDP_RESPONSE_INVALID_FORMAT,
 Model0_PORT_VDP_RESPONSE_INSUFFICIENT_RESOURCES,
 Model0_PORT_VDP_RESPONSE_UNUSED_VTID,
 Model0_PORT_VDP_RESPONSE_VTID_VIOLATION,
 Model0_PORT_VDP_RESPONSE_VTID_VERSION_VIOALTION,
 Model0_PORT_VDP_RESPONSE_OUT_OF_SYNC,
 /* 0x08-0xFF reserved for future VDP use */
 Model0_PORT_PROFILE_RESPONSE_SUCCESS = 0x100,
 Model0_PORT_PROFILE_RESPONSE_INPROGRESS,
 Model0_PORT_PROFILE_RESPONSE_INVALID,
 Model0_PORT_PROFILE_RESPONSE_BADSTATE,
 Model0_PORT_PROFILE_RESPONSE_INSUFFICIENT_RESOURCES,
 Model0_PORT_PROFILE_RESPONSE_ERROR,
};

struct Model0_ifla_port_vsi {
 __u8 Model0_vsi_mgr_id;
 __u8 Model0_vsi_type_id[3];
 __u8 Model0_vsi_type_version;
 __u8 Model0_pad[3];
};


/* IPoIB section */

enum {
 Model0_IFLA_IPOIB_UNSPEC,
 Model0_IFLA_IPOIB_PKEY,
 Model0_IFLA_IPOIB_MODE,
 Model0_IFLA_IPOIB_UMCAST,
 Model0___IFLA_IPOIB_MAX
};

enum {
 Model0_IPOIB_MODE_DATAGRAM = 0, /* using unreliable datagram QPs */
 Model0_IPOIB_MODE_CONNECTED = 1, /* using connected QPs */
};




/* HSR section */

enum {
 Model0_IFLA_HSR_UNSPEC,
 Model0_IFLA_HSR_SLAVE1,
 Model0_IFLA_HSR_SLAVE2,
 Model0_IFLA_HSR_MULTICAST_SPEC, /* Last byte of supervision addr */
 Model0_IFLA_HSR_SUPERVISION_ADDR, /* Supervision frame multicast addr */
 Model0_IFLA_HSR_SEQ_NR,
 Model0_IFLA_HSR_VERSION, /* HSR version */
 Model0___IFLA_HSR_MAX,
};



/* STATS section */

struct Model0_if_stats_msg {
 __u8 Model0_family;
 __u8 Model0_pad1;
 Model0___u16 Model0_pad2;
 __u32 Model0_ifindex;
 __u32 Model0_filter_mask;
};

/* A stats attribute can be netdev specific or a global stat.
 * For netdev stats, lets use the prefix IFLA_STATS_LINK_*
 */
enum {
 Model0_IFLA_STATS_UNSPEC, /* also used as 64bit pad attribute */
 Model0_IFLA_STATS_LINK_64,
 Model0_IFLA_STATS_LINK_XSTATS,
 Model0_IFLA_STATS_LINK_XSTATS_SLAVE,
 Model0___IFLA_STATS_MAX,
};





/* These are embedded into IFLA_STATS_LINK_XSTATS:
 * [IFLA_STATS_LINK_XSTATS]
 * -> [LINK_XSTATS_TYPE_xxx]
 *    -> [rtnl link type specific attributes]
 */
enum {
 Model0_LINK_XSTATS_TYPE_UNSPEC,
 Model0_LINK_XSTATS_TYPE_BRIDGE,
 Model0___LINK_XSTATS_TYPE_MAX
};


/* XDP section */

enum {
 Model0_IFLA_XDP_UNSPEC,
 Model0_IFLA_XDP_FD,
 Model0_IFLA_XDP_ATTACHED,
 Model0___IFLA_XDP_MAX,
};


/* We don't want this structure exposed to user space */
struct Model0_ifla_vf_stats {
 __u64 Model0_rx_packets;
 __u64 Model0_tx_packets;
 __u64 Model0_rx_bytes;
 __u64 Model0_tx_bytes;
 __u64 Model0_broadcast;
 __u64 Model0_multicast;
};

struct Model0_ifla_vf_info {
 __u32 Model0_vf;
 __u8 Model0_mac[32];
 __u32 Model0_vlan;
 __u32 Model0_qos;
 __u32 Model0_spoofchk;
 __u32 Model0_linkstate;
 __u32 Model0_min_tx_rate;
 __u32 Model0_max_tx_rate;
 __u32 Model0_rss_query_en;
 __u32 Model0_trusted;
};




/* Initial net device group. All devices belong to group 0 by default. */



/* interface name assignment types (sysfs name_assign_type attribute) */






/* Media selection options. */
enum {
        Model0_IF_PORT_UNKNOWN = 0,
        Model0_IF_PORT_10BASE2,
        Model0_IF_PORT_10BASET,
        Model0_IF_PORT_AUI,
        Model0_IF_PORT_100BASET,
        Model0_IF_PORT_100BASETX,
        Model0_IF_PORT_100BASEFX
};

/* hardware address assignment types */
/*
 * Bond several ethernet interfaces into a Cisco, running 'Etherchannel'.
 *
 *
 * Portions are (c) Copyright 1995 Simon "Guru Aleph-Null" Janes
 * NCM: Network and Communications Management, Inc.
 *
 * BUT, I'm the one who modified it for ethernet, so:
 * (c) Copyright 1999, Thomas Davis, tadavis@lbl.gov
 *
 *	This software may be used and distributed according to the terms
 *	of the GNU Public License, incorporated herein by reference.
 *
 * 2003/03/18 - Amir Noam <amir.noam at intel dot com>
 *	- Added support for getting slave's speed and duplex via ethtool.
 *	  Needed for 802.3ad and other future modes.
 *
 * 2003/03/18 - Tsippy Mendelson <tsippy.mendelson at intel dot com> and
 *		Shmulik Hen <shmulik.hen at intel dot com>
 *	- Enable support of modes that need to use the unique mac address of
 *	  each slave.
 *
 * 2003/03/18 - Tsippy Mendelson <tsippy.mendelson at intel dot com> and
 *		Amir Noam <amir.noam at intel dot com>
 *	- Moved driver's private data types to bonding.h
 *
 * 2003/03/18 - Amir Noam <amir.noam at intel dot com>,
 *		Tsippy Mendelson <tsippy.mendelson at intel dot com> and
 *		Shmulik Hen <shmulik.hen at intel dot com>
 *	- Added support for IEEE 802.3ad Dynamic link aggregation mode.
 *
 * 2003/05/01 - Amir Noam <amir.noam at intel dot com>
 *	- Added ABI version control to restore compatibility between
 *	  new/old ifenslave and new/old bonding.
 *
 * 2003/12/01 - Shmulik Hen <shmulik.hen at intel dot com>
 *	- Code cleanup and style changes
 *
 * 2005/05/05 - Jason Gabler <jygabler at lbl dot gov>
 *      - added definitions for various XOR hashing policies
 */
/* userland - kernel ABI version (2003/05/08) */


/*
 * We can remove these ioctl definitions in 2.5.  People should use the
 * SIOC*** versions of them instead
 */
/* each slave's link has 4 states */





/* each slave has several states */
/* hashing types */






typedef struct Model0_ifbond {
 Model0___s32 Model0_bond_mode;
 Model0___s32 Model0_num_slaves;
 Model0___s32 Model0_miimon;
} Model0_ifbond;

typedef struct Model0_ifslave {
 Model0___s32 Model0_slave_id; /* Used as an IN param to the BOND_SLAVE_INFO_QUERY ioctl */
 char Model0_slave_name[16];
 Model0___s8 Model0_link;
 Model0___s8 Model0_state;
 __u32 Model0_link_failure_count;
} Model0_ifslave;

struct Model0_ad_info {
 Model0___u16 Model0_aggregator_id;
 Model0___u16 Model0_ports;
 Model0___u16 Model0_actor_key;
 Model0___u16 Model0_partner_key;
 __u8 Model0_partner_system[6];
};



/*
 * Local variables:
 *  version-control: t
 *  kept-new-versions: 5
 *  c-indent-level: 8
 *  c-basic-offset: 8
 *  tab-width: 8
 * End:
 */









/* Logical priority bands not depending on specific packet scheduler.
   Every scheduler will map them to real traffic classes, if it has
   no more precise mechanism to classify packets.

   These numbers have no special meaning, though their coincidence
   with obsolete IPv6 values is not occasional :-). New IPv6 drafts
   preferred full anarchy inspired by diffserv group.

   Note: TC_PRIO_BESTEFFORT does not mean that it is the most unhappy
   class, actually, as rule it will be handled with more care than
   filler or even bulk.
 */
/* Generic queue statistics, available for all the elements.
   Particular schedulers may have also their private records.
 */

struct Model0_tc_stats {
 __u64 Model0_bytes; /* Number of enqueued bytes */
 __u32 Model0_packets; /* Number of enqueued packets	*/
 __u32 Model0_drops; /* Packets dropped because of lack of resources */
 __u32 Model0_overlimits; /* Number of throttle events when this
					 * flow goes out of allocated bandwidth */
 __u32 Model0_bps; /* Current flow byte rate */
 __u32 Model0_pps; /* Current flow packet rate */
 __u32 Model0_qlen;
 __u32 Model0_backlog;
};

struct Model0_tc_estimator {
 signed char Model0_interval;
 unsigned char Model0_ewma_log;
};

/* "Handles"
   ---------

    All the traffic control objects have 32bit identifiers, or "handles".

    They can be considered as opaque numbers from user API viewpoint,
    but actually they always consist of two fields: major and
    minor numbers, which are interpreted by kernel specially,
    that may be used by applications, though not recommended.

    F.e. qdisc handles always have minor number equal to zero,
    classes (or flows) have major equal to parent qdisc major, and
    minor uniquely identifying class inside qdisc.

    Macros to manipulate handles:
 */
/* Need to corrospond to iproute2 tc/tc_core.h "enum link_layer" */
enum Model0_tc_link_layer {
 Model0_TC_LINKLAYER_UNAWARE, /* Indicate unaware old iproute2 util */
 Model0_TC_LINKLAYER_ETHERNET,
 Model0_TC_LINKLAYER_ATM,
};


struct Model0_tc_ratespec {
 unsigned char Model0_cell_log;
 __u8 Model0_linklayer; /* lower 4 bits */
 unsigned short Model0_overhead;
 short Model0_cell_align;
 unsigned short Model0_mpu;
 __u32 Model0_rate;
};



struct Model0_tc_sizespec {
 unsigned char Model0_cell_log;
 unsigned char Model0_size_log;
 short Model0_cell_align;
 int Model0_overhead;
 unsigned int Model0_linklayer;
 unsigned int Model0_mpu;
 unsigned int Model0_mtu;
 unsigned int Model0_tsize;
};

enum {
 Model0_TCA_STAB_UNSPEC,
 Model0_TCA_STAB_BASE,
 Model0_TCA_STAB_DATA,
 Model0___TCA_STAB_MAX
};



/* FIFO section */

struct Model0_tc_fifo_qopt {
 __u32 Model0_limit; /* Queue length: bytes for bfifo, packets for pfifo */
};

/* PRIO section */




struct Model0_tc_prio_qopt {
 int Model0_bands; /* Number of bands */
 __u8 Model0_priomap[15 +1]; /* Map: logical priority -> PRIO band */
};

/* MULTIQ section */

struct Model0_tc_multiq_qopt {
 Model0___u16 Model0_bands; /* Number of bands */
 Model0___u16 Model0_max_bands; /* Maximum number of queues */
};

/* PLUG section */






struct Model0_tc_plug_qopt {
 /* TCQ_PLUG_BUFFER: Inset a plug into the queue and
	 *  buffer any incoming packets
	 * TCQ_PLUG_RELEASE_ONE: Dequeue packets from queue head
	 *   to beginning of the next plug.
	 * TCQ_PLUG_RELEASE_INDEFINITE: Dequeue all packets from queue.
	 *   Stop buffering packets until the next TCQ_PLUG_BUFFER
	 *   command is received (just act as a pass-thru queue).
	 * TCQ_PLUG_LIMIT: Increase/decrease queue size
	 */
 int Model0_action;
 __u32 Model0_limit;
};

/* TBF section */

struct Model0_tc_tbf_qopt {
 struct Model0_tc_ratespec Model0_rate;
 struct Model0_tc_ratespec Model0_peakrate;
 __u32 Model0_limit;
 __u32 Model0_buffer;
 __u32 Model0_mtu;
};

enum {
 Model0_TCA_TBF_UNSPEC,
 Model0_TCA_TBF_PARMS,
 Model0_TCA_TBF_RTAB,
 Model0_TCA_TBF_PTAB,
 Model0_TCA_TBF_RATE64,
 Model0_TCA_TBF_PRATE64,
 Model0_TCA_TBF_BURST,
 Model0_TCA_TBF_PBURST,
 Model0_TCA_TBF_PAD,
 Model0___TCA_TBF_MAX,
};




/* TEQL section */

/* TEQL does not require any parameters */

/* SFQ section */

struct Model0_tc_sfq_qopt {
 unsigned Model0_quantum; /* Bytes per round allocated to flow */
 int Model0_perturb_period; /* Period of hash perturbation */
 __u32 Model0_limit; /* Maximal packets in queue */
 unsigned Model0_divisor; /* Hash divisor  */
 unsigned Model0_flows; /* Maximal number of flows  */
};

struct Model0_tc_sfqred_stats {
 __u32 Model0_prob_drop; /* Early drops, below max threshold */
 __u32 Model0_forced_drop; /* Early drops, after max threshold */
 __u32 Model0_prob_mark; /* Marked packets, below max threshold */
 __u32 Model0_forced_mark; /* Marked packets, after max threshold */
 __u32 Model0_prob_mark_head; /* Marked packets, below max threshold */
 __u32 Model0_forced_mark_head;/* Marked packets, after max threshold */
};

struct Model0_tc_sfq_qopt_v1 {
 struct Model0_tc_sfq_qopt Model0_v0;
 unsigned int Model0_depth; /* max number of packets per flow */
 unsigned int Model0_headdrop;
/* SFQRED parameters */
 __u32 Model0_limit; /* HARD maximal flow queue length (bytes) */
 __u32 Model0_qth_min; /* Min average length threshold (bytes) */
 __u32 Model0_qth_max; /* Max average length threshold (bytes) */
 unsigned char Model0_Wlog; /* log(W)		*/
 unsigned char Model0_Plog; /* log(P_max/(qth_max-qth_min))	*/
 unsigned char Model0_Scell_log; /* cell size for idle damping */
 unsigned char Model0_flags;
 __u32 Model0_max_P; /* probability, high resolution */
/* SFQRED stats */
 struct Model0_tc_sfqred_stats Model0_stats;
};


struct Model0_tc_sfq_xstats {
 Model0___s32 Model0_allot;
};

/* RED section */

enum {
 Model0_TCA_RED_UNSPEC,
 Model0_TCA_RED_PARMS,
 Model0_TCA_RED_STAB,
 Model0_TCA_RED_MAX_P,
 Model0___TCA_RED_MAX,
};



struct Model0_tc_red_qopt {
 __u32 Model0_limit; /* HARD maximal queue length (bytes)	*/
 __u32 Model0_qth_min; /* Min average length threshold (bytes) */
 __u32 Model0_qth_max; /* Max average length threshold (bytes) */
 unsigned char Model0_Wlog; /* log(W)		*/
 unsigned char Model0_Plog; /* log(P_max/(qth_max-qth_min))	*/
 unsigned char Model0_Scell_log; /* cell size for idle damping */
 unsigned char Model0_flags;



};

struct Model0_tc_red_xstats {
 __u32 Model0_early; /* Early drops */
 __u32 Model0_pdrop; /* Drops due to queue limits */
 __u32 Model0_other; /* Drops due to drop() calls */
 __u32 Model0_marked; /* Marked packets */
};

/* GRED section */



enum {
       Model0_TCA_GRED_UNSPEC,
       Model0_TCA_GRED_PARMS,
       Model0_TCA_GRED_STAB,
       Model0_TCA_GRED_DPS,
       Model0_TCA_GRED_MAX_P,
       Model0_TCA_GRED_LIMIT,
       Model0___TCA_GRED_MAX,
};



struct Model0_tc_gred_qopt {
 __u32 Model0_limit; /* HARD maximal queue length (bytes)    */
 __u32 Model0_qth_min; /* Min average length threshold (bytes) */
 __u32 Model0_qth_max; /* Max average length threshold (bytes) */
 __u32 Model0_DP; /* up to 2^32 DPs */
 __u32 Model0_backlog;
 __u32 Model0_qave;
 __u32 Model0_forced;
 __u32 Model0_early;
 __u32 Model0_other;
 __u32 Model0_pdrop;
 __u8 Model0_Wlog; /* log(W)               */
 __u8 Model0_Plog; /* log(P_max/(qth_max-qth_min)) */
 __u8 Model0_Scell_log; /* cell size for idle damping */
 __u8 Model0_prio; /* prio of this VQ */
 __u32 Model0_packets;
 __u32 Model0_bytesin;
};

/* gred setup */
struct Model0_tc_gred_sopt {
 __u32 Model0_DPs;
 __u32 Model0_def_DP;
 __u8 Model0_grio;
 __u8 Model0_flags;
 Model0___u16 Model0_pad1;
};

/* CHOKe section */

enum {
 Model0_TCA_CHOKE_UNSPEC,
 Model0_TCA_CHOKE_PARMS,
 Model0_TCA_CHOKE_STAB,
 Model0_TCA_CHOKE_MAX_P,
 Model0___TCA_CHOKE_MAX,
};



struct Model0_tc_choke_qopt {
 __u32 Model0_limit; /* Hard queue length (packets)	*/
 __u32 Model0_qth_min; /* Min average threshold (packets) */
 __u32 Model0_qth_max; /* Max average threshold (packets) */
 unsigned char Model0_Wlog; /* log(W)		*/
 unsigned char Model0_Plog; /* log(P_max/(qth_max-qth_min))	*/
 unsigned char Model0_Scell_log; /* cell size for idle damping */
 unsigned char Model0_flags; /* see RED flags */
};

struct Model0_tc_choke_xstats {
 __u32 Model0_early; /* Early drops */
 __u32 Model0_pdrop; /* Drops due to queue limits */
 __u32 Model0_other; /* Drops due to drop() calls */
 __u32 Model0_marked; /* Marked packets */
 __u32 Model0_matched; /* Drops due to flow match */
};

/* HTB section */




struct Model0_tc_htb_opt {
 struct Model0_tc_ratespec Model0_rate;
 struct Model0_tc_ratespec Model0_ceil;
 __u32 Model0_buffer;
 __u32 Model0_cbuffer;
 __u32 Model0_quantum;
 __u32 Model0_level; /* out only */
 __u32 Model0_prio;
};
struct Model0_tc_htb_glob {
 __u32 Model0_version; /* to match HTB/TC */
     __u32 Model0_rate2quantum; /* bps->quantum divisor */
     __u32 Model0_defcls; /* default class number */
 __u32 Model0_debug; /* debug flags */

 /* stats */
 __u32 Model0_direct_pkts; /* count of non shaped packets */
};
enum {
 Model0_TCA_HTB_UNSPEC,
 Model0_TCA_HTB_PARMS,
 Model0_TCA_HTB_INIT,
 Model0_TCA_HTB_CTAB,
 Model0_TCA_HTB_RTAB,
 Model0_TCA_HTB_DIRECT_QLEN,
 Model0_TCA_HTB_RATE64,
 Model0_TCA_HTB_CEIL64,
 Model0_TCA_HTB_PAD,
 Model0___TCA_HTB_MAX,
};



struct Model0_tc_htb_xstats {
 __u32 Model0_lends;
 __u32 Model0_borrows;
 __u32 Model0_giants; /* too big packets (rate will not be accurate) */
 __u32 Model0_tokens;
 __u32 Model0_ctokens;
};

/* HFSC section */

struct Model0_tc_hfsc_qopt {
 Model0___u16 Model0_defcls; /* default class */
};

struct Model0_tc_service_curve {
 __u32 Model0_m1; /* slope of the first segment in bps */
 __u32 Model0_d; /* x-projection of the first segment in us */
 __u32 Model0_m2; /* slope of the second segment in bps */
};

struct Model0_tc_hfsc_stats {
 __u64 Model0_work; /* total work done */
 __u64 Model0_rtwork; /* work done by real-time criteria */
 __u32 Model0_period; /* current period */
 __u32 Model0_level; /* class level in hierarchy */
};

enum {
 Model0_TCA_HFSC_UNSPEC,
 Model0_TCA_HFSC_RSC,
 Model0_TCA_HFSC_FSC,
 Model0_TCA_HFSC_USC,
 Model0___TCA_HFSC_MAX,
};




/* CBQ section */





struct Model0_tc_cbq_lssopt {
 unsigned char Model0_change;
 unsigned char Model0_flags;


 unsigned char Model0_ewma_log;
 unsigned char Model0_level;






 __u32 Model0_maxidle;
 __u32 Model0_minidle;
 __u32 Model0_offtime;
 __u32 Model0_avpkt;
};

struct Model0_tc_cbq_wrropt {
 unsigned char Model0_flags;
 unsigned char Model0_priority;
 unsigned char Model0_cpriority;
 unsigned char Model0___reserved;
 __u32 Model0_allot;
 __u32 Model0_weight;
};

struct Model0_tc_cbq_ovl {
 unsigned char Model0_strategy;





 unsigned char Model0_priority2;
 Model0___u16 Model0_pad;
 __u32 Model0_penalty;
};

struct Model0_tc_cbq_police {
 unsigned char Model0_police;
 unsigned char Model0___res1;
 unsigned short Model0___res2;
};

struct Model0_tc_cbq_fopt {
 __u32 Model0_split;
 __u32 Model0_defmap;
 __u32 Model0_defchange;
};

struct Model0_tc_cbq_xstats {
 __u32 Model0_borrows;
 __u32 Model0_overactions;
 Model0___s32 Model0_avgidle;
 Model0___s32 Model0_undertime;
};

enum {
 Model0_TCA_CBQ_UNSPEC,
 Model0_TCA_CBQ_LSSOPT,
 Model0_TCA_CBQ_WRROPT,
 Model0_TCA_CBQ_FOPT,
 Model0_TCA_CBQ_OVL_STRATEGY,
 Model0_TCA_CBQ_RATE,
 Model0_TCA_CBQ_RTAB,
 Model0_TCA_CBQ_POLICE,
 Model0___TCA_CBQ_MAX,
};



/* dsmark section */

enum {
 Model0_TCA_DSMARK_UNSPEC,
 Model0_TCA_DSMARK_INDICES,
 Model0_TCA_DSMARK_DEFAULT_INDEX,
 Model0_TCA_DSMARK_SET_TC_INDEX,
 Model0_TCA_DSMARK_MASK,
 Model0_TCA_DSMARK_VALUE,
 Model0___TCA_DSMARK_MAX,
};



/* ATM  section */

enum {
 Model0_TCA_ATM_UNSPEC,
 Model0_TCA_ATM_FD, /* file/socket descriptor */
 Model0_TCA_ATM_PTR, /* pointer to descriptor - later */
 Model0_TCA_ATM_HDR, /* LL header */
 Model0_TCA_ATM_EXCESS, /* excess traffic class (0 for CLP)  */
 Model0_TCA_ATM_ADDR, /* PVC address (for output only) */
 Model0_TCA_ATM_STATE, /* VC state (ATM_VS_*; for output only) */
 Model0___TCA_ATM_MAX,
};



/* Network emulator */

enum {
 Model0_TCA_NETEM_UNSPEC,
 Model0_TCA_NETEM_CORR,
 Model0_TCA_NETEM_DELAY_DIST,
 Model0_TCA_NETEM_REORDER,
 Model0_TCA_NETEM_CORRUPT,
 Model0_TCA_NETEM_LOSS,
 Model0_TCA_NETEM_RATE,
 Model0_TCA_NETEM_ECN,
 Model0_TCA_NETEM_RATE64,
 Model0_TCA_NETEM_PAD,
 Model0___TCA_NETEM_MAX,
};



struct Model0_tc_netem_qopt {
 __u32 Model0_latency; /* added delay (us) */
 __u32 Model0_limit; /* fifo limit (packets) */
 __u32 Model0_loss; /* random packet loss (0=none ~0=100%) */
 __u32 Model0_gap; /* re-ordering gap (0 for none) */
 __u32 Model0_duplicate; /* random packet dup  (0=none ~0=100%) */
 __u32 Model0_jitter; /* random jitter in latency (us) */
};

struct Model0_tc_netem_corr {
 __u32 Model0_delay_corr; /* delay correlation */
 __u32 Model0_loss_corr; /* packet loss correlation */
 __u32 Model0_dup_corr; /* duplicate correlation  */
};

struct Model0_tc_netem_reorder {
 __u32 Model0_probability;
 __u32 Model0_correlation;
};

struct Model0_tc_netem_corrupt {
 __u32 Model0_probability;
 __u32 Model0_correlation;
};

struct Model0_tc_netem_rate {
 __u32 Model0_rate; /* byte/s */
 Model0___s32 Model0_packet_overhead;
 __u32 Model0_cell_size;
 Model0___s32 Model0_cell_overhead;
};

enum {
 Model0_NETEM_LOSS_UNSPEC,
 Model0_NETEM_LOSS_GI, /* General Intuitive - 4 state model */
 Model0_NETEM_LOSS_GE, /* Gilbert Elliot models */
 Model0___NETEM_LOSS_MAX
};


/* State transition probabilities for 4 state model */
struct Model0_tc_netem_gimodel {
 __u32 Model0_p13;
 __u32 Model0_p31;
 __u32 Model0_p32;
 __u32 Model0_p14;
 __u32 Model0_p23;
};

/* Gilbert-Elliot models */
struct Model0_tc_netem_gemodel {
 __u32 Model0_p;
 __u32 Model0_r;
 __u32 Model0_h;
 __u32 Model0_k1;
};




/* DRR */

enum {
 Model0_TCA_DRR_UNSPEC,
 Model0_TCA_DRR_QUANTUM,
 Model0___TCA_DRR_MAX
};



struct Model0_tc_drr_stats {
 __u32 Model0_deficit;
};

/* MQPRIO */



struct Model0_tc_mqprio_qopt {
 __u8 Model0_num_tc;
 __u8 Model0_prio_tc_map[15 + 1];
 __u8 Model0_hw;
 Model0___u16 Model0_count[16];
 Model0___u16 Model0_offset[16];
};

/* SFB */

enum {
 Model0_TCA_SFB_UNSPEC,
 Model0_TCA_SFB_PARMS,
 Model0___TCA_SFB_MAX,
};



/*
 * Note: increment, decrement are Q0.16 fixed-point values.
 */
struct Model0_tc_sfb_qopt {
 __u32 Model0_rehash_interval; /* delay between hash move, in ms */
 __u32 Model0_warmup_time; /* double buffering warmup time in ms (warmup_time < rehash_interval) */
 __u32 Model0_max; /* max len of qlen_min */
 __u32 Model0_bin_size; /* maximum queue length per bin */
 __u32 Model0_increment; /* probability increment, (d1 in Blue) */
 __u32 Model0_decrement; /* probability decrement, (d2 in Blue) */
 __u32 Model0_limit; /* max SFB queue length */
 __u32 Model0_penalty_rate; /* inelastic flows are rate limited to 'rate' pps */
 __u32 Model0_penalty_burst;
};

struct Model0_tc_sfb_xstats {
 __u32 Model0_earlydrop;
 __u32 Model0_penaltydrop;
 __u32 Model0_bucketdrop;
 __u32 Model0_queuedrop;
 __u32 Model0_childdrop; /* drops in child qdisc */
 __u32 Model0_marked;
 __u32 Model0_maxqlen;
 __u32 Model0_maxprob;
 __u32 Model0_avgprob;
};



/* QFQ */
enum {
 Model0_TCA_QFQ_UNSPEC,
 Model0_TCA_QFQ_WEIGHT,
 Model0_TCA_QFQ_LMAX,
 Model0___TCA_QFQ_MAX
};



struct Model0_tc_qfq_stats {
 __u32 Model0_weight;
 __u32 Model0_lmax;
};

/* CODEL */

enum {
 Model0_TCA_CODEL_UNSPEC,
 Model0_TCA_CODEL_TARGET,
 Model0_TCA_CODEL_LIMIT,
 Model0_TCA_CODEL_INTERVAL,
 Model0_TCA_CODEL_ECN,
 Model0_TCA_CODEL_CE_THRESHOLD,
 Model0___TCA_CODEL_MAX
};



struct Model0_tc_codel_xstats {
 __u32 Model0_maxpacket; /* largest packet we've seen so far */
 __u32 Model0_count; /* how many drops we've done since the last time we
			    * entered dropping state
			    */
 __u32 Model0_lastcount; /* count at entry to dropping state */
 __u32 Model0_ldelay; /* in-queue delay seen by most recently dequeued packet */
 Model0___s32 Model0_drop_next; /* time to drop next packet */
 __u32 Model0_drop_overlimit; /* number of time max qdisc packet limit was hit */
 __u32 Model0_ecn_mark; /* number of packets we ECN marked instead of dropped */
 __u32 Model0_dropping; /* are we in dropping state ? */
 __u32 Model0_ce_mark; /* number of CE marked packets because of ce_threshold */
};

/* FQ_CODEL */

enum {
 Model0_TCA_FQ_CODEL_UNSPEC,
 Model0_TCA_FQ_CODEL_TARGET,
 Model0_TCA_FQ_CODEL_LIMIT,
 Model0_TCA_FQ_CODEL_INTERVAL,
 Model0_TCA_FQ_CODEL_ECN,
 Model0_TCA_FQ_CODEL_FLOWS,
 Model0_TCA_FQ_CODEL_QUANTUM,
 Model0_TCA_FQ_CODEL_CE_THRESHOLD,
 Model0_TCA_FQ_CODEL_DROP_BATCH_SIZE,
 Model0_TCA_FQ_CODEL_MEMORY_LIMIT,
 Model0___TCA_FQ_CODEL_MAX
};



enum {
 Model0_TCA_FQ_CODEL_XSTATS_QDISC,
 Model0_TCA_FQ_CODEL_XSTATS_CLASS,
};

struct Model0_tc_fq_codel_qd_stats {
 __u32 Model0_maxpacket; /* largest packet we've seen so far */
 __u32 Model0_drop_overlimit; /* number of time max qdisc
				 * packet limit was hit
				 */
 __u32 Model0_ecn_mark; /* number of packets we ECN marked
				 * instead of being dropped
				 */
 __u32 Model0_new_flow_count; /* number of time packets
				 * created a 'new flow'
				 */
 __u32 Model0_new_flows_len; /* count of flows in new list */
 __u32 Model0_old_flows_len; /* count of flows in old list */
 __u32 Model0_ce_mark; /* packets above ce_threshold */
 __u32 Model0_memory_usage; /* in bytes */
 __u32 Model0_drop_overmemory;
};

struct Model0_tc_fq_codel_cl_stats {
 Model0___s32 Model0_deficit;
 __u32 Model0_ldelay; /* in-queue delay seen by most recently
				 * dequeued packet
				 */
 __u32 Model0_count;
 __u32 Model0_lastcount;
 __u32 Model0_dropping;
 Model0___s32 Model0_drop_next;
};

struct Model0_tc_fq_codel_xstats {
 __u32 Model0_type;
 union {
  struct Model0_tc_fq_codel_qd_stats Model0_qdisc_stats;
  struct Model0_tc_fq_codel_cl_stats Model0_class_stats;
 };
};

/* FQ */

enum {
 Model0_TCA_FQ_UNSPEC,

 Model0_TCA_FQ_PLIMIT, /* limit of total number of packets in queue */

 Model0_TCA_FQ_FLOW_PLIMIT, /* limit of packets per flow */

 Model0_TCA_FQ_QUANTUM, /* RR quantum */

 Model0_TCA_FQ_INITIAL_QUANTUM, /* RR quantum for new flow */

 Model0_TCA_FQ_RATE_ENABLE, /* enable/disable rate limiting */

 Model0_TCA_FQ_FLOW_DEFAULT_RATE,/* obsolete, do not use */

 Model0_TCA_FQ_FLOW_MAX_RATE, /* per flow max rate */

 Model0_TCA_FQ_BUCKETS_LOG, /* log2(number of buckets) */

 Model0_TCA_FQ_FLOW_REFILL_DELAY, /* flow credit refill delay in usec */

 Model0_TCA_FQ_ORPHAN_MASK, /* mask applied to orphaned skb hashes */

 Model0___TCA_FQ_MAX
};



struct Model0_tc_fq_qd_stats {
 __u64 Model0_gc_flows;
 __u64 Model0_highprio_packets;
 __u64 Model0_tcp_retrans;
 __u64 Model0_throttled;
 __u64 Model0_flows_plimit;
 __u64 Model0_pkts_too_long;
 __u64 Model0_allocation_errors;
 Model0___s64 Model0_time_next_delayed_flow;
 __u32 Model0_flows;
 __u32 Model0_inactive_flows;
 __u32 Model0_throttled_flows;
 __u32 Model0_pad;
};

/* Heavy-Hitter Filter */

enum {
 Model0_TCA_HHF_UNSPEC,
 Model0_TCA_HHF_BACKLOG_LIMIT,
 Model0_TCA_HHF_QUANTUM,
 Model0_TCA_HHF_HH_FLOWS_LIMIT,
 Model0_TCA_HHF_RESET_TIMEOUT,
 Model0_TCA_HHF_ADMIT_BYTES,
 Model0_TCA_HHF_EVICT_TIMEOUT,
 Model0_TCA_HHF_NON_HH_WEIGHT,
 Model0___TCA_HHF_MAX
};



struct Model0_tc_hhf_xstats {
 __u32 Model0_drop_overlimit; /* number of times max qdisc packet limit
				 * was hit
				 */
 __u32 Model0_hh_overlimit; /* number of times max heavy-hitters was hit */
 __u32 Model0_hh_tot_count; /* number of captured heavy-hitters so far */
 __u32 Model0_hh_cur_count; /* number of current heavy-hitters */
};

/* PIE */
enum {
 Model0_TCA_PIE_UNSPEC,
 Model0_TCA_PIE_TARGET,
 Model0_TCA_PIE_LIMIT,
 Model0_TCA_PIE_TUPDATE,
 Model0_TCA_PIE_ALPHA,
 Model0_TCA_PIE_BETA,
 Model0_TCA_PIE_ECN,
 Model0_TCA_PIE_BYTEMODE,
 Model0___TCA_PIE_MAX
};


struct Model0_tc_pie_xstats {
 __u32 Model0_prob; /* current probability */
 __u32 Model0_delay; /* current delay in ms */
 __u32 Model0_avg_dq_rate; /* current average dq_rate in bits/pie_time */
 __u32 Model0_packets_in; /* total number of packets enqueued */
 __u32 Model0_dropped; /* packets dropped due to pie_action */
 __u32 Model0_overlimit; /* dropped due to lack of space in queue */
 __u32 Model0_maxq; /* maximum queue size */
 __u32 Model0_ecn_mark; /* packets marked with ecn*/
};


/* I think i could have done better macros ; for now this is stolen from
 * some arch/mips code - jhs
*/







/* verdict bit breakdown 
 *
bit 0: when set -> this packet has been munged already

bit 1: when set -> It is ok to munge this packet

bit 2,3,4,5: Reclassify counter - sort of reverse TTL - if exceeded
assume loop

bit 6,7: Where this packet was last seen 
0: Above the transmit example at the socket level
1: on the Ingress
2: on the Egress

bit 8: when set --> Request not to classify on ingress. 

bits 9,10,11: redirect counter -  redirect TTL. Loop avoidance

 *
 * */
/* Action attributes */
enum {
 Model0_TCA_ACT_UNSPEC,
 Model0_TCA_ACT_KIND,
 Model0_TCA_ACT_OPTIONS,
 Model0_TCA_ACT_INDEX,
 Model0_TCA_ACT_STATS,
 Model0_TCA_ACT_PAD,
 Model0___TCA_ACT_MAX
};
/* Action type identifiers*/
enum {
 Model0_TCA_ID_UNSPEC=0,
 Model0_TCA_ID_POLICE=1,
 /* other actions go here */
 Model0___TCA_ID_MAX=255
};



struct Model0_tc_police {
 __u32 Model0_index;
 int Model0_action;






 __u32 Model0_limit;
 __u32 Model0_burst;
 __u32 Model0_mtu;
 struct Model0_tc_ratespec Model0_rate;
 struct Model0_tc_ratespec Model0_peakrate;
 int Model0_refcnt;
 int Model0_bindcnt;
 __u32 Model0_capab;
};

struct Model0_tcf_t {
 __u64 Model0_install;
 __u64 Model0_lastuse;
 __u64 Model0_expires;
 __u64 Model0_firstuse;
};

struct Model0_tc_cnt {
 int Model0_refcnt;
 int Model0_bindcnt;
};
enum {
 Model0_TCA_POLICE_UNSPEC,
 Model0_TCA_POLICE_TBF,
 Model0_TCA_POLICE_RATE,
 Model0_TCA_POLICE_PEAKRATE,
 Model0_TCA_POLICE_AVRATE,
 Model0_TCA_POLICE_RESULT,
 Model0_TCA_POLICE_TM,
 Model0_TCA_POLICE_PAD,
 Model0___TCA_POLICE_MAX

};



/* tca flags definitions */



/* U32 filters */
enum {
 Model0_TCA_U32_UNSPEC,
 Model0_TCA_U32_CLASSID,
 Model0_TCA_U32_HASH,
 Model0_TCA_U32_LINK,
 Model0_TCA_U32_DIVISOR,
 Model0_TCA_U32_SEL,
 Model0_TCA_U32_POLICE,
 Model0_TCA_U32_ACT,
 Model0_TCA_U32_INDEV,
 Model0_TCA_U32_PCNT,
 Model0_TCA_U32_MARK,
 Model0_TCA_U32_FLAGS,
 Model0_TCA_U32_PAD,
 Model0___TCA_U32_MAX
};



struct Model0_tc_u32_key {
 Model0___be32 Model0_mask;
 Model0___be32 Model0_val;
 int Model0_off;
 int Model0_offmask;
};

struct Model0_tc_u32_sel {
 unsigned char Model0_flags;
 unsigned char Model0_offshift;
 unsigned char Model0_nkeys;

 Model0___be16 Model0_offmask;
 Model0___u16 Model0_off;
 short Model0_offoff;

 short Model0_hoff;
 Model0___be32 Model0_hmask;
 struct Model0_tc_u32_key Model0_keys[0];
};

struct Model0_tc_u32_mark {
 __u32 Model0_val;
 __u32 Model0_mask;
 __u32 Model0_success;
};

struct Model0_tc_u32_pcnt {
 __u64 Model0_rcnt;
 __u64 Model0_rhit;
 __u64 Model0_kcnts[0];
};

/* Flags */
/* RSVP filter */

enum {
 Model0_TCA_RSVP_UNSPEC,
 Model0_TCA_RSVP_CLASSID,
 Model0_TCA_RSVP_DST,
 Model0_TCA_RSVP_SRC,
 Model0_TCA_RSVP_PINFO,
 Model0_TCA_RSVP_POLICE,
 Model0_TCA_RSVP_ACT,
 Model0___TCA_RSVP_MAX
};



struct Model0_tc_rsvp_gpi {
 __u32 Model0_key;
 __u32 Model0_mask;
 int Model0_offset;
};

struct Model0_tc_rsvp_pinfo {
 struct Model0_tc_rsvp_gpi Model0_dpi;
 struct Model0_tc_rsvp_gpi Model0_spi;
 __u8 Model0_protocol;
 __u8 Model0_tunnelid;
 __u8 Model0_tunnelhdr;
 __u8 Model0_pad;
};

/* ROUTE filter */

enum {
 Model0_TCA_ROUTE4_UNSPEC,
 Model0_TCA_ROUTE4_CLASSID,
 Model0_TCA_ROUTE4_TO,
 Model0_TCA_ROUTE4_FROM,
 Model0_TCA_ROUTE4_IIF,
 Model0_TCA_ROUTE4_POLICE,
 Model0_TCA_ROUTE4_ACT,
 Model0___TCA_ROUTE4_MAX
};




/* FW filter */

enum {
 Model0_TCA_FW_UNSPEC,
 Model0_TCA_FW_CLASSID,
 Model0_TCA_FW_POLICE,
 Model0_TCA_FW_INDEV, /*  used by CONFIG_NET_CLS_IND */
 Model0_TCA_FW_ACT, /* used by CONFIG_NET_CLS_ACT */
 Model0_TCA_FW_MASK,
 Model0___TCA_FW_MAX
};



/* TC index filter */

enum {
 Model0_TCA_TCINDEX_UNSPEC,
 Model0_TCA_TCINDEX_HASH,
 Model0_TCA_TCINDEX_MASK,
 Model0_TCA_TCINDEX_SHIFT,
 Model0_TCA_TCINDEX_FALL_THROUGH,
 Model0_TCA_TCINDEX_CLASSID,
 Model0_TCA_TCINDEX_POLICE,
 Model0_TCA_TCINDEX_ACT,
 Model0___TCA_TCINDEX_MAX
};



/* Flow filter */

enum {
 Model0_FLOW_KEY_SRC,
 Model0_FLOW_KEY_DST,
 Model0_FLOW_KEY_PROTO,
 Model0_FLOW_KEY_PROTO_SRC,
 Model0_FLOW_KEY_PROTO_DST,
 Model0_FLOW_KEY_IIF,
 Model0_FLOW_KEY_PRIORITY,
 Model0_FLOW_KEY_MARK,
 Model0_FLOW_KEY_NFCT,
 Model0_FLOW_KEY_NFCT_SRC,
 Model0_FLOW_KEY_NFCT_DST,
 Model0_FLOW_KEY_NFCT_PROTO_SRC,
 Model0_FLOW_KEY_NFCT_PROTO_DST,
 Model0_FLOW_KEY_RTCLASSID,
 Model0_FLOW_KEY_SKUID,
 Model0_FLOW_KEY_SKGID,
 Model0_FLOW_KEY_VLAN_TAG,
 Model0_FLOW_KEY_RXHASH,
 Model0___FLOW_KEY_MAX,
};



enum {
 Model0_FLOW_MODE_MAP,
 Model0_FLOW_MODE_HASH,
};

enum {
 Model0_TCA_FLOW_UNSPEC,
 Model0_TCA_FLOW_KEYS,
 Model0_TCA_FLOW_MODE,
 Model0_TCA_FLOW_BASECLASS,
 Model0_TCA_FLOW_RSHIFT,
 Model0_TCA_FLOW_ADDEND,
 Model0_TCA_FLOW_MASK,
 Model0_TCA_FLOW_XOR,
 Model0_TCA_FLOW_DIVISOR,
 Model0_TCA_FLOW_ACT,
 Model0_TCA_FLOW_POLICE,
 Model0_TCA_FLOW_EMATCHES,
 Model0_TCA_FLOW_PERTURB,
 Model0___TCA_FLOW_MAX
};



/* Basic filter */

enum {
 Model0_TCA_BASIC_UNSPEC,
 Model0_TCA_BASIC_CLASSID,
 Model0_TCA_BASIC_EMATCHES,
 Model0_TCA_BASIC_ACT,
 Model0_TCA_BASIC_POLICE,
 Model0___TCA_BASIC_MAX
};




/* Cgroup classifier */

enum {
 Model0_TCA_CGROUP_UNSPEC,
 Model0_TCA_CGROUP_ACT,
 Model0_TCA_CGROUP_POLICE,
 Model0_TCA_CGROUP_EMATCHES,
 Model0___TCA_CGROUP_MAX,
};



/* BPF classifier */



enum {
 Model0_TCA_BPF_UNSPEC,
 Model0_TCA_BPF_ACT,
 Model0_TCA_BPF_POLICE,
 Model0_TCA_BPF_CLASSID,
 Model0_TCA_BPF_OPS_LEN,
 Model0_TCA_BPF_OPS,
 Model0_TCA_BPF_FD,
 Model0_TCA_BPF_NAME,
 Model0_TCA_BPF_FLAGS,
 Model0___TCA_BPF_MAX,
};



/* Flower classifier */

enum {
 Model0_TCA_FLOWER_UNSPEC,
 Model0_TCA_FLOWER_CLASSID,
 Model0_TCA_FLOWER_INDEV,
 Model0_TCA_FLOWER_ACT,
 Model0_TCA_FLOWER_KEY_ETH_DST, /* ETH_ALEN */
 Model0_TCA_FLOWER_KEY_ETH_DST_MASK, /* ETH_ALEN */
 Model0_TCA_FLOWER_KEY_ETH_SRC, /* ETH_ALEN */
 Model0_TCA_FLOWER_KEY_ETH_SRC_MASK, /* ETH_ALEN */
 Model0_TCA_FLOWER_KEY_ETH_TYPE, /* be16 */
 Model0_TCA_FLOWER_KEY_IP_PROTO, /* u8 */
 Model0_TCA_FLOWER_KEY_IPV4_SRC, /* be32 */
 Model0_TCA_FLOWER_KEY_IPV4_SRC_MASK, /* be32 */
 Model0_TCA_FLOWER_KEY_IPV4_DST, /* be32 */
 Model0_TCA_FLOWER_KEY_IPV4_DST_MASK, /* be32 */
 Model0_TCA_FLOWER_KEY_IPV6_SRC, /* struct in6_addr */
 Model0_TCA_FLOWER_KEY_IPV6_SRC_MASK, /* struct in6_addr */
 Model0_TCA_FLOWER_KEY_IPV6_DST, /* struct in6_addr */
 Model0_TCA_FLOWER_KEY_IPV6_DST_MASK, /* struct in6_addr */
 Model0_TCA_FLOWER_KEY_TCP_SRC, /* be16 */
 Model0_TCA_FLOWER_KEY_TCP_DST, /* be16 */
 Model0_TCA_FLOWER_KEY_UDP_SRC, /* be16 */
 Model0_TCA_FLOWER_KEY_UDP_DST, /* be16 */

 Model0_TCA_FLOWER_FLAGS,
 Model0___TCA_FLOWER_MAX,
};



/* Match-all classifier */

enum {
 Model0_TCA_MATCHALL_UNSPEC,
 Model0_TCA_MATCHALL_CLASSID,
 Model0_TCA_MATCHALL_ACT,
 Model0_TCA_MATCHALL_FLAGS,
 Model0___TCA_MATCHALL_MAX,
};



/* Extended Matches */

struct Model0_tcf_ematch_tree_hdr {
 Model0___u16 Model0_nmatches;
 Model0___u16 Model0_progid;
};

enum {
 Model0_TCA_EMATCH_TREE_UNSPEC,
 Model0_TCA_EMATCH_TREE_HDR,
 Model0_TCA_EMATCH_TREE_LIST,
 Model0___TCA_EMATCH_TREE_MAX
};


struct Model0_tcf_ematch_hdr {
 Model0___u16 Model0_matchid;
 Model0___u16 Model0_kind;
 Model0___u16 Model0_flags;
 Model0___u16 Model0_pad; /* currently unused */
};

/*  0                   1
 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 
 * +-----------------------+-+-+---+
 * |         Unused        |S|I| R |
 * +-----------------------+-+-+---+
 *
 * R(2) ::= relation to next ematch
 *          where: 0 0 END (last ematch)
 *                 0 1 AND
 *                 1 0 OR
 *                 1 1 Unused (invalid)
 * I(1) ::= invert result
 * S(1) ::= simple payload
 */
enum {
 Model0_TCF_LAYER_LINK,
 Model0_TCF_LAYER_NETWORK,
 Model0_TCF_LAYER_TRANSPORT,
 Model0___TCF_LAYER_MAX
};


/* Ematch type assignments
 *   1..32767		Reserved for ematches inside kernel tree
 *   32768..65535	Free to use, not reliable
 */
enum {
 Model0_TCF_EM_PROG_TC
};

enum {
 Model0_TCF_EM_OPND_EQ,
 Model0_TCF_EM_OPND_GT,
 Model0_TCF_EM_OPND_LT
};

struct Model0_netpoll_info;
struct Model0_device;
struct Model0_phy_device;
/* 802.11 specific */
struct Model0_wireless_dev;
/* 802.15.4 specific */
struct Model0_wpan_dev;
struct Model0_mpls_dev;
/* UDP Tunnel offloads */
struct Model0_udp_tunnel_info;
struct Model0_bpf_prog;

void Model0_netdev_set_default_ethtool_ops(struct Model0_net_device *Model0_dev,
        const struct Model0_ethtool_ops *Model0_ops);

/* Backlog congestion levels */



/*
 * Transmit return codes: transmit return codes originate from three different
 * namespaces:
 *
 * - qdisc return codes
 * - driver transmit return codes
 * - errno values
 *
 * Drivers are allowed to return any one of those in their hard_start_xmit()
 * function. Real network devices commonly used with qdiscs should only return
 * the driver transmit return codes though - when qdiscs are used, the actual
 * transmission happens asynchronously, so the value is not propagated to
 * higher layers. Virtual network devices transmit synchronously; in this case
 * the driver transmit return codes are consumed by dev_queue_xmit(), and all
 * others are propagated to higher layers.
 */

/* qdisc ->enqueue() return codes. */





/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It
 * indicates that the device will soon be dropping packets, or already drops
 * some packets of the same priority; prompting us to send less aggressively. */



/* Driver transmit return codes */


enum Model0_netdev_tx {
 Model0___NETDEV_TX_MIN = (-((int)(~0U>>1)) - 1), /* make sure enum is signed */
 Model0_NETDEV_TX_OK = 0x00, /* driver took care of packet */
 Model0_NETDEV_TX_BUSY = 0x10, /* driver tx path was busy*/
};
typedef enum Model0_netdev_tx Model0_netdev_tx_t;

/*
 * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;
 * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.
 */
static inline __attribute__((no_instrument_function)) bool Model0_dev_xmit_complete(int Model0_rc)
{
 /*
	 * Positive cases with an skb consumed by a driver:
	 * - successful transmission (rc == NETDEV_TX_OK)
	 * - error while transmitting (rc < 0)
	 * - error while queueing to a different device (rc & NET_XMIT_MASK)
	 */
 if (__builtin_expect(!!(Model0_rc < 0x0f), 1))
  return true;

 return false;
}

/*
 *	Compute the worst-case header length according to the protocols
 *	used.
 */
/*
 *	Old network device statistics. Fields are native words
 *	(unsigned long) so they can be read and written atomically.
 */

struct Model0_net_device_stats {
 unsigned long Model0_rx_packets;
 unsigned long Model0_tx_packets;
 unsigned long Model0_rx_bytes;
 unsigned long Model0_tx_bytes;
 unsigned long Model0_rx_errors;
 unsigned long Model0_tx_errors;
 unsigned long Model0_rx_dropped;
 unsigned long Model0_tx_dropped;
 unsigned long Model0_multicast;
 unsigned long Model0_collisions;
 unsigned long Model0_rx_length_errors;
 unsigned long Model0_rx_over_errors;
 unsigned long Model0_rx_crc_errors;
 unsigned long Model0_rx_frame_errors;
 unsigned long Model0_rx_fifo_errors;
 unsigned long Model0_rx_missed_errors;
 unsigned long Model0_tx_aborted_errors;
 unsigned long Model0_tx_carrier_errors;
 unsigned long Model0_tx_fifo_errors;
 unsigned long Model0_tx_heartbeat_errors;
 unsigned long Model0_tx_window_errors;
 unsigned long Model0_rx_compressed;
 unsigned long Model0_tx_compressed;
};







extern struct Model0_static_key Model0_rps_needed;


struct Model0_neighbour;
struct Model0_neigh_parms;
struct Model0_sk_buff;

struct Model0_netdev_hw_addr {
 struct Model0_list_head Model0_list;
 unsigned char Model0_addr[32];
 unsigned char Model0_type;





 bool Model0_global_use;
 int Model0_sync_cnt;
 int Model0_refcount;
 int Model0_synced;
 struct Model0_callback_head Model0_callback_head;
};

struct Model0_netdev_hw_addr_list {
 struct Model0_list_head Model0_list;
 int Model0_count;
};
struct Model0_hh_cache {
 Model0_u16 Model0_hh_len;
 Model0_u16 Model0___pad;
 Model0_seqlock_t Model0_hh_lock;

 /* cached hardware header; allow for machine alignment needs.        */





 unsigned long Model0_hh_data[(((96)+(16 -1))&~(16 - 1)) / sizeof(long)];
};

/* Reserve HH_DATA_MOD byte-aligned hard_header_len, but at least that much.
 * Alternative is:
 *   dev->hard_header_len ? (dev->hard_header_len +
 *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0
 *
 * We could use other alignment values, but we must maintain the
 * relationship HH alignment <= LL alignment.
 */





struct Model0_header_ops {
 int (*Model0_create) (struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
      unsigned short Model0_type, const void *Model0_daddr,
      const void *Model0_saddr, unsigned int Model0_len);
 int (*Model0_parse)(const struct Model0_sk_buff *Model0_skb, unsigned char *Model0_haddr);
 int (*Model0_cache)(const struct Model0_neighbour *Model0_neigh, struct Model0_hh_cache *Model0_hh, Model0___be16 Model0_type);
 void (*Model0_cache_update)(struct Model0_hh_cache *Model0_hh,
    const struct Model0_net_device *Model0_dev,
    const unsigned char *Model0_haddr);
 bool (*Model0_validate)(const char *Model0_ll_header, unsigned int Model0_len);
};

/* These flag bits are private to the generic network queueing
 * layer; they may not be explicitly referenced by any other
 * code.
 */

enum Model0_netdev_state_t {
 Model0___LINK_STATE_START,
 Model0___LINK_STATE_PRESENT,
 Model0___LINK_STATE_NOCARRIER,
 Model0___LINK_STATE_LINKWATCH_PENDING,
 Model0___LINK_STATE_DORMANT,
};


/*
 * This structure holds boot-time configured netdevice settings. They
 * are then used in the device probing.
 */
struct Model0_netdev_boot_setup {
 char Model0_name[16];
 struct Model0_ifmap Model0_map;
};


int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_netdev_boot_setup(char *Model0_str);

/*
 * Structure for NAPI scheduling similar to tasklet but with weighting
 */
struct Model0_napi_struct {
 /* The poll_list must only be managed by the entity which
	 * changes the state of the NAPI_STATE_SCHED bit.  This means
	 * whoever atomically sets that bit can add this napi_struct
	 * to the per-CPU poll_list, and whoever clears that bit
	 * can remove from the list right before clearing the bit.
	 */
 struct Model0_list_head Model0_poll_list;

 unsigned long Model0_state;
 int Model0_weight;
 unsigned int Model0_gro_count;
 int (*Model0_poll)(struct Model0_napi_struct *, int);

 Model0_spinlock_t Model0_poll_lock;
 int Model0_poll_owner;

 struct Model0_net_device *Model0_dev;
 struct Model0_sk_buff *Model0_gro_list;
 struct Model0_sk_buff *Model0_skb;
 struct Model0_hrtimer Model0_timer;
 struct Model0_list_head Model0_dev_list;
 struct Model0_hlist_node Model0_napi_hash_node;
 unsigned int Model0_napi_id;
};

enum {
 Model0_NAPI_STATE_SCHED, /* Poll is scheduled */
 Model0_NAPI_STATE_DISABLE, /* Disable pending */
 Model0_NAPI_STATE_NPSVC, /* Netpoll - don't dequeue from poll_list */
 Model0_NAPI_STATE_HASHED, /* In NAPI hash (busy polling possible) */
 Model0_NAPI_STATE_NO_BUSY_POLL,/* Do not add in napi_hash, no busy polling */
};

enum Model0_gro_result {
 Model0_GRO_MERGED,
 Model0_GRO_MERGED_FREE,
 Model0_GRO_HELD,
 Model0_GRO_NORMAL,
 Model0_GRO_DROP,
};
typedef enum Model0_gro_result Model0_gro_result_t;

/*
 * enum rx_handler_result - Possible return values for rx_handlers.
 * @RX_HANDLER_CONSUMED: skb was consumed by rx_handler, do not process it
 * further.
 * @RX_HANDLER_ANOTHER: Do another round in receive path. This is indicated in
 * case skb->dev was changed by rx_handler.
 * @RX_HANDLER_EXACT: Force exact delivery, no wildcard.
 * @RX_HANDLER_PASS: Do nothing, pass the skb as if no rx_handler was called.
 *
 * rx_handlers are functions called from inside __netif_receive_skb(), to do
 * special processing of the skb, prior to delivery to protocol handlers.
 *
 * Currently, a net_device can only have a single rx_handler registered. Trying
 * to register a second rx_handler will return -EBUSY.
 *
 * To register a rx_handler on a net_device, use netdev_rx_handler_register().
 * To unregister a rx_handler on a net_device, use
 * netdev_rx_handler_unregister().
 *
 * Upon return, rx_handler is expected to tell __netif_receive_skb() what to
 * do with the skb.
 *
 * If the rx_handler consumed the skb in some way, it should return
 * RX_HANDLER_CONSUMED. This is appropriate when the rx_handler arranged for
 * the skb to be delivered in some other way.
 *
 * If the rx_handler changed skb->dev, to divert the skb to another
 * net_device, it should return RX_HANDLER_ANOTHER. The rx_handler for the
 * new device will be called if it exists.
 *
 * If the rx_handler decides the skb should be ignored, it should return
 * RX_HANDLER_EXACT. The skb will only be delivered to protocol handlers that
 * are registered on exact device (ptype->dev == skb->dev).
 *
 * If the rx_handler didn't change skb->dev, but wants the skb to be normally
 * delivered, it should return RX_HANDLER_PASS.
 *
 * A device without a registered rx_handler will behave as if rx_handler
 * returned RX_HANDLER_PASS.
 */

enum Model0_rx_handler_result {
 Model0_RX_HANDLER_CONSUMED,
 Model0_RX_HANDLER_ANOTHER,
 Model0_RX_HANDLER_EXACT,
 Model0_RX_HANDLER_PASS,
};
typedef enum Model0_rx_handler_result Model0_rx_handler_result_t;
typedef Model0_rx_handler_result_t Model0_rx_handler_func_t(struct Model0_sk_buff **Model0_pskb);

void Model0___napi_schedule(struct Model0_napi_struct *Model0_n);
void Model0___napi_schedule_irqoff(struct Model0_napi_struct *Model0_n);

static inline __attribute__((no_instrument_function)) bool Model0_napi_disable_pending(struct Model0_napi_struct *Model0_n)
{
 return (__builtin_constant_p((Model0_NAPI_STATE_DISABLE)) ? Model0_constant_test_bit((Model0_NAPI_STATE_DISABLE), (&Model0_n->Model0_state)) : Model0_variable_test_bit((Model0_NAPI_STATE_DISABLE), (&Model0_n->Model0_state)));
}

/**
 *	napi_schedule_prep - check if NAPI can be scheduled
 *	@n: NAPI context
 *
 * Test if NAPI routine is already running, and if not mark
 * it as running.  This is used as a condition variable to
 * insure only one NAPI poll instance runs.  We also make
 * sure there is no pending NAPI disable.
 */
static inline __attribute__((no_instrument_function)) bool Model0_napi_schedule_prep(struct Model0_napi_struct *Model0_n)
{
 return !Model0_napi_disable_pending(Model0_n) &&
  !Model0_test_and_set_bit(Model0_NAPI_STATE_SCHED, &Model0_n->Model0_state);
}

/**
 *	napi_schedule - schedule NAPI poll
 *	@n: NAPI context
 *
 * Schedule NAPI poll routine to be called if it is not already
 * running.
 */
static inline __attribute__((no_instrument_function)) void Model0_napi_schedule(struct Model0_napi_struct *Model0_n)
{
 if (Model0_napi_schedule_prep(Model0_n))
  Model0___napi_schedule(Model0_n);
}

/**
 *	napi_schedule_irqoff - schedule NAPI poll
 *	@n: NAPI context
 *
 * Variant of napi_schedule(), assuming hard irqs are masked.
 */
static inline __attribute__((no_instrument_function)) void Model0_napi_schedule_irqoff(struct Model0_napi_struct *Model0_n)
{
 if (Model0_napi_schedule_prep(Model0_n))
  Model0___napi_schedule_irqoff(Model0_n);
}

/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */
static inline __attribute__((no_instrument_function)) bool Model0_napi_reschedule(struct Model0_napi_struct *Model0_napi)
{
 if (Model0_napi_schedule_prep(Model0_napi)) {
  Model0___napi_schedule(Model0_napi);
  return true;
 }
 return false;
}

void Model0___napi_complete(struct Model0_napi_struct *Model0_n);
void Model0_napi_complete_done(struct Model0_napi_struct *Model0_n, int Model0_work_done);
/**
 *	napi_complete - NAPI processing complete
 *	@n: NAPI context
 *
 * Mark NAPI processing as complete.
 * Consider using napi_complete_done() instead.
 */
static inline __attribute__((no_instrument_function)) void Model0_napi_complete(struct Model0_napi_struct *Model0_n)
{
 return Model0_napi_complete_done(Model0_n, 0);
}

/**
 *	napi_hash_add - add a NAPI to global hashtable
 *	@napi: NAPI context
 *
 * Generate a new napi_id and store a @napi under it in napi_hash.
 * Used for busy polling (CONFIG_NET_RX_BUSY_POLL).
 * Note: This is normally automatically done from netif_napi_add(),
 * so might disappear in a future Linux version.
 */
void Model0_napi_hash_add(struct Model0_napi_struct *Model0_napi);

/**
 *	napi_hash_del - remove a NAPI from global table
 *	@napi: NAPI context
 *
 * Warning: caller must observe RCU grace period
 * before freeing memory containing @napi, if
 * this function returns true.
 * Note: core networking stack automatically calls it
 * from netif_napi_del().
 * Drivers might want to call this helper to combine all
 * the needed RCU grace periods into a single one.
 */
bool Model0_napi_hash_del(struct Model0_napi_struct *Model0_napi);

/**
 *	napi_disable - prevent NAPI from scheduling
 *	@n: NAPI context
 *
 * Stop NAPI from being scheduled on this context.
 * Waits till any outstanding processing completes.
 */
void Model0_napi_disable(struct Model0_napi_struct *Model0_n);

/**
 *	napi_enable - enable NAPI scheduling
 *	@n: NAPI context
 *
 * Resume NAPI from being scheduled on this context.
 * Must be paired with napi_disable.
 */
static inline __attribute__((no_instrument_function)) void Model0_napi_enable(struct Model0_napi_struct *Model0_n)
{
 do { if (__builtin_expect(!!(!(__builtin_constant_p((Model0_NAPI_STATE_SCHED)) ? Model0_constant_test_bit((Model0_NAPI_STATE_SCHED), (&Model0_n->Model0_state)) : Model0_variable_test_bit((Model0_NAPI_STATE_SCHED), (&Model0_n->Model0_state)))), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/netdevice.h"), "i" (512), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 Model0_clear_bit(Model0_NAPI_STATE_SCHED, &Model0_n->Model0_state);
 Model0_clear_bit(Model0_NAPI_STATE_NPSVC, &Model0_n->Model0_state);
}

/**
 *	napi_synchronize - wait until NAPI is not running
 *	@n: NAPI context
 *
 * Wait until NAPI is done being scheduled on this context.
 * Waits till any outstanding processing completes but
 * does not disable future activations.
 */
static inline __attribute__((no_instrument_function)) void Model0_napi_synchronize(const struct Model0_napi_struct *Model0_n)
{
 if (1)
  while ((__builtin_constant_p((Model0_NAPI_STATE_SCHED)) ? Model0_constant_test_bit((Model0_NAPI_STATE_SCHED), (&Model0_n->Model0_state)) : Model0_variable_test_bit((Model0_NAPI_STATE_SCHED), (&Model0_n->Model0_state))))
   Model0_msleep(1);
 else
  __asm__ __volatile__("": : :"memory");
}

enum Model0_netdev_queue_state_t {
 Model0___QUEUE_STATE_DRV_XOFF,
 Model0___QUEUE_STATE_STACK_XOFF,
 Model0___QUEUE_STATE_FROZEN,
};
/*
 * __QUEUE_STATE_DRV_XOFF is used by drivers to stop the transmit queue.  The
 * netif_tx_* functions below are used to manipulate this flag.  The
 * __QUEUE_STATE_STACK_XOFF flag is used by the stack to stop the transmit
 * queue independently.  The netif_xmit_*stopped functions below are called
 * to check if the queue has been stopped by the driver or stack (either
 * of the XOFF bits are set in the state).  Drivers should not need to call
 * netif_xmit*stopped functions, they should only be using netif_tx_*.
 */

struct Model0_netdev_queue {
/*
 * read-mostly part
 */
 struct Model0_net_device *Model0_dev;
 struct Model0_Qdisc *Model0_qdisc;
 struct Model0_Qdisc *Model0_qdisc_sleeping;

 struct Model0_kobject Model0_kobj;


 int Model0_numa_node;

 unsigned long Model0_tx_maxrate;
 /*
	 * Number of TX timeouts for this queue
	 * (/sys/class/net/DEV/Q/trans_timeout)
	 */
 unsigned long Model0_trans_timeout;
/*
 * write-mostly part
 */
 Model0_spinlock_t Model0__xmit_lock __attribute__((__aligned__((1 << (6)))));
 int Model0_xmit_lock_owner;
 /*
	 * Time (in jiffies) of last Tx
	 */
 unsigned long Model0_trans_start;

 unsigned long Model0_state;


 struct Model0_dql Model0_dql;

} __attribute__((__aligned__((1 << (6)))));

static inline __attribute__((no_instrument_function)) int Model0_netdev_queue_numa_node_read(const struct Model0_netdev_queue *Model0_q)
{

 return Model0_q->Model0_numa_node;



}

static inline __attribute__((no_instrument_function)) void Model0_netdev_queue_numa_node_write(struct Model0_netdev_queue *Model0_q, int Model0_node)
{

 Model0_q->Model0_numa_node = Model0_node;

}


/*
 * This structure holds an RPS map which can be of variable length.  The
 * map is an array of CPUs.
 */
struct Model0_rps_map {
 unsigned int Model0_len;
 struct Model0_callback_head Model0_rcu;
 Model0_u16 Model0_cpus[0];
};


/*
 * The rps_dev_flow structure contains the mapping of a flow to a CPU, the
 * tail pointer for that CPU's input queue at the time of last enqueue, and
 * a hardware filter index.
 */
struct Model0_rps_dev_flow {
 Model0_u16 Model0_cpu;
 Model0_u16 Model0_filter;
 unsigned int Model0_last_qtail;
};


/*
 * The rps_dev_flow_table structure contains a table of flow mappings.
 */
struct Model0_rps_dev_flow_table {
 unsigned int Model0_mask;
 struct Model0_callback_head Model0_rcu;
 struct Model0_rps_dev_flow Model0_flows[0];
};



/*
 * The rps_sock_flow_table contains mappings of flows to the last CPU
 * on which they were processed by the application (set in recvmsg).
 * Each entry is a 32bit value. Upper part is the high-order bits
 * of flow hash, lower part is CPU number.
 * rps_cpu_mask is used to partition the space, depending on number of
 * possible CPUs : rps_cpu_mask = roundup_pow_of_two(nr_cpu_ids) - 1
 * For example, if 64 CPUs are possible, rps_cpu_mask = 0x3f,
 * meaning we use 32-6=26 bits for the hash.
 */
struct Model0_rps_sock_flow_table {
 Model0_u32 Model0_mask;

 Model0_u32 Model0_ents[0] __attribute__((__aligned__((1 << (6)))));
};




extern Model0_u32 Model0_rps_cpu_mask;
extern struct Model0_rps_sock_flow_table *Model0_rps_sock_flow_table;

static inline __attribute__((no_instrument_function)) void Model0_rps_record_sock_flow(struct Model0_rps_sock_flow_table *Model0_table,
     Model0_u32 Model0_hash)
{
 if (Model0_table && Model0_hash) {
  unsigned int Model0_index = Model0_hash & Model0_table->Model0_mask;
  Model0_u32 Model0_val = Model0_hash & ~Model0_rps_cpu_mask;

  /* We only give a hint, preemption can change CPU under us */
  Model0_val |= (({ typeof(Model0_cpu_number) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_number)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_cpu_number)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; }));

  if (Model0_table->Model0_ents[Model0_index] != Model0_val)
   Model0_table->Model0_ents[Model0_index] = Model0_val;
 }
}


bool Model0_rps_may_expire_flow(struct Model0_net_device *Model0_dev, Model0_u16 Model0_rxq_index, Model0_u32 Model0_flow_id,
    Model0_u16 Model0_filter_id);



/* This structure contains an instance of an RX queue. */
struct Model0_netdev_rx_queue {

 struct Model0_rps_map *Model0_rps_map;
 struct Model0_rps_dev_flow_table *Model0_rps_flow_table;

 struct Model0_kobject Model0_kobj;
 struct Model0_net_device *Model0_dev;
} __attribute__((__aligned__((1 << (6)))));

/*
 * RX queue sysfs structures and functions.
 */
struct Model0_rx_queue_attribute {
 struct Model0_attribute Model0_attr;
 Model0_ssize_t (*Model0_show)(struct Model0_netdev_rx_queue *Model0_queue,
     struct Model0_rx_queue_attribute *Model0_attr, char *Model0_buf);
 Model0_ssize_t (*Model0_store)(struct Model0_netdev_rx_queue *Model0_queue,
     struct Model0_rx_queue_attribute *Model0_attr, const char *Model0_buf, Model0_size_t Model0_len);
};


/*
 * This structure holds an XPS map which can be of variable length.  The
 * map is an array of queues.
 */
struct Model0_xps_map {
 unsigned int Model0_len;
 unsigned int Model0_alloc_len;
 struct Model0_callback_head Model0_rcu;
 Model0_u16 Model0_queues[0];
};




/*
 * This structure holds all XPS maps for device.  Maps are indexed by CPU.
 */
struct Model0_xps_dev_maps {
 struct Model0_callback_head Model0_rcu;
 struct Model0_xps_map *Model0_cpu_map[0];
};






/* HW offloaded queuing disciplines txq count and offset maps */
struct Model0_netdev_tc_txq {
 Model0_u16 Model0_count;
 Model0_u16 Model0_offset;
};
/* This structure holds a unique identifier to identify some
 * physical item (port for example) used by a netdevice.
 */
struct Model0_netdev_phys_item_id {
 unsigned char Model0_id[32];
 unsigned char Model0_id_len;
};

static inline __attribute__((no_instrument_function)) bool Model0_netdev_phys_item_id_same(struct Model0_netdev_phys_item_id *Model0_a,
         struct Model0_netdev_phys_item_id *Model0_b)
{
 return Model0_a->Model0_id_len == Model0_b->Model0_id_len &&
        Model0_memcmp(Model0_a->Model0_id, Model0_b->Model0_id, Model0_a->Model0_id_len) == 0;
}

typedef Model0_u16 (*Model0_select_queue_fallback_t)(struct Model0_net_device *Model0_dev,
           struct Model0_sk_buff *Model0_skb);

/* These structures hold the attributes of qdisc and classifiers
 * that are being passed to the netdevice through the setup_tc op.
 */
enum {
 Model0_TC_SETUP_MQPRIO,
 Model0_TC_SETUP_CLSU32,
 Model0_TC_SETUP_CLSFLOWER,
 Model0_TC_SETUP_MATCHALL,
};

struct Model0_tc_cls_u32_offload;

struct Model0_tc_to_netdev {
 unsigned int Model0_type;
 union {
  Model0_u8 Model0_tc;
  struct Model0_tc_cls_u32_offload *Model0_cls_u32;
  struct Model0_tc_cls_flower_offload *Model0_cls_flower;
  struct Model0_tc_cls_matchall_offload *Model0_cls_mall;
 };
};

/* These structures hold the attributes of xdp state that are being passed
 * to the netdevice through the xdp op.
 */
enum Model0_xdp_netdev_command {
 /* Set or clear a bpf program used in the earliest stages of packet
	 * rx. The prog will have been loaded as BPF_PROG_TYPE_XDP. The callee
	 * is responsible for calling bpf_prog_put on any old progs that are
	 * stored. In case of error, the callee need not release the new prog
	 * reference, but on success it takes ownership and must bpf_prog_put
	 * when it is no longer used.
	 */
 Model0_XDP_SETUP_PROG,
 /* Check if a bpf program is set on the device.  The callee should
	 * return true if a program is currently attached and running.
	 */
 Model0_XDP_QUERY_PROG,
};

struct Model0_netdev_xdp {
 enum Model0_xdp_netdev_command Model0_command;
 union {
  /* XDP_SETUP_PROG */
  struct Model0_bpf_prog *Model0_prog;
  /* XDP_QUERY_PROG */
  bool Model0_prog_attached;
 };
};

/*
 * This structure defines the management hooks for network devices.
 * The following hooks can be defined; unless noted otherwise, they are
 * optional and can be filled with a null pointer.
 *
 * int (*ndo_init)(struct net_device *dev);
 *     This function is called once when a network device is registered.
 *     The network device can use this for any late stage initialization
 *     or semantic validation. It can fail with an error code which will
 *     be propagated back to register_netdev.
 *
 * void (*ndo_uninit)(struct net_device *dev);
 *     This function is called when device is unregistered or when registration
 *     fails. It is not called if init fails.
 *
 * int (*ndo_open)(struct net_device *dev);
 *     This function is called when a network device transitions to the up
 *     state.
 *
 * int (*ndo_stop)(struct net_device *dev);
 *     This function is called when a network device transitions to the down
 *     state.
 *
 * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,
 *                               struct net_device *dev);
 *	Called when a packet needs to be transmitted.
 *	Returns NETDEV_TX_OK.  Can return NETDEV_TX_BUSY, but you should stop
 *	the queue before that can happen; it's for obsolete devices and weird
 *	corner cases, but the stack really does a non-trivial amount
 *	of useless work if you return NETDEV_TX_BUSY.
 *	Required; cannot be NULL.
 *
 * netdev_features_t (*ndo_fix_features)(struct net_device *dev,
 *		netdev_features_t features);
 *	Adjusts the requested feature flags according to device-specific
 *	constraints, and returns the resulting flags. Must not modify
 *	the device state.
 *
 * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb,
 *                         void *accel_priv, select_queue_fallback_t fallback);
 *	Called to decide which queue to use when device supports multiple
 *	transmit queues.
 *
 * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);
 *	This function is called to allow device receiver to make
 *	changes to configuration when multicast or promiscuous is enabled.
 *
 * void (*ndo_set_rx_mode)(struct net_device *dev);
 *	This function is called device changes address list filtering.
 *	If driver handles unicast address filtering, it should set
 *	IFF_UNICAST_FLT in its priv_flags.
 *
 * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);
 *	This function  is called when the Media Access Control address
 *	needs to be changed. If this interface is not defined, the
 *	MAC address can not be changed.
 *
 * int (*ndo_validate_addr)(struct net_device *dev);
 *	Test if Media Access Control address is valid for the device.
 *
 * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);
 *	Called when a user requests an ioctl which can't be handled by
 *	the generic interface code. If not defined ioctls return
 *	not supported error code.
 *
 * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);
 *	Used to set network devices bus interface parameters. This interface
 *	is retained for legacy reasons; new devices should use the bus
 *	interface (PCI) for low level management.
 *
 * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);
 *	Called when a user wants to change the Maximum Transfer Unit
 *	of a device. If not defined, any request to change MTU will
 *	will return an error.
 *
 * void (*ndo_tx_timeout)(struct net_device *dev);
 *	Callback used when the transmitter has not made any progress
 *	for dev->watchdog ticks.
 *
 * struct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,
 *                      struct rtnl_link_stats64 *storage);
 * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);
 *	Called when a user wants to get the network device usage
 *	statistics. Drivers must do one of the following:
 *	1. Define @ndo_get_stats64 to fill in a zero-initialised
 *	   rtnl_link_stats64 structure passed by the caller.
 *	2. Define @ndo_get_stats to update a net_device_stats structure
 *	   (which should normally be dev->stats) and return a pointer to
 *	   it. The structure may be changed asynchronously only if each
 *	   field is written atomically.
 *	3. Update dev->stats asynchronously and atomically, and define
 *	   neither operation.
 *
 * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid);
 *	If device supports VLAN filtering this function is called when a
 *	VLAN id is registered.
 *
 * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid);
 *	If device supports VLAN filtering this function is called when a
 *	VLAN id is unregistered.
 *
 * void (*ndo_poll_controller)(struct net_device *dev);
 *
 *	SR-IOV management functions.
 * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);
 * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan, u8 qos);
 * int (*ndo_set_vf_rate)(struct net_device *dev, int vf, int min_tx_rate,
 *			  int max_tx_rate);
 * int (*ndo_set_vf_spoofchk)(struct net_device *dev, int vf, bool setting);
 * int (*ndo_set_vf_trust)(struct net_device *dev, int vf, bool setting);
 * int (*ndo_get_vf_config)(struct net_device *dev,
 *			    int vf, struct ifla_vf_info *ivf);
 * int (*ndo_set_vf_link_state)(struct net_device *dev, int vf, int link_state);
 * int (*ndo_set_vf_port)(struct net_device *dev, int vf,
 *			  struct nlattr *port[]);
 *
 *      Enable or disable the VF ability to query its RSS Redirection Table and
 *      Hash Key. This is needed since on some devices VF share this information
 *      with PF and querying it may introduce a theoretical security risk.
 * int (*ndo_set_vf_rss_query_en)(struct net_device *dev, int vf, bool setting);
 * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);
 * int (*ndo_setup_tc)(struct net_device *dev, u8 tc)
 * 	Called to setup 'tc' number of traffic classes in the net device. This
 * 	is always called from the stack with the rtnl lock held and netif tx
 * 	queues stopped. This allows the netdevice to perform queue management
 * 	safely.
 *
 *	Fiber Channel over Ethernet (FCoE) offload functions.
 * int (*ndo_fcoe_enable)(struct net_device *dev);
 *	Called when the FCoE protocol stack wants to start using LLD for FCoE
 *	so the underlying device can perform whatever needed configuration or
 *	initialization to support acceleration of FCoE traffic.
 *
 * int (*ndo_fcoe_disable)(struct net_device *dev);
 *	Called when the FCoE protocol stack wants to stop using LLD for FCoE
 *	so the underlying device can perform whatever needed clean-ups to
 *	stop supporting acceleration of FCoE traffic.
 *
 * int (*ndo_fcoe_ddp_setup)(struct net_device *dev, u16 xid,
 *			     struct scatterlist *sgl, unsigned int sgc);
 *	Called when the FCoE Initiator wants to initialize an I/O that
 *	is a possible candidate for Direct Data Placement (DDP). The LLD can
 *	perform necessary setup and returns 1 to indicate the device is set up
 *	successfully to perform DDP on this I/O, otherwise this returns 0.
 *
 * int (*ndo_fcoe_ddp_done)(struct net_device *dev,  u16 xid);
 *	Called when the FCoE Initiator/Target is done with the DDPed I/O as
 *	indicated by the FC exchange id 'xid', so the underlying device can
 *	clean up and reuse resources for later DDP requests.
 *
 * int (*ndo_fcoe_ddp_target)(struct net_device *dev, u16 xid,
 *			      struct scatterlist *sgl, unsigned int sgc);
 *	Called when the FCoE Target wants to initialize an I/O that
 *	is a possible candidate for Direct Data Placement (DDP). The LLD can
 *	perform necessary setup and returns 1 to indicate the device is set up
 *	successfully to perform DDP on this I/O, otherwise this returns 0.
 *
 * int (*ndo_fcoe_get_hbainfo)(struct net_device *dev,
 *			       struct netdev_fcoe_hbainfo *hbainfo);
 *	Called when the FCoE Protocol stack wants information on the underlying
 *	device. This information is utilized by the FCoE protocol stack to
 *	register attributes with Fiber Channel management service as per the
 *	FC-GS Fabric Device Management Information(FDMI) specification.
 *
 * int (*ndo_fcoe_get_wwn)(struct net_device *dev, u64 *wwn, int type);
 *	Called when the underlying device wants to override default World Wide
 *	Name (WWN) generation mechanism in FCoE protocol stack to pass its own
 *	World Wide Port Name (WWPN) or World Wide Node Name (WWNN) to the FCoE
 *	protocol stack to use.
 *
 *	RFS acceleration.
 * int (*ndo_rx_flow_steer)(struct net_device *dev, const struct sk_buff *skb,
 *			    u16 rxq_index, u32 flow_id);
 *	Set hardware filter for RFS.  rxq_index is the target queue index;
 *	flow_id is a flow ID to be passed to rps_may_expire_flow() later.
 *	Return the filter ID on success, or a negative error code.
 *
 *	Slave management functions (for bridge, bonding, etc).
 * int (*ndo_add_slave)(struct net_device *dev, struct net_device *slave_dev);
 *	Called to make another netdev an underling.
 *
 * int (*ndo_del_slave)(struct net_device *dev, struct net_device *slave_dev);
 *	Called to release previously enslaved netdev.
 *
 *      Feature/offload setting functions.
 * int (*ndo_set_features)(struct net_device *dev, netdev_features_t features);
 *	Called to update device configuration to new features. Passed
 *	feature set might be less than what was returned by ndo_fix_features()).
 *	Must return >0 or -errno if it changed dev->features itself.
 *
 * int (*ndo_fdb_add)(struct ndmsg *ndm, struct nlattr *tb[],
 *		      struct net_device *dev,
 *		      const unsigned char *addr, u16 vid, u16 flags)
 *	Adds an FDB entry to dev for addr.
 * int (*ndo_fdb_del)(struct ndmsg *ndm, struct nlattr *tb[],
 *		      struct net_device *dev,
 *		      const unsigned char *addr, u16 vid)
 *	Deletes the FDB entry from dev coresponding to addr.
 * int (*ndo_fdb_dump)(struct sk_buff *skb, struct netlink_callback *cb,
 *		       struct net_device *dev, struct net_device *filter_dev,
 *		       int idx)
 *	Used to add FDB entries to dump requests. Implementers should add
 *	entries to skb and update idx with the number of entries.
 *
 * int (*ndo_bridge_setlink)(struct net_device *dev, struct nlmsghdr *nlh,
 *			     u16 flags)
 * int (*ndo_bridge_getlink)(struct sk_buff *skb, u32 pid, u32 seq,
 *			     struct net_device *dev, u32 filter_mask,
 *			     int nlflags)
 * int (*ndo_bridge_dellink)(struct net_device *dev, struct nlmsghdr *nlh,
 *			     u16 flags);
 *
 * int (*ndo_change_carrier)(struct net_device *dev, bool new_carrier);
 *	Called to change device carrier. Soft-devices (like dummy, team, etc)
 *	which do not represent real hardware may define this to allow their
 *	userspace components to manage their virtual carrier state. Devices
 *	that determine carrier state from physical hardware properties (eg
 *	network cables) or protocol-dependent mechanisms (eg
 *	USB_CDC_NOTIFY_NETWORK_CONNECTION) should NOT implement this function.
 *
 * int (*ndo_get_phys_port_id)(struct net_device *dev,
 *			       struct netdev_phys_item_id *ppid);
 *	Called to get ID of physical port of this device. If driver does
 *	not implement this, it is assumed that the hw is not able to have
 *	multiple net devices on single physical port.
 *
 * void (*ndo_udp_tunnel_add)(struct net_device *dev,
 *			      struct udp_tunnel_info *ti);
 *	Called by UDP tunnel to notify a driver about the UDP port and socket
 *	address family that a UDP tunnel is listnening to. It is called only
 *	when a new port starts listening. The operation is protected by the
 *	RTNL.
 *
 * void (*ndo_udp_tunnel_del)(struct net_device *dev,
 *			      struct udp_tunnel_info *ti);
 *	Called by UDP tunnel to notify the driver about a UDP port and socket
 *	address family that the UDP tunnel is not listening to anymore. The
 *	operation is protected by the RTNL.
 *
 * void* (*ndo_dfwd_add_station)(struct net_device *pdev,
 *				 struct net_device *dev)
 *	Called by upper layer devices to accelerate switching or other
 *	station functionality into hardware. 'pdev is the lowerdev
 *	to use for the offload and 'dev' is the net device that will
 *	back the offload. Returns a pointer to the private structure
 *	the upper layer will maintain.
 * void (*ndo_dfwd_del_station)(struct net_device *pdev, void *priv)
 *	Called by upper layer device to delete the station created
 *	by 'ndo_dfwd_add_station'. 'pdev' is the net device backing
 *	the station and priv is the structure returned by the add
 *	operation.
 * netdev_tx_t (*ndo_dfwd_start_xmit)(struct sk_buff *skb,
 *				      struct net_device *dev,
 *				      void *priv);
 *	Callback to use for xmit over the accelerated station. This
 *	is used in place of ndo_start_xmit on accelerated net
 *	devices.
 * netdev_features_t (*ndo_features_check)(struct sk_buff *skb,
 *					   struct net_device *dev
 *					   netdev_features_t features);
 *	Called by core transmit path to determine if device is capable of
 *	performing offload operations on a given packet. This is to give
 *	the device an opportunity to implement any restrictions that cannot
 *	be otherwise expressed by feature flags. The check is called with
 *	the set of features that the stack has calculated and it returns
 *	those the driver believes to be appropriate.
 * int (*ndo_set_tx_maxrate)(struct net_device *dev,
 *			     int queue_index, u32 maxrate);
 *	Called when a user wants to set a max-rate limitation of specific
 *	TX queue.
 * int (*ndo_get_iflink)(const struct net_device *dev);
 *	Called to get the iflink value of this device.
 * void (*ndo_change_proto_down)(struct net_device *dev,
 *				 bool proto_down);
 *	This function is used to pass protocol port error state information
 *	to the switch driver. The switch driver can react to the proto_down
 *      by doing a phys down on the associated switch port.
 * int (*ndo_fill_metadata_dst)(struct net_device *dev, struct sk_buff *skb);
 *	This function is used to get egress tunnel information for given skb.
 *	This is useful for retrieving outer tunnel header parameters while
 *	sampling packet.
 * void (*ndo_set_rx_headroom)(struct net_device *dev, int needed_headroom);
 *	This function is used to specify the headroom that the skb must
 *	consider when allocation skb during packet reception. Setting
 *	appropriate rx headroom value allows avoiding skb head copy on
 *	forward. Setting a negative value resets the rx headroom to the
 *	default value.
 * int (*ndo_xdp)(struct net_device *dev, struct netdev_xdp *xdp);
 *	This function is used to set or query state related to XDP on the
 *	netdevice. See definition of enum xdp_netdev_command for details.
 *
 */
struct Model0_net_device_ops {
 int (*Model0_ndo_init)(struct Model0_net_device *Model0_dev);
 void (*Model0_ndo_uninit)(struct Model0_net_device *Model0_dev);
 int (*Model0_ndo_open)(struct Model0_net_device *Model0_dev);
 int (*Model0_ndo_stop)(struct Model0_net_device *Model0_dev);
 Model0_netdev_tx_t (*Model0_ndo_start_xmit)(struct Model0_sk_buff *Model0_skb,
        struct Model0_net_device *Model0_dev);
 Model0_netdev_features_t (*Model0_ndo_features_check)(struct Model0_sk_buff *Model0_skb,
            struct Model0_net_device *Model0_dev,
            Model0_netdev_features_t Model0_features);
 Model0_u16 (*Model0_ndo_select_queue)(struct Model0_net_device *Model0_dev,
          struct Model0_sk_buff *Model0_skb,
          void *Model0_accel_priv,
          Model0_select_queue_fallback_t Model0_fallback);
 void (*Model0_ndo_change_rx_flags)(struct Model0_net_device *Model0_dev,
             int Model0_flags);
 void (*Model0_ndo_set_rx_mode)(struct Model0_net_device *Model0_dev);
 int (*Model0_ndo_set_mac_address)(struct Model0_net_device *Model0_dev,
             void *Model0_addr);
 int (*Model0_ndo_validate_addr)(struct Model0_net_device *Model0_dev);
 int (*Model0_ndo_do_ioctl)(struct Model0_net_device *Model0_dev,
             struct Model0_ifreq *Model0_ifr, int Model0_cmd);
 int (*Model0_ndo_set_config)(struct Model0_net_device *Model0_dev,
               struct Model0_ifmap *Model0_map);
 int (*Model0_ndo_change_mtu)(struct Model0_net_device *Model0_dev,
        int Model0_new_mtu);
 int (*Model0_ndo_neigh_setup)(struct Model0_net_device *Model0_dev,
         struct Model0_neigh_parms *);
 void (*Model0_ndo_tx_timeout) (struct Model0_net_device *Model0_dev);

 struct Model0_rtnl_link_stats64* (*Model0_ndo_get_stats64)(struct Model0_net_device *Model0_dev,
           struct Model0_rtnl_link_stats64 *Model0_storage);
 struct Model0_net_device_stats* (*Model0_ndo_get_stats)(struct Model0_net_device *Model0_dev);

 int (*Model0_ndo_vlan_rx_add_vid)(struct Model0_net_device *Model0_dev,
             Model0___be16 Model0_proto, Model0_u16 Model0_vid);
 int (*Model0_ndo_vlan_rx_kill_vid)(struct Model0_net_device *Model0_dev,
              Model0___be16 Model0_proto, Model0_u16 Model0_vid);

 void (*Model0_ndo_poll_controller)(struct Model0_net_device *Model0_dev);
 int (*Model0_ndo_netpoll_setup)(struct Model0_net_device *Model0_dev,
           struct Model0_netpoll_info *Model0_info);
 void (*Model0_ndo_netpoll_cleanup)(struct Model0_net_device *Model0_dev);


 int (*Model0_ndo_busy_poll)(struct Model0_napi_struct *Model0_dev);

 int (*Model0_ndo_set_vf_mac)(struct Model0_net_device *Model0_dev,
        int Model0_queue, Model0_u8 *Model0_mac);
 int (*Model0_ndo_set_vf_vlan)(struct Model0_net_device *Model0_dev,
         int Model0_queue, Model0_u16 Model0_vlan, Model0_u8 Model0_qos);
 int (*Model0_ndo_set_vf_rate)(struct Model0_net_device *Model0_dev,
         int Model0_vf, int Model0_min_tx_rate,
         int Model0_max_tx_rate);
 int (*Model0_ndo_set_vf_spoofchk)(struct Model0_net_device *Model0_dev,
             int Model0_vf, bool Model0_setting);
 int (*Model0_ndo_set_vf_trust)(struct Model0_net_device *Model0_dev,
          int Model0_vf, bool Model0_setting);
 int (*Model0_ndo_get_vf_config)(struct Model0_net_device *Model0_dev,
           int Model0_vf,
           struct Model0_ifla_vf_info *Model0_ivf);
 int (*Model0_ndo_set_vf_link_state)(struct Model0_net_device *Model0_dev,
        int Model0_vf, int Model0_link_state);
 int (*Model0_ndo_get_vf_stats)(struct Model0_net_device *Model0_dev,
          int Model0_vf,
          struct Model0_ifla_vf_stats
          *Model0_vf_stats);
 int (*Model0_ndo_set_vf_port)(struct Model0_net_device *Model0_dev,
         int Model0_vf,
         struct Model0_nlattr *Model0_port[]);
 int (*Model0_ndo_get_vf_port)(struct Model0_net_device *Model0_dev,
         int Model0_vf, struct Model0_sk_buff *Model0_skb);
 int (*Model0_ndo_set_vf_guid)(struct Model0_net_device *Model0_dev,
         int Model0_vf, Model0_u64 Model0_guid,
         int Model0_guid_type);
 int (*Model0_ndo_set_vf_rss_query_en)(
         struct Model0_net_device *Model0_dev,
         int Model0_vf, bool Model0_setting);
 int (*Model0_ndo_setup_tc)(struct Model0_net_device *Model0_dev,
      Model0_u32 Model0_handle,
      Model0___be16 Model0_protocol,
      struct Model0_tc_to_netdev *Model0_tc);
 int (*Model0_ndo_rx_flow_steer)(struct Model0_net_device *Model0_dev,
           const struct Model0_sk_buff *Model0_skb,
           Model0_u16 Model0_rxq_index,
           Model0_u32 Model0_flow_id);

 int (*Model0_ndo_add_slave)(struct Model0_net_device *Model0_dev,
       struct Model0_net_device *Model0_slave_dev);
 int (*Model0_ndo_del_slave)(struct Model0_net_device *Model0_dev,
       struct Model0_net_device *Model0_slave_dev);
 Model0_netdev_features_t (*Model0_ndo_fix_features)(struct Model0_net_device *Model0_dev,
          Model0_netdev_features_t Model0_features);
 int (*Model0_ndo_set_features)(struct Model0_net_device *Model0_dev,
          Model0_netdev_features_t Model0_features);
 int (*Model0_ndo_neigh_construct)(struct Model0_net_device *Model0_dev,
             struct Model0_neighbour *Model0_n);
 void (*Model0_ndo_neigh_destroy)(struct Model0_net_device *Model0_dev,
           struct Model0_neighbour *Model0_n);

 int (*Model0_ndo_fdb_add)(struct Model0_ndmsg *Model0_ndm,
            struct Model0_nlattr *Model0_tb[],
            struct Model0_net_device *Model0_dev,
            const unsigned char *Model0_addr,
            Model0_u16 Model0_vid,
            Model0_u16 Model0_flags);
 int (*Model0_ndo_fdb_del)(struct Model0_ndmsg *Model0_ndm,
            struct Model0_nlattr *Model0_tb[],
            struct Model0_net_device *Model0_dev,
            const unsigned char *Model0_addr,
            Model0_u16 Model0_vid);
 int (*Model0_ndo_fdb_dump)(struct Model0_sk_buff *Model0_skb,
      struct Model0_netlink_callback *Model0_cb,
      struct Model0_net_device *Model0_dev,
      struct Model0_net_device *Model0_filter_dev,
      int Model0_idx);

 int (*Model0_ndo_bridge_setlink)(struct Model0_net_device *Model0_dev,
            struct Model0_nlmsghdr *Model0_nlh,
            Model0_u16 Model0_flags);
 int (*Model0_ndo_bridge_getlink)(struct Model0_sk_buff *Model0_skb,
            Model0_u32 Model0_pid, Model0_u32 Model0_seq,
            struct Model0_net_device *Model0_dev,
            Model0_u32 Model0_filter_mask,
            int Model0_nlflags);
 int (*Model0_ndo_bridge_dellink)(struct Model0_net_device *Model0_dev,
            struct Model0_nlmsghdr *Model0_nlh,
            Model0_u16 Model0_flags);
 int (*Model0_ndo_change_carrier)(struct Model0_net_device *Model0_dev,
            bool Model0_new_carrier);
 int (*Model0_ndo_get_phys_port_id)(struct Model0_net_device *Model0_dev,
       struct Model0_netdev_phys_item_id *Model0_ppid);
 int (*Model0_ndo_get_phys_port_name)(struct Model0_net_device *Model0_dev,
         char *Model0_name, Model0_size_t Model0_len);
 void (*Model0_ndo_udp_tunnel_add)(struct Model0_net_device *Model0_dev,
            struct Model0_udp_tunnel_info *Model0_ti);
 void (*Model0_ndo_udp_tunnel_del)(struct Model0_net_device *Model0_dev,
            struct Model0_udp_tunnel_info *Model0_ti);
 void* (*Model0_ndo_dfwd_add_station)(struct Model0_net_device *Model0_pdev,
       struct Model0_net_device *Model0_dev);
 void (*Model0_ndo_dfwd_del_station)(struct Model0_net_device *Model0_pdev,
       void *Model0_priv);

 Model0_netdev_tx_t (*Model0_ndo_dfwd_start_xmit) (struct Model0_sk_buff *Model0_skb,
       struct Model0_net_device *Model0_dev,
       void *Model0_priv);
 int (*Model0_ndo_get_lock_subclass)(struct Model0_net_device *Model0_dev);
 int (*Model0_ndo_set_tx_maxrate)(struct Model0_net_device *Model0_dev,
            int Model0_queue_index,
            Model0_u32 Model0_maxrate);
 int (*Model0_ndo_get_iflink)(const struct Model0_net_device *Model0_dev);
 int (*Model0_ndo_change_proto_down)(struct Model0_net_device *Model0_dev,
        bool Model0_proto_down);
 int (*Model0_ndo_fill_metadata_dst)(struct Model0_net_device *Model0_dev,
             struct Model0_sk_buff *Model0_skb);
 void (*Model0_ndo_set_rx_headroom)(struct Model0_net_device *Model0_dev,
             int Model0_needed_headroom);
 int (*Model0_ndo_xdp)(struct Model0_net_device *Model0_dev,
        struct Model0_netdev_xdp *Model0_xdp);
};

/**
 * enum net_device_priv_flags - &struct net_device priv_flags
 *
 * These are the &struct net_device, they are only set internally
 * by drivers and used in the kernel. These flags are invisible to
 * userspace; this means that the order of these flags can change
 * during any kernel release.
 *
 * You should have a pretty good reason to be extending these flags.
 *
 * @IFF_802_1Q_VLAN: 802.1Q VLAN device
 * @IFF_EBRIDGE: Ethernet bridging device
 * @IFF_BONDING: bonding master or slave
 * @IFF_ISATAP: ISATAP interface (RFC4214)
 * @IFF_WAN_HDLC: WAN HDLC device
 * @IFF_XMIT_DST_RELEASE: dev_hard_start_xmit() is allowed to
 *	release skb->dst
 * @IFF_DONT_BRIDGE: disallow bridging this ether dev
 * @IFF_DISABLE_NETPOLL: disable netpoll at run-time
 * @IFF_MACVLAN_PORT: device used as macvlan port
 * @IFF_BRIDGE_PORT: device used as bridge port
 * @IFF_OVS_DATAPATH: device used as Open vSwitch datapath port
 * @IFF_TX_SKB_SHARING: The interface supports sharing skbs on transmit
 * @IFF_UNICAST_FLT: Supports unicast filtering
 * @IFF_TEAM_PORT: device used as team port
 * @IFF_SUPP_NOFCS: device supports sending custom FCS
 * @IFF_LIVE_ADDR_CHANGE: device supports hardware address
 *	change when it's running
 * @IFF_MACVLAN: Macvlan device
 * @IFF_XMIT_DST_RELEASE_PERM: IFF_XMIT_DST_RELEASE not taking into account
 *	underlying stacked devices
 * @IFF_IPVLAN_MASTER: IPvlan master device
 * @IFF_IPVLAN_SLAVE: IPvlan slave device
 * @IFF_L3MDEV_MASTER: device is an L3 master device
 * @IFF_NO_QUEUE: device can run without qdisc attached
 * @IFF_OPENVSWITCH: device is a Open vSwitch master
 * @IFF_L3MDEV_SLAVE: device is enslaved to an L3 master device
 * @IFF_TEAM: device is a team device
 * @IFF_RXFH_CONFIGURED: device has had Rx Flow indirection table configured
 * @IFF_PHONY_HEADROOM: the headroom value is controlled by an external
 *	entity (i.e. the master device for bridged veth)
 * @IFF_MACSEC: device is a MACsec device
 */
enum Model0_netdev_priv_flags {
 Model0_IFF_802_1Q_VLAN = 1<<0,
 Model0_IFF_EBRIDGE = 1<<1,
 Model0_IFF_BONDING = 1<<2,
 Model0_IFF_ISATAP = 1<<3,
 Model0_IFF_WAN_HDLC = 1<<4,
 Model0_IFF_XMIT_DST_RELEASE = 1<<5,
 Model0_IFF_DONT_BRIDGE = 1<<6,
 Model0_IFF_DISABLE_NETPOLL = 1<<7,
 Model0_IFF_MACVLAN_PORT = 1<<8,
 Model0_IFF_BRIDGE_PORT = 1<<9,
 Model0_IFF_OVS_DATAPATH = 1<<10,
 Model0_IFF_TX_SKB_SHARING = 1<<11,
 Model0_IFF_UNICAST_FLT = 1<<12,
 Model0_IFF_TEAM_PORT = 1<<13,
 Model0_IFF_SUPP_NOFCS = 1<<14,
 Model0_IFF_LIVE_ADDR_CHANGE = 1<<15,
 Model0_IFF_MACVLAN = 1<<16,
 Model0_IFF_XMIT_DST_RELEASE_PERM = 1<<17,
 Model0_IFF_IPVLAN_MASTER = 1<<18,
 Model0_IFF_IPVLAN_SLAVE = 1<<19,
 Model0_IFF_L3MDEV_MASTER = 1<<20,
 Model0_IFF_NO_QUEUE = 1<<21,
 Model0_IFF_OPENVSWITCH = 1<<22,
 Model0_IFF_L3MDEV_SLAVE = 1<<23,
 Model0_IFF_TEAM = 1<<24,
 Model0_IFF_RXFH_CONFIGURED = 1<<25,
 Model0_IFF_PHONY_HEADROOM = 1<<26,
 Model0_IFF_MACSEC = 1<<27,
};
/**
 *	struct net_device - The DEVICE structure.
 *		Actually, this whole structure is a big mistake.  It mixes I/O
 *		data with strictly "high-level" data, and it has to know about
 *		almost every data structure used in the INET module.
 *
 *	@name:	This is the first field of the "visible" part of this structure
 *		(i.e. as seen by users in the "Space.c" file).  It is the name
 *	 	of the interface.
 *
 *	@name_hlist: 	Device name hash chain, please keep it close to name[]
 *	@ifalias:	SNMP alias
 *	@mem_end:	Shared memory end
 *	@mem_start:	Shared memory start
 *	@base_addr:	Device I/O address
 *	@irq:		Device IRQ number
 *
 *	@carrier_changes:	Stats to monitor carrier on<->off transitions
 *
 *	@state:		Generic network queuing layer state, see netdev_state_t
 *	@dev_list:	The global list of network devices
 *	@napi_list:	List entry used for polling NAPI devices
 *	@unreg_list:	List entry  when we are unregistering the
 *			device; see the function unregister_netdev
 *	@close_list:	List entry used when we are closing the device
 *	@ptype_all:     Device-specific packet handlers for all protocols
 *	@ptype_specific: Device-specific, protocol-specific packet handlers
 *
 *	@adj_list:	Directly linked devices, like slaves for bonding
 *	@all_adj_list:	All linked devices, *including* neighbours
 *	@features:	Currently active device features
 *	@hw_features:	User-changeable features
 *
 *	@wanted_features:	User-requested features
 *	@vlan_features:		Mask of features inheritable by VLAN devices
 *
 *	@hw_enc_features:	Mask of features inherited by encapsulating devices
 *				This field indicates what encapsulation
 *				offloads the hardware is capable of doing,
 *				and drivers will need to set them appropriately.
 *
 *	@mpls_features:	Mask of features inheritable by MPLS
 *
 *	@ifindex:	interface index
 *	@group:		The group the device belongs to
 *
 *	@stats:		Statistics struct, which was left as a legacy, use
 *			rtnl_link_stats64 instead
 *
 *	@rx_dropped:	Dropped packets by core network,
 *			do not use this in drivers
 *	@tx_dropped:	Dropped packets by core network,
 *			do not use this in drivers
 *	@rx_nohandler:	nohandler dropped packets by core network on
 *			inactive devices, do not use this in drivers
 *
 *	@wireless_handlers:	List of functions to handle Wireless Extensions,
 *				instead of ioctl,
 *				see <net/iw_handler.h> for details.
 *	@wireless_data:	Instance data managed by the core of wireless extensions
 *
 *	@netdev_ops:	Includes several pointers to callbacks,
 *			if one wants to override the ndo_*() functions
 *	@ethtool_ops:	Management operations
 *	@ndisc_ops:	Includes callbacks for different IPv6 neighbour
 *			discovery handling. Necessary for e.g. 6LoWPAN.
 *	@header_ops:	Includes callbacks for creating,parsing,caching,etc
 *			of Layer 2 headers.
 *
 *	@flags:		Interface flags (a la BSD)
 *	@priv_flags:	Like 'flags' but invisible to userspace,
 *			see if.h for the definitions
 *	@gflags:	Global flags ( kept as legacy )
 *	@padded:	How much padding added by alloc_netdev()
 *	@operstate:	RFC2863 operstate
 *	@link_mode:	Mapping policy to operstate
 *	@if_port:	Selectable AUI, TP, ...
 *	@dma:		DMA channel
 *	@mtu:		Interface MTU value
 *	@type:		Interface hardware type
 *	@hard_header_len: Maximum hardware header length.
 *
 *	@needed_headroom: Extra headroom the hardware may need, but not in all
 *			  cases can this be guaranteed
 *	@needed_tailroom: Extra tailroom the hardware may need, but not in all
 *			  cases can this be guaranteed. Some cases also use
 *			  LL_MAX_HEADER instead to allocate the skb
 *
 *	interface address info:
 *
 * 	@perm_addr:		Permanent hw address
 * 	@addr_assign_type:	Hw address assignment type
 * 	@addr_len:		Hardware address length
 *	@neigh_priv_len:	Used in neigh_alloc()
 * 	@dev_id:		Used to differentiate devices that share
 * 				the same link layer address
 * 	@dev_port:		Used to differentiate devices that share
 * 				the same function
 *	@addr_list_lock:	XXX: need comments on this one
 *	@uc_promisc:		Counter that indicates promiscuous mode
 *				has been enabled due to the need to listen to
 *				additional unicast addresses in a device that
 *				does not implement ndo_set_rx_mode()
 *	@uc:			unicast mac addresses
 *	@mc:			multicast mac addresses
 *	@dev_addrs:		list of device hw addresses
 *	@queues_kset:		Group of all Kobjects in the Tx and RX queues
 *	@promiscuity:		Number of times the NIC is told to work in
 *				promiscuous mode; if it becomes 0 the NIC will
 *				exit promiscuous mode
 *	@allmulti:		Counter, enables or disables allmulticast mode
 *
 *	@vlan_info:	VLAN info
 *	@dsa_ptr:	dsa specific data
 *	@tipc_ptr:	TIPC specific data
 *	@atalk_ptr:	AppleTalk link
 *	@ip_ptr:	IPv4 specific data
 *	@dn_ptr:	DECnet specific data
 *	@ip6_ptr:	IPv6 specific data
 *	@ax25_ptr:	AX.25 specific data
 *	@ieee80211_ptr:	IEEE 802.11 specific data, assign before registering
 *
 *	@last_rx:	Time of last Rx
 *	@dev_addr:	Hw address (before bcast,
 *			because most packets are unicast)
 *
 *	@_rx:			Array of RX queues
 *	@num_rx_queues:		Number of RX queues
 *				allocated at register_netdev() time
 *	@real_num_rx_queues: 	Number of RX queues currently active in device
 *
 *	@rx_handler:		handler for received packets
 *	@rx_handler_data: 	XXX: need comments on this one
 *	@ingress_queue:		XXX: need comments on this one
 *	@broadcast:		hw bcast address
 *
 *	@rx_cpu_rmap:	CPU reverse-mapping for RX completion interrupts,
 *			indexed by RX queue number. Assigned by driver.
 *			This must only be set if the ndo_rx_flow_steer
 *			operation is defined
 *	@index_hlist:		Device index hash chain
 *
 *	@_tx:			Array of TX queues
 *	@num_tx_queues:		Number of TX queues allocated at alloc_netdev_mq() time
 *	@real_num_tx_queues: 	Number of TX queues currently active in device
 *	@qdisc:			Root qdisc from userspace point of view
 *	@tx_queue_len:		Max frames per queue allowed
 *	@tx_global_lock: 	XXX: need comments on this one
 *
 *	@xps_maps:	XXX: need comments on this one
 *
 *	@offload_fwd_mark:	Offload device fwding mark
 *
 *	@watchdog_timeo:	Represents the timeout that is used by
 *				the watchdog (see dev_watchdog())
 *	@watchdog_timer:	List of timers
 *
 *	@pcpu_refcnt:		Number of references to this device
 *	@todo_list:		Delayed register/unregister
 *	@link_watch_list:	XXX: need comments on this one
 *
 *	@reg_state:		Register/unregister state machine
 *	@dismantle:		Device is going to be freed
 *	@rtnl_link_state:	This enum represents the phases of creating
 *				a new link
 *
 *	@destructor:		Called from unregister,
 *				can be used to call free_netdev
 *	@npinfo:		XXX: need comments on this one
 * 	@nd_net:		Network namespace this network device is inside
 *
 * 	@ml_priv:	Mid-layer private
 * 	@lstats:	Loopback statistics
 * 	@tstats:	Tunnel statistics
 * 	@dstats:	Dummy statistics
 * 	@vstats:	Virtual ethernet statistics
 *
 *	@garp_port:	GARP
 *	@mrp_port:	MRP
 *
 *	@dev:		Class/net/name entry
 *	@sysfs_groups:	Space for optional device, statistics and wireless
 *			sysfs groups
 *
 *	@sysfs_rx_queue_group:	Space for optional per-rx queue attributes
 *	@rtnl_link_ops:	Rtnl_link_ops
 *
 *	@gso_max_size:	Maximum size of generic segmentation offload
 *	@gso_max_segs:	Maximum number of segments that can be passed to the
 *			NIC for GSO
 *
 *	@dcbnl_ops:	Data Center Bridging netlink ops
 *	@num_tc:	Number of traffic classes in the net device
 *	@tc_to_txq:	XXX: need comments on this one
 *	@prio_tc_map	XXX: need comments on this one
 *
 *	@fcoe_ddp_xid:	Max exchange id for FCoE LRO by ddp
 *
 *	@priomap:	XXX: need comments on this one
 *	@phydev:	Physical device may attach itself
 *			for hardware timestamping
 *
 *	@qdisc_tx_busylock: lockdep class annotating Qdisc->busylock spinlock
 *	@qdisc_running_key: lockdep class annotating Qdisc->running seqcount
 *
 *	@proto_down:	protocol port state information can be sent to the
 *			switch driver and used to set the phys state of the
 *			switch port.
 *
 *	FIXME: cleanup struct net_device such that network protocol info
 *	moves out.
 */

struct Model0_net_device {
 char Model0_name[16];
 struct Model0_hlist_node Model0_name_hlist;
 char *Model0_ifalias;
 /*
	 *	I/O specific fields
	 *	FIXME: Merge these and struct ifmap into one
	 */
 unsigned long Model0_mem_end;
 unsigned long Model0_mem_start;
 unsigned long Model0_base_addr;
 int Model0_irq;

 Model0_atomic_t Model0_carrier_changes;

 /*
	 *	Some hardware also needs these fields (state,dev_list,
	 *	napi_list,unreg_list,close_list) but they are not
	 *	part of the usual set specified in Space.c.
	 */

 unsigned long Model0_state;

 struct Model0_list_head Model0_dev_list;
 struct Model0_list_head Model0_napi_list;
 struct Model0_list_head Model0_unreg_list;
 struct Model0_list_head Model0_close_list;
 struct Model0_list_head Model0_ptype_all;
 struct Model0_list_head Model0_ptype_specific;

 struct {
  struct Model0_list_head Model0_upper;
  struct Model0_list_head Model0_lower;
 } Model0_adj_list;

 struct {
  struct Model0_list_head Model0_upper;
  struct Model0_list_head Model0_lower;
 } Model0_all_adj_list;

 Model0_netdev_features_t Model0_features;
 Model0_netdev_features_t Model0_hw_features;
 Model0_netdev_features_t Model0_wanted_features;
 Model0_netdev_features_t Model0_vlan_features;
 Model0_netdev_features_t Model0_hw_enc_features;
 Model0_netdev_features_t Model0_mpls_features;
 Model0_netdev_features_t Model0_gso_partial_features;

 int Model0_ifindex;
 int Model0_group;

 struct Model0_net_device_stats Model0_stats;

 Model0_atomic_long_t Model0_rx_dropped;
 Model0_atomic_long_t Model0_tx_dropped;
 Model0_atomic_long_t Model0_rx_nohandler;





 const struct Model0_net_device_ops *Model0_netdev_ops;
 const struct Model0_ethtool_ops *Model0_ethtool_ops;







 const struct Model0_ndisc_ops *Model0_ndisc_ops;


 const struct Model0_header_ops *Model0_header_ops;

 unsigned int Model0_flags;
 unsigned int Model0_priv_flags;

 unsigned short Model0_gflags;
 unsigned short Model0_padded;

 unsigned char Model0_operstate;
 unsigned char Model0_link_mode;

 unsigned char Model0_if_port;
 unsigned char Model0_dma;

 unsigned int Model0_mtu;
 unsigned short Model0_type;
 unsigned short Model0_hard_header_len;

 unsigned short Model0_needed_headroom;
 unsigned short Model0_needed_tailroom;

 /* Interface address info. */
 unsigned char Model0_perm_addr[32];
 unsigned char Model0_addr_assign_type;
 unsigned char Model0_addr_len;
 unsigned short Model0_neigh_priv_len;
 unsigned short Model0_dev_id;
 unsigned short Model0_dev_port;
 Model0_spinlock_t Model0_addr_list_lock;
 unsigned char Model0_name_assign_type;
 bool Model0_uc_promisc;
 struct Model0_netdev_hw_addr_list Model0_uc;
 struct Model0_netdev_hw_addr_list Model0_mc;
 struct Model0_netdev_hw_addr_list Model0_dev_addrs;


 struct Model0_kset *Model0_queues_kset;

 unsigned int Model0_promiscuity;
 unsigned int Model0_allmulti;


 /* Protocol-specific pointers */
 void *Model0_atalk_ptr;
 struct Model0_in_device *Model0_ip_ptr;
 struct Model0_dn_dev *Model0_dn_ptr;
 struct Model0_inet6_dev *Model0_ip6_ptr;
 void *Model0_ax25_ptr;
 struct Model0_wireless_dev *Model0_ieee80211_ptr;
 struct Model0_wpan_dev *Model0_ieee802154_ptr;




/*
 * Cache lines mostly used on receive path (including eth_type_trans())
 */
 unsigned long Model0_last_rx;

 /* Interface address info used in eth_type_trans() */
 unsigned char *Model0_dev_addr;


 struct Model0_netdev_rx_queue *Model0__rx;

 unsigned int Model0_num_rx_queues;
 unsigned int Model0_real_num_rx_queues;


 unsigned long Model0_gro_flush_timeout;
 Model0_rx_handler_func_t *Model0_rx_handler;
 void *Model0_rx_handler_data;


 struct Model0_tcf_proto *Model0_ingress_cl_list;

 struct Model0_netdev_queue *Model0_ingress_queue;

 struct Model0_list_head Model0_nf_hooks_ingress;


 unsigned char Model0_broadcast[32];

 struct Model0_cpu_rmap *Model0_rx_cpu_rmap;

 struct Model0_hlist_node Model0_index_hlist;

/*
 * Cache lines mostly used on transmit path
 */
 struct Model0_netdev_queue *Model0__tx __attribute__((__aligned__((1 << (6)))));
 unsigned int Model0_num_tx_queues;
 unsigned int Model0_real_num_tx_queues;
 struct Model0_Qdisc *Model0_qdisc;
 unsigned long Model0_tx_queue_len;
 Model0_spinlock_t Model0_tx_global_lock;
 int Model0_watchdog_timeo;


 struct Model0_xps_dev_maps *Model0_xps_maps;


 struct Model0_tcf_proto *Model0_egress_cl_list;





 /* These may be needed for future network-power-down code. */
 struct Model0_timer_list Model0_watchdog_timer;

 int *Model0_pcpu_refcnt;
 struct Model0_list_head Model0_todo_list;

 struct Model0_list_head Model0_link_watch_list;

 enum { Model0_NETREG_UNINITIALIZED=0,
        Model0_NETREG_REGISTERED, /* completed register_netdevice */
        Model0_NETREG_UNREGISTERING, /* called unregister_netdevice */
        Model0_NETREG_UNREGISTERED, /* completed unregister todo */
        Model0_NETREG_RELEASED, /* called free_netdev */
        Model0_NETREG_DUMMY, /* dummy device for NAPI poll */
 } Model0_reg_state:8;

 bool Model0_dismantle;

 enum {
  Model0_RTNL_LINK_INITIALIZED,
  Model0_RTNL_LINK_INITIALIZING,
 } Model0_rtnl_link_state:16;

 void (*Model0_destructor)(struct Model0_net_device *Model0_dev);


 struct Model0_netpoll_info *Model0_npinfo;


 Model0_possible_net_t Model0_nd_net;

 /* mid-layer private */
 union {
  void *Model0_ml_priv;
  struct Model0_pcpu_lstats *Model0_lstats;
  struct Model0_pcpu_sw_netstats *Model0_tstats;
  struct Model0_pcpu_dstats *Model0_dstats;
  struct Model0_pcpu_vstats *Model0_vstats;
 };

 struct Model0_garp_port *Model0_garp_port;
 struct Model0_mrp_port *Model0_mrp_port;

 struct Model0_device Model0_dev;
 const struct Model0_attribute_group *Model0_sysfs_groups[4];
 const struct Model0_attribute_group *Model0_sysfs_rx_queue_group;

 const struct Model0_rtnl_link_ops *Model0_rtnl_link_ops;

 /* for setting kernel sock attribute on TCP connection setup */

 unsigned int Model0_gso_max_size;

 Model0_u16 Model0_gso_max_segs;




 Model0_u8 Model0_num_tc;
 struct Model0_netdev_tc_txq Model0_tc_to_txq[16];
 Model0_u8 Model0_prio_tc_map[15 + 1];







 struct Model0_phy_device *Model0_phydev;
 struct Model0_lock_class_key *Model0_qdisc_tx_busylock;
 struct Model0_lock_class_key *Model0_qdisc_running_key;
 bool Model0_proto_down;
};




static inline __attribute__((no_instrument_function))
int Model0_netdev_get_prio_tc_map(const struct Model0_net_device *Model0_dev, Model0_u32 Model0_prio)
{
 return Model0_dev->Model0_prio_tc_map[Model0_prio & 15];
}

static inline __attribute__((no_instrument_function))
int Model0_netdev_set_prio_tc_map(struct Model0_net_device *Model0_dev, Model0_u8 Model0_prio, Model0_u8 Model0_tc)
{
 if (Model0_tc >= Model0_dev->Model0_num_tc)
  return -22;

 Model0_dev->Model0_prio_tc_map[Model0_prio & 15] = Model0_tc & 15;
 return 0;
}

static inline __attribute__((no_instrument_function))
void Model0_netdev_reset_tc(struct Model0_net_device *Model0_dev)
{
 Model0_dev->Model0_num_tc = 0;
 memset(Model0_dev->Model0_tc_to_txq, 0, sizeof(Model0_dev->Model0_tc_to_txq));
 memset(Model0_dev->Model0_prio_tc_map, 0, sizeof(Model0_dev->Model0_prio_tc_map));
}

static inline __attribute__((no_instrument_function))
int Model0_netdev_set_tc_queue(struct Model0_net_device *Model0_dev, Model0_u8 Model0_tc, Model0_u16 Model0_count, Model0_u16 Model0_offset)
{
 if (Model0_tc >= Model0_dev->Model0_num_tc)
  return -22;

 Model0_dev->Model0_tc_to_txq[Model0_tc].Model0_count = Model0_count;
 Model0_dev->Model0_tc_to_txq[Model0_tc].Model0_offset = Model0_offset;
 return 0;
}

static inline __attribute__((no_instrument_function))
int Model0_netdev_set_num_tc(struct Model0_net_device *Model0_dev, Model0_u8 Model0_num_tc)
{
 if (Model0_num_tc > 16)
  return -22;

 Model0_dev->Model0_num_tc = Model0_num_tc;
 return 0;
}

static inline __attribute__((no_instrument_function))
int Model0_netdev_get_num_tc(struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_num_tc;
}

static inline __attribute__((no_instrument_function))
struct Model0_netdev_queue *Model0_netdev_get_tx_queue(const struct Model0_net_device *Model0_dev,
      unsigned int Model0_index)
{
 return &Model0_dev->Model0__tx[Model0_index];
}

static inline __attribute__((no_instrument_function)) struct Model0_netdev_queue *Model0_skb_get_tx_queue(const struct Model0_net_device *Model0_dev,
          const struct Model0_sk_buff *Model0_skb)
{
 return Model0_netdev_get_tx_queue(Model0_dev, Model0_skb_get_queue_mapping(Model0_skb));
}

static inline __attribute__((no_instrument_function)) void Model0_netdev_for_each_tx_queue(struct Model0_net_device *Model0_dev,
         void (*Model0_f)(struct Model0_net_device *,
            struct Model0_netdev_queue *,
            void *),
         void *Model0_arg)
{
 unsigned int Model0_i;

 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++)
  Model0_f(Model0_dev, &Model0_dev->Model0__tx[Model0_i], Model0_arg);
}
struct Model0_netdev_queue *Model0_netdev_pick_tx(struct Model0_net_device *Model0_dev,
        struct Model0_sk_buff *Model0_skb,
        void *Model0_accel_priv);

/* returns the headroom that the master device needs to take in account
 * when forwarding to this dev
 */
static inline __attribute__((no_instrument_function)) unsigned Model0_netdev_get_fwd_headroom(struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_PHONY_HEADROOM ? 0 : Model0_dev->Model0_needed_headroom;
}

static inline __attribute__((no_instrument_function)) void Model0_netdev_set_rx_headroom(struct Model0_net_device *Model0_dev, int Model0_new_hr)
{
 if (Model0_dev->Model0_netdev_ops->Model0_ndo_set_rx_headroom)
  Model0_dev->Model0_netdev_ops->Model0_ndo_set_rx_headroom(Model0_dev, Model0_new_hr);
}

/* set the device rx headroom to the dev's default */
static inline __attribute__((no_instrument_function)) void Model0_netdev_reset_rx_headroom(struct Model0_net_device *Model0_dev)
{
 Model0_netdev_set_rx_headroom(Model0_dev, -1);
}

/*
 * Net namespace inlines
 */
static inline __attribute__((no_instrument_function))
struct Model0_net *Model0_dev_net(const struct Model0_net_device *Model0_dev)
{
 return Model0_read_pnet(&Model0_dev->Model0_nd_net);
}

static inline __attribute__((no_instrument_function))
void Model0_dev_net_set(struct Model0_net_device *Model0_dev, struct Model0_net *Model0_net)
{
 Model0_write_pnet(&Model0_dev->Model0_nd_net, Model0_net);
}

static inline __attribute__((no_instrument_function)) bool Model0_netdev_uses_dsa(struct Model0_net_device *Model0_dev)
{




 return false;
}

/**
 *	netdev_priv - access network device private data
 *	@dev: network device
 *
 * Get network device private data
 */
static inline __attribute__((no_instrument_function)) void *Model0_netdev_priv(const struct Model0_net_device *Model0_dev)
{
 return (char *)Model0_dev + ((((sizeof(struct Model0_net_device))) + ((typeof((sizeof(struct Model0_net_device))))((32)) - 1)) & ~((typeof((sizeof(struct Model0_net_device))))((32)) - 1));
}

/* Set the sysfs physical device reference for the network logical device
 * if set prior to registration will cause a symlink during initialization.
 */


/* Set the sysfs device type for the network logical device to allow
 * fine-grained identification of different network device types. For
 * example Ethernet, Wireless LAN, Bluetooth, WiMAX etc.
 */


/* Default NAPI poll() weight
 * Device drivers are strongly advised to not use bigger value
 */


/**
 *	netif_napi_add - initialize a NAPI context
 *	@dev:  network device
 *	@napi: NAPI context
 *	@poll: polling function
 *	@weight: default weight
 *
 * netif_napi_add() must be used to initialize a NAPI context prior to calling
 * *any* of the other NAPI-related functions.
 */
void Model0_netif_napi_add(struct Model0_net_device *Model0_dev, struct Model0_napi_struct *Model0_napi,
      int (*Model0_poll)(struct Model0_napi_struct *, int), int Model0_weight);

/**
 *	netif_tx_napi_add - initialize a NAPI context
 *	@dev:  network device
 *	@napi: NAPI context
 *	@poll: polling function
 *	@weight: default weight
 *
 * This variant of netif_napi_add() should be used from drivers using NAPI
 * to exclusively poll a TX queue.
 * This will avoid we add it into napi_hash[], thus polluting this hash table.
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_tx_napi_add(struct Model0_net_device *Model0_dev,
         struct Model0_napi_struct *Model0_napi,
         int (*Model0_poll)(struct Model0_napi_struct *, int),
         int Model0_weight)
{
 Model0_set_bit(Model0_NAPI_STATE_NO_BUSY_POLL, &Model0_napi->Model0_state);
 Model0_netif_napi_add(Model0_dev, Model0_napi, Model0_poll, Model0_weight);
}

/**
 *  netif_napi_del - remove a NAPI context
 *  @napi: NAPI context
 *
 *  netif_napi_del() removes a NAPI context from the network device NAPI list
 */
void Model0_netif_napi_del(struct Model0_napi_struct *Model0_napi);

struct Model0_napi_gro_cb {
 /* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */
 void *Model0_frag0;

 /* Length of frag0. */
 unsigned int Model0_frag0_len;

 /* This indicates where we are processing relative to skb->data. */
 int Model0_data_offset;

 /* This is non-zero if the packet cannot be merged with the new skb. */
 Model0_u16 Model0_flush;

 /* Save the IP ID here and check when we get to the transport layer */
 Model0_u16 Model0_flush_id;

 /* Number of segments aggregated. */
 Model0_u16 Model0_count;

 /* Start offset for remote checksum offload */
 Model0_u16 Model0_gro_remcsum_start;

 /* jiffies when first packet was created/queued */
 unsigned long Model0_age;

 /* Used in ipv6_gro_receive() and foo-over-udp */
 Model0_u16 Model0_proto;

 /* This is non-zero if the packet may be of the same flow. */
 Model0_u8 Model0_same_flow:1;

 /* Used in tunnel GRO receive */
 Model0_u8 Model0_encap_mark:1;

 /* GRO checksum is valid */
 Model0_u8 Model0_csum_valid:1;

 /* Number of checksums via CHECKSUM_UNNECESSARY */
 Model0_u8 Model0_csum_cnt:3;

 /* Free the skb? */
 Model0_u8 Model0_free:2;



 /* Used in foo-over-udp, set in udp[46]_gro_receive */
 Model0_u8 Model0_is_ipv6:1;

 /* Used in GRE, set in fou/gue_gro_receive */
 Model0_u8 Model0_is_fou:1;

 /* Used to determine if flush_id can be ignored */
 Model0_u8 Model0_is_atomic:1;

 /* 5 bit hole */

 /* used to support CHECKSUM_COMPLETE for tunneling protocols */
 Model0___wsum Model0_csum;

 /* used in skb_gro_receive() slow path */
 struct Model0_sk_buff *Model0_last;
};



struct Model0_packet_type {
 Model0___be16 Model0_type; /* This is really htons(ether_type). */
 struct Model0_net_device *Model0_dev; /* NULL is wildcarded here	     */
 int (*func) (struct Model0_sk_buff *,
      struct Model0_net_device *,
      struct Model0_packet_type *,
      struct Model0_net_device *);
 bool (*Model0_id_match)(struct Model0_packet_type *Model0_ptype,
         struct Model0_sock *Model0_sk);
 void *Model0_af_packet_priv;
 struct Model0_list_head Model0_list;
};

struct Model0_offload_callbacks {
 struct Model0_sk_buff *(*Model0_gso_segment)(struct Model0_sk_buff *Model0_skb,
      Model0_netdev_features_t Model0_features);
 struct Model0_sk_buff **(*Model0_gro_receive)(struct Model0_sk_buff **Model0_head,
       struct Model0_sk_buff *Model0_skb);
 int (*Model0_gro_complete)(struct Model0_sk_buff *Model0_skb, int Model0_nhoff);
};

struct Model0_packet_offload {
 Model0___be16 Model0_type; /* This is really htons(ether_type). */
 Model0_u16 Model0_priority;
 struct Model0_offload_callbacks Model0_callbacks;
 struct Model0_list_head Model0_list;
};

/* often modified stats are per-CPU, other are shared (netdev->stats) */
struct Model0_pcpu_sw_netstats {
 Model0_u64 Model0_rx_packets;
 Model0_u64 Model0_rx_bytes;
 Model0_u64 Model0_tx_packets;
 Model0_u64 Model0_tx_bytes;
 struct Model0_u64_stats_sync Model0_syncp;
};
enum Model0_netdev_lag_tx_type {
 Model0_NETDEV_LAG_TX_TYPE_UNKNOWN,
 Model0_NETDEV_LAG_TX_TYPE_RANDOM,
 Model0_NETDEV_LAG_TX_TYPE_BROADCAST,
 Model0_NETDEV_LAG_TX_TYPE_ROUNDROBIN,
 Model0_NETDEV_LAG_TX_TYPE_ACTIVEBACKUP,
 Model0_NETDEV_LAG_TX_TYPE_HASH,
};

struct Model0_netdev_lag_upper_info {
 enum Model0_netdev_lag_tx_type Model0_tx_type;
};

struct Model0_netdev_lag_lower_state_info {
 Model0_u8 Model0_link_up : 1,
    Model0_tx_enabled : 1;
};



/* netdevice notifier chain. Please remember to update the rtnetlink
 * notification exclusion list in rtnetlink_event() when adding new
 * types.
 */
int Model0_register_netdevice_notifier(struct Model0_notifier_block *Model0_nb);
int Model0_unregister_netdevice_notifier(struct Model0_notifier_block *Model0_nb);

struct Model0_netdev_notifier_info {
 struct Model0_net_device *Model0_dev;
};

struct Model0_netdev_notifier_change_info {
 struct Model0_netdev_notifier_info Model0_info; /* must be first */
 unsigned int Model0_flags_changed;
};

struct Model0_netdev_notifier_changeupper_info {
 struct Model0_netdev_notifier_info Model0_info; /* must be first */
 struct Model0_net_device *Model0_upper_dev; /* new upper dev */
 bool Model0_master; /* is upper dev master */
 bool Model0_linking; /* is the notification for link or unlink */
 void *Model0_upper_info; /* upper dev info */
};

struct Model0_netdev_notifier_changelowerstate_info {
 struct Model0_netdev_notifier_info Model0_info; /* must be first */
 void *Model0_lower_state_info; /* is lower dev state */
};

static inline __attribute__((no_instrument_function)) void Model0_netdev_notifier_info_init(struct Model0_netdev_notifier_info *Model0_info,
          struct Model0_net_device *Model0_dev)
{
 Model0_info->Model0_dev = Model0_dev;
}

static inline __attribute__((no_instrument_function)) struct Model0_net_device *
Model0_netdev_notifier_info_to_dev(const struct Model0_netdev_notifier_info *Model0_info)
{
 return Model0_info->Model0_dev;
}

int Model0_call_netdevice_notifiers(unsigned long Model0_val, struct Model0_net_device *Model0_dev);


extern Model0_rwlock_t Model0_dev_base_lock; /* Device list lock */
static inline __attribute__((no_instrument_function)) struct Model0_net_device *Model0_next_net_device(struct Model0_net_device *Model0_dev)
{
 struct Model0_list_head *Model0_lh;
 struct Model0_net *Model0_net;

 Model0_net = Model0_dev_net(Model0_dev);
 Model0_lh = Model0_dev->Model0_dev_list.Model0_next;
 return Model0_lh == &Model0_net->Model0_dev_base_head ? ((void *)0) : ({ const typeof( ((struct Model0_net_device *)0)->Model0_dev_list ) *Model0___mptr = (Model0_lh); (struct Model0_net_device *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_net_device, Model0_dev_list) );});
}

static inline __attribute__((no_instrument_function)) struct Model0_net_device *Model0_next_net_device_rcu(struct Model0_net_device *Model0_dev)
{
 struct Model0_list_head *Model0_lh;
 struct Model0_net *Model0_net;

 Model0_net = Model0_dev_net(Model0_dev);
 Model0_lh = ({ typeof(*((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next))))) *Model0_________p1 = (typeof(*((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next))))) *)({ typeof(((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next))))) Model0__________p1 = ({ union { typeof(((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next))))) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next)))))); else Model0___read_once_size_nocheck(&(((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next)))))); Model0___u.Model0___val; }); typeof(*(((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next)))))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*((*((struct Model0_list_head **)(&(&Model0_dev->Model0_dev_list)->Model0_next))))) *)(Model0_________p1)); });
 return Model0_lh == &Model0_net->Model0_dev_base_head ? ((void *)0) : ({ const typeof( ((struct Model0_net_device *)0)->Model0_dev_list ) *Model0___mptr = (Model0_lh); (struct Model0_net_device *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_net_device, Model0_dev_list) );});
}

static inline __attribute__((no_instrument_function)) struct Model0_net_device *Model0_first_net_device(struct Model0_net *Model0_net)
{
 return Model0_list_empty(&Model0_net->Model0_dev_base_head) ? ((void *)0) :
  ({ const typeof( ((struct Model0_net_device *)0)->Model0_dev_list ) *Model0___mptr = (Model0_net->Model0_dev_base_head.Model0_next); (struct Model0_net_device *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_net_device, Model0_dev_list) );});
}

static inline __attribute__((no_instrument_function)) struct Model0_net_device *Model0_first_net_device_rcu(struct Model0_net *Model0_net)
{
 struct Model0_list_head *Model0_lh = ({ typeof(*((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next))))) *Model0_________p1 = (typeof(*((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next))))) *)({ typeof(((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next))))) Model0__________p1 = ({ union { typeof(((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next))))) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next)))))); else Model0___read_once_size_nocheck(&(((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next))))), Model0___u.Model0___c, sizeof(((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next)))))); Model0___u.Model0___val; }); typeof(*(((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next)))))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*((*((struct Model0_list_head **)(&(&Model0_net->Model0_dev_base_head)->Model0_next))))) *)(Model0_________p1)); });

 return Model0_lh == &Model0_net->Model0_dev_base_head ? ((void *)0) : ({ const typeof( ((struct Model0_net_device *)0)->Model0_dev_list ) *Model0___mptr = (Model0_lh); (struct Model0_net_device *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_net_device, Model0_dev_list) );});
}

int Model0_netdev_boot_setup_check(struct Model0_net_device *Model0_dev);
unsigned long Model0_netdev_boot_base(const char *Model0_prefix, int Model0_unit);
struct Model0_net_device *Model0_dev_getbyhwaddr_rcu(struct Model0_net *Model0_net, unsigned short Model0_type,
           const char *Model0_hwaddr);
struct Model0_net_device *Model0_dev_getfirstbyhwtype(struct Model0_net *Model0_net, unsigned short Model0_type);
struct Model0_net_device *Model0___dev_getfirstbyhwtype(struct Model0_net *Model0_net, unsigned short Model0_type);
void Model0_dev_add_pack(struct Model0_packet_type *Model0_pt);
void Model0_dev_remove_pack(struct Model0_packet_type *Model0_pt);
void Model0___dev_remove_pack(struct Model0_packet_type *Model0_pt);
void Model0_dev_add_offload(struct Model0_packet_offload *Model0_po);
void Model0_dev_remove_offload(struct Model0_packet_offload *Model0_po);

int Model0_dev_get_iflink(const struct Model0_net_device *Model0_dev);
int Model0_dev_fill_metadata_dst(struct Model0_net_device *Model0_dev, struct Model0_sk_buff *Model0_skb);
struct Model0_net_device *Model0___dev_get_by_flags(struct Model0_net *Model0_net, unsigned short Model0_flags,
          unsigned short Model0_mask);
struct Model0_net_device *Model0_dev_get_by_name(struct Model0_net *Model0_net, const char *Model0_name);
struct Model0_net_device *Model0_dev_get_by_name_rcu(struct Model0_net *Model0_net, const char *Model0_name);
struct Model0_net_device *Model0___dev_get_by_name(struct Model0_net *Model0_net, const char *Model0_name);
int Model0_dev_alloc_name(struct Model0_net_device *Model0_dev, const char *Model0_name);
int Model0_dev_open(struct Model0_net_device *Model0_dev);
int Model0_dev_close(struct Model0_net_device *Model0_dev);
int Model0_dev_close_many(struct Model0_list_head *Model0_head, bool Model0_unlink);
void Model0_dev_disable_lro(struct Model0_net_device *Model0_dev);
int Model0_dev_loopback_xmit(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_newskb);
int Model0_dev_queue_xmit(struct Model0_sk_buff *Model0_skb);
int Model0_dev_queue_xmit_accel(struct Model0_sk_buff *Model0_skb, void *Model0_accel_priv);
int Model0_register_netdevice(struct Model0_net_device *Model0_dev);
void Model0_unregister_netdevice_queue(struct Model0_net_device *Model0_dev, struct Model0_list_head *Model0_head);
void Model0_unregister_netdevice_many(struct Model0_list_head *Model0_head);
static inline __attribute__((no_instrument_function)) void Model0_unregister_netdevice(struct Model0_net_device *Model0_dev)
{
 Model0_unregister_netdevice_queue(Model0_dev, ((void *)0));
}

int Model0_netdev_refcnt_read(const struct Model0_net_device *Model0_dev);
void Model0_free_netdev(struct Model0_net_device *Model0_dev);
void Model0_netdev_freemem(struct Model0_net_device *Model0_dev);
void Model0_synchronize_net(void);
int Model0_init_dummy_netdev(struct Model0_net_device *Model0_dev);

extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model0_xmit_recursion;


static inline __attribute__((no_instrument_function)) int Model0_dev_recursion_level(void)
{
 return ({ typeof(Model0_xmit_recursion) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_xmit_recursion)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_xmit_recursion)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_xmit_recursion) Model0_pfo_ret__; switch (sizeof(Model0_xmit_recursion)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_xmit_recursion) Model0_pfo_ret__; switch (sizeof(Model0_xmit_recursion)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_xmit_recursion) Model0_pfo_ret__; switch (sizeof(Model0_xmit_recursion)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_xmit_recursion) Model0_pfo_ret__; switch (sizeof(Model0_xmit_recursion)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_xmit_recursion)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; });
}

struct Model0_net_device *Model0_dev_get_by_index(struct Model0_net *Model0_net, int Model0_ifindex);
struct Model0_net_device *Model0___dev_get_by_index(struct Model0_net *Model0_net, int Model0_ifindex);
struct Model0_net_device *Model0_dev_get_by_index_rcu(struct Model0_net *Model0_net, int Model0_ifindex);
int Model0_netdev_get_name(struct Model0_net *Model0_net, char *Model0_name, int Model0_ifindex);
int Model0_dev_restart(struct Model0_net_device *Model0_dev);
int Model0_skb_gro_receive(struct Model0_sk_buff **Model0_head, struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) unsigned int Model0_skb_gro_offset(const struct Model0_sk_buff *Model0_skb)
{
 return ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_data_offset;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_skb_gro_len(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_len - ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_data_offset;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_gro_pull(struct Model0_sk_buff *Model0_skb, unsigned int Model0_len)
{
 ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_data_offset += Model0_len;
}

static inline __attribute__((no_instrument_function)) void *Model0_skb_gro_header_fast(struct Model0_sk_buff *Model0_skb,
     unsigned int Model0_offset)
{
 return ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_frag0 + Model0_offset;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_gro_header_hard(struct Model0_sk_buff *Model0_skb, unsigned int Model0_hlen)
{
 return ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_frag0_len < Model0_hlen;
}

static inline __attribute__((no_instrument_function)) void *Model0_skb_gro_header_slow(struct Model0_sk_buff *Model0_skb, unsigned int Model0_hlen,
     unsigned int Model0_offset)
{
 if (!Model0_pskb_may_pull(Model0_skb, Model0_hlen))
  return ((void *)0);

 ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_frag0 = ((void *)0);
 ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_frag0_len = 0;
 return Model0_skb->Model0_data + Model0_offset;
}

static inline __attribute__((no_instrument_function)) void *Model0_skb_gro_network_header(struct Model0_sk_buff *Model0_skb)
{
 return (((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_frag0 ?: Model0_skb->Model0_data) +
        Model0_skb_network_offset(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_skb_gro_postpull_rcsum(struct Model0_sk_buff *Model0_skb,
     const void *Model0_start, unsigned int Model0_len)
{
 if (((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_valid)
  ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum = Model0_csum_sub(((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum,
        Model0_csum_partial(Model0_start, Model0_len, 0));
}

/* GRO checksum functions. These are logical equivalents of the normal
 * checksum functions (in skbuff.h) except that they operate on the GRO
 * offsets and fields in sk_buff.
 */

Model0___sum16 Model0___skb_gro_checksum_complete(struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) bool Model0_skb_at_gro_remcsum_start(struct Model0_sk_buff *Model0_skb)
{
 return (((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_gro_remcsum_start == Model0_skb_gro_offset(Model0_skb));
}

static inline __attribute__((no_instrument_function)) bool Model0___skb_gro_checksum_validate_needed(struct Model0_sk_buff *Model0_skb,
            bool Model0_zero_okay,
            Model0___sum16 Model0_check)
{
 return ((Model0_skb->Model0_ip_summed != 3 ||
  Model0_skb_checksum_start_offset(Model0_skb) <
   Model0_skb_gro_offset(Model0_skb)) &&
  !Model0_skb_at_gro_remcsum_start(Model0_skb) &&
  ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_cnt == 0 &&
  (!Model0_zero_okay || Model0_check));
}

static inline __attribute__((no_instrument_function)) Model0___sum16 Model0___skb_gro_checksum_validate_complete(struct Model0_sk_buff *Model0_skb,
          Model0___wsum Model0_psum)
{
 if (((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_valid &&
     !Model0_csum_fold(Model0_csum_add(Model0_psum, ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum)))
  return 0;

 ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum = Model0_psum;

 return Model0___skb_gro_checksum_complete(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_skb_gro_incr_csum_unnecessary(struct Model0_sk_buff *Model0_skb)
{
 if (((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_cnt > 0) {
  /* Consume a checksum from CHECKSUM_UNNECESSARY */
  ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_cnt--;
 } else {
  /* Update skb for CHECKSUM_UNNECESSARY and csum_level when we
		 * verified a new top level checksum or an encapsulated one
		 * during GRO. This saves work if we fallback to normal path.
		 */
  Model0___skb_incr_checksum_unnecessary(Model0_skb);
 }
}
static inline __attribute__((no_instrument_function)) bool Model0___skb_gro_checksum_convert_check(struct Model0_sk_buff *Model0_skb)
{
 return (((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_cnt == 0 &&
  !((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_valid);
}

static inline __attribute__((no_instrument_function)) void Model0___skb_gro_checksum_convert(struct Model0_sk_buff *Model0_skb,
           Model0___sum16 Model0_check, Model0___wsum Model0_pseudo)
{
 ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum = ~Model0_pseudo;
 ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_valid = 1;
}
struct Model0_gro_remcsum {
 int Model0_offset;
 Model0___wsum Model0_delta;
};

static inline __attribute__((no_instrument_function)) void Model0_skb_gro_remcsum_init(struct Model0_gro_remcsum *Model0_grc)
{
 Model0_grc->Model0_offset = 0;
 Model0_grc->Model0_delta = 0;
}

static inline __attribute__((no_instrument_function)) void *Model0_skb_gro_remcsum_process(struct Model0_sk_buff *Model0_skb, void *Model0_ptr,
         unsigned int Model0_off, Model0_size_t Model0_hdrlen,
         int Model0_start, int Model0_offset,
         struct Model0_gro_remcsum *Model0_grc,
         bool Model0_nopartial)
{
 Model0___wsum Model0_delta;
 Model0_size_t Model0_plen = Model0_hdrlen + ({ Model0_size_t Model0___max1 = (Model0_offset + sizeof(Model0_u16)); Model0_size_t Model0___max2 = (Model0_start); Model0___max1 > Model0___max2 ? Model0___max1: Model0___max2; });

 do { if (__builtin_expect(!!(!((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum_valid), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/netdevice.h"), "i" (2592), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);

 if (!Model0_nopartial) {
  ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_gro_remcsum_start = Model0_off + Model0_hdrlen + Model0_start;
  return Model0_ptr;
 }

 Model0_ptr = Model0_skb_gro_header_fast(Model0_skb, Model0_off);
 if (Model0_skb_gro_header_hard(Model0_skb, Model0_off + Model0_plen)) {
  Model0_ptr = Model0_skb_gro_header_slow(Model0_skb, Model0_off + Model0_plen, Model0_off);
  if (!Model0_ptr)
   return ((void *)0);
 }

 Model0_delta = Model0_remcsum_adjust(Model0_ptr + Model0_hdrlen, ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum,
          Model0_start, Model0_offset);

 /* Adjust skb->csum since we changed the packet */
 ((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum = Model0_csum_add(((struct Model0_napi_gro_cb *)(Model0_skb)->Model0_cb)->Model0_csum, Model0_delta);

 Model0_grc->Model0_offset = Model0_off + Model0_hdrlen + Model0_offset;
 Model0_grc->Model0_delta = Model0_delta;

 return Model0_ptr;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_gro_remcsum_cleanup(struct Model0_sk_buff *Model0_skb,
        struct Model0_gro_remcsum *Model0_grc)
{
 void *Model0_ptr;
 Model0_size_t Model0_plen = Model0_grc->Model0_offset + sizeof(Model0_u16);

 if (!Model0_grc->Model0_delta)
  return;

 Model0_ptr = Model0_skb_gro_header_fast(Model0_skb, Model0_grc->Model0_offset);
 if (Model0_skb_gro_header_hard(Model0_skb, Model0_grc->Model0_offset + sizeof(Model0_u16))) {
  Model0_ptr = Model0_skb_gro_header_slow(Model0_skb, Model0_plen, Model0_grc->Model0_offset);
  if (!Model0_ptr)
   return;
 }

 Model0_remcsum_unadjust((Model0___sum16 *)Model0_ptr, Model0_grc->Model0_delta);
}

struct Model0_skb_csum_offl_spec {
 Model0___u16 Model0_ipv4_okay:1,
   Model0_ipv6_okay:1,
   Model0_encap_okay:1,
   Model0_ip_options_okay:1,
   Model0_ext_hdrs_okay:1,
   Model0_tcp_okay:1,
   Model0_udp_okay:1,
   Model0_sctp_okay:1,
   Model0_vlan_okay:1,
   Model0_no_encapped_ipv6:1,
   Model0_no_not_encapped:1;
};

bool Model0___skb_csum_offload_chk(struct Model0_sk_buff *Model0_skb,
       const struct Model0_skb_csum_offl_spec *Model0_spec,
       bool *Model0_csum_encapped,
       bool Model0_csum_help);

static inline __attribute__((no_instrument_function)) bool Model0_skb_csum_offload_chk(struct Model0_sk_buff *Model0_skb,
     const struct Model0_skb_csum_offl_spec *Model0_spec,
     bool *Model0_csum_encapped,
     bool Model0_csum_help)
{
 if (Model0_skb->Model0_ip_summed != 3)
  return false;

 return Model0___skb_csum_offload_chk(Model0_skb, Model0_spec, Model0_csum_encapped, Model0_csum_help);
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_csum_offload_chk_help(struct Model0_sk_buff *Model0_skb,
          const struct Model0_skb_csum_offl_spec *Model0_spec)
{
 bool Model0_csum_encapped;

 return Model0_skb_csum_offload_chk(Model0_skb, Model0_spec, &Model0_csum_encapped, true);
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_csum_off_chk_help_cmn(struct Model0_sk_buff *Model0_skb)
{
 static const struct Model0_skb_csum_offl_spec Model0_csum_offl_spec = {
  .Model0_ipv4_okay = 1,
  .Model0_ip_options_okay = 1,
  .Model0_ipv6_okay = 1,
  .Model0_vlan_okay = 1,
  .Model0_tcp_okay = 1,
  .Model0_udp_okay = 1,
 };

 return Model0_skb_csum_offload_chk_help(Model0_skb, &Model0_csum_offl_spec);
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_csum_off_chk_help_cmn_v4_only(struct Model0_sk_buff *Model0_skb)
{
 static const struct Model0_skb_csum_offl_spec Model0_csum_offl_spec = {
  .Model0_ipv4_okay = 1,
  .Model0_ip_options_okay = 1,
  .Model0_tcp_okay = 1,
  .Model0_udp_okay = 1,
  .Model0_vlan_okay = 1,
 };

 return Model0_skb_csum_offload_chk_help(Model0_skb, &Model0_csum_offl_spec);
}

static inline __attribute__((no_instrument_function)) int Model0_dev_hard_header(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
      unsigned short Model0_type,
      const void *Model0_daddr, const void *Model0_saddr,
      unsigned int Model0_len)
{
 if (!Model0_dev->Model0_header_ops || !Model0_dev->Model0_header_ops->Model0_create)
  return 0;

 return Model0_dev->Model0_header_ops->Model0_create(Model0_skb, Model0_dev, Model0_type, Model0_daddr, Model0_saddr, Model0_len);
}

static inline __attribute__((no_instrument_function)) int Model0_dev_parse_header(const struct Model0_sk_buff *Model0_skb,
       unsigned char *Model0_haddr)
{
 const struct Model0_net_device *Model0_dev = Model0_skb->Model0_dev;

 if (!Model0_dev->Model0_header_ops || !Model0_dev->Model0_header_ops->Model0_parse)
  return 0;
 return Model0_dev->Model0_header_ops->Model0_parse(Model0_skb, Model0_haddr);
}

/* ll_header must have at least hard_header_len allocated */
static inline __attribute__((no_instrument_function)) bool Model0_dev_validate_header(const struct Model0_net_device *Model0_dev,
           char *Model0_ll_header, int Model0_len)
{
 if (__builtin_expect(!!(Model0_len >= Model0_dev->Model0_hard_header_len), 1))
  return true;

 if (Model0_capable(17)) {
  memset(Model0_ll_header + Model0_len, 0, Model0_dev->Model0_hard_header_len - Model0_len);
  return true;
 }

 if (Model0_dev->Model0_header_ops && Model0_dev->Model0_header_ops->Model0_validate)
  return Model0_dev->Model0_header_ops->Model0_validate(Model0_ll_header, Model0_len);

 return false;
}

typedef int Model0_gifconf_func_t(struct Model0_net_device * Model0_dev, char * Model0_bufptr, int Model0_len);
int Model0_register_gifconf(unsigned int Model0_family, Model0_gifconf_func_t *Model0_gifconf);
static inline __attribute__((no_instrument_function)) int Model0_unregister_gifconf(unsigned int Model0_family)
{
 return Model0_register_gifconf(Model0_family, ((void *)0));
}



struct Model0_sd_flow_limit {
 Model0_u64 Model0_count;
 unsigned int Model0_num_buckets;
 unsigned int Model0_history_head;
 Model0_u16 Model0_history[(1 << 7)];
 Model0_u8 Model0_buckets[];
};

extern int Model0_netdev_flow_limit_table_len;


/*
 * Incoming packets are placed on per-CPU queues
 */
struct Model0_softnet_data {
 struct Model0_list_head Model0_poll_list;
 struct Model0_sk_buff_head Model0_process_queue;

 /* stats */
 unsigned int Model0_processed;
 unsigned int Model0_time_squeeze;
 unsigned int Model0_received_rps;

 struct Model0_softnet_data *Model0_rps_ipi_list;


 struct Model0_sd_flow_limit *Model0_flow_limit;

 struct Model0_Qdisc *Model0_output_queue;
 struct Model0_Qdisc **Model0_output_queue_tailp;
 struct Model0_sk_buff *Model0_completion_queue;


 /* input_queue_head should be written by cpu owning this struct,
	 * and only read by other cpus. Worth using a cache line.
	 */
 unsigned int Model0_input_queue_head __attribute__((__aligned__((1 << (6)))));

 /* Elements below can be accessed between CPUs for RPS/RFS */
 struct Model0_call_single_data Model0_csd __attribute__((__aligned__((1 << (6)))));
 struct Model0_softnet_data *Model0_rps_ipi_next;
 unsigned int Model0_cpu;
 unsigned int Model0_input_queue_tail;

 unsigned int Model0_dropped;
 struct Model0_sk_buff_head Model0_input_pkt_queue;
 struct Model0_napi_struct Model0_backlog;

};

static inline __attribute__((no_instrument_function)) void Model0_input_queue_head_incr(struct Model0_softnet_data *Model0_sd)
{

 Model0_sd->Model0_input_queue_head++;

}

static inline __attribute__((no_instrument_function)) void Model0_input_queue_tail_incr_save(struct Model0_softnet_data *Model0_sd,
           unsigned int *Model0_qtail)
{

 *Model0_qtail = ++Model0_sd->Model0_input_queue_tail;

}

extern __attribute__((section(".data..percpu" "..shared_aligned"))) __typeof__(struct Model0_softnet_data) Model0_softnet_data __attribute__((__aligned__((1 << (6)))));

void Model0___netif_schedule(struct Model0_Qdisc *Model0_q);
void Model0_netif_schedule_queue(struct Model0_netdev_queue *Model0_txq);

static inline __attribute__((no_instrument_function)) void Model0_netif_tx_schedule_all(struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;

 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++)
  Model0_netif_schedule_queue(Model0_netdev_get_tx_queue(Model0_dev, Model0_i));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_netif_tx_start_queue(struct Model0_netdev_queue *Model0_dev_queue)
{
 Model0_clear_bit(Model0___QUEUE_STATE_DRV_XOFF, &Model0_dev_queue->Model0_state);
}

/**
 *	netif_start_queue - allow transmit
 *	@dev: network device
 *
 *	Allow upper layers to call the device hard_start_xmit routine.
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_start_queue(struct Model0_net_device *Model0_dev)
{
 Model0_netif_tx_start_queue(Model0_netdev_get_tx_queue(Model0_dev, 0));
}

static inline __attribute__((no_instrument_function)) void Model0_netif_tx_start_all_queues(struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;

 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_i);
  Model0_netif_tx_start_queue(Model0_txq);
 }
}

void Model0_netif_tx_wake_queue(struct Model0_netdev_queue *Model0_dev_queue);

/**
 *	netif_wake_queue - restart transmit
 *	@dev: network device
 *
 *	Allow upper layers to call the device hard_start_xmit routine.
 *	Used for flow control when transmit resources are available.
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_wake_queue(struct Model0_net_device *Model0_dev)
{
 Model0_netif_tx_wake_queue(Model0_netdev_get_tx_queue(Model0_dev, 0));
}

static inline __attribute__((no_instrument_function)) void Model0_netif_tx_wake_all_queues(struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;

 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_i);
  Model0_netif_tx_wake_queue(Model0_txq);
 }
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_netif_tx_stop_queue(struct Model0_netdev_queue *Model0_dev_queue)
{
 Model0_set_bit(Model0___QUEUE_STATE_DRV_XOFF, &Model0_dev_queue->Model0_state);
}

/**
 *	netif_stop_queue - stop transmitted packets
 *	@dev: network device
 *
 *	Stop upper layers calling the device hard_start_xmit routine.
 *	Used for flow control when transmit resources are unavailable.
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_stop_queue(struct Model0_net_device *Model0_dev)
{
 Model0_netif_tx_stop_queue(Model0_netdev_get_tx_queue(Model0_dev, 0));
}

void Model0_netif_tx_stop_all_queues(struct Model0_net_device *Model0_dev);

static inline __attribute__((no_instrument_function)) bool Model0_netif_tx_queue_stopped(const struct Model0_netdev_queue *Model0_dev_queue)
{
 return (__builtin_constant_p((Model0___QUEUE_STATE_DRV_XOFF)) ? Model0_constant_test_bit((Model0___QUEUE_STATE_DRV_XOFF), (&Model0_dev_queue->Model0_state)) : Model0_variable_test_bit((Model0___QUEUE_STATE_DRV_XOFF), (&Model0_dev_queue->Model0_state)));
}

/**
 *	netif_queue_stopped - test if transmit queue is flowblocked
 *	@dev: network device
 *
 *	Test if transmit queue on device is currently unable to send.
 */
static inline __attribute__((no_instrument_function)) bool Model0_netif_queue_stopped(const struct Model0_net_device *Model0_dev)
{
 return Model0_netif_tx_queue_stopped(Model0_netdev_get_tx_queue(Model0_dev, 0));
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_xmit_stopped(const struct Model0_netdev_queue *Model0_dev_queue)
{
 return Model0_dev_queue->Model0_state & ((1 << Model0___QUEUE_STATE_DRV_XOFF) | (1 << Model0___QUEUE_STATE_STACK_XOFF));
}

static inline __attribute__((no_instrument_function)) bool
Model0_netif_xmit_frozen_or_stopped(const struct Model0_netdev_queue *Model0_dev_queue)
{
 return Model0_dev_queue->Model0_state & (((1 << Model0___QUEUE_STATE_DRV_XOFF) | (1 << Model0___QUEUE_STATE_STACK_XOFF)) | (1 << Model0___QUEUE_STATE_FROZEN));
}

static inline __attribute__((no_instrument_function)) bool
Model0_netif_xmit_frozen_or_drv_stopped(const struct Model0_netdev_queue *Model0_dev_queue)
{
 return Model0_dev_queue->Model0_state & ((1 << Model0___QUEUE_STATE_DRV_XOFF) | (1 << Model0___QUEUE_STATE_FROZEN));
}

/**
 *	netdev_txq_bql_enqueue_prefetchw - prefetch bql data for write
 *	@dev_queue: pointer to transmit queue
 *
 * BQL enabled drivers might use this helper in their ndo_start_xmit(),
 * to give appropriate hint to the CPU.
 */
static inline __attribute__((no_instrument_function)) void Model0_netdev_txq_bql_enqueue_prefetchw(struct Model0_netdev_queue *Model0_dev_queue)
{

 Model0_prefetchw(&Model0_dev_queue->Model0_dql.Model0_num_queued);

}

/**
 *	netdev_txq_bql_complete_prefetchw - prefetch bql data for write
 *	@dev_queue: pointer to transmit queue
 *
 * BQL enabled drivers might use this helper in their TX completion path,
 * to give appropriate hint to the CPU.
 */
static inline __attribute__((no_instrument_function)) void Model0_netdev_txq_bql_complete_prefetchw(struct Model0_netdev_queue *Model0_dev_queue)
{

 Model0_prefetchw(&Model0_dev_queue->Model0_dql.Model0_limit);

}

static inline __attribute__((no_instrument_function)) void Model0_netdev_tx_sent_queue(struct Model0_netdev_queue *Model0_dev_queue,
     unsigned int Model0_bytes)
{

 Model0_dql_queued(&Model0_dev_queue->Model0_dql, Model0_bytes);

 if (__builtin_expect(!!(Model0_dql_avail(&Model0_dev_queue->Model0_dql) >= 0), 1))
  return;

 Model0_set_bit(Model0___QUEUE_STATE_STACK_XOFF, &Model0_dev_queue->Model0_state);

 /*
	 * The XOFF flag must be set before checking the dql_avail below,
	 * because in netdev_tx_completed_queue we update the dql_completed
	 * before checking the XOFF flag.
	 */
 asm volatile("mfence":::"memory");

 /* check again in case another CPU has just made room avail */
 if (__builtin_expect(!!(Model0_dql_avail(&Model0_dev_queue->Model0_dql) >= 0), 0))
  Model0_clear_bit(Model0___QUEUE_STATE_STACK_XOFF, &Model0_dev_queue->Model0_state);

}

/**
 * 	netdev_sent_queue - report the number of bytes queued to hardware
 * 	@dev: network device
 * 	@bytes: number of bytes queued to the hardware device queue
 *
 * 	Report the number of bytes queued for sending/completion to the network
 * 	device hardware queue. @bytes should be a good approximation and should
 * 	exactly match netdev_completed_queue() @bytes
 */
static inline __attribute__((no_instrument_function)) void Model0_netdev_sent_queue(struct Model0_net_device *Model0_dev, unsigned int Model0_bytes)
{
 Model0_netdev_tx_sent_queue(Model0_netdev_get_tx_queue(Model0_dev, 0), Model0_bytes);
}

static inline __attribute__((no_instrument_function)) void Model0_netdev_tx_completed_queue(struct Model0_netdev_queue *Model0_dev_queue,
          unsigned int Model0_pkts, unsigned int Model0_bytes)
{

 if (__builtin_expect(!!(!Model0_bytes), 0))
  return;

 Model0_dql_completed(&Model0_dev_queue->Model0_dql, Model0_bytes);

 /*
	 * Without the memory barrier there is a small possiblity that
	 * netdev_tx_sent_queue will miss the update and cause the queue to
	 * be stopped forever
	 */
 asm volatile("mfence":::"memory");

 if (Model0_dql_avail(&Model0_dev_queue->Model0_dql) < 0)
  return;

 if (Model0_test_and_clear_bit(Model0___QUEUE_STATE_STACK_XOFF, &Model0_dev_queue->Model0_state))
  Model0_netif_schedule_queue(Model0_dev_queue);

}

/**
 * 	netdev_completed_queue - report bytes and packets completed by device
 * 	@dev: network device
 * 	@pkts: actual number of packets sent over the medium
 * 	@bytes: actual number of bytes sent over the medium
 *
 * 	Report the number of bytes and packets transmitted by the network device
 * 	hardware queue over the physical medium, @bytes must exactly match the
 * 	@bytes amount passed to netdev_sent_queue()
 */
static inline __attribute__((no_instrument_function)) void Model0_netdev_completed_queue(struct Model0_net_device *Model0_dev,
       unsigned int Model0_pkts, unsigned int Model0_bytes)
{
 Model0_netdev_tx_completed_queue(Model0_netdev_get_tx_queue(Model0_dev, 0), Model0_pkts, Model0_bytes);
}

static inline __attribute__((no_instrument_function)) void Model0_netdev_tx_reset_queue(struct Model0_netdev_queue *Model0_q)
{

 Model0_clear_bit(Model0___QUEUE_STATE_STACK_XOFF, &Model0_q->Model0_state);
 Model0_dql_reset(&Model0_q->Model0_dql);

}

/**
 * 	netdev_reset_queue - reset the packets and bytes count of a network device
 * 	@dev_queue: network device
 *
 * 	Reset the bytes and packet count of a network device and clear the
 * 	software flow control OFF bit for this network device
 */
static inline __attribute__((no_instrument_function)) void Model0_netdev_reset_queue(struct Model0_net_device *Model0_dev_queue)
{
 Model0_netdev_tx_reset_queue(Model0_netdev_get_tx_queue(Model0_dev_queue, 0));
}

/**
 * 	netdev_cap_txqueue - check if selected tx queue exceeds device queues
 * 	@dev: network device
 * 	@queue_index: given tx queue index
 *
 * 	Returns 0 if given tx queue index >= number of device tx queues,
 * 	otherwise returns the originally passed tx queue index.
 */
static inline __attribute__((no_instrument_function)) Model0_u16 Model0_netdev_cap_txqueue(struct Model0_net_device *Model0_dev, Model0_u16 Model0_queue_index)
{
 if (__builtin_expect(!!(Model0_queue_index >= Model0_dev->Model0_real_num_tx_queues), 0)) {
  do { if (Model0_net_ratelimit()) Model0_printk("\001" "4" "TCP: " "%s selects TX queue %d, but real number of TX queues is %d\n", Model0_dev->Model0_name, Model0_queue_index, Model0_dev->Model0_real_num_tx_queues); } while (0);


  return 0;
 }

 return Model0_queue_index;
}

/**
 *	netif_running - test if up
 *	@dev: network device
 *
 *	Test if the device has been brought up.
 */
static inline __attribute__((no_instrument_function)) bool Model0_netif_running(const struct Model0_net_device *Model0_dev)
{
 return (__builtin_constant_p((Model0___LINK_STATE_START)) ? Model0_constant_test_bit((Model0___LINK_STATE_START), (&Model0_dev->Model0_state)) : Model0_variable_test_bit((Model0___LINK_STATE_START), (&Model0_dev->Model0_state)));
}

/*
 * Routines to manage the subqueues on a device.  We only need start,
 * stop, and a check if it's stopped.  All other device management is
 * done at the overall netdevice level.
 * Also test the device if we're multiqueue.
 */

/**
 *	netif_start_subqueue - allow sending packets on subqueue
 *	@dev: network device
 *	@queue_index: sub queue index
 *
 * Start individual transmit queue of a device with multiple transmit queues.
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_start_subqueue(struct Model0_net_device *Model0_dev, Model0_u16 Model0_queue_index)
{
 struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_queue_index);

 Model0_netif_tx_start_queue(Model0_txq);
}

/**
 *	netif_stop_subqueue - stop sending packets on subqueue
 *	@dev: network device
 *	@queue_index: sub queue index
 *
 * Stop individual transmit queue of a device with multiple transmit queues.
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_stop_subqueue(struct Model0_net_device *Model0_dev, Model0_u16 Model0_queue_index)
{
 struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_queue_index);
 Model0_netif_tx_stop_queue(Model0_txq);
}

/**
 *	netif_subqueue_stopped - test status of subqueue
 *	@dev: network device
 *	@queue_index: sub queue index
 *
 * Check individual transmit queue of a device with multiple transmit queues.
 */
static inline __attribute__((no_instrument_function)) bool Model0___netif_subqueue_stopped(const struct Model0_net_device *Model0_dev,
         Model0_u16 Model0_queue_index)
{
 struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_queue_index);

 return Model0_netif_tx_queue_stopped(Model0_txq);
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_subqueue_stopped(const struct Model0_net_device *Model0_dev,
       struct Model0_sk_buff *Model0_skb)
{
 return Model0___netif_subqueue_stopped(Model0_dev, Model0_skb_get_queue_mapping(Model0_skb));
}

void Model0_netif_wake_subqueue(struct Model0_net_device *Model0_dev, Model0_u16 Model0_queue_index);


int Model0_netif_set_xps_queue(struct Model0_net_device *Model0_dev, const struct Model0_cpumask *Model0_mask,
   Model0_u16 Model0_index);
Model0_u16 Model0___skb_tx_hash(const struct Model0_net_device *Model0_dev, struct Model0_sk_buff *Model0_skb,
    unsigned int Model0_num_tx_queues);

/*
 * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used
 * as a distribution range limit for the returned value.
 */
static inline __attribute__((no_instrument_function)) Model0_u16 Model0_skb_tx_hash(const struct Model0_net_device *Model0_dev,
         struct Model0_sk_buff *Model0_skb)
{
 return Model0___skb_tx_hash(Model0_dev, Model0_skb, Model0_dev->Model0_real_num_tx_queues);
}

/**
 *	netif_is_multiqueue - test if device has multiple transmit queues
 *	@dev: network device
 *
 * Check if device has multiple transmit queues
 */
static inline __attribute__((no_instrument_function)) bool Model0_netif_is_multiqueue(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_num_tx_queues > 1;
}

int Model0_netif_set_real_num_tx_queues(struct Model0_net_device *Model0_dev, unsigned int Model0_txq);


int Model0_netif_set_real_num_rx_queues(struct Model0_net_device *Model0_dev, unsigned int Model0_rxq);
static inline __attribute__((no_instrument_function)) unsigned int Model0_get_netdev_rx_queue_index(
  struct Model0_netdev_rx_queue *Model0_queue)
{
 struct Model0_net_device *Model0_dev = Model0_queue->Model0_dev;
 int Model0_index = Model0_queue - Model0_dev->Model0__rx;

 do { if (__builtin_expect(!!(Model0_index >= Model0_dev->Model0_num_rx_queues), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/netdevice.h"), "i" (3199), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);
 return Model0_index;
}



int Model0_netif_get_num_default_rss_queues(void);

enum Model0_skb_free_reason {
 Model0_SKB_REASON_CONSUMED,
 Model0_SKB_REASON_DROPPED,
};

void Model0___dev_kfree_skb_irq(struct Model0_sk_buff *Model0_skb, enum Model0_skb_free_reason Model0_reason);
void Model0___dev_kfree_skb_any(struct Model0_sk_buff *Model0_skb, enum Model0_skb_free_reason Model0_reason);

/*
 * It is not allowed to call kfree_skb() or consume_skb() from hardware
 * interrupt context or with hardware interrupts being disabled.
 * (in_irq() || irqs_disabled())
 *
 * We provide four helpers that can be used in following contexts :
 *
 * dev_kfree_skb_irq(skb) when caller drops a packet from irq context,
 *  replacing kfree_skb(skb)
 *
 * dev_consume_skb_irq(skb) when caller consumes a packet from irq context.
 *  Typically used in place of consume_skb(skb) in TX completion path
 *
 * dev_kfree_skb_any(skb) when caller doesn't know its current irq context,
 *  replacing kfree_skb(skb)
 *
 * dev_consume_skb_any(skb) when caller doesn't know its current irq context,
 *  and consumed a packet. Used in place of consume_skb(skb)
 */
static inline __attribute__((no_instrument_function)) void Model0_dev_kfree_skb_irq(struct Model0_sk_buff *Model0_skb)
{
 Model0___dev_kfree_skb_irq(Model0_skb, Model0_SKB_REASON_DROPPED);
}

static inline __attribute__((no_instrument_function)) void Model0_dev_consume_skb_irq(struct Model0_sk_buff *Model0_skb)
{
 Model0___dev_kfree_skb_irq(Model0_skb, Model0_SKB_REASON_CONSUMED);
}

static inline __attribute__((no_instrument_function)) void Model0_dev_kfree_skb_any(struct Model0_sk_buff *Model0_skb)
{
 Model0___dev_kfree_skb_any(Model0_skb, Model0_SKB_REASON_DROPPED);
}

static inline __attribute__((no_instrument_function)) void Model0_dev_consume_skb_any(struct Model0_sk_buff *Model0_skb)
{
 Model0___dev_kfree_skb_any(Model0_skb, Model0_SKB_REASON_CONSUMED);
}

int Model0_netif_rx(struct Model0_sk_buff *Model0_skb);
int Model0_netif_rx_ni(struct Model0_sk_buff *Model0_skb);
int Model0_netif_receive_skb(struct Model0_sk_buff *Model0_skb);
Model0_gro_result_t Model0_napi_gro_receive(struct Model0_napi_struct *Model0_napi, struct Model0_sk_buff *Model0_skb);
void Model0_napi_gro_flush(struct Model0_napi_struct *Model0_napi, bool Model0_flush_old);
struct Model0_sk_buff *Model0_napi_get_frags(struct Model0_napi_struct *Model0_napi);
Model0_gro_result_t Model0_napi_gro_frags(struct Model0_napi_struct *Model0_napi);
struct Model0_packet_offload *Model0_gro_find_receive_by_type(Model0___be16 Model0_type);
struct Model0_packet_offload *Model0_gro_find_complete_by_type(Model0___be16 Model0_type);

static inline __attribute__((no_instrument_function)) void Model0_napi_free_frags(struct Model0_napi_struct *Model0_napi)
{
 Model0_kfree_skb(Model0_napi->Model0_skb);
 Model0_napi->Model0_skb = ((void *)0);
}

bool Model0_netdev_is_rx_handler_busy(struct Model0_net_device *Model0_dev);
int Model0_netdev_rx_handler_register(struct Model0_net_device *Model0_dev,
          Model0_rx_handler_func_t *Model0_rx_handler,
          void *Model0_rx_handler_data);
void Model0_netdev_rx_handler_unregister(struct Model0_net_device *Model0_dev);

bool Model0_dev_valid_name(const char *Model0_name);
int Model0_dev_ioctl(struct Model0_net *Model0_net, unsigned int Model0_cmd, void *);
int Model0_dev_ethtool(struct Model0_net *Model0_net, struct Model0_ifreq *);
unsigned int Model0_dev_get_flags(const struct Model0_net_device *);
int Model0___dev_change_flags(struct Model0_net_device *, unsigned int Model0_flags);
int Model0_dev_change_flags(struct Model0_net_device *, unsigned int);
void Model0___dev_notify_flags(struct Model0_net_device *, unsigned int Model0_old_flags,
   unsigned int Model0_gchanges);
int Model0_dev_change_name(struct Model0_net_device *, const char *);
int Model0_dev_set_alias(struct Model0_net_device *, const char *, Model0_size_t);
int Model0_dev_change_net_namespace(struct Model0_net_device *, struct Model0_net *, const char *);
int Model0_dev_set_mtu(struct Model0_net_device *, int);
void Model0_dev_set_group(struct Model0_net_device *, int);
int Model0_dev_set_mac_address(struct Model0_net_device *, struct Model0_sockaddr *);
int Model0_dev_change_carrier(struct Model0_net_device *, bool Model0_new_carrier);
int Model0_dev_get_phys_port_id(struct Model0_net_device *Model0_dev,
    struct Model0_netdev_phys_item_id *Model0_ppid);
int Model0_dev_get_phys_port_name(struct Model0_net_device *Model0_dev,
      char *Model0_name, Model0_size_t Model0_len);
int Model0_dev_change_proto_down(struct Model0_net_device *Model0_dev, bool Model0_proto_down);
int Model0_dev_change_xdp_fd(struct Model0_net_device *Model0_dev, int Model0_fd);
struct Model0_sk_buff *Model0_validate_xmit_skb_list(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev);
struct Model0_sk_buff *Model0_dev_hard_start_xmit(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
        struct Model0_netdev_queue *Model0_txq, int *Model0_ret);
int Model0___dev_forward_skb(struct Model0_net_device *Model0_dev, struct Model0_sk_buff *Model0_skb);
int Model0_dev_forward_skb(struct Model0_net_device *Model0_dev, struct Model0_sk_buff *Model0_skb);
bool Model0_is_skb_forwardable(const struct Model0_net_device *Model0_dev,
   const struct Model0_sk_buff *Model0_skb);

void Model0_dev_queue_xmit_nit(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev);

extern int Model0_netdev_budget;

/* Called by rtnetlink.c:rtnl_unlock() */
void Model0_netdev_run_todo(void);

/**
 *	dev_put - release reference to device
 *	@dev: network device
 *
 * Release reference to device to allow it to be freed.
 */
static inline __attribute__((no_instrument_function)) void Model0_dev_put(struct Model0_net_device *Model0_dev)
{
 do { do { const void *Model0___vpp_verify = (typeof((&(*Model0_dev->Model0_pcpu_refcnt)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(*Model0_dev->Model0_pcpu_refcnt)) { case 1: do { typedef typeof((*Model0_dev->Model0_pcpu_refcnt)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) && ((-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) == 1 || (-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) == -1)) ? (int)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_dev->Model0_pcpu_refcnt))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "qi" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "re" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model0_dev->Model0_pcpu_refcnt)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) && ((-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) == 1 || (-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) == -1)) ? (int)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_dev->Model0_pcpu_refcnt))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "qi" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "re" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model0_dev->Model0_pcpu_refcnt)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) && ((-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) == 1 || (-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) == -1)) ? (int)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_dev->Model0_pcpu_refcnt))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "qi" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "re" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model0_dev->Model0_pcpu_refcnt)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) && ((-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) == 1 || (-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) == -1)) ? (int)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_dev->Model0_pcpu_refcnt))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "qi" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "re" ((Model0_pao_T__)(-(typeof(*Model0_dev->Model0_pcpu_refcnt))(1)))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
}

/**
 *	dev_hold - get reference to device
 *	@dev: network device
 *
 * Hold reference to device to keep it from being freed.
 */
static inline __attribute__((no_instrument_function)) void Model0_dev_hold(struct Model0_net_device *Model0_dev)
{
 do { do { const void *Model0___vpp_verify = (typeof((&(*Model0_dev->Model0_pcpu_refcnt)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(*Model0_dev->Model0_pcpu_refcnt)) { case 1: do { typedef typeof((*Model0_dev->Model0_pcpu_refcnt)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_dev->Model0_pcpu_refcnt))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model0_dev->Model0_pcpu_refcnt)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_dev->Model0_pcpu_refcnt))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model0_dev->Model0_pcpu_refcnt)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_dev->Model0_pcpu_refcnt))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model0_dev->Model0_pcpu_refcnt)) Model0_pao_T__; const int Model0_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model0_pao_T__ Model0_pao_tmp__; Model0_pao_tmp__ = (1); (void)Model0_pao_tmp__; } switch (sizeof((*Model0_dev->Model0_pcpu_refcnt))) { case 1: if (Model0_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "qi" ((Model0_pao_T__)(1))); break; case 2: if (Model0_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(1))); break; case 4: if (Model0_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "ri" ((Model0_pao_T__)(1))); break; case 8: if (Model0_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else if (Model0_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model0_dev->Model0_pcpu_refcnt)) : "re" ((Model0_pao_T__)(1))); break; default: Model0___bad_percpu_size(); } } while (0);break; default: Model0___bad_size_call_parameter();break; } } while (0);
}

/* Carrier loss detection, dial on demand. The functions netif_carrier_on
 * and _off may be called from IRQ context, but it is caller
 * who is responsible for serialization of these calls.
 *
 * The name carrier is inappropriate, these functions should really be
 * called netif_lowerlayer_*() because they represent the state of any
 * kind of lower layer not just hardware media.
 */

void Model0_linkwatch_init_dev(struct Model0_net_device *Model0_dev);
void Model0_linkwatch_fire_event(struct Model0_net_device *Model0_dev);
void Model0_linkwatch_forget_dev(struct Model0_net_device *Model0_dev);

/**
 *	netif_carrier_ok - test if carrier present
 *	@dev: network device
 *
 * Check if carrier is present on device
 */
static inline __attribute__((no_instrument_function)) bool Model0_netif_carrier_ok(const struct Model0_net_device *Model0_dev)
{
 return !(__builtin_constant_p((Model0___LINK_STATE_NOCARRIER)) ? Model0_constant_test_bit((Model0___LINK_STATE_NOCARRIER), (&Model0_dev->Model0_state)) : Model0_variable_test_bit((Model0___LINK_STATE_NOCARRIER), (&Model0_dev->Model0_state)));
}

unsigned long Model0_dev_trans_start(struct Model0_net_device *Model0_dev);

void Model0___netdev_watchdog_up(struct Model0_net_device *Model0_dev);

void Model0_netif_carrier_on(struct Model0_net_device *Model0_dev);

void Model0_netif_carrier_off(struct Model0_net_device *Model0_dev);

/**
 *	netif_dormant_on - mark device as dormant.
 *	@dev: network device
 *
 * Mark device as dormant (as per RFC2863).
 *
 * The dormant state indicates that the relevant interface is not
 * actually in a condition to pass packets (i.e., it is not 'up') but is
 * in a "pending" state, waiting for some external event.  For "on-
 * demand" interfaces, this new state identifies the situation where the
 * interface is waiting for events to place it in the up state.
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_dormant_on(struct Model0_net_device *Model0_dev)
{
 if (!Model0_test_and_set_bit(Model0___LINK_STATE_DORMANT, &Model0_dev->Model0_state))
  Model0_linkwatch_fire_event(Model0_dev);
}

/**
 *	netif_dormant_off - set device as not dormant.
 *	@dev: network device
 *
 * Device is not in dormant state.
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_dormant_off(struct Model0_net_device *Model0_dev)
{
 if (Model0_test_and_clear_bit(Model0___LINK_STATE_DORMANT, &Model0_dev->Model0_state))
  Model0_linkwatch_fire_event(Model0_dev);
}

/**
 *	netif_dormant - test if carrier present
 *	@dev: network device
 *
 * Check if carrier is present on device
 */
static inline __attribute__((no_instrument_function)) bool Model0_netif_dormant(const struct Model0_net_device *Model0_dev)
{
 return (__builtin_constant_p((Model0___LINK_STATE_DORMANT)) ? Model0_constant_test_bit((Model0___LINK_STATE_DORMANT), (&Model0_dev->Model0_state)) : Model0_variable_test_bit((Model0___LINK_STATE_DORMANT), (&Model0_dev->Model0_state)));
}


/**
 *	netif_oper_up - test if device is operational
 *	@dev: network device
 *
 * Check if carrier is operational
 */
static inline __attribute__((no_instrument_function)) bool Model0_netif_oper_up(const struct Model0_net_device *Model0_dev)
{
 return (Model0_dev->Model0_operstate == Model0_IF_OPER_UP ||
  Model0_dev->Model0_operstate == Model0_IF_OPER_UNKNOWN /* backward compat */);
}

/**
 *	netif_device_present - is device available or removed
 *	@dev: network device
 *
 * Check if device has not been removed from system.
 */
static inline __attribute__((no_instrument_function)) bool Model0_netif_device_present(struct Model0_net_device *Model0_dev)
{
 return (__builtin_constant_p((Model0___LINK_STATE_PRESENT)) ? Model0_constant_test_bit((Model0___LINK_STATE_PRESENT), (&Model0_dev->Model0_state)) : Model0_variable_test_bit((Model0___LINK_STATE_PRESENT), (&Model0_dev->Model0_state)));
}

void Model0_netif_device_detach(struct Model0_net_device *Model0_dev);

void Model0_netif_device_attach(struct Model0_net_device *Model0_dev);

/*
 * Network interface message level settings
 */

enum {
 Model0_NETIF_MSG_DRV = 0x0001,
 Model0_NETIF_MSG_PROBE = 0x0002,
 Model0_NETIF_MSG_LINK = 0x0004,
 Model0_NETIF_MSG_TIMER = 0x0008,
 Model0_NETIF_MSG_IFDOWN = 0x0010,
 Model0_NETIF_MSG_IFUP = 0x0020,
 Model0_NETIF_MSG_RX_ERR = 0x0040,
 Model0_NETIF_MSG_TX_ERR = 0x0080,
 Model0_NETIF_MSG_TX_QUEUED = 0x0100,
 Model0_NETIF_MSG_INTR = 0x0200,
 Model0_NETIF_MSG_TX_DONE = 0x0400,
 Model0_NETIF_MSG_RX_STATUS = 0x0800,
 Model0_NETIF_MSG_PKTDATA = 0x1000,
 Model0_NETIF_MSG_HW = 0x2000,
 Model0_NETIF_MSG_WOL = 0x4000,
};
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_netif_msg_init(int Model0_debug_value, int Model0_default_msg_enable_bits)
{
 /* use default */
 if (Model0_debug_value < 0 || Model0_debug_value >= (sizeof(Model0_u32) * 8))
  return Model0_default_msg_enable_bits;
 if (Model0_debug_value == 0) /* no output */
  return 0;
 /* set low N bits */
 return (1 << Model0_debug_value) - 1;
}

static inline __attribute__((no_instrument_function)) void Model0___netif_tx_lock(struct Model0_netdev_queue *Model0_txq, int Model0_cpu)
{
 Model0_spin_lock(&Model0_txq->Model0__xmit_lock);
 Model0_txq->Model0_xmit_lock_owner = Model0_cpu;
}

static inline __attribute__((no_instrument_function)) void Model0___netif_tx_lock_bh(struct Model0_netdev_queue *Model0_txq)
{
 Model0_spin_lock_bh(&Model0_txq->Model0__xmit_lock);
 Model0_txq->Model0_xmit_lock_owner = (({ typeof(Model0_cpu_number) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_number)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_cpu_number)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; }));
}

static inline __attribute__((no_instrument_function)) bool Model0___netif_tx_trylock(struct Model0_netdev_queue *Model0_txq)
{
 bool Model0_ok = Model0_spin_trylock(&Model0_txq->Model0__xmit_lock);
 if (__builtin_expect(!!(Model0_ok), 1))
  Model0_txq->Model0_xmit_lock_owner = (({ typeof(Model0_cpu_number) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_number)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_cpu_number)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; }));
 return Model0_ok;
}

static inline __attribute__((no_instrument_function)) void Model0___netif_tx_unlock(struct Model0_netdev_queue *Model0_txq)
{
 Model0_txq->Model0_xmit_lock_owner = -1;
 Model0_spin_unlock(&Model0_txq->Model0__xmit_lock);
}

static inline __attribute__((no_instrument_function)) void Model0___netif_tx_unlock_bh(struct Model0_netdev_queue *Model0_txq)
{
 Model0_txq->Model0_xmit_lock_owner = -1;
 Model0_spin_unlock_bh(&Model0_txq->Model0__xmit_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_txq_trans_update(struct Model0_netdev_queue *Model0_txq)
{
 if (Model0_txq->Model0_xmit_lock_owner != -1)
  Model0_txq->Model0_trans_start = Model0_jiffies;
}

/* legacy drivers only, netdev_start_xmit() sets txq->trans_start */
static inline __attribute__((no_instrument_function)) void Model0_netif_trans_update(struct Model0_net_device *Model0_dev)
{
 struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, 0);

 if (Model0_txq->Model0_trans_start != Model0_jiffies)
  Model0_txq->Model0_trans_start = Model0_jiffies;
}

/**
 *	netif_tx_lock - grab network device transmit lock
 *	@dev: network device
 *
 * Get network device transmit lock
 */
static inline __attribute__((no_instrument_function)) void Model0_netif_tx_lock(struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;
 int Model0_cpu;

 Model0_spin_lock(&Model0_dev->Model0_tx_global_lock);
 Model0_cpu = (({ typeof(Model0_cpu_number) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_number)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_cpu_number)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; }));
 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_i);

  /* We are the only thread of execution doing a
		 * freeze, but we have to grab the _xmit_lock in
		 * order to synchronize with threads which are in
		 * the ->hard_start_xmit() handler and already
		 * checked the frozen bit.
		 */
  Model0___netif_tx_lock(Model0_txq, Model0_cpu);
  Model0_set_bit(Model0___QUEUE_STATE_FROZEN, &Model0_txq->Model0_state);
  Model0___netif_tx_unlock(Model0_txq);
 }
}

static inline __attribute__((no_instrument_function)) void Model0_netif_tx_lock_bh(struct Model0_net_device *Model0_dev)
{
 Model0_local_bh_disable();
 Model0_netif_tx_lock(Model0_dev);
}

static inline __attribute__((no_instrument_function)) void Model0_netif_tx_unlock(struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;

 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_i);

  /* No need to grab the _xmit_lock here.  If the
		 * queue is not stopped for another reason, we
		 * force a schedule.
		 */
  Model0_clear_bit(Model0___QUEUE_STATE_FROZEN, &Model0_txq->Model0_state);
  Model0_netif_schedule_queue(Model0_txq);
 }
 Model0_spin_unlock(&Model0_dev->Model0_tx_global_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_netif_tx_unlock_bh(struct Model0_net_device *Model0_dev)
{
 Model0_netif_tx_unlock(Model0_dev);
 Model0_local_bh_enable();
}
static inline __attribute__((no_instrument_function)) void Model0_netif_tx_disable(struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;
 int Model0_cpu;

 Model0_local_bh_disable();
 Model0_cpu = (({ typeof(Model0_cpu_number) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_number)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_cpu_number)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; }));
 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_i);

  Model0___netif_tx_lock(Model0_txq, Model0_cpu);
  Model0_netif_tx_stop_queue(Model0_txq);
  Model0___netif_tx_unlock(Model0_txq);
 }
 Model0_local_bh_enable();
}

static inline __attribute__((no_instrument_function)) void Model0_netif_addr_lock(struct Model0_net_device *Model0_dev)
{
 Model0_spin_lock(&Model0_dev->Model0_addr_list_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_netif_addr_lock_nested(struct Model0_net_device *Model0_dev)
{
 int Model0_subclass = 1;

 if (Model0_dev->Model0_netdev_ops->Model0_ndo_get_lock_subclass)
  Model0_subclass = Model0_dev->Model0_netdev_ops->Model0_ndo_get_lock_subclass(Model0_dev);

 do { Model0__raw_spin_lock(((void)(Model0_subclass), (Model0_spinlock_check(&Model0_dev->Model0_addr_list_lock)))); } while (0);
}

static inline __attribute__((no_instrument_function)) void Model0_netif_addr_lock_bh(struct Model0_net_device *Model0_dev)
{
 Model0_spin_lock_bh(&Model0_dev->Model0_addr_list_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_netif_addr_unlock(struct Model0_net_device *Model0_dev)
{
 Model0_spin_unlock(&Model0_dev->Model0_addr_list_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_netif_addr_unlock_bh(struct Model0_net_device *Model0_dev)
{
 Model0_spin_unlock_bh(&Model0_dev->Model0_addr_list_lock);
}

/*
 * dev_addrs walker. Should be used only for read access. Call with
 * rcu_read_lock held.
 */



/* These functions live elsewhere (drivers/net/net_init.c, but related) */

void Model0_ether_setup(struct Model0_net_device *Model0_dev);

/* Support for loadable net-drivers */
struct Model0_net_device *Model0_alloc_netdev_mqs(int Model0_sizeof_priv, const char *Model0_name,
        unsigned char Model0_name_assign_type,
        void (*Model0_setup)(struct Model0_net_device *),
        unsigned int Model0_txqs, unsigned int Model0_rxqs);







int Model0_register_netdev(struct Model0_net_device *Model0_dev);
void Model0_unregister_netdev(struct Model0_net_device *Model0_dev);

/* General hardware address lists handling functions */
int Model0___hw_addr_sync(struct Model0_netdev_hw_addr_list *Model0_to_list,
     struct Model0_netdev_hw_addr_list *Model0_from_list, int Model0_addr_len);
void Model0___hw_addr_unsync(struct Model0_netdev_hw_addr_list *Model0_to_list,
        struct Model0_netdev_hw_addr_list *Model0_from_list, int Model0_addr_len);
int Model0___hw_addr_sync_dev(struct Model0_netdev_hw_addr_list *Model0_list,
         struct Model0_net_device *Model0_dev,
         int (*Model0_sync)(struct Model0_net_device *, const unsigned char *),
         int (*Model0_unsync)(struct Model0_net_device *,
         const unsigned char *));
void Model0___hw_addr_unsync_dev(struct Model0_netdev_hw_addr_list *Model0_list,
     struct Model0_net_device *Model0_dev,
     int (*Model0_unsync)(struct Model0_net_device *,
     const unsigned char *));
void Model0___hw_addr_init(struct Model0_netdev_hw_addr_list *Model0_list);

/* Functions used for device addresses handling */
int Model0_dev_addr_add(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr,
   unsigned char Model0_addr_type);
int Model0_dev_addr_del(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr,
   unsigned char Model0_addr_type);
void Model0_dev_addr_flush(struct Model0_net_device *Model0_dev);
int Model0_dev_addr_init(struct Model0_net_device *Model0_dev);

/* Functions used for unicast addresses handling */
int Model0_dev_uc_add(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr);
int Model0_dev_uc_add_excl(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr);
int Model0_dev_uc_del(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr);
int Model0_dev_uc_sync(struct Model0_net_device *Model0_to, struct Model0_net_device *Model0_from);
int Model0_dev_uc_sync_multiple(struct Model0_net_device *Model0_to, struct Model0_net_device *Model0_from);
void Model0_dev_uc_unsync(struct Model0_net_device *Model0_to, struct Model0_net_device *Model0_from);
void Model0_dev_uc_flush(struct Model0_net_device *Model0_dev);
void Model0_dev_uc_init(struct Model0_net_device *Model0_dev);

/**
 *  __dev_uc_sync - Synchonize device's unicast list
 *  @dev:  device to sync
 *  @sync: function to call if address should be added
 *  @unsync: function to call if address should be removed
 *
 *  Add newly added addresses to the interface, and release
 *  addresses that have been deleted.
 */
static inline __attribute__((no_instrument_function)) int Model0___dev_uc_sync(struct Model0_net_device *Model0_dev,
    int (*Model0_sync)(struct Model0_net_device *,
         const unsigned char *),
    int (*Model0_unsync)(struct Model0_net_device *,
           const unsigned char *))
{
 return Model0___hw_addr_sync_dev(&Model0_dev->Model0_uc, Model0_dev, Model0_sync, Model0_unsync);
}

/**
 *  __dev_uc_unsync - Remove synchronized addresses from device
 *  @dev:  device to sync
 *  @unsync: function to call if address should be removed
 *
 *  Remove all addresses that were added to the device by dev_uc_sync().
 */
static inline __attribute__((no_instrument_function)) void Model0___dev_uc_unsync(struct Model0_net_device *Model0_dev,
       int (*Model0_unsync)(struct Model0_net_device *,
       const unsigned char *))
{
 Model0___hw_addr_unsync_dev(&Model0_dev->Model0_uc, Model0_dev, Model0_unsync);
}

/* Functions used for multicast addresses handling */
int Model0_dev_mc_add(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr);
int Model0_dev_mc_add_global(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr);
int Model0_dev_mc_add_excl(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr);
int Model0_dev_mc_del(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr);
int Model0_dev_mc_del_global(struct Model0_net_device *Model0_dev, const unsigned char *Model0_addr);
int Model0_dev_mc_sync(struct Model0_net_device *Model0_to, struct Model0_net_device *Model0_from);
int Model0_dev_mc_sync_multiple(struct Model0_net_device *Model0_to, struct Model0_net_device *Model0_from);
void Model0_dev_mc_unsync(struct Model0_net_device *Model0_to, struct Model0_net_device *Model0_from);
void Model0_dev_mc_flush(struct Model0_net_device *Model0_dev);
void Model0_dev_mc_init(struct Model0_net_device *Model0_dev);

/**
 *  __dev_mc_sync - Synchonize device's multicast list
 *  @dev:  device to sync
 *  @sync: function to call if address should be added
 *  @unsync: function to call if address should be removed
 *
 *  Add newly added addresses to the interface, and release
 *  addresses that have been deleted.
 */
static inline __attribute__((no_instrument_function)) int Model0___dev_mc_sync(struct Model0_net_device *Model0_dev,
    int (*Model0_sync)(struct Model0_net_device *,
         const unsigned char *),
    int (*Model0_unsync)(struct Model0_net_device *,
           const unsigned char *))
{
 return Model0___hw_addr_sync_dev(&Model0_dev->Model0_mc, Model0_dev, Model0_sync, Model0_unsync);
}

/**
 *  __dev_mc_unsync - Remove synchronized addresses from device
 *  @dev:  device to sync
 *  @unsync: function to call if address should be removed
 *
 *  Remove all addresses that were added to the device by dev_mc_sync().
 */
static inline __attribute__((no_instrument_function)) void Model0___dev_mc_unsync(struct Model0_net_device *Model0_dev,
       int (*Model0_unsync)(struct Model0_net_device *,
       const unsigned char *))
{
 Model0___hw_addr_unsync_dev(&Model0_dev->Model0_mc, Model0_dev, Model0_unsync);
}

/* Functions used for secondary unicast and multicast support */
void Model0_dev_set_rx_mode(struct Model0_net_device *Model0_dev);
void Model0___dev_set_rx_mode(struct Model0_net_device *Model0_dev);
int Model0_dev_set_promiscuity(struct Model0_net_device *Model0_dev, int Model0_inc);
int Model0_dev_set_allmulti(struct Model0_net_device *Model0_dev, int Model0_inc);
void Model0_netdev_state_change(struct Model0_net_device *Model0_dev);
void Model0_netdev_notify_peers(struct Model0_net_device *Model0_dev);
void Model0_netdev_features_change(struct Model0_net_device *Model0_dev);
/* Load a device via the kmod */
void Model0_dev_load(struct Model0_net *Model0_net, const char *Model0_name);
struct Model0_rtnl_link_stats64 *Model0_dev_get_stats(struct Model0_net_device *Model0_dev,
     struct Model0_rtnl_link_stats64 *Model0_storage);
void Model0_netdev_stats_to_stats64(struct Model0_rtnl_link_stats64 *Model0_stats64,
        const struct Model0_net_device_stats *Model0_netdev_stats);

extern int Model0_netdev_max_backlog;
extern int Model0_netdev_tstamp_prequeue;
extern int Model0_weight_p;

bool Model0_netdev_has_upper_dev(struct Model0_net_device *Model0_dev, struct Model0_net_device *Model0_upper_dev);
struct Model0_net_device *Model0_netdev_upper_get_next_dev_rcu(struct Model0_net_device *Model0_dev,
           struct Model0_list_head **Model0_iter);
struct Model0_net_device *Model0_netdev_all_upper_get_next_dev_rcu(struct Model0_net_device *Model0_dev,
           struct Model0_list_head **Model0_iter);

/* iterate through upper list, must be called under RCU read lock */






/* iterate through upper list, must be called under RCU read lock */






void *Model0_netdev_lower_get_next_private(struct Model0_net_device *Model0_dev,
        struct Model0_list_head **Model0_iter);
void *Model0_netdev_lower_get_next_private_rcu(struct Model0_net_device *Model0_dev,
     struct Model0_list_head **Model0_iter);
void *Model0_netdev_lower_get_next(struct Model0_net_device *Model0_dev,
    struct Model0_list_head **Model0_iter);







struct Model0_net_device *Model0_netdev_all_lower_get_next(struct Model0_net_device *Model0_dev,
          struct Model0_list_head **Model0_iter);
struct Model0_net_device *Model0_netdev_all_lower_get_next_rcu(struct Model0_net_device *Model0_dev,
       struct Model0_list_head **Model0_iter);
void *Model0_netdev_adjacent_get_private(struct Model0_list_head *Model0_adj_list);
void *Model0_netdev_lower_get_first_private_rcu(struct Model0_net_device *Model0_dev);
struct Model0_net_device *Model0_netdev_master_upper_dev_get(struct Model0_net_device *Model0_dev);
struct Model0_net_device *Model0_netdev_master_upper_dev_get_rcu(struct Model0_net_device *Model0_dev);
int Model0_netdev_upper_dev_link(struct Model0_net_device *Model0_dev, struct Model0_net_device *Model0_upper_dev);
int Model0_netdev_master_upper_dev_link(struct Model0_net_device *Model0_dev,
     struct Model0_net_device *Model0_upper_dev,
     void *Model0_upper_priv, void *Model0_upper_info);
void Model0_netdev_upper_dev_unlink(struct Model0_net_device *Model0_dev,
        struct Model0_net_device *Model0_upper_dev);
void Model0_netdev_adjacent_rename_links(struct Model0_net_device *Model0_dev, char *Model0_oldname);
void *Model0_netdev_lower_dev_get_private(struct Model0_net_device *Model0_dev,
       struct Model0_net_device *Model0_lower_dev);
void Model0_netdev_lower_state_changed(struct Model0_net_device *Model0_lower_dev,
    void *Model0_lower_state_info);
int Model0_netdev_default_l2upper_neigh_construct(struct Model0_net_device *Model0_dev,
        struct Model0_neighbour *Model0_n);
void Model0_netdev_default_l2upper_neigh_destroy(struct Model0_net_device *Model0_dev,
       struct Model0_neighbour *Model0_n);

/* RSS keys are 40 or 52 bytes long */

extern Model0_u8 Model0_netdev_rss_key[52] __attribute__((__section__(".data..read_mostly")));
void Model0_netdev_rss_key_fill(void *Model0_buffer, Model0_size_t Model0_len);

int Model0_dev_get_nest_level(struct Model0_net_device *Model0_dev);
int Model0_skb_checksum_help(struct Model0_sk_buff *Model0_skb);
struct Model0_sk_buff *Model0___skb_gso_segment(struct Model0_sk_buff *Model0_skb,
      Model0_netdev_features_t Model0_features, bool Model0_tx_path);
struct Model0_sk_buff *Model0_skb_mac_gso_segment(struct Model0_sk_buff *Model0_skb,
        Model0_netdev_features_t Model0_features);

struct Model0_netdev_bonding_info {
 Model0_ifslave Model0_slave;
 Model0_ifbond Model0_master;
};

struct Model0_netdev_notifier_bonding_info {
 struct Model0_netdev_notifier_info Model0_info; /* must be first */
 struct Model0_netdev_bonding_info Model0_bonding_info;
};

void Model0_netdev_bonding_info_change(struct Model0_net_device *Model0_dev,
    struct Model0_netdev_bonding_info *Model0_bonding_info);

static inline __attribute__((no_instrument_function))
struct Model0_sk_buff *Model0_skb_gso_segment(struct Model0_sk_buff *Model0_skb, Model0_netdev_features_t Model0_features)
{
 return Model0___skb_gso_segment(Model0_skb, Model0_features, true);
}
Model0___be16 Model0_skb_network_protocol(struct Model0_sk_buff *Model0_skb, int *Model0_depth);

static inline __attribute__((no_instrument_function)) bool Model0_can_checksum_protocol(Model0_netdev_features_t Model0_features,
      Model0___be16 Model0_protocol)
{
 if (Model0_protocol == (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x8906))) ? ((Model0___u16)( (((Model0___u16)((0x8906)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x8906)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x8906)))))
  return !!(Model0_features & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_FCOE_CRC_BIT)));

 /* Assume this is an IP checksum (not SCTP CRC) */

 if (Model0_features & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_HW_CSUM_BIT))) {
  /* Can checksum everything */
  return true;
 }

 switch (Model0_protocol) {
 case (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x0800))) ? ((Model0___u16)( (((Model0___u16)((0x0800)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x0800)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x0800)))):
  return !!(Model0_features & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_IP_CSUM_BIT)));
 case (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x86DD))) ? ((Model0___u16)( (((Model0___u16)((0x86DD)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x86DD)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x86DD)))):
  return !!(Model0_features & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_IPV6_CSUM_BIT)));
 default:
  return false;
 }
}

/* Map an ethertype into IP protocol if possible */
static inline __attribute__((no_instrument_function)) int Model0_eproto_to_ipproto(int Model0_eproto)
{
 switch (Model0_eproto) {
 case (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x0800))) ? ((Model0___u16)( (((Model0___u16)((0x0800)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x0800)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x0800)))):
  return Model0_IPPROTO_IP;
 case (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x86DD))) ? ((Model0___u16)( (((Model0___u16)((0x86DD)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x86DD)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x86DD)))):
  return Model0_IPPROTO_IPV6;
 default:
  return -1;
 }
}


void Model0_netdev_rx_csum_fault(struct Model0_net_device *Model0_dev);





/* rx skb timestamps */
void Model0_net_enable_timestamp(void);
void Model0_net_disable_timestamp(void);


int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model0_dev_proc_init(void);




static inline __attribute__((no_instrument_function)) Model0_netdev_tx_t Model0___netdev_start_xmit(const struct Model0_net_device_ops *Model0_ops,
           struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
           bool Model0_more)
{
 Model0_skb->Model0_xmit_more = Model0_more ? 1 : 0;
 return Model0_ops->Model0_ndo_start_xmit(Model0_skb, Model0_dev);
}

static inline __attribute__((no_instrument_function)) Model0_netdev_tx_t Model0_netdev_start_xmit(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
         struct Model0_netdev_queue *Model0_txq, bool Model0_more)
{
 const struct Model0_net_device_ops *Model0_ops = Model0_dev->Model0_netdev_ops;
 int Model0_rc;

 Model0_rc = Model0___netdev_start_xmit(Model0_ops, Model0_skb, Model0_dev, Model0_more);
 if (Model0_rc == Model0_NETDEV_TX_OK)
  Model0_txq_trans_update(Model0_txq);

 return Model0_rc;
}

int Model0_netdev_class_create_file_ns(struct Model0_class_attribute *Model0_class_attr,
    const void *Model0_ns);
void Model0_netdev_class_remove_file_ns(struct Model0_class_attribute *Model0_class_attr,
     const void *Model0_ns);

static inline __attribute__((no_instrument_function)) int Model0_netdev_class_create_file(struct Model0_class_attribute *Model0_class_attr)
{
 return Model0_netdev_class_create_file_ns(Model0_class_attr, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model0_netdev_class_remove_file(struct Model0_class_attribute *Model0_class_attr)
{
 Model0_netdev_class_remove_file_ns(Model0_class_attr, ((void *)0));
}

extern struct Model0_kobj_ns_type_operations Model0_net_ns_type_operations;

const char *Model0_netdev_drivername(const struct Model0_net_device *Model0_dev);

void Model0_linkwatch_run_queue(void);

static inline __attribute__((no_instrument_function)) Model0_netdev_features_t Model0_netdev_intersect_features(Model0_netdev_features_t Model0_f1,
         Model0_netdev_features_t Model0_f2)
{
 if ((Model0_f1 ^ Model0_f2) & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_HW_CSUM_BIT))) {
  if (Model0_f1 & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_HW_CSUM_BIT)))
   Model0_f1 |= (((Model0_netdev_features_t)1 << (Model0_NETIF_F_IP_CSUM_BIT))|((Model0_netdev_features_t)1 << (Model0_NETIF_F_IPV6_CSUM_BIT)));
  else
   Model0_f2 |= (((Model0_netdev_features_t)1 << (Model0_NETIF_F_IP_CSUM_BIT))|((Model0_netdev_features_t)1 << (Model0_NETIF_F_IPV6_CSUM_BIT)));
 }

 return Model0_f1 & Model0_f2;
}

static inline __attribute__((no_instrument_function)) Model0_netdev_features_t Model0_netdev_get_wanted_features(
 struct Model0_net_device *Model0_dev)
{
 return (Model0_dev->Model0_features & ~Model0_dev->Model0_hw_features) | Model0_dev->Model0_wanted_features;
}
Model0_netdev_features_t Model0_netdev_increment_features(Model0_netdev_features_t Model0_all,
 Model0_netdev_features_t Model0_one, Model0_netdev_features_t Model0_mask);

/* Allow TSO being used on stacked device :
 * Performing the GSO segmentation before last device
 * is a performance improvement.
 */
static inline __attribute__((no_instrument_function)) Model0_netdev_features_t Model0_netdev_add_tso_features(Model0_netdev_features_t Model0_features,
       Model0_netdev_features_t Model0_mask)
{
 return Model0_netdev_increment_features(Model0_features, (((Model0_netdev_features_t)1 << (Model0_NETIF_F_TSO_BIT)) | ((Model0_netdev_features_t)1 << (Model0_NETIF_F_TSO6_BIT)) | ((Model0_netdev_features_t)1 << (Model0_NETIF_F_TSO_ECN_BIT)) | ((Model0_netdev_features_t)1 << (Model0_NETIF_F_TSO_MANGLEID_BIT))), Model0_mask);
}

int Model0___netdev_update_features(struct Model0_net_device *Model0_dev);
void Model0_netdev_update_features(struct Model0_net_device *Model0_dev);
void Model0_netdev_change_features(struct Model0_net_device *Model0_dev);

void Model0_netif_stacked_transfer_operstate(const struct Model0_net_device *Model0_rootdev,
     struct Model0_net_device *Model0_dev);

Model0_netdev_features_t Model0_passthru_features_check(struct Model0_sk_buff *Model0_skb,
       struct Model0_net_device *Model0_dev,
       Model0_netdev_features_t Model0_features);
Model0_netdev_features_t Model0_netif_skb_features(struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) bool Model0_net_gso_ok(Model0_netdev_features_t Model0_features, int Model0_gso_type)
{
 Model0_netdev_features_t Model0_feature = (Model0_netdev_features_t)Model0_gso_type << Model0_NETIF_F_GSO_SHIFT;

 /* check flags correspondence */
 do { bool Model0___cond = !(!(Model0_SKB_GSO_TCPV4 != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_TSO_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4065(void) ; if (Model0___cond) Model0___compiletime_assert_4065(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_UDP != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_UFO_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4066(void) ; if (Model0___cond) Model0___compiletime_assert_4066(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_DODGY != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_ROBUST_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4067(void) ; if (Model0___cond) Model0___compiletime_assert_4067(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_TCP_ECN != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_TSO_ECN_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4068(void) ; if (Model0___cond) Model0___compiletime_assert_4068(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_TCP_FIXEDID != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_TSO_MANGLEID_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4069(void) ; if (Model0___cond) Model0___compiletime_assert_4069(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_TCPV6 != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_TSO6_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4070(void) ; if (Model0___cond) Model0___compiletime_assert_4070(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_FCOE != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_FSO_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4071(void) ; if (Model0___cond) Model0___compiletime_assert_4071(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_GRE != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_GRE_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4072(void) ; if (Model0___cond) Model0___compiletime_assert_4072(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_GRE_CSUM != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_GRE_CSUM_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4073(void) ; if (Model0___cond) Model0___compiletime_assert_4073(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_IPXIP4 != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_IPXIP4_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4074(void) ; if (Model0___cond) Model0___compiletime_assert_4074(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_IPXIP6 != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_IPXIP6_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4075(void) ; if (Model0___cond) Model0___compiletime_assert_4075(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_UDP_TUNNEL != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_UDP_TUNNEL_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4076(void) ; if (Model0___cond) Model0___compiletime_assert_4076(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_UDP_TUNNEL_CSUM != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_UDP_TUNNEL_CSUM_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4077(void) ; if (Model0___cond) Model0___compiletime_assert_4077(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_PARTIAL != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_PARTIAL_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4078(void) ; if (Model0___cond) Model0___compiletime_assert_4078(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_TUNNEL_REMCSUM != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_TUNNEL_REMCSUM_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4079(void) ; if (Model0___cond) Model0___compiletime_assert_4079(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(Model0_SKB_GSO_SCTP != (((Model0_netdev_features_t)1 << (Model0_NETIF_F_GSO_SCTP_BIT)) >> Model0_NETIF_F_GSO_SHIFT))); extern void Model0___compiletime_assert_4080(void) ; if (Model0___cond) Model0___compiletime_assert_4080(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);

 return (Model0_features & Model0_feature) == Model0_feature;
}

static inline __attribute__((no_instrument_function)) bool Model0_skb_gso_ok(struct Model0_sk_buff *Model0_skb, Model0_netdev_features_t Model0_features)
{
 return Model0_net_gso_ok(Model0_features, ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_gso_type) &&
        (!Model0_skb_has_frag_list(Model0_skb) || (Model0_features & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_FRAGLIST_BIT))));
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_needs_gso(struct Model0_sk_buff *Model0_skb,
       Model0_netdev_features_t Model0_features)
{
 return Model0_skb_is_gso(Model0_skb) && (!Model0_skb_gso_ok(Model0_skb, Model0_features) ||
  __builtin_expect(!!((Model0_skb->Model0_ip_summed != 3) && (Model0_skb->Model0_ip_summed != 1)), 0));

}

static inline __attribute__((no_instrument_function)) void Model0_netif_set_gso_max_size(struct Model0_net_device *Model0_dev,
       unsigned int Model0_size)
{
 Model0_dev->Model0_gso_max_size = Model0_size;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_gso_error_unwind(struct Model0_sk_buff *Model0_skb, Model0___be16 Model0_protocol,
     int Model0_pulled_hlen, Model0_u16 Model0_mac_offset,
     int Model0_mac_len)
{
 Model0_skb->Model0_protocol = Model0_protocol;
 Model0_skb->Model0_encapsulation = 1;
 Model0_skb_push(Model0_skb, Model0_pulled_hlen);
 Model0_skb_reset_transport_header(Model0_skb);
 Model0_skb->Model0_mac_header = Model0_mac_offset;
 Model0_skb->Model0_network_header = Model0_skb->Model0_mac_header + Model0_mac_len;
 Model0_skb->Model0_mac_len = Model0_mac_len;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_macsec(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_MACSEC;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_macvlan(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_MACVLAN;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_macvlan_port(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_MACVLAN_PORT;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_ipvlan(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_IPVLAN_SLAVE;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_ipvlan_port(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_IPVLAN_MASTER;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_bond_master(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_flags & Model0_IFF_MASTER && Model0_dev->Model0_priv_flags & Model0_IFF_BONDING;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_bond_slave(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_flags & Model0_IFF_SLAVE && Model0_dev->Model0_priv_flags & Model0_IFF_BONDING;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_supports_nofcs(struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_SUPP_NOFCS;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_l3_master(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_L3MDEV_MASTER;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_l3_slave(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_L3MDEV_SLAVE;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_bridge_master(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_EBRIDGE;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_bridge_port(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_BRIDGE_PORT;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_ovs_master(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_OPENVSWITCH;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_team_master(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_TEAM;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_team_port(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_TEAM_PORT;
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_lag_master(const struct Model0_net_device *Model0_dev)
{
 return Model0_netif_is_bond_master(Model0_dev) || Model0_netif_is_team_master(Model0_dev);
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_lag_port(const struct Model0_net_device *Model0_dev)
{
 return Model0_netif_is_bond_slave(Model0_dev) || Model0_netif_is_team_port(Model0_dev);
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_is_rxfh_configured(const struct Model0_net_device *Model0_dev)
{
 return Model0_dev->Model0_priv_flags & Model0_IFF_RXFH_CONFIGURED;
}

/* This device needs to keep skb dst for qdisc enqueue or ndo_start_xmit() */
static inline __attribute__((no_instrument_function)) void Model0_netif_keep_dst(struct Model0_net_device *Model0_dev)
{
 Model0_dev->Model0_priv_flags &= ~(Model0_IFF_XMIT_DST_RELEASE | Model0_IFF_XMIT_DST_RELEASE_PERM);
}

/* return true if dev can't cope with mtu frames that need vlan tag insertion */
static inline __attribute__((no_instrument_function)) bool Model0_netif_reduces_vlan_mtu(struct Model0_net_device *Model0_dev)
{
 /* TODO: reserve and use an additional IFF bit, if we get more users */
 return Model0_dev->Model0_priv_flags & Model0_IFF_MACSEC;
}

extern struct Model0_pernet_operations Model0_loopback_net_ops;

/* Logging, debugging and troubleshooting/diagnostic helpers. */

/* netdev_printk helpers, similar to dev_printk */

static inline __attribute__((no_instrument_function)) const char *Model0_netdev_name(const struct Model0_net_device *Model0_dev)
{
 if (!Model0_dev->Model0_name[0] || Model0_strchr(Model0_dev->Model0_name, '%'))
  return "(unnamed net_device)";
 return Model0_dev->Model0_name;
}

static inline __attribute__((no_instrument_function)) const char *Model0_netdev_reg_state(const struct Model0_net_device *Model0_dev)
{
 switch (Model0_dev->Model0_reg_state) {
 case Model0_NETREG_UNINITIALIZED: return " (uninitialized)";
 case Model0_NETREG_REGISTERED: return "";
 case Model0_NETREG_UNREGISTERING: return " (unregistering)";
 case Model0_NETREG_UNREGISTERED: return " (unregistered)";
 case Model0_NETREG_RELEASED: return " (released)";
 case Model0_NETREG_DUMMY: return " (dummy)";
 }

 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(1); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_fmt("./include/linux/netdevice.h", 4245, "%s: unknown reg_state %d\n", Model0_dev->Model0_name, Model0_dev->Model0_reg_state); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });
 return " (unknown)";
}

__attribute__((format(printf, 3, 4)))
void Model0_netdev_printk(const char *Model0_level, const struct Model0_net_device *Model0_dev,
     const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model0_netdev_emerg(const struct Model0_net_device *Model0_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model0_netdev_alert(const struct Model0_net_device *Model0_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model0_netdev_crit(const struct Model0_net_device *Model0_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model0_netdev_err(const struct Model0_net_device *Model0_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model0_netdev_warn(const struct Model0_net_device *Model0_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model0_netdev_notice(const struct Model0_net_device *Model0_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model0_netdev_info(const struct Model0_net_device *Model0_dev, const char *format, ...);
/*
 * netdev_WARN() acts like dev_printk(), but with the key difference
 * of using a WARN/WARN_ON to get the message out, including the
 * file/line information and a backtrace.
 */




/* netif printk helpers, similar to netdev_printk */
/*
 *	The list of packet types we will receive (as opposed to discard)
 *	and the routines to invoke.
 *
 *	Why 16. Because with 16 the only overlap we get on a hash of the
 *	low nibble of the protocol value is RARP/SNAP/X.25.
 *
 *      NOTE:  That is no longer true with the addition of VLAN tags.  Not
 *             sure which should go first, but I bet it won't make much
 *             difference if we are running VLANs.  The good news is that
 *             this protocol won't be in the list unless compiled in, so
 *             the average user (w/out VLANs) will not be adversely affected.
 *             --BLG
 *
 *		0800	IP
 *		8100    802.1Q VLAN
 *		0001	802.3
 *		0002	AX.25
 *		0004	802.2
 *		8035	RARP
 *		0005	SNAP
 *		0805	X.25
 *		0806	ARP
 *		8137	IPX
 *		0009	Localtalk
 *		86DD	IPv6
 */



















struct Model0_ifaddrmsg {
 __u8 Model0_ifa_family;
 __u8 Model0_ifa_prefixlen; /* The prefix length		*/
 __u8 Model0_ifa_flags; /* Flags			*/
 __u8 Model0_ifa_scope; /* Address scope		*/
 __u32 Model0_ifa_index; /* Link index			*/
};

/*
 * Important comment:
 * IFA_ADDRESS is prefix address, rather than local interface address.
 * It makes no difference for normally configured broadcast interfaces,
 * but for point-to-point IFA_ADDRESS is DESTINATION address,
 * local address is supplied in IFA_LOCAL attribute.
 *
 * IFA_FLAGS is a u32 attribute that extends the u8 field ifa_flags.
 * If present, the value from struct ifaddrmsg will be ignored.
 */
enum {
 Model0_IFA_UNSPEC,
 Model0_IFA_ADDRESS,
 Model0_IFA_LOCAL,
 Model0_IFA_LABEL,
 Model0_IFA_BROADCAST,
 Model0_IFA_ANYCAST,
 Model0_IFA_CACHEINFO,
 Model0_IFA_MULTICAST,
 Model0_IFA_FLAGS,
 Model0___IFA_MAX,
};



/* ifa_flags */
struct Model0_ifa_cacheinfo {
 __u32 Model0_ifa_prefered;
 __u32 Model0_ifa_valid;
 __u32 Model0_cstamp; /* created timestamp, hundredths of seconds */
 __u32 Model0_tstamp; /* updated timestamp, hundredths of seconds */
};

/* backwards compatibility for userspace */


/* rtnetlink families. Values up to 127 are reserved for real address
 * families, values above 128 may be used arbitrarily.
 */




/****
 *		Routing/neighbour discovery messages.
 ****/

/* Types of messages */

enum {
 Model0_RTM_BASE = 16,


 Model0_RTM_NEWLINK = 16,

 Model0_RTM_DELLINK,

 Model0_RTM_GETLINK,

 Model0_RTM_SETLINK,


 Model0_RTM_NEWADDR = 20,

 Model0_RTM_DELADDR,

 Model0_RTM_GETADDR,


 Model0_RTM_NEWROUTE = 24,

 Model0_RTM_DELROUTE,

 Model0_RTM_GETROUTE,


 Model0_RTM_NEWNEIGH = 28,

 Model0_RTM_DELNEIGH,

 Model0_RTM_GETNEIGH,


 Model0_RTM_NEWRULE = 32,

 Model0_RTM_DELRULE,

 Model0_RTM_GETRULE,


 Model0_RTM_NEWQDISC = 36,

 Model0_RTM_DELQDISC,

 Model0_RTM_GETQDISC,


 Model0_RTM_NEWTCLASS = 40,

 Model0_RTM_DELTCLASS,

 Model0_RTM_GETTCLASS,


 Model0_RTM_NEWTFILTER = 44,

 Model0_RTM_DELTFILTER,

 Model0_RTM_GETTFILTER,


 Model0_RTM_NEWACTION = 48,

 Model0_RTM_DELACTION,

 Model0_RTM_GETACTION,


 Model0_RTM_NEWPREFIX = 52,


 Model0_RTM_GETMULTICAST = 58,


 Model0_RTM_GETANYCAST = 62,


 Model0_RTM_NEWNEIGHTBL = 64,

 Model0_RTM_GETNEIGHTBL = 66,

 Model0_RTM_SETNEIGHTBL,


 Model0_RTM_NEWNDUSEROPT = 68,


 Model0_RTM_NEWADDRLABEL = 72,

 Model0_RTM_DELADDRLABEL,

 Model0_RTM_GETADDRLABEL,


 Model0_RTM_GETDCB = 78,

 Model0_RTM_SETDCB,


 Model0_RTM_NEWNETCONF = 80,

 Model0_RTM_GETNETCONF = 82,


 Model0_RTM_NEWMDB = 84,

 Model0_RTM_DELMDB = 85,

 Model0_RTM_GETMDB = 86,


 Model0_RTM_NEWNSID = 88,

 Model0_RTM_DELNSID = 89,

 Model0_RTM_GETNSID = 90,


 Model0_RTM_NEWSTATS = 92,

 Model0_RTM_GETSTATS = 94,


 Model0___RTM_MAX,

};





/* 
   Generic structure for encapsulation of optional route information.
   It is reminiscent of sockaddr, but with sa_family replaced
   with attribute type.
 */

struct Model0_rtattr {
 unsigned short Model0_rta_len;
 unsigned short Model0_rta_type;
};

/* Macros to handle rtattributes */
/******************************************************************************
 *		Definitions used in routing table administration.
 ****/

struct Model0_rtmsg {
 unsigned char Model0_rtm_family;
 unsigned char Model0_rtm_dst_len;
 unsigned char Model0_rtm_src_len;
 unsigned char Model0_rtm_tos;

 unsigned char Model0_rtm_table; /* Routing table id */
 unsigned char Model0_rtm_protocol; /* Routing protocol; see below	*/
 unsigned char Model0_rtm_scope; /* See below */
 unsigned char Model0_rtm_type; /* See below	*/

 unsigned Model0_rtm_flags;
};

/* rtm_type */

enum {
 Model0_RTN_UNSPEC,
 Model0_RTN_UNICAST, /* Gateway or direct route	*/
 Model0_RTN_LOCAL, /* Accept locally		*/
 Model0_RTN_BROADCAST, /* Accept locally as broadcast,
				   send as broadcast */
 Model0_RTN_ANYCAST, /* Accept locally as broadcast,
				   but send as unicast */
 Model0_RTN_MULTICAST, /* Multicast route		*/
 Model0_RTN_BLACKHOLE, /* Drop				*/
 Model0_RTN_UNREACHABLE, /* Destination is unreachable   */
 Model0_RTN_PROHIBIT, /* Administratively prohibited	*/
 Model0_RTN_THROW, /* Not in this table		*/
 Model0_RTN_NAT, /* Translate this address	*/
 Model0_RTN_XRESOLVE, /* Use external resolver	*/
 Model0___RTN_MAX
};




/* rtm_protocol */
/* Values of protocol >= RTPROT_STATIC are not interpreted by kernel;
   they are just passed from user and back as is.
   It will be used by hypothetical multiple routing daemons.
   Note that protocol values should be standardized in order to
   avoid conflicts.
 */
/* rtm_scope

   Really it is not scope, but sort of distance to the destination.
   NOWHERE are reserved for not existing destinations, HOST is our
   local addresses, LINK are destinations, located on directly attached
   link and UNIVERSE is everywhere in the Universe.

   Intermediate values are also possible f.e. interior routes
   could be assigned a value between UNIVERSE and LINK.
*/

enum Model0_rt_scope_t {
 Model0_RT_SCOPE_UNIVERSE=0,
/* User defined values  */
 Model0_RT_SCOPE_SITE=200,
 Model0_RT_SCOPE_LINK=253,
 Model0_RT_SCOPE_HOST=254,
 Model0_RT_SCOPE_NOWHERE=255
};

/* rtm_flags */







/* Reserved table identifiers */

enum Model0_rt_class_t {
 Model0_RT_TABLE_UNSPEC=0,
/* User defined values */
 Model0_RT_TABLE_COMPAT=252,
 Model0_RT_TABLE_DEFAULT=253,
 Model0_RT_TABLE_MAIN=254,
 Model0_RT_TABLE_LOCAL=255,
 Model0_RT_TABLE_MAX=0xFFFFFFFF
};


/* Routing message attributes */

enum Model0_rtattr_type_t {
 Model0_RTA_UNSPEC,
 Model0_RTA_DST,
 Model0_RTA_SRC,
 Model0_RTA_IIF,
 Model0_RTA_OIF,
 Model0_RTA_GATEWAY,
 Model0_RTA_PRIORITY,
 Model0_RTA_PREFSRC,
 Model0_RTA_METRICS,
 Model0_RTA_MULTIPATH,
 Model0_RTA_PROTOINFO, /* no longer used */
 Model0_RTA_FLOW,
 Model0_RTA_CACHEINFO,
 Model0_RTA_SESSION, /* no longer used */
 Model0_RTA_MP_ALGO, /* no longer used */
 Model0_RTA_TABLE,
 Model0_RTA_MARK,
 Model0_RTA_MFC_STATS,
 Model0_RTA_VIA,
 Model0_RTA_NEWDST,
 Model0_RTA_PREF,
 Model0_RTA_ENCAP_TYPE,
 Model0_RTA_ENCAP,
 Model0_RTA_EXPIRES,
 Model0_RTA_PAD,
 Model0___RTA_MAX
};






/* RTM_MULTIPATH --- array of struct rtnexthop.
 *
 * "struct rtnexthop" describes all necessary nexthop information,
 * i.e. parameters of path to a destination via this nexthop.
 *
 * At the moment it is impossible to set different prefsrc, mtu, window
 * and rtt for different paths from multipath.
 */

struct Model0_rtnexthop {
 unsigned short Model0_rtnh_len;
 unsigned char Model0_rtnh_flags;
 unsigned char Model0_rtnh_hops;
 int Model0_rtnh_ifindex;
};

/* rtnh_flags */
/* Macros to handle hexthops */
/* RTA_VIA */
struct Model0_rtvia {
 Model0___kernel_sa_family_t Model0_rtvia_family;
 __u8 Model0_rtvia_addr[0];
};

/* RTM_CACHEINFO */

struct Model0_rta_cacheinfo {
 __u32 Model0_rta_clntref;
 __u32 Model0_rta_lastuse;
 Model0___s32 Model0_rta_expires;
 __u32 Model0_rta_error;
 __u32 Model0_rta_used;


 __u32 Model0_rta_id;
 __u32 Model0_rta_ts;
 __u32 Model0_rta_tsage;
};

/* RTM_METRICS --- array of struct rtattr with types of RTAX_* */

enum {
 Model0_RTAX_UNSPEC,

 Model0_RTAX_LOCK,

 Model0_RTAX_MTU,

 Model0_RTAX_WINDOW,

 Model0_RTAX_RTT,

 Model0_RTAX_RTTVAR,

 Model0_RTAX_SSTHRESH,

 Model0_RTAX_CWND,

 Model0_RTAX_ADVMSS,

 Model0_RTAX_REORDERING,

 Model0_RTAX_HOPLIMIT,

 Model0_RTAX_INITCWND,

 Model0_RTAX_FEATURES,

 Model0_RTAX_RTO_MIN,

 Model0_RTAX_INITRWND,

 Model0_RTAX_QUICKACK,

 Model0_RTAX_CC_ALGO,

 Model0___RTAX_MAX
};
struct Model0_rta_session {
 __u8 Model0_proto;
 __u8 Model0_pad1;
 Model0___u16 Model0_pad2;

 union {
  struct {
   Model0___u16 Model0_sport;
   Model0___u16 Model0_dport;
  } Model0_ports;

  struct {
   __u8 Model0_type;
   __u8 Model0_code;
   Model0___u16 Model0_ident;
  } Model0_icmpt;

  __u32 Model0_spi;
 } Model0_u;
};

struct Model0_rta_mfc_stats {
 __u64 Model0_mfcs_packets;
 __u64 Model0_mfcs_bytes;
 __u64 Model0_mfcs_wrong_if;
};

/****
 *		General form of address family dependent message.
 ****/

struct Model0_rtgenmsg {
 unsigned char Model0_rtgen_family;
};

/*****************************************************************
 *		Link layer specific messages.
 ****/

/* struct ifinfomsg
 * passes link level specific information, not dependent
 * on network protocol.
 */

struct Model0_ifinfomsg {
 unsigned char Model0_ifi_family;
 unsigned char Model0___ifi_pad;
 unsigned short Model0_ifi_type; /* ARPHRD_* */
 int Model0_ifi_index; /* Link index	*/
 unsigned Model0_ifi_flags; /* IFF_* flags	*/
 unsigned Model0_ifi_change; /* IFF_* change mask */
};

/********************************************************************
 *		prefix information 
 ****/

struct Model0_prefixmsg {
 unsigned char Model0_prefix_family;
 unsigned char Model0_prefix_pad1;
 unsigned short Model0_prefix_pad2;
 int Model0_prefix_ifindex;
 unsigned char Model0_prefix_type;
 unsigned char Model0_prefix_len;
 unsigned char Model0_prefix_flags;
 unsigned char Model0_prefix_pad3;
};

enum
{
 Model0_PREFIX_UNSPEC,
 Model0_PREFIX_ADDRESS,
 Model0_PREFIX_CACHEINFO,
 Model0___PREFIX_MAX
};



struct Model0_prefix_cacheinfo {
 __u32 Model0_preferred_time;
 __u32 Model0_valid_time;
};


/*****************************************************************
 *		Traffic control messages.
 ****/

struct Model0_tcmsg {
 unsigned char Model0_tcm_family;
 unsigned char Model0_tcm__pad1;
 unsigned short Model0_tcm__pad2;
 int Model0_tcm_ifindex;
 __u32 Model0_tcm_handle;
 __u32 Model0_tcm_parent;
 __u32 Model0_tcm_info;
};

enum {
 Model0_TCA_UNSPEC,
 Model0_TCA_KIND,
 Model0_TCA_OPTIONS,
 Model0_TCA_STATS,
 Model0_TCA_XSTATS,
 Model0_TCA_RATE,
 Model0_TCA_FCNT,
 Model0_TCA_STATS2,
 Model0_TCA_STAB,
 Model0_TCA_PAD,
 Model0___TCA_MAX
};






/********************************************************************
 *		Neighbor Discovery userland options
 ****/

struct Model0_nduseroptmsg {
 unsigned char Model0_nduseropt_family;
 unsigned char Model0_nduseropt_pad1;
 unsigned short Model0_nduseropt_opts_len; /* Total length of options */
 int Model0_nduseropt_ifindex;
 __u8 Model0_nduseropt_icmp_type;
 __u8 Model0_nduseropt_icmp_code;
 unsigned short Model0_nduseropt_pad2;
 unsigned int Model0_nduseropt_pad3;
 /* Followed by one or more ND options */
};

enum {
 Model0_NDUSEROPT_UNSPEC,
 Model0_NDUSEROPT_SRCADDR,
 Model0___NDUSEROPT_MAX
};
/* RTnetlink multicast groups */
enum Model0_rtnetlink_groups {
 Model0_RTNLGRP_NONE,

 Model0_RTNLGRP_LINK,

 Model0_RTNLGRP_NOTIFY,

 Model0_RTNLGRP_NEIGH,

 Model0_RTNLGRP_TC,

 Model0_RTNLGRP_IPV4_IFADDR,

 Model0_RTNLGRP_IPV4_MROUTE,

 Model0_RTNLGRP_IPV4_ROUTE,

 Model0_RTNLGRP_IPV4_RULE,

 Model0_RTNLGRP_IPV6_IFADDR,

 Model0_RTNLGRP_IPV6_MROUTE,

 Model0_RTNLGRP_IPV6_ROUTE,

 Model0_RTNLGRP_IPV6_IFINFO,

 Model0_RTNLGRP_DECnet_IFADDR,

 Model0_RTNLGRP_NOP2,
 Model0_RTNLGRP_DECnet_ROUTE,

 Model0_RTNLGRP_DECnet_RULE,

 Model0_RTNLGRP_NOP4,
 Model0_RTNLGRP_IPV6_PREFIX,

 Model0_RTNLGRP_IPV6_RULE,

 Model0_RTNLGRP_ND_USEROPT,

 Model0_RTNLGRP_PHONET_IFADDR,

 Model0_RTNLGRP_PHONET_ROUTE,

 Model0_RTNLGRP_DCB,

 Model0_RTNLGRP_IPV4_NETCONF,

 Model0_RTNLGRP_IPV6_NETCONF,

 Model0_RTNLGRP_MDB,

 Model0_RTNLGRP_MPLS_ROUTE,

 Model0_RTNLGRP_NSID,

 Model0___RTNLGRP_MAX
};


/* TC action piece */
struct Model0_tcamsg {
 unsigned char Model0_tca_family;
 unsigned char Model0_tca__pad1;
 unsigned short Model0_tca__pad2;
};





/* New extended info filters for IFLA_EXT_MASK */





/* End of information exported to user level */

extern int Model0_rtnetlink_send(struct Model0_sk_buff *Model0_skb, struct Model0_net *Model0_net, Model0_u32 Model0_pid, Model0_u32 Model0_group, int Model0_echo);
extern int Model0_rtnl_unicast(struct Model0_sk_buff *Model0_skb, struct Model0_net *Model0_net, Model0_u32 Model0_pid);
extern void Model0_rtnl_notify(struct Model0_sk_buff *Model0_skb, struct Model0_net *Model0_net, Model0_u32 Model0_pid,
   Model0_u32 Model0_group, struct Model0_nlmsghdr *Model0_nlh, Model0_gfp_t Model0_flags);
extern void Model0_rtnl_set_sk_err(struct Model0_net *Model0_net, Model0_u32 Model0_group, int error);
extern int Model0_rtnetlink_put_metrics(struct Model0_sk_buff *Model0_skb, Model0_u32 *Model0_metrics);
extern int Model0_rtnl_put_cacheinfo(struct Model0_sk_buff *Model0_skb, struct Model0_dst_entry *Model0_dst,
         Model0_u32 Model0_id, long Model0_expires, Model0_u32 error);

void Model0_rtmsg_ifinfo(int Model0_type, struct Model0_net_device *Model0_dev, unsigned Model0_change, Model0_gfp_t Model0_flags);
struct Model0_sk_buff *Model0_rtmsg_ifinfo_build_skb(int Model0_type, struct Model0_net_device *Model0_dev,
           unsigned Model0_change, Model0_gfp_t Model0_flags);
void Model0_rtmsg_ifinfo_send(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
         Model0_gfp_t Model0_flags);


/* RTNL is used as a global lock for all changes to network configuration  */
extern void Model0_rtnl_lock(void);
extern void Model0_rtnl_unlock(void);
extern int Model0_rtnl_trylock(void);
extern int Model0_rtnl_is_locked(void);

extern Model0_wait_queue_head_t Model0_netdev_unregistering_wq;
extern struct Model0_mutex Model0_net_mutex;




static inline __attribute__((no_instrument_function)) bool Model0_lockdep_rtnl_is_held(void)
{
 return true;
}


/**
 * rcu_dereference_rtnl - rcu_dereference with debug checking
 * @p: The pointer to read, prior to dereferencing
 *
 * Do an rcu_dereference(p), but check caller either holds rcu_read_lock()
 * or RTNL. Note : Please prefer rtnl_dereference() or rcu_dereference()
 */



/**
 * rcu_dereference_bh_rtnl - rcu_dereference_bh with debug checking
 * @p: The pointer to read, prior to dereference
 *
 * Do an rcu_dereference_bh(p), but check caller either holds rcu_read_lock_bh()
 * or RTNL. Note : Please prefer rtnl_dereference() or rcu_dereference_bh()
 */



/**
 * rtnl_dereference - fetch RCU pointer when updates are prevented by RTNL
 * @p: The pointer to read, prior to dereferencing
 *
 * Return the value of the specified RCU-protected pointer, but omit
 * both the smp_read_barrier_depends() and the ACCESS_ONCE(), because
 * caller holds RTNL.
 */



static inline __attribute__((no_instrument_function)) struct Model0_netdev_queue *Model0_dev_ingress_queue(struct Model0_net_device *Model0_dev)
{
 return ({ do { } while (0); ; ((typeof(*(Model0_dev->Model0_ingress_queue)) *)((Model0_dev->Model0_ingress_queue))); });
}

struct Model0_netdev_queue *Model0_dev_ingress_queue_create(struct Model0_net_device *Model0_dev);


void Model0_net_inc_ingress_queue(void);
void Model0_net_dec_ingress_queue(void);







void Model0_rtnetlink_init(void);
void Model0___rtnl_unlock(void);
void Model0_rtnl_kfree_skbs(struct Model0_sk_buff *Model0_head, struct Model0_sk_buff *Model0_tail);
extern int Model0_ndo_dflt_fdb_dump(struct Model0_sk_buff *Model0_skb,
        struct Model0_netlink_callback *Model0_cb,
        struct Model0_net_device *Model0_dev,
        struct Model0_net_device *Model0_filter_dev,
        int Model0_idx);
extern int Model0_ndo_dflt_fdb_add(struct Model0_ndmsg *Model0_ndm,
       struct Model0_nlattr *Model0_tb[],
       struct Model0_net_device *Model0_dev,
       const unsigned char *Model0_addr,
       Model0_u16 Model0_vid,
       Model0_u16 Model0_flags);
extern int Model0_ndo_dflt_fdb_del(struct Model0_ndmsg *Model0_ndm,
       struct Model0_nlattr *Model0_tb[],
       struct Model0_net_device *Model0_dev,
       const unsigned char *Model0_addr,
       Model0_u16 Model0_vid);

extern int Model0_ndo_dflt_bridge_getlink(struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_pid, Model0_u32 Model0_seq,
       struct Model0_net_device *Model0_dev, Model0_u16 Model0_mode,
       Model0_u32 Model0_flags, Model0_u32 Model0_mask, int Model0_nlflags,
       Model0_u32 Model0_filter_mask,
       int (*Model0_vlan_fill)(struct Model0_sk_buff *Model0_skb,
          struct Model0_net_device *Model0_dev,
          Model0_u32 Model0_filter_mask));








/*
 *	Generic neighbour manipulation
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>
 *	Alexey Kuznetsov	<kuznet@ms2.inr.ac.ru>
 *
 * 	Changes:
 *
 *	Harald Welte:		<laforge@gnumonks.org>
 *		- Add neighbour cache statistics like rtstat
 */












/* ========================================================================
 *         Netlink Messages and Attributes Interface (As Seen On TV)
 * ------------------------------------------------------------------------
 *                          Messages Interface
 * ------------------------------------------------------------------------
 *
 * Message Format:
 *    <--- nlmsg_total_size(payload)  --->
 *    <-- nlmsg_msg_size(payload) ->
 *   +----------+- - -+-------------+- - -+-------- - -
 *   | nlmsghdr | Pad |   Payload   | Pad | nlmsghdr
 *   +----------+- - -+-------------+- - -+-------- - -
 *   nlmsg_data(nlh)---^                   ^
 *   nlmsg_next(nlh)-----------------------+
 *
 * Payload Format:
 *    <---------------------- nlmsg_len(nlh) --------------------->
 *    <------ hdrlen ------>       <- nlmsg_attrlen(nlh, hdrlen) ->
 *   +----------------------+- - -+--------------------------------+
 *   |     Family Header    | Pad |           Attributes           |
 *   +----------------------+- - -+--------------------------------+
 *   nlmsg_attrdata(nlh, hdrlen)---^
 *
 * Data Structures:
 *   struct nlmsghdr			netlink message header
 *
 * Message Construction:
 *   nlmsg_new()			create a new netlink message
 *   nlmsg_put()			add a netlink message to an skb
 *   nlmsg_put_answer()			callback based nlmsg_put()
 *   nlmsg_end()			finalize netlink message
 *   nlmsg_get_pos()			return current position in message
 *   nlmsg_trim()			trim part of message
 *   nlmsg_cancel()			cancel message construction
 *   nlmsg_free()			free a netlink message
 *
 * Message Sending:
 *   nlmsg_multicast()			multicast message to several groups
 *   nlmsg_unicast()			unicast a message to a single socket
 *   nlmsg_notify()			send notification message
 *
 * Message Length Calculations:
 *   nlmsg_msg_size(payload)		length of message w/o padding
 *   nlmsg_total_size(payload)		length of message w/ padding
 *   nlmsg_padlen(payload)		length of padding at tail
 *
 * Message Payload Access:
 *   nlmsg_data(nlh)			head of message payload
 *   nlmsg_len(nlh)			length of message payload
 *   nlmsg_attrdata(nlh, hdrlen)	head of attributes data
 *   nlmsg_attrlen(nlh, hdrlen)		length of attributes data
 *
 * Message Parsing:
 *   nlmsg_ok(nlh, remaining)		does nlh fit into remaining bytes?
 *   nlmsg_next(nlh, remaining)		get next netlink message
 *   nlmsg_parse()			parse attributes of a message
 *   nlmsg_find_attr()			find an attribute in a message
 *   nlmsg_for_each_msg()		loop over all messages
 *   nlmsg_validate()			validate netlink message incl. attrs
 *   nlmsg_for_each_attr()		loop over all attributes
 *
 * Misc:
 *   nlmsg_report()			report back to application?
 *
 * ------------------------------------------------------------------------
 *                          Attributes Interface
 * ------------------------------------------------------------------------
 *
 * Attribute Format:
 *    <------- nla_total_size(payload) ------->
 *    <---- nla_attr_size(payload) ----->
 *   +----------+- - -+- - - - - - - - - +- - -+-------- - -
 *   |  Header  | Pad |     Payload      | Pad |  Header
 *   +----------+- - -+- - - - - - - - - +- - -+-------- - -
 *                     <- nla_len(nla) ->      ^
 *   nla_data(nla)----^                        |
 *   nla_next(nla)-----------------------------'
 *
 * Data Structures:
 *   struct nlattr			netlink attribute header
 *
 * Attribute Construction:
 *   nla_reserve(skb, type, len)	reserve room for an attribute
 *   nla_reserve_nohdr(skb, len)	reserve room for an attribute w/o hdr
 *   nla_put(skb, type, len, data)	add attribute to skb
 *   nla_put_nohdr(skb, len, data)	add attribute w/o hdr
 *   nla_append(skb, len, data)		append data to skb
 *
 * Attribute Construction for Basic Types:
 *   nla_put_u8(skb, type, value)	add u8 attribute to skb
 *   nla_put_u16(skb, type, value)	add u16 attribute to skb
 *   nla_put_u32(skb, type, value)	add u32 attribute to skb
 *   nla_put_u64_64bits(skb, type,
 *			value, padattr)	add u64 attribute to skb
 *   nla_put_s8(skb, type, value)	add s8 attribute to skb
 *   nla_put_s16(skb, type, value)	add s16 attribute to skb
 *   nla_put_s32(skb, type, value)	add s32 attribute to skb
 *   nla_put_s64(skb, type, value,
 *               padattr)		add s64 attribute to skb
 *   nla_put_string(skb, type, str)	add string attribute to skb
 *   nla_put_flag(skb, type)		add flag attribute to skb
 *   nla_put_msecs(skb, type, jiffies,
 *                 padattr)		add msecs attribute to skb
 *   nla_put_in_addr(skb, type, addr)	add IPv4 address attribute to skb
 *   nla_put_in6_addr(skb, type, addr)	add IPv6 address attribute to skb
 *
 * Nested Attributes Construction:
 *   nla_nest_start(skb, type)		start a nested attribute
 *   nla_nest_end(skb, nla)		finalize a nested attribute
 *   nla_nest_cancel(skb, nla)		cancel nested attribute construction
 *
 * Attribute Length Calculations:
 *   nla_attr_size(payload)		length of attribute w/o padding
 *   nla_total_size(payload)		length of attribute w/ padding
 *   nla_padlen(payload)		length of padding
 *
 * Attribute Payload Access:
 *   nla_data(nla)			head of attribute payload
 *   nla_len(nla)			length of attribute payload
 *
 * Attribute Payload Access for Basic Types:
 *   nla_get_u8(nla)			get payload for a u8 attribute
 *   nla_get_u16(nla)			get payload for a u16 attribute
 *   nla_get_u32(nla)			get payload for a u32 attribute
 *   nla_get_u64(nla)			get payload for a u64 attribute
 *   nla_get_s8(nla)			get payload for a s8 attribute
 *   nla_get_s16(nla)			get payload for a s16 attribute
 *   nla_get_s32(nla)			get payload for a s32 attribute
 *   nla_get_s64(nla)			get payload for a s64 attribute
 *   nla_get_flag(nla)			return 1 if flag is true
 *   nla_get_msecs(nla)			get payload for a msecs attribute
 *
 * Attribute Misc:
 *   nla_memcpy(dest, nla, count)	copy attribute into memory
 *   nla_memcmp(nla, data, size)	compare attribute with memory area
 *   nla_strlcpy(dst, nla, size)	copy attribute to a sized string
 *   nla_strcmp(nla, str)		compare attribute with string
 *
 * Attribute Parsing:
 *   nla_ok(nla, remaining)		does nla fit into remaining bytes?
 *   nla_next(nla, remaining)		get next netlink attribute
 *   nla_validate()			validate a stream of attributes
 *   nla_validate_nested()		validate a stream of nested attributes
 *   nla_find()				find attribute in stream of attributes
 *   nla_find_nested()			find attribute in nested attributes
 *   nla_parse()			parse and validate stream of attrs
 *   nla_parse_nested()			parse nested attribuets
 *   nla_for_each_attr()		loop over all attributes
 *   nla_for_each_nested()		loop over the nested attributes
 *=========================================================================
 */

 /**
  * Standard attribute types to specify validation policy
  */
enum {
 Model0_NLA_UNSPEC,
 Model0_NLA_U8,
 Model0_NLA_U16,
 Model0_NLA_U32,
 Model0_NLA_U64,
 Model0_NLA_STRING,
 Model0_NLA_FLAG,
 Model0_NLA_MSECS,
 Model0_NLA_NESTED,
 Model0_NLA_NESTED_COMPAT,
 Model0_NLA_NUL_STRING,
 Model0_NLA_BINARY,
 Model0_NLA_S8,
 Model0_NLA_S16,
 Model0_NLA_S32,
 Model0_NLA_S64,
 Model0___NLA_TYPE_MAX,
};



/**
 * struct nla_policy - attribute validation policy
 * @type: Type of attribute or NLA_UNSPEC
 * @len: Type specific length of payload
 *
 * Policies are defined as arrays of this struct, the array must be
 * accessible by attribute type up to the highest identifier to be expected.
 *
 * Meaning of `len' field:
 *    NLA_STRING           Maximum length of string
 *    NLA_NUL_STRING       Maximum length of string (excluding NUL)
 *    NLA_FLAG             Unused
 *    NLA_BINARY           Maximum length of attribute payload
 *    NLA_NESTED           Don't use `len' field -- length verification is
 *                         done by checking len of nested header (or empty)
 *    NLA_NESTED_COMPAT    Minimum length of structure payload
 *    NLA_U8, NLA_U16,
 *    NLA_U32, NLA_U64,
 *    NLA_S8, NLA_S16,
 *    NLA_S32, NLA_S64,
 *    NLA_MSECS            Leaving the length field zero will verify the
 *                         given type fits, using it verifies minimum length
 *                         just like "All other"
 *    All other            Minimum length of attribute payload
 *
 * Example:
 * static const struct nla_policy my_policy[ATTR_MAX+1] = {
 * 	[ATTR_FOO] = { .type = NLA_U16 },
 *	[ATTR_BAR] = { .type = NLA_STRING, .len = BARSIZ },
 *	[ATTR_BAZ] = { .len = sizeof(struct mystruct) },
 * };
 */
struct Model0_nla_policy {
 Model0_u16 Model0_type;
 Model0_u16 Model0_len;
};

/**
 * struct nl_info - netlink source information
 * @nlh: Netlink message header of original request
 * @portid: Netlink PORTID of requesting application
 */
struct Model0_nl_info {
 struct Model0_nlmsghdr *Model0_nlh;
 struct Model0_net *Model0_nl_net;
 Model0_u32 Model0_portid;
};

int Model0_netlink_rcv_skb(struct Model0_sk_buff *Model0_skb,
      int (*Model0_cb)(struct Model0_sk_buff *, struct Model0_nlmsghdr *));
int Model0_nlmsg_notify(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_portid,
   unsigned int Model0_group, int Model0_report, Model0_gfp_t Model0_flags);

int Model0_nla_validate(const struct Model0_nlattr *Model0_head, int Model0_len, int Model0_maxtype,
   const struct Model0_nla_policy *Model0_policy);
int Model0_nla_parse(struct Model0_nlattr **Model0_tb, int Model0_maxtype, const struct Model0_nlattr *Model0_head,
       int Model0_len, const struct Model0_nla_policy *Model0_policy);
int Model0_nla_policy_len(const struct Model0_nla_policy *, int);
struct Model0_nlattr *Model0_nla_find(const struct Model0_nlattr *Model0_head, int Model0_len, int Model0_attrtype);
Model0_size_t Model0_nla_strlcpy(char *Model0_dst, const struct Model0_nlattr *Model0_nla, Model0_size_t Model0_dstsize);
int Model0_nla_memcpy(void *Model0_dest, const struct Model0_nlattr *Model0_src, int Model0_count);
int Model0_nla_memcmp(const struct Model0_nlattr *Model0_nla, const void *Model0_data, Model0_size_t Model0_size);
int Model0_nla_strcmp(const struct Model0_nlattr *Model0_nla, const char *Model0_str);
struct Model0_nlattr *Model0___nla_reserve(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, int Model0_attrlen);
struct Model0_nlattr *Model0___nla_reserve_64bit(struct Model0_sk_buff *Model0_skb, int Model0_attrtype,
       int Model0_attrlen, int Model0_padattr);
void *Model0___nla_reserve_nohdr(struct Model0_sk_buff *Model0_skb, int Model0_attrlen);
struct Model0_nlattr *Model0_nla_reserve(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, int Model0_attrlen);
struct Model0_nlattr *Model0_nla_reserve_64bit(struct Model0_sk_buff *Model0_skb, int Model0_attrtype,
     int Model0_attrlen, int Model0_padattr);
void *Model0_nla_reserve_nohdr(struct Model0_sk_buff *Model0_skb, int Model0_attrlen);
void Model0___nla_put(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, int Model0_attrlen,
        const void *Model0_data);
void Model0___nla_put_64bit(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, int Model0_attrlen,
       const void *Model0_data, int Model0_padattr);
void Model0___nla_put_nohdr(struct Model0_sk_buff *Model0_skb, int Model0_attrlen, const void *Model0_data);
int Model0_nla_put(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, int Model0_attrlen, const void *Model0_data);
int Model0_nla_put_64bit(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, int Model0_attrlen,
    const void *Model0_data, int Model0_padattr);
int Model0_nla_put_nohdr(struct Model0_sk_buff *Model0_skb, int Model0_attrlen, const void *Model0_data);
int Model0_nla_append(struct Model0_sk_buff *Model0_skb, int Model0_attrlen, const void *Model0_data);

/**************************************************************************
 * Netlink Messages
 **************************************************************************/

/**
 * nlmsg_msg_size - length of netlink message not including padding
 * @payload: length of message payload
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_msg_size(int Model0_payload)
{
 return ((int) ( ((sizeof(struct Model0_nlmsghdr))+4U -1) & ~(4U -1) )) + Model0_payload;
}

/**
 * nlmsg_total_size - length of netlink message including padding
 * @payload: length of message payload
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_total_size(int Model0_payload)
{
 return ( ((Model0_nlmsg_msg_size(Model0_payload))+4U -1) & ~(4U -1) );
}

/**
 * nlmsg_padlen - length of padding at the message's tail
 * @payload: length of message payload
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_padlen(int Model0_payload)
{
 return Model0_nlmsg_total_size(Model0_payload) - Model0_nlmsg_msg_size(Model0_payload);
}

/**
 * nlmsg_data - head of message payload
 * @nlh: netlink message header
 */
static inline __attribute__((no_instrument_function)) void *Model0_nlmsg_data(const struct Model0_nlmsghdr *Model0_nlh)
{
 return (unsigned char *) Model0_nlh + ((int) ( ((sizeof(struct Model0_nlmsghdr))+4U -1) & ~(4U -1) ));
}

/**
 * nlmsg_len - length of message payload
 * @nlh: netlink message header
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_len(const struct Model0_nlmsghdr *Model0_nlh)
{
 return Model0_nlh->Model0_nlmsg_len - ((int) ( ((sizeof(struct Model0_nlmsghdr))+4U -1) & ~(4U -1) ));
}

/**
 * nlmsg_attrdata - head of attributes data
 * @nlh: netlink message header
 * @hdrlen: length of family specific header
 */
static inline __attribute__((no_instrument_function)) struct Model0_nlattr *Model0_nlmsg_attrdata(const struct Model0_nlmsghdr *Model0_nlh,
         int Model0_hdrlen)
{
 unsigned char *Model0_data = Model0_nlmsg_data(Model0_nlh);
 return (struct Model0_nlattr *) (Model0_data + ( ((Model0_hdrlen)+4U -1) & ~(4U -1) ));
}

/**
 * nlmsg_attrlen - length of attributes data
 * @nlh: netlink message header
 * @hdrlen: length of family specific header
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_attrlen(const struct Model0_nlmsghdr *Model0_nlh, int Model0_hdrlen)
{
 return Model0_nlmsg_len(Model0_nlh) - ( ((Model0_hdrlen)+4U -1) & ~(4U -1) );
}

/**
 * nlmsg_ok - check if the netlink message fits into the remaining bytes
 * @nlh: netlink message header
 * @remaining: number of bytes remaining in message stream
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_ok(const struct Model0_nlmsghdr *Model0_nlh, int Model0_remaining)
{
 return (Model0_remaining >= (int) sizeof(struct Model0_nlmsghdr) &&
  Model0_nlh->Model0_nlmsg_len >= sizeof(struct Model0_nlmsghdr) &&
  Model0_nlh->Model0_nlmsg_len <= Model0_remaining);
}

/**
 * nlmsg_next - next netlink message in message stream
 * @nlh: netlink message header
 * @remaining: number of bytes remaining in message stream
 *
 * Returns the next netlink message in the message stream and
 * decrements remaining by the size of the current message.
 */
static inline __attribute__((no_instrument_function)) struct Model0_nlmsghdr *
Model0_nlmsg_next(const struct Model0_nlmsghdr *Model0_nlh, int *Model0_remaining)
{
 int Model0_totlen = ( ((Model0_nlh->Model0_nlmsg_len)+4U -1) & ~(4U -1) );

 *Model0_remaining -= Model0_totlen;

 return (struct Model0_nlmsghdr *) ((unsigned char *) Model0_nlh + Model0_totlen);
}

/**
 * nlmsg_parse - parse attributes of a netlink message
 * @nlh: netlink message header
 * @hdrlen: length of family specific header
 * @tb: destination array with maxtype+1 elements
 * @maxtype: maximum attribute type to be expected
 * @policy: validation policy
 *
 * See nla_parse()
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_parse(const struct Model0_nlmsghdr *Model0_nlh, int Model0_hdrlen,
         struct Model0_nlattr *Model0_tb[], int Model0_maxtype,
         const struct Model0_nla_policy *Model0_policy)
{
 if (Model0_nlh->Model0_nlmsg_len < Model0_nlmsg_msg_size(Model0_hdrlen))
  return -22;

 return Model0_nla_parse(Model0_tb, Model0_maxtype, Model0_nlmsg_attrdata(Model0_nlh, Model0_hdrlen),
    Model0_nlmsg_attrlen(Model0_nlh, Model0_hdrlen), Model0_policy);
}

/**
 * nlmsg_find_attr - find a specific attribute in a netlink message
 * @nlh: netlink message header
 * @hdrlen: length of familiy specific header
 * @attrtype: type of attribute to look for
 *
 * Returns the first attribute which matches the specified type.
 */
static inline __attribute__((no_instrument_function)) struct Model0_nlattr *Model0_nlmsg_find_attr(const struct Model0_nlmsghdr *Model0_nlh,
          int Model0_hdrlen, int Model0_attrtype)
{
 return Model0_nla_find(Model0_nlmsg_attrdata(Model0_nlh, Model0_hdrlen),
   Model0_nlmsg_attrlen(Model0_nlh, Model0_hdrlen), Model0_attrtype);
}

/**
 * nlmsg_validate - validate a netlink message including attributes
 * @nlh: netlinket message header
 * @hdrlen: length of familiy specific header
 * @maxtype: maximum attribute type to be expected
 * @policy: validation policy
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_validate(const struct Model0_nlmsghdr *Model0_nlh,
     int Model0_hdrlen, int Model0_maxtype,
     const struct Model0_nla_policy *Model0_policy)
{
 if (Model0_nlh->Model0_nlmsg_len < Model0_nlmsg_msg_size(Model0_hdrlen))
  return -22;

 return Model0_nla_validate(Model0_nlmsg_attrdata(Model0_nlh, Model0_hdrlen),
       Model0_nlmsg_attrlen(Model0_nlh, Model0_hdrlen), Model0_maxtype, Model0_policy);
}

/**
 * nlmsg_report - need to report back to application?
 * @nlh: netlink message header
 *
 * Returns 1 if a report back to the application is requested.
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_report(const struct Model0_nlmsghdr *Model0_nlh)
{
 return !!(Model0_nlh->Model0_nlmsg_flags & 8);
}

/**
 * nlmsg_for_each_attr - iterate over a stream of attributes
 * @pos: loop counter, set to current attribute
 * @nlh: netlink message header
 * @hdrlen: length of familiy specific header
 * @rem: initialized to len, holds bytes currently remaining in stream
 */




/**
 * nlmsg_put - Add a new netlink message to an skb
 * @skb: socket buffer to store message in
 * @portid: netlink PORTID of requesting application
 * @seq: sequence number of message
 * @type: message type
 * @payload: length of message payload
 * @flags: message flags
 *
 * Returns NULL if the tailroom of the skb is insufficient to store
 * the message header and payload.
 */
static inline __attribute__((no_instrument_function)) struct Model0_nlmsghdr *Model0_nlmsg_put(struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_portid, Model0_u32 Model0_seq,
      int Model0_type, int Model0_payload, int Model0_flags)
{
 if (__builtin_expect(!!(Model0_skb_tailroom(Model0_skb) < Model0_nlmsg_total_size(Model0_payload)), 0))
  return ((void *)0);

 return Model0___nlmsg_put(Model0_skb, Model0_portid, Model0_seq, Model0_type, Model0_payload, Model0_flags);
}

/**
 * nlmsg_put_answer - Add a new callback based netlink message to an skb
 * @skb: socket buffer to store message in
 * @cb: netlink callback
 * @type: message type
 * @payload: length of message payload
 * @flags: message flags
 *
 * Returns NULL if the tailroom of the skb is insufficient to store
 * the message header and payload.
 */
static inline __attribute__((no_instrument_function)) struct Model0_nlmsghdr *Model0_nlmsg_put_answer(struct Model0_sk_buff *Model0_skb,
      struct Model0_netlink_callback *Model0_cb,
      int Model0_type, int Model0_payload,
      int Model0_flags)
{
 return Model0_nlmsg_put(Model0_skb, (*(struct Model0_netlink_skb_parms*)&((Model0_cb->Model0_skb)->Model0_cb)).Model0_portid, Model0_cb->Model0_nlh->Model0_nlmsg_seq,
    Model0_type, Model0_payload, Model0_flags);
}

/**
 * nlmsg_new - Allocate a new netlink message
 * @payload: size of the message payload
 * @flags: the type of memory to allocate.
 *
 * Use NLMSG_DEFAULT_SIZE if the size of the payload isn't known
 * and a good default is needed.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_nlmsg_new(Model0_size_t Model0_payload, Model0_gfp_t Model0_flags)
{
 return Model0_alloc_skb(Model0_nlmsg_total_size(Model0_payload), Model0_flags);
}

/**
 * nlmsg_end - Finalize a netlink message
 * @skb: socket buffer the message is stored in
 * @nlh: netlink message header
 *
 * Corrects the netlink message header to include the appeneded
 * attributes. Only necessary if attributes have been added to
 * the message.
 */
static inline __attribute__((no_instrument_function)) void Model0_nlmsg_end(struct Model0_sk_buff *Model0_skb, struct Model0_nlmsghdr *Model0_nlh)
{
 Model0_nlh->Model0_nlmsg_len = Model0_skb_tail_pointer(Model0_skb) - (unsigned char *)Model0_nlh;
}

/**
 * nlmsg_get_pos - return current position in netlink message
 * @skb: socket buffer the message is stored in
 *
 * Returns a pointer to the current tail of the message.
 */
static inline __attribute__((no_instrument_function)) void *Model0_nlmsg_get_pos(struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_tail_pointer(Model0_skb);
}

/**
 * nlmsg_trim - Trim message to a mark
 * @skb: socket buffer the message is stored in
 * @mark: mark to trim to
 *
 * Trims the message to the provided mark.
 */
static inline __attribute__((no_instrument_function)) void Model0_nlmsg_trim(struct Model0_sk_buff *Model0_skb, const void *Model0_mark)
{
 if (Model0_mark) {
  ({ int Model0___ret_warn_on = !!((unsigned char *) Model0_mark < Model0_skb->Model0_data); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/net/netlink.h", 534); __builtin_expect(!!(Model0___ret_warn_on), 0); });
  Model0_skb_trim(Model0_skb, (unsigned char *) Model0_mark - Model0_skb->Model0_data);
 }
}

/**
 * nlmsg_cancel - Cancel construction of a netlink message
 * @skb: socket buffer the message is stored in
 * @nlh: netlink message header
 *
 * Removes the complete netlink message including all
 * attributes from the socket buffer again.
 */
static inline __attribute__((no_instrument_function)) void Model0_nlmsg_cancel(struct Model0_sk_buff *Model0_skb, struct Model0_nlmsghdr *Model0_nlh)
{
 Model0_nlmsg_trim(Model0_skb, Model0_nlh);
}

/**
 * nlmsg_free - free a netlink message
 * @skb: socket buffer of netlink message
 */
static inline __attribute__((no_instrument_function)) void Model0_nlmsg_free(struct Model0_sk_buff *Model0_skb)
{
 Model0_kfree_skb(Model0_skb);
}

/**
 * nlmsg_multicast - multicast a netlink message
 * @sk: netlink socket to spread messages to
 * @skb: netlink message as socket buffer
 * @portid: own netlink portid to avoid sending to yourself
 * @group: multicast group id
 * @flags: allocation flags
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_multicast(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
      Model0_u32 Model0_portid, unsigned int Model0_group, Model0_gfp_t Model0_flags)
{
 int err;

 (*(struct Model0_netlink_skb_parms*)&((Model0_skb)->Model0_cb)).Model0_dst_group = Model0_group;

 err = Model0_netlink_broadcast(Model0_sk, Model0_skb, Model0_portid, Model0_group, Model0_flags);
 if (err > 0)
  err = 0;

 return err;
}

/**
 * nlmsg_unicast - unicast a netlink message
 * @sk: netlink socket to spread message to
 * @skb: netlink message as socket buffer
 * @portid: netlink portid of the destination socket
 */
static inline __attribute__((no_instrument_function)) int Model0_nlmsg_unicast(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_portid)
{
 int err;

 err = Model0_netlink_unicast(Model0_sk, Model0_skb, Model0_portid, 0x40);
 if (err > 0)
  err = 0;

 return err;
}

/**
 * nlmsg_for_each_msg - iterate over a stream of messages
 * @pos: loop counter, set to current message
 * @head: head of message stream
 * @len: length of message stream
 * @rem: initialized to len, holds bytes currently remaining in stream
 */





/**
 * nl_dump_check_consistent - check if sequence is consistent and advertise if not
 * @cb: netlink callback structure that stores the sequence number
 * @nlh: netlink message header to write the flag to
 *
 * This function checks if the sequence (generation) number changed during dump
 * and if it did, advertises it in the netlink message header.
 *
 * The correct way to use it is to set cb->seq to the generation counter when
 * all locks for dumping have been acquired, and then call this function for
 * each message that is generated.
 *
 * Note that due to initialisation concerns, 0 is an invalid sequence number
 * and must not be used by code that uses this functionality.
 */
static inline __attribute__((no_instrument_function)) void
Model0_nl_dump_check_consistent(struct Model0_netlink_callback *Model0_cb,
    struct Model0_nlmsghdr *Model0_nlh)
{
 if (Model0_cb->Model0_prev_seq && Model0_cb->Model0_seq != Model0_cb->Model0_prev_seq)
  Model0_nlh->Model0_nlmsg_flags |= 16;
 Model0_cb->Model0_prev_seq = Model0_cb->Model0_seq;
}

/**************************************************************************
 * Netlink Attributes
 **************************************************************************/

/**
 * nla_attr_size - length of attribute not including padding
 * @payload: length of payload
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_attr_size(int Model0_payload)
{
 return ((int) (((sizeof(struct Model0_nlattr)) + 4 - 1) & ~(4 - 1))) + Model0_payload;
}

/**
 * nla_total_size - total length of attribute including padding
 * @payload: length of payload
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_total_size(int Model0_payload)
{
 return (((Model0_nla_attr_size(Model0_payload)) + 4 - 1) & ~(4 - 1));
}

/**
 * nla_padlen - length of padding at the tail of attribute
 * @payload: length of payload
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_padlen(int Model0_payload)
{
 return Model0_nla_total_size(Model0_payload) - Model0_nla_attr_size(Model0_payload);
}

/**
 * nla_type - attribute type
 * @nla: netlink attribute
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_type(const struct Model0_nlattr *Model0_nla)
{
 return Model0_nla->Model0_nla_type & ~((1 << 15) | (1 << 14));
}

/**
 * nla_data - head of payload
 * @nla: netlink attribute
 */
static inline __attribute__((no_instrument_function)) void *Model0_nla_data(const struct Model0_nlattr *Model0_nla)
{
 return (char *) Model0_nla + ((int) (((sizeof(struct Model0_nlattr)) + 4 - 1) & ~(4 - 1)));
}

/**
 * nla_len - length of payload
 * @nla: netlink attribute
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_len(const struct Model0_nlattr *Model0_nla)
{
 return Model0_nla->Model0_nla_len - ((int) (((sizeof(struct Model0_nlattr)) + 4 - 1) & ~(4 - 1)));
}

/**
 * nla_ok - check if the netlink attribute fits into the remaining bytes
 * @nla: netlink attribute
 * @remaining: number of bytes remaining in attribute stream
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_ok(const struct Model0_nlattr *Model0_nla, int Model0_remaining)
{
 return Model0_remaining >= (int) sizeof(*Model0_nla) &&
        Model0_nla->Model0_nla_len >= sizeof(*Model0_nla) &&
        Model0_nla->Model0_nla_len <= Model0_remaining;
}

/**
 * nla_next - next netlink attribute in attribute stream
 * @nla: netlink attribute
 * @remaining: number of bytes remaining in attribute stream
 *
 * Returns the next netlink attribute in the attribute stream and
 * decrements remaining by the size of the current attribute.
 */
static inline __attribute__((no_instrument_function)) struct Model0_nlattr *Model0_nla_next(const struct Model0_nlattr *Model0_nla, int *Model0_remaining)
{
 int Model0_totlen = (((Model0_nla->Model0_nla_len) + 4 - 1) & ~(4 - 1));

 *Model0_remaining -= Model0_totlen;
 return (struct Model0_nlattr *) ((char *) Model0_nla + Model0_totlen);
}

/**
 * nla_find_nested - find attribute in a set of nested attributes
 * @nla: attribute containing the nested attributes
 * @attrtype: type of attribute to look for
 *
 * Returns the first attribute which matches the specified type.
 */
static inline __attribute__((no_instrument_function)) struct Model0_nlattr *
Model0_nla_find_nested(const struct Model0_nlattr *Model0_nla, int Model0_attrtype)
{
 return Model0_nla_find(Model0_nla_data(Model0_nla), Model0_nla_len(Model0_nla), Model0_attrtype);
}

/**
 * nla_parse_nested - parse nested attributes
 * @tb: destination array with maxtype+1 elements
 * @maxtype: maximum attribute type to be expected
 * @nla: attribute containing the nested attributes
 * @policy: validation policy
 *
 * See nla_parse()
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_parse_nested(struct Model0_nlattr *Model0_tb[], int Model0_maxtype,
       const struct Model0_nlattr *Model0_nla,
       const struct Model0_nla_policy *Model0_policy)
{
 return Model0_nla_parse(Model0_tb, Model0_maxtype, Model0_nla_data(Model0_nla), Model0_nla_len(Model0_nla), Model0_policy);
}

/**
 * nla_put_u8 - Add a u8 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_u8(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0_u8 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0_u8), &Model0_value);
}

/**
 * nla_put_u16 - Add a u16 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_u16(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0_u16 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0_u16), &Model0_value);
}

/**
 * nla_put_be16 - Add a __be16 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_be16(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___be16 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0___be16), &Model0_value);
}

/**
 * nla_put_net16 - Add 16-bit network byte order netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_net16(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___be16 Model0_value)
{
 return Model0_nla_put_be16(Model0_skb, Model0_attrtype | (1 << 14), Model0_value);
}

/**
 * nla_put_le16 - Add a __le16 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_le16(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___le16 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0___le16), &Model0_value);
}

/**
 * nla_put_u32 - Add a u32 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_u32(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0_u32 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0_u32), &Model0_value);
}

/**
 * nla_put_be32 - Add a __be32 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_be32(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___be32 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0___be32), &Model0_value);
}

/**
 * nla_put_net32 - Add 32-bit network byte order netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_net32(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___be32 Model0_value)
{
 return Model0_nla_put_be32(Model0_skb, Model0_attrtype | (1 << 14), Model0_value);
}

/**
 * nla_put_le32 - Add a __le32 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_le32(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___le32 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0___le32), &Model0_value);
}

/**
 * nla_put_u64_64bit - Add a u64 netlink attribute to a skb and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_u64_64bit(struct Model0_sk_buff *Model0_skb, int Model0_attrtype,
        Model0_u64 Model0_value, int Model0_padattr)
{
 return Model0_nla_put_64bit(Model0_skb, Model0_attrtype, sizeof(Model0_u64), &Model0_value, Model0_padattr);
}

/**
 * nla_put_be64 - Add a __be64 netlink attribute to a socket buffer and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_be64(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___be64 Model0_value,
          int Model0_padattr)
{
 return Model0_nla_put_64bit(Model0_skb, Model0_attrtype, sizeof(Model0___be64), &Model0_value, Model0_padattr);
}

/**
 * nla_put_net64 - Add 64-bit network byte order nlattr to a skb and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_net64(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___be64 Model0_value,
    int Model0_padattr)
{
 return Model0_nla_put_be64(Model0_skb, Model0_attrtype | (1 << 14), Model0_value,
       Model0_padattr);
}

/**
 * nla_put_le64 - Add a __le64 netlink attribute to a socket buffer and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_le64(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0___le64 Model0_value,
          int Model0_padattr)
{
 return Model0_nla_put_64bit(Model0_skb, Model0_attrtype, sizeof(Model0___le64), &Model0_value, Model0_padattr);
}

/**
 * nla_put_s8 - Add a s8 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_s8(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0_s8 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0_s8), &Model0_value);
}

/**
 * nla_put_s16 - Add a s16 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_s16(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0_s16 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0_s16), &Model0_value);
}

/**
 * nla_put_s32 - Add a s32 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_s32(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0_s32 Model0_value)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(Model0_s32), &Model0_value);
}

/**
 * nla_put_s64 - Add a s64 netlink attribute to a socket buffer and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_s64(struct Model0_sk_buff *Model0_skb, int Model0_attrtype, Model0_s64 Model0_value,
         int Model0_padattr)
{
 return Model0_nla_put_64bit(Model0_skb, Model0_attrtype, sizeof(Model0_s64), &Model0_value, Model0_padattr);
}

/**
 * nla_put_string - Add a string netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @str: NUL terminated string
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_string(struct Model0_sk_buff *Model0_skb, int Model0_attrtype,
     const char *Model0_str)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, Model0_strlen(Model0_str) + 1, Model0_str);
}

/**
 * nla_put_flag - Add a flag netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_flag(struct Model0_sk_buff *Model0_skb, int Model0_attrtype)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, 0, ((void *)0));
}

/**
 * nla_put_msecs - Add a msecs netlink attribute to a skb and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @njiffies: number of jiffies to convert to msecs
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_msecs(struct Model0_sk_buff *Model0_skb, int Model0_attrtype,
    unsigned long Model0_njiffies, int Model0_padattr)
{
 Model0_u64 Model0_tmp = Model0_jiffies_to_msecs(Model0_njiffies);

 return Model0_nla_put_64bit(Model0_skb, Model0_attrtype, sizeof(Model0_u64), &Model0_tmp, Model0_padattr);
}

/**
 * nla_put_in_addr - Add an IPv4 address netlink attribute to a socket
 * buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @addr: IPv4 address
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_in_addr(struct Model0_sk_buff *Model0_skb, int Model0_attrtype,
      Model0___be32 Model0_addr)
{
 return Model0_nla_put_be32(Model0_skb, Model0_attrtype, Model0_addr);
}

/**
 * nla_put_in6_addr - Add an IPv6 address netlink attribute to a socket
 * buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @addr: IPv6 address
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_put_in6_addr(struct Model0_sk_buff *Model0_skb, int Model0_attrtype,
       const struct Model0_in6_addr *Model0_addr)
{
 return Model0_nla_put(Model0_skb, Model0_attrtype, sizeof(*Model0_addr), Model0_addr);
}

/**
 * nla_get_u32 - return payload of u32 attribute
 * @nla: u32 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_nla_get_u32(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0_u32 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_be32 - return payload of __be32 attribute
 * @nla: __be32 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0___be32 Model0_nla_get_be32(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0___be32 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_le32 - return payload of __le32 attribute
 * @nla: __le32 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0___le32 Model0_nla_get_le32(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0___le32 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_u16 - return payload of u16 attribute
 * @nla: u16 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0_u16 Model0_nla_get_u16(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0_u16 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_be16 - return payload of __be16 attribute
 * @nla: __be16 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0___be16 Model0_nla_get_be16(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0___be16 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_le16 - return payload of __le16 attribute
 * @nla: __le16 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0___le16 Model0_nla_get_le16(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0___le16 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_u8 - return payload of u8 attribute
 * @nla: u8 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0_u8 Model0_nla_get_u8(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0_u8 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_u64 - return payload of u64 attribute
 * @nla: u64 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0_u64 Model0_nla_get_u64(const struct Model0_nlattr *Model0_nla)
{
 Model0_u64 Model0_tmp;

 Model0_nla_memcpy(&Model0_tmp, Model0_nla, sizeof(Model0_tmp));

 return Model0_tmp;
}

/**
 * nla_get_be64 - return payload of __be64 attribute
 * @nla: __be64 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0___be64 Model0_nla_get_be64(const struct Model0_nlattr *Model0_nla)
{
 Model0___be64 Model0_tmp;

 Model0_nla_memcpy(&Model0_tmp, Model0_nla, sizeof(Model0_tmp));

 return Model0_tmp;
}

/**
 * nla_get_le64 - return payload of __le64 attribute
 * @nla: __le64 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0___le64 Model0_nla_get_le64(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0___le64 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_s32 - return payload of s32 attribute
 * @nla: s32 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0_s32 Model0_nla_get_s32(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0_s32 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_s16 - return payload of s16 attribute
 * @nla: s16 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0_s16 Model0_nla_get_s16(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0_s16 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_s8 - return payload of s8 attribute
 * @nla: s8 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0_s8 Model0_nla_get_s8(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0_s8 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_s64 - return payload of s64 attribute
 * @nla: s64 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0_s64 Model0_nla_get_s64(const struct Model0_nlattr *Model0_nla)
{
 Model0_s64 Model0_tmp;

 Model0_nla_memcpy(&Model0_tmp, Model0_nla, sizeof(Model0_tmp));

 return Model0_tmp;
}

/**
 * nla_get_flag - return payload of flag attribute
 * @nla: flag netlink attribute
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_get_flag(const struct Model0_nlattr *Model0_nla)
{
 return !!Model0_nla;
}

/**
 * nla_get_msecs - return payload of msecs attribute
 * @nla: msecs netlink attribute
 *
 * Returns the number of milliseconds in jiffies.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_nla_get_msecs(const struct Model0_nlattr *Model0_nla)
{
 Model0_u64 Model0_msecs = Model0_nla_get_u64(Model0_nla);

 return Model0_msecs_to_jiffies((unsigned long) Model0_msecs);
}

/**
 * nla_get_in_addr - return payload of IPv4 address attribute
 * @nla: IPv4 address netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model0___be32 Model0_nla_get_in_addr(const struct Model0_nlattr *Model0_nla)
{
 return *(Model0___be32 *) Model0_nla_data(Model0_nla);
}

/**
 * nla_get_in6_addr - return payload of IPv6 address attribute
 * @nla: IPv6 address netlink attribute
 */
static inline __attribute__((no_instrument_function)) struct Model0_in6_addr Model0_nla_get_in6_addr(const struct Model0_nlattr *Model0_nla)
{
 struct Model0_in6_addr Model0_tmp;

 Model0_nla_memcpy(&Model0_tmp, Model0_nla, sizeof(Model0_tmp));
 return Model0_tmp;
}

/**
 * nla_nest_start - Start a new level of nested attributes
 * @skb: socket buffer to add attributes to
 * @attrtype: attribute type of container
 *
 * Returns the container attribute
 */
static inline __attribute__((no_instrument_function)) struct Model0_nlattr *Model0_nla_nest_start(struct Model0_sk_buff *Model0_skb, int Model0_attrtype)
{
 struct Model0_nlattr *Model0_start = (struct Model0_nlattr *)Model0_skb_tail_pointer(Model0_skb);

 if (Model0_nla_put(Model0_skb, Model0_attrtype, 0, ((void *)0)) < 0)
  return ((void *)0);

 return Model0_start;
}

/**
 * nla_nest_end - Finalize nesting of attributes
 * @skb: socket buffer the attributes are stored in
 * @start: container attribute
 *
 * Corrects the container attribute header to include the all
 * appeneded attributes.
 *
 * Returns the total data length of the skb.
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_nest_end(struct Model0_sk_buff *Model0_skb, struct Model0_nlattr *Model0_start)
{
 Model0_start->Model0_nla_len = Model0_skb_tail_pointer(Model0_skb) - (unsigned char *)Model0_start;
 return Model0_skb->Model0_len;
}

/**
 * nla_nest_cancel - Cancel nesting of attributes
 * @skb: socket buffer the message is stored in
 * @start: container attribute
 *
 * Removes the container attribute and including all nested
 * attributes. Returns -EMSGSIZE
 */
static inline __attribute__((no_instrument_function)) void Model0_nla_nest_cancel(struct Model0_sk_buff *Model0_skb, struct Model0_nlattr *Model0_start)
{
 Model0_nlmsg_trim(Model0_skb, Model0_start);
}

/**
 * nla_validate_nested - Validate a stream of nested attributes
 * @start: container attribute
 * @maxtype: maximum attribute type to be expected
 * @policy: validation policy
 *
 * Validates all attributes in the nested attribute stream against the
 * specified policy. Attributes with a type exceeding maxtype will be
 * ignored. See documenation of struct nla_policy for more details.
 *
 * Returns 0 on success or a negative error code.
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_validate_nested(const struct Model0_nlattr *Model0_start, int Model0_maxtype,
          const struct Model0_nla_policy *Model0_policy)
{
 return Model0_nla_validate(Model0_nla_data(Model0_start), Model0_nla_len(Model0_start), Model0_maxtype, Model0_policy);
}

/**
 * nla_need_padding_for_64bit - test 64-bit alignment of the next attribute
 * @skb: socket buffer the message is stored in
 *
 * Return true if padding is needed to align the next attribute (nla_data()) to
 * a 64-bit aligned area.
 */
static inline __attribute__((no_instrument_function)) bool Model0_nla_need_padding_for_64bit(struct Model0_sk_buff *Model0_skb)
{
 return false;
}

/**
 * nla_align_64bit - 64-bit align the nla_data() of next attribute
 * @skb: socket buffer the message is stored in
 * @padattr: attribute type for the padding
 *
 * Conditionally emit a padding netlink attribute in order to make
 * the next attribute we emit have a 64-bit aligned nla_data() area.
 * This will only be done in architectures which do not have
 * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS defined.
 *
 * Returns zero on success or a negative error code.
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_align_64bit(struct Model0_sk_buff *Model0_skb, int Model0_padattr)
{
 if (Model0_nla_need_padding_for_64bit(Model0_skb) &&
     !Model0_nla_reserve(Model0_skb, Model0_padattr, 0))
  return -90;

 return 0;
}

/**
 * nla_total_size_64bit - total length of attribute including padding
 * @payload: length of payload
 */
static inline __attribute__((no_instrument_function)) int Model0_nla_total_size_64bit(int Model0_payload)
{
 return (((Model0_nla_attr_size(Model0_payload)) + 4 - 1) & ~(4 - 1))



  ;
}

/**
 * nla_for_each_attr - iterate over a stream of attributes
 * @pos: loop counter, set to current attribute
 * @head: head of attribute stream
 * @len: length of attribute stream
 * @rem: initialized to len, holds bytes currently remaining in stream
 */





/**
 * nla_for_each_nested - iterate over nested attributes
 * @pos: loop counter, set to current attribute
 * @nla: attribute containing the nested attributes
 * @rem: initialized to len, holds bytes currently remaining in stream
 */



/**
 * nla_is_last - Test if attribute is last in stream
 * @nla: attribute to test
 * @rem: bytes remaining in stream
 */
static inline __attribute__((no_instrument_function)) bool Model0_nla_is_last(const struct Model0_nlattr *Model0_nla, int Model0_rem)
{
 return Model0_nla->Model0_nla_len == Model0_rem;
}

typedef int (*Model0_rtnl_doit_func)(struct Model0_sk_buff *, struct Model0_nlmsghdr *);
typedef int (*Model0_rtnl_dumpit_func)(struct Model0_sk_buff *, struct Model0_netlink_callback *);
typedef Model0_u16 (*Model0_rtnl_calcit_func)(struct Model0_sk_buff *, struct Model0_nlmsghdr *);

int Model0___rtnl_register(int Model0_protocol, int Model0_msgtype,
      Model0_rtnl_doit_func, Model0_rtnl_dumpit_func, Model0_rtnl_calcit_func);
void Model0_rtnl_register(int Model0_protocol, int Model0_msgtype,
     Model0_rtnl_doit_func, Model0_rtnl_dumpit_func, Model0_rtnl_calcit_func);
int Model0_rtnl_unregister(int Model0_protocol, int Model0_msgtype);
void Model0_rtnl_unregister_all(int Model0_protocol);

static inline __attribute__((no_instrument_function)) int Model0_rtnl_msg_family(const struct Model0_nlmsghdr *Model0_nlh)
{
 if (Model0_nlmsg_len(Model0_nlh) >= sizeof(struct Model0_rtgenmsg))
  return ((struct Model0_rtgenmsg *) Model0_nlmsg_data(Model0_nlh))->Model0_rtgen_family;
 else
  return 0;
}

/**
 *	struct rtnl_link_ops - rtnetlink link operations
 *
 *	@list: Used internally
 *	@kind: Identifier
 *	@maxtype: Highest device specific netlink attribute number
 *	@policy: Netlink policy for device specific attribute validation
 *	@validate: Optional validation function for netlink/changelink parameters
 *	@priv_size: sizeof net_device private space
 *	@setup: net_device setup function
 *	@newlink: Function for configuring and registering a new device
 *	@changelink: Function for changing parameters of an existing device
 *	@dellink: Function to remove a device
 *	@get_size: Function to calculate required room for dumping device
 *		   specific netlink attributes
 *	@fill_info: Function to dump device specific netlink attributes
 *	@get_xstats_size: Function to calculate required room for dumping device
 *			  specific statistics
 *	@fill_xstats: Function to dump device specific statistics
 *	@get_num_tx_queues: Function to determine number of transmit queues
 *			    to create when creating a new device.
 *	@get_num_rx_queues: Function to determine number of receive queues
 *			    to create when creating a new device.
 *	@get_link_net: Function to get the i/o netns of the device
 *	@get_linkxstats_size: Function to calculate the required room for
 *			      dumping device-specific extended link stats
 *	@fill_linkxstats: Function to dump device-specific extended link stats
 */
struct Model0_rtnl_link_ops {
 struct Model0_list_head Model0_list;

 const char *Model0_kind;

 Model0_size_t Model0_priv_size;
 void (*Model0_setup)(struct Model0_net_device *Model0_dev);

 int Model0_maxtype;
 const struct Model0_nla_policy *Model0_policy;
 int (*Model0_validate)(struct Model0_nlattr *Model0_tb[],
         struct Model0_nlattr *Model0_data[]);

 int (*Model0_newlink)(struct Model0_net *Model0_src_net,
        struct Model0_net_device *Model0_dev,
        struct Model0_nlattr *Model0_tb[],
        struct Model0_nlattr *Model0_data[]);
 int (*Model0_changelink)(struct Model0_net_device *Model0_dev,
           struct Model0_nlattr *Model0_tb[],
           struct Model0_nlattr *Model0_data[]);
 void (*Model0_dellink)(struct Model0_net_device *Model0_dev,
        struct Model0_list_head *Model0_head);

 Model0_size_t (*Model0_get_size)(const struct Model0_net_device *Model0_dev);
 int (*Model0_fill_info)(struct Model0_sk_buff *Model0_skb,
          const struct Model0_net_device *Model0_dev);

 Model0_size_t (*Model0_get_xstats_size)(const struct Model0_net_device *Model0_dev);
 int (*Model0_fill_xstats)(struct Model0_sk_buff *Model0_skb,
            const struct Model0_net_device *Model0_dev);
 unsigned int (*Model0_get_num_tx_queues)(void);
 unsigned int (*Model0_get_num_rx_queues)(void);

 int Model0_slave_maxtype;
 const struct Model0_nla_policy *Model0_slave_policy;
 int (*Model0_slave_validate)(struct Model0_nlattr *Model0_tb[],
        struct Model0_nlattr *Model0_data[]);
 int (*Model0_slave_changelink)(struct Model0_net_device *Model0_dev,
          struct Model0_net_device *Model0_slave_dev,
          struct Model0_nlattr *Model0_tb[],
          struct Model0_nlattr *Model0_data[]);
 Model0_size_t (*Model0_get_slave_size)(const struct Model0_net_device *Model0_dev,
        const struct Model0_net_device *Model0_slave_dev);
 int (*Model0_fill_slave_info)(struct Model0_sk_buff *Model0_skb,
         const struct Model0_net_device *Model0_dev,
         const struct Model0_net_device *Model0_slave_dev);
 struct Model0_net *(*Model0_get_link_net)(const struct Model0_net_device *Model0_dev);
 Model0_size_t (*Model0_get_linkxstats_size)(const struct Model0_net_device *Model0_dev,
             int Model0_attr);
 int (*Model0_fill_linkxstats)(struct Model0_sk_buff *Model0_skb,
         const struct Model0_net_device *Model0_dev,
         int *Model0_prividx, int Model0_attr);
};

int Model0___rtnl_link_register(struct Model0_rtnl_link_ops *Model0_ops);
void Model0___rtnl_link_unregister(struct Model0_rtnl_link_ops *Model0_ops);

int Model0_rtnl_link_register(struct Model0_rtnl_link_ops *Model0_ops);
void Model0_rtnl_link_unregister(struct Model0_rtnl_link_ops *Model0_ops);

/**
 * 	struct rtnl_af_ops - rtnetlink address family operations
 *
 *	@list: Used internally
 * 	@family: Address family
 * 	@fill_link_af: Function to fill IFLA_AF_SPEC with address family
 * 		       specific netlink attributes.
 * 	@get_link_af_size: Function to calculate size of address family specific
 * 			   netlink attributes.
 *	@validate_link_af: Validate a IFLA_AF_SPEC attribute, must check attr
 *			   for invalid configuration settings.
 * 	@set_link_af: Function to parse a IFLA_AF_SPEC attribute and modify
 *		      net_device accordingly.
 */
struct Model0_rtnl_af_ops {
 struct Model0_list_head Model0_list;
 int Model0_family;

 int (*Model0_fill_link_af)(struct Model0_sk_buff *Model0_skb,
      const struct Model0_net_device *Model0_dev,
      Model0_u32 Model0_ext_filter_mask);
 Model0_size_t (*Model0_get_link_af_size)(const struct Model0_net_device *Model0_dev,
          Model0_u32 Model0_ext_filter_mask);

 int (*Model0_validate_link_af)(const struct Model0_net_device *Model0_dev,
          const struct Model0_nlattr *Model0_attr);
 int (*Model0_set_link_af)(struct Model0_net_device *Model0_dev,
            const struct Model0_nlattr *Model0_attr);
};

void Model0___rtnl_af_unregister(struct Model0_rtnl_af_ops *Model0_ops);

void Model0_rtnl_af_register(struct Model0_rtnl_af_ops *Model0_ops);
void Model0_rtnl_af_unregister(struct Model0_rtnl_af_ops *Model0_ops);

struct Model0_net *Model0_rtnl_link_get_net(struct Model0_net *Model0_src_net, struct Model0_nlattr *Model0_tb[]);
struct Model0_net_device *Model0_rtnl_create_link(struct Model0_net *Model0_net, const char *Model0_ifname,
        unsigned char Model0_name_assign_type,
        const struct Model0_rtnl_link_ops *Model0_ops,
        struct Model0_nlattr *Model0_tb[]);
int Model0_rtnl_delete_link(struct Model0_net_device *Model0_dev);
int Model0_rtnl_configure_link(struct Model0_net_device *Model0_dev, const struct Model0_ifinfomsg *Model0_ifm);

int Model0_rtnl_nla_parse_ifla(struct Model0_nlattr **Model0_tb, const struct Model0_nlattr *Model0_head, int Model0_len);

/*
 * NUD stands for "neighbor unreachability detection"
 */





struct Model0_neighbour;

enum {
 Model0_NEIGH_VAR_MCAST_PROBES,
 Model0_NEIGH_VAR_UCAST_PROBES,
 Model0_NEIGH_VAR_APP_PROBES,
 Model0_NEIGH_VAR_MCAST_REPROBES,
 Model0_NEIGH_VAR_RETRANS_TIME,
 Model0_NEIGH_VAR_BASE_REACHABLE_TIME,
 Model0_NEIGH_VAR_DELAY_PROBE_TIME,
 Model0_NEIGH_VAR_GC_STALETIME,
 Model0_NEIGH_VAR_QUEUE_LEN_BYTES,
 Model0_NEIGH_VAR_PROXY_QLEN,
 Model0_NEIGH_VAR_ANYCAST_DELAY,
 Model0_NEIGH_VAR_PROXY_DELAY,
 Model0_NEIGH_VAR_LOCKTIME,

 /* Following are used as a second way to access one of the above */
 Model0_NEIGH_VAR_QUEUE_LEN, /* same data as NEIGH_VAR_QUEUE_LEN_BYTES */
 Model0_NEIGH_VAR_RETRANS_TIME_MS, /* same data as NEIGH_VAR_RETRANS_TIME */
 Model0_NEIGH_VAR_BASE_REACHABLE_TIME_MS, /* same data as NEIGH_VAR_BASE_REACHABLE_TIME */
 /* Following are used by "default" only */
 Model0_NEIGH_VAR_GC_INTERVAL,
 Model0_NEIGH_VAR_GC_THRESH1,
 Model0_NEIGH_VAR_GC_THRESH2,
 Model0_NEIGH_VAR_GC_THRESH3,
 Model0_NEIGH_VAR_MAX
};

struct Model0_neigh_parms {
 Model0_possible_net_t Model0_net;
 struct Model0_net_device *Model0_dev;
 struct Model0_list_head Model0_list;
 int (*Model0_neigh_setup)(struct Model0_neighbour *);
 void (*Model0_neigh_cleanup)(struct Model0_neighbour *);
 struct Model0_neigh_table *Model0_tbl;

 void *Model0_sysctl_table;

 int Model0_dead;
 Model0_atomic_t Model0_refcnt;
 struct Model0_callback_head Model0_callback_head;

 int Model0_reachable_time;
 int Model0_data[(Model0_NEIGH_VAR_LOCKTIME + 1)];
 unsigned long Model0_data_state[((((Model0_NEIGH_VAR_LOCKTIME + 1)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
};

static inline __attribute__((no_instrument_function)) void Model0_neigh_var_set(struct Model0_neigh_parms *Model0_p, int Model0_index, int Model0_val)
{
 Model0_set_bit(Model0_index, Model0_p->Model0_data_state);
 Model0_p->Model0_data[Model0_index] = Model0_val;
}



/* In ndo_neigh_setup, NEIGH_VAR_INIT should be used.
 * In other cases, NEIGH_VAR_SET should be used.
 */



static inline __attribute__((no_instrument_function)) void Model0_neigh_parms_data_state_setall(struct Model0_neigh_parms *Model0_p)
{
 Model0_bitmap_fill(Model0_p->Model0_data_state, (Model0_NEIGH_VAR_LOCKTIME + 1));
}

static inline __attribute__((no_instrument_function)) void Model0_neigh_parms_data_state_cleanall(struct Model0_neigh_parms *Model0_p)
{
 Model0_bitmap_zero(Model0_p->Model0_data_state, (Model0_NEIGH_VAR_LOCKTIME + 1));
}

struct Model0_neigh_statistics {
 unsigned long Model0_allocs; /* number of allocated neighs */
 unsigned long Model0_destroys; /* number of destroyed neighs */
 unsigned long Model0_hash_grows; /* number of hash resizes */

 unsigned long Model0_res_failed; /* number of failed resolutions */

 unsigned long Model0_lookups; /* number of lookups */
 unsigned long Model0_hits; /* number of hits (among lookups) */

 unsigned long Model0_rcv_probes_mcast; /* number of received mcast ipv6 */
 unsigned long Model0_rcv_probes_ucast; /* number of received ucast ipv6 */

 unsigned long Model0_periodic_gc_runs; /* number of periodic GC runs */
 unsigned long Model0_forced_gc_runs; /* number of forced GC runs */

 unsigned long Model0_unres_discards; /* number of unresolved drops */
 unsigned long Model0_table_fulls; /* times even gc couldn't help */
};



struct Model0_neighbour {
 struct Model0_neighbour *Model0_next;
 struct Model0_neigh_table *Model0_tbl;
 struct Model0_neigh_parms *Model0_parms;
 unsigned long Model0_confirmed;
 unsigned long Model0_updated;
 Model0_rwlock_t Model0_lock;
 Model0_atomic_t Model0_refcnt;
 struct Model0_sk_buff_head Model0_arp_queue;
 unsigned int Model0_arp_queue_len_bytes;
 struct Model0_timer_list Model0_timer;
 unsigned long Model0_used;
 Model0_atomic_t Model0_probes;
 __u8 Model0_flags;
 __u8 Model0_nud_state;
 __u8 Model0_type;
 __u8 Model0_dead;
 Model0_seqlock_t Model0_ha_lock;
 unsigned char Model0_ha[((((32)) + ((typeof((32)))((sizeof(unsigned long))) - 1)) & ~((typeof((32)))((sizeof(unsigned long))) - 1))];
 struct Model0_hh_cache Model0_hh;
 int (*Model0_output)(struct Model0_neighbour *, struct Model0_sk_buff *);
 const struct Model0_neigh_ops *Model0_ops;
 struct Model0_callback_head Model0_rcu;
 struct Model0_net_device *Model0_dev;
 Model0_u8 Model0_primary_key[0];
};

struct Model0_neigh_ops {
 int Model0_family;
 void (*Model0_solicit)(struct Model0_neighbour *, struct Model0_sk_buff *);
 void (*Model0_error_report)(struct Model0_neighbour *, struct Model0_sk_buff *);
 int (*Model0_output)(struct Model0_neighbour *, struct Model0_sk_buff *);
 int (*Model0_connected_output)(struct Model0_neighbour *, struct Model0_sk_buff *);
};

struct Model0_pneigh_entry {
 struct Model0_pneigh_entry *Model0_next;
 Model0_possible_net_t Model0_net;
 struct Model0_net_device *Model0_dev;
 Model0_u8 Model0_flags;
 Model0_u8 Model0_key[0];
};

/*
 *	neighbour table manipulation
 */



struct Model0_neigh_hash_table {
 struct Model0_neighbour **Model0_hash_buckets;
 unsigned int Model0_hash_shift;
 __u32 Model0_hash_rnd[4];
 struct Model0_callback_head Model0_rcu;
};


struct Model0_neigh_table {
 int Model0_family;
 int Model0_entry_size;
 int Model0_key_len;
 Model0___be16 Model0_protocol;
 __u32 (*Model0_hash)(const void *Model0_pkey,
     const struct Model0_net_device *Model0_dev,
     __u32 *Model0_hash_rnd);
 bool (*Model0_key_eq)(const struct Model0_neighbour *, const void *Model0_pkey);
 int (*Model0_constructor)(struct Model0_neighbour *);
 int (*Model0_pconstructor)(struct Model0_pneigh_entry *);
 void (*Model0_pdestructor)(struct Model0_pneigh_entry *);
 void (*Model0_proxy_redo)(struct Model0_sk_buff *Model0_skb);
 char *Model0_id;
 struct Model0_neigh_parms Model0_parms;
 struct Model0_list_head Model0_parms_list;
 int Model0_gc_interval;
 int Model0_gc_thresh1;
 int Model0_gc_thresh2;
 int Model0_gc_thresh3;
 unsigned long Model0_last_flush;
 struct Model0_delayed_work Model0_gc_work;
 struct Model0_timer_list Model0_proxy_timer;
 struct Model0_sk_buff_head Model0_proxy_queue;
 Model0_atomic_t Model0_entries;
 Model0_rwlock_t Model0_lock;
 unsigned long Model0_last_rand;
 struct Model0_neigh_statistics *Model0_stats;
 struct Model0_neigh_hash_table *Model0_nht;
 struct Model0_pneigh_entry **Model0_phash_buckets;
};

enum {
 Model0_NEIGH_ARP_TABLE = 0,
 Model0_NEIGH_ND_TABLE = 1,
 Model0_NEIGH_DN_TABLE = 2,
 Model0_NEIGH_NR_TABLES,
 Model0_NEIGH_LINK_TABLE = Model0_NEIGH_NR_TABLES /* Pseudo table for neigh_xmit */
};

static inline __attribute__((no_instrument_function)) int Model0_neigh_parms_family(struct Model0_neigh_parms *Model0_p)
{
 return Model0_p->Model0_tbl->Model0_family;
}




static inline __attribute__((no_instrument_function)) void *Model0_neighbour_priv(const struct Model0_neighbour *Model0_n)
{
 return (char *)Model0_n + Model0_n->Model0_tbl->Model0_entry_size;
}

/* flags for neigh_update() */







static inline __attribute__((no_instrument_function)) bool Model0_neigh_key_eq16(const struct Model0_neighbour *Model0_n, const void *Model0_pkey)
{
 return *(const Model0_u16 *)Model0_n->Model0_primary_key == *(const Model0_u16 *)Model0_pkey;
}

static inline __attribute__((no_instrument_function)) bool Model0_neigh_key_eq32(const struct Model0_neighbour *Model0_n, const void *Model0_pkey)
{
 return *(const Model0_u32 *)Model0_n->Model0_primary_key == *(const Model0_u32 *)Model0_pkey;
}

static inline __attribute__((no_instrument_function)) bool Model0_neigh_key_eq128(const struct Model0_neighbour *Model0_n, const void *Model0_pkey)
{
 const Model0_u32 *Model0_n32 = (const Model0_u32 *)Model0_n->Model0_primary_key;
 const Model0_u32 *Model0_p32 = Model0_pkey;

 return ((Model0_n32[0] ^ Model0_p32[0]) | (Model0_n32[1] ^ Model0_p32[1]) |
  (Model0_n32[2] ^ Model0_p32[2]) | (Model0_n32[3] ^ Model0_p32[3])) == 0;
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour *Model0____neigh_lookup_noref(
 struct Model0_neigh_table *Model0_tbl,
 bool (*Model0_key_eq)(const struct Model0_neighbour *Model0_n, const void *Model0_pkey),
 __u32 (*Model0_hash)(const void *Model0_pkey,
        const struct Model0_net_device *Model0_dev,
        __u32 *Model0_hash_rnd),
 const void *Model0_pkey,
 struct Model0_net_device *Model0_dev)
{
 struct Model0_neigh_hash_table *Model0_nht = ({ typeof(*(Model0_tbl->Model0_nht)) *Model0_________p1 = (typeof(*(Model0_tbl->Model0_nht)) *)({ typeof((Model0_tbl->Model0_nht)) Model0__________p1 = ({ union { typeof((Model0_tbl->Model0_nht)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_tbl->Model0_nht)), Model0___u.Model0___c, sizeof((Model0_tbl->Model0_nht))); else Model0___read_once_size_nocheck(&((Model0_tbl->Model0_nht)), Model0___u.Model0___c, sizeof((Model0_tbl->Model0_nht))); Model0___u.Model0___val; }); typeof(*((Model0_tbl->Model0_nht))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_tbl->Model0_nht)) *)(Model0_________p1)); });
 struct Model0_neighbour *Model0_n;
 Model0_u32 Model0_hash_val;

 Model0_hash_val = Model0_hash(Model0_pkey, Model0_dev, Model0_nht->Model0_hash_rnd) >> (32 - Model0_nht->Model0_hash_shift);
 for (Model0_n = ({ typeof(*(Model0_nht->Model0_hash_buckets[Model0_hash_val])) *Model0_________p1 = (typeof(*(Model0_nht->Model0_hash_buckets[Model0_hash_val])) *)({ typeof((Model0_nht->Model0_hash_buckets[Model0_hash_val])) Model0__________p1 = ({ union { typeof((Model0_nht->Model0_hash_buckets[Model0_hash_val])) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_nht->Model0_hash_buckets[Model0_hash_val])), Model0___u.Model0___c, sizeof((Model0_nht->Model0_hash_buckets[Model0_hash_val]))); else Model0___read_once_size_nocheck(&((Model0_nht->Model0_hash_buckets[Model0_hash_val])), Model0___u.Model0___c, sizeof((Model0_nht->Model0_hash_buckets[Model0_hash_val]))); Model0___u.Model0___val; }); typeof(*((Model0_nht->Model0_hash_buckets[Model0_hash_val]))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_nht->Model0_hash_buckets[Model0_hash_val])) *)(Model0_________p1)); });
      Model0_n != ((void *)0);
      Model0_n = ({ typeof(*(Model0_n->Model0_next)) *Model0_________p1 = (typeof(*(Model0_n->Model0_next)) *)({ typeof((Model0_n->Model0_next)) Model0__________p1 = ({ union { typeof((Model0_n->Model0_next)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_n->Model0_next)), Model0___u.Model0___c, sizeof((Model0_n->Model0_next))); else Model0___read_once_size_nocheck(&((Model0_n->Model0_next)), Model0___u.Model0___c, sizeof((Model0_n->Model0_next))); Model0___u.Model0___val; }); typeof(*((Model0_n->Model0_next))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_n->Model0_next)) *)(Model0_________p1)); })) {
  if (Model0_n->Model0_dev == Model0_dev && Model0_key_eq(Model0_n, Model0_pkey))
   return Model0_n;
 }

 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour *Model0___neigh_lookup_noref(struct Model0_neigh_table *Model0_tbl,
           const void *Model0_pkey,
           struct Model0_net_device *Model0_dev)
{
 return Model0____neigh_lookup_noref(Model0_tbl, Model0_tbl->Model0_key_eq, Model0_tbl->Model0_hash, Model0_pkey, Model0_dev);
}

void Model0_neigh_table_init(int Model0_index, struct Model0_neigh_table *Model0_tbl);
int Model0_neigh_table_clear(int Model0_index, struct Model0_neigh_table *Model0_tbl);
struct Model0_neighbour *Model0_neigh_lookup(struct Model0_neigh_table *Model0_tbl, const void *Model0_pkey,
          struct Model0_net_device *Model0_dev);
struct Model0_neighbour *Model0_neigh_lookup_nodev(struct Model0_neigh_table *Model0_tbl, struct Model0_net *Model0_net,
         const void *Model0_pkey);
struct Model0_neighbour *Model0___neigh_create(struct Model0_neigh_table *Model0_tbl, const void *Model0_pkey,
     struct Model0_net_device *Model0_dev, bool Model0_want_ref);
static inline __attribute__((no_instrument_function)) struct Model0_neighbour *Model0_neigh_create(struct Model0_neigh_table *Model0_tbl,
          const void *Model0_pkey,
          struct Model0_net_device *Model0_dev)
{
 return Model0___neigh_create(Model0_tbl, Model0_pkey, Model0_dev, true);
}
void Model0_neigh_destroy(struct Model0_neighbour *Model0_neigh);
int Model0___neigh_event_send(struct Model0_neighbour *Model0_neigh, struct Model0_sk_buff *Model0_skb);
int Model0_neigh_update(struct Model0_neighbour *Model0_neigh, const Model0_u8 *Model0_lladdr, Model0_u8 Model0_new, Model0_u32 Model0_flags);
void Model0___neigh_set_probe_once(struct Model0_neighbour *Model0_neigh);
void Model0_neigh_changeaddr(struct Model0_neigh_table *Model0_tbl, struct Model0_net_device *Model0_dev);
int Model0_neigh_ifdown(struct Model0_neigh_table *Model0_tbl, struct Model0_net_device *Model0_dev);
int Model0_neigh_resolve_output(struct Model0_neighbour *Model0_neigh, struct Model0_sk_buff *Model0_skb);
int Model0_neigh_connected_output(struct Model0_neighbour *Model0_neigh, struct Model0_sk_buff *Model0_skb);
int Model0_neigh_direct_output(struct Model0_neighbour *Model0_neigh, struct Model0_sk_buff *Model0_skb);
struct Model0_neighbour *Model0_neigh_event_ns(struct Model0_neigh_table *Model0_tbl,
      Model0_u8 *Model0_lladdr, void *Model0_saddr,
      struct Model0_net_device *Model0_dev);

struct Model0_neigh_parms *Model0_neigh_parms_alloc(struct Model0_net_device *Model0_dev,
          struct Model0_neigh_table *Model0_tbl);
void Model0_neigh_parms_release(struct Model0_neigh_table *Model0_tbl, struct Model0_neigh_parms *Model0_parms);

static inline __attribute__((no_instrument_function))
struct Model0_net *Model0_neigh_parms_net(const struct Model0_neigh_parms *Model0_parms)
{
 return Model0_read_pnet(&Model0_parms->Model0_net);
}

unsigned long Model0_neigh_rand_reach_time(unsigned long Model0_base);

void Model0_pneigh_enqueue(struct Model0_neigh_table *Model0_tbl, struct Model0_neigh_parms *Model0_p,
      struct Model0_sk_buff *Model0_skb);
struct Model0_pneigh_entry *Model0_pneigh_lookup(struct Model0_neigh_table *Model0_tbl, struct Model0_net *Model0_net,
       const void *Model0_key, struct Model0_net_device *Model0_dev,
       int Model0_creat);
struct Model0_pneigh_entry *Model0___pneigh_lookup(struct Model0_neigh_table *Model0_tbl, struct Model0_net *Model0_net,
         const void *Model0_key, struct Model0_net_device *Model0_dev);
int Model0_pneigh_delete(struct Model0_neigh_table *Model0_tbl, struct Model0_net *Model0_net, const void *Model0_key,
    struct Model0_net_device *Model0_dev);

static inline __attribute__((no_instrument_function)) struct Model0_net *Model0_pneigh_net(const struct Model0_pneigh_entry *Model0_pneigh)
{
 return Model0_read_pnet(&Model0_pneigh->Model0_net);
}

void Model0_neigh_app_ns(struct Model0_neighbour *Model0_n);
void Model0_neigh_for_each(struct Model0_neigh_table *Model0_tbl,
      void (*Model0_cb)(struct Model0_neighbour *, void *), void *Model0_cookie);
void Model0___neigh_for_each_release(struct Model0_neigh_table *Model0_tbl,
         int (*Model0_cb)(struct Model0_neighbour *));
int Model0_neigh_xmit(int Model0_fam, struct Model0_net_device *, const void *, struct Model0_sk_buff *);
void Model0_pneigh_for_each(struct Model0_neigh_table *Model0_tbl,
       void (*Model0_cb)(struct Model0_pneigh_entry *));

struct Model0_neigh_seq_state {
 struct Model0_seq_net_private Model0_p;
 struct Model0_neigh_table *Model0_tbl;
 struct Model0_neigh_hash_table *Model0_nht;
 void *(*Model0_neigh_sub_iter)(struct Model0_neigh_seq_state *Model0_state,
    struct Model0_neighbour *Model0_n, Model0_loff_t *Model0_pos);
 unsigned int Model0_bucket;
 unsigned int Model0_flags;



};
void *Model0_neigh_seq_start(struct Model0_seq_file *, Model0_loff_t *, struct Model0_neigh_table *,
        unsigned int);
void *Model0_neigh_seq_next(struct Model0_seq_file *, void *, Model0_loff_t *);
void Model0_neigh_seq_stop(struct Model0_seq_file *, void *);

int Model0_neigh_proc_dointvec(struct Model0_ctl_table *Model0_ctl, int Model0_write,
   void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);
int Model0_neigh_proc_dointvec_jiffies(struct Model0_ctl_table *Model0_ctl, int Model0_write,
    void *Model0_buffer,
    Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);
int Model0_neigh_proc_dointvec_ms_jiffies(struct Model0_ctl_table *Model0_ctl, int Model0_write,
       void *Model0_buffer,
       Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);

int Model0_neigh_sysctl_register(struct Model0_net_device *Model0_dev, struct Model0_neigh_parms *Model0_p,
     Model0_proc_handler *Model0_proc_handler);
void Model0_neigh_sysctl_unregister(struct Model0_neigh_parms *Model0_p);

static inline __attribute__((no_instrument_function)) void Model0___neigh_parms_put(struct Model0_neigh_parms *Model0_parms)
{
 Model0_atomic_dec(&Model0_parms->Model0_refcnt);
}

static inline __attribute__((no_instrument_function)) struct Model0_neigh_parms *Model0_neigh_parms_clone(struct Model0_neigh_parms *Model0_parms)
{
 Model0_atomic_inc(&Model0_parms->Model0_refcnt);
 return Model0_parms;
}

/*
 *	Neighbour references
 */

static inline __attribute__((no_instrument_function)) void Model0_neigh_release(struct Model0_neighbour *Model0_neigh)
{
 if (Model0_atomic_dec_and_test(&Model0_neigh->Model0_refcnt))
  Model0_neigh_destroy(Model0_neigh);
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour * Model0_neigh_clone(struct Model0_neighbour *Model0_neigh)
{
 if (Model0_neigh)
  Model0_atomic_inc(&Model0_neigh->Model0_refcnt);
 return Model0_neigh;
}



static inline __attribute__((no_instrument_function)) int Model0_neigh_event_send(struct Model0_neighbour *Model0_neigh, struct Model0_sk_buff *Model0_skb)
{
 unsigned long Model0_now = Model0_jiffies;

 if (Model0_neigh->Model0_used != Model0_now)
  Model0_neigh->Model0_used = Model0_now;
 if (!(Model0_neigh->Model0_nud_state&((0x80|0x40|0x02)|0x08|0x10)))
  return Model0___neigh_event_send(Model0_neigh, Model0_skb);
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model0_neigh_hh_output(const struct Model0_hh_cache *Model0_hh, struct Model0_sk_buff *Model0_skb)
{
 unsigned int Model0_seq;
 int Model0_hh_len;

 do {
  Model0_seq = Model0_read_seqbegin(&Model0_hh->Model0_hh_lock);
  Model0_hh_len = Model0_hh->Model0_hh_len;
  if (__builtin_expect(!!(Model0_hh_len <= 16), 1)) {
   /* this is inlined by gcc */
   ({ Model0_size_t Model0___len = (16); void *Model0___ret; if (__builtin_constant_p(16) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_skb->Model0_data - 16), (Model0_hh->Model0_hh_data), Model0___len); else Model0___ret = __builtin_memcpy((Model0_skb->Model0_data - 16), (Model0_hh->Model0_hh_data), Model0___len); Model0___ret; });
  } else {
   int Model0_hh_alen = (((Model0_hh_len)+(16 -1))&~(16 - 1));

   ({ Model0_size_t Model0___len = (Model0_hh_alen); void *Model0___ret; if (__builtin_constant_p(Model0_hh_alen) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_skb->Model0_data - Model0_hh_alen), (Model0_hh->Model0_hh_data), Model0___len); else Model0___ret = __builtin_memcpy((Model0_skb->Model0_data - Model0_hh_alen), (Model0_hh->Model0_hh_data), Model0___len); Model0___ret; });
  }
 } while (Model0_read_seqretry(&Model0_hh->Model0_hh_lock, Model0_seq));

 Model0_skb_push(Model0_skb, Model0_hh_len);
 return Model0_dev_queue_xmit(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour *
Model0___neigh_lookup(struct Model0_neigh_table *Model0_tbl, const void *Model0_pkey, struct Model0_net_device *Model0_dev, int Model0_creat)
{
 struct Model0_neighbour *Model0_n = Model0_neigh_lookup(Model0_tbl, Model0_pkey, Model0_dev);

 if (Model0_n || !Model0_creat)
  return Model0_n;

 Model0_n = Model0_neigh_create(Model0_tbl, Model0_pkey, Model0_dev);
 return Model0_IS_ERR(Model0_n) ? ((void *)0) : Model0_n;
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour *
Model0___neigh_lookup_errno(struct Model0_neigh_table *Model0_tbl, const void *Model0_pkey,
  struct Model0_net_device *Model0_dev)
{
 struct Model0_neighbour *Model0_n = Model0_neigh_lookup(Model0_tbl, Model0_pkey, Model0_dev);

 if (Model0_n)
  return Model0_n;

 return Model0_neigh_create(Model0_tbl, Model0_pkey, Model0_dev);
}

struct Model0_neighbour_cb {
 unsigned long Model0_sched_next;
 unsigned int Model0_flags;
};





static inline __attribute__((no_instrument_function)) void Model0_neigh_ha_snapshot(char *Model0_dst, const struct Model0_neighbour *Model0_n,
         const struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_seq;

 do {
  Model0_seq = Model0_read_seqbegin(&Model0_n->Model0_ha_lock);
  ({ Model0_size_t Model0___len = (Model0_dev->Model0_addr_len); void *Model0___ret; if (__builtin_constant_p(Model0_dev->Model0_addr_len) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_dst), (Model0_n->Model0_ha), Model0___len); else Model0___ret = __builtin_memcpy((Model0_dst), (Model0_n->Model0_ha), Model0___len); Model0___ret; });
 } while (Model0_read_seqretry(&Model0_n->Model0_ha_lock, Model0_seq));
}






/* Each dst_entry has reference count and sits in some parent list(s).
 * When it is removed from parent list, it is "freed" (dst_free).
 * After this it enters dead state (dst->obsolete > 0) and if its refcnt
 * is zero, it can be destroyed immediately, otherwise it is added
 * to gc list and garbage collector periodically checks the refcnt.
 */

struct Model0_sk_buff;

struct Model0_dst_entry {
 struct Model0_callback_head Model0_callback_head;
 struct Model0_dst_entry *Model0_child;
 struct Model0_net_device *Model0_dev;
 struct Model0_dst_ops *Model0_ops;
 unsigned long Model0__metrics;
 unsigned long Model0_expires;
 struct Model0_dst_entry *Model0_path;
 struct Model0_dst_entry *Model0_from;

 struct Model0_xfrm_state *Model0_xfrm;



 int (*Model0_input)(struct Model0_sk_buff *);
 int (*Model0_output)(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

 unsigned short Model0_flags;
 unsigned short Model0_pending_confirm;

 short error;

 /* A non-zero value of dst->obsolete forces by-hand validation
	 * of the route entry.  Positive values are set by the generic
	 * dst layer to indicate that the entry has been forcefully
	 * destroyed.
	 *
	 * Negative values are used by the implementation layer code to
	 * force invocation of the dst_ops->check() method.
	 */
 short Model0_obsolete;




 unsigned short Model0_header_len; /* more space at head required */
 unsigned short Model0_trailer_len; /* space to reserve at tail */



 __u32 Model0___pad2;



 /*
	 * Align __refcnt to a 64 bytes alignment
	 * (L1_CACHE_SIZE would be too much)
	 */
 long Model0___pad_to_align_refcnt[2];

 /*
	 * __refcnt wants to be on a different cache line from
	 * input/output/ops or performance tanks badly
	 */
 Model0_atomic_t Model0___refcnt; /* client references	*/
 int Model0___use;
 unsigned long Model0_lastuse;
 struct Model0_lwtunnel_state *Model0_lwtstate;
 union {
  struct Model0_dst_entry *Model0_next;
  struct Model0_rtable *Model0_rt_next;
  struct Model0_rt6_info *Model0_rt6_next;
  struct Model0_dn_route *Model0_dn_next;
 };
};

Model0_u32 *Model0_dst_cow_metrics_generic(struct Model0_dst_entry *Model0_dst, unsigned long old);
extern const Model0_u32 Model0_dst_default_metrics[];







static inline __attribute__((no_instrument_function)) bool Model0_dst_metrics_read_only(const struct Model0_dst_entry *Model0_dst)
{
 return Model0_dst->Model0__metrics & 0x1UL;
}

void Model0___dst_destroy_metrics_generic(struct Model0_dst_entry *Model0_dst, unsigned long old);

static inline __attribute__((no_instrument_function)) void Model0_dst_destroy_metrics_generic(struct Model0_dst_entry *Model0_dst)
{
 unsigned long Model0_val = Model0_dst->Model0__metrics;
 if (!(Model0_val & 0x1UL))
  Model0___dst_destroy_metrics_generic(Model0_dst, Model0_val);
}

static inline __attribute__((no_instrument_function)) Model0_u32 *Model0_dst_metrics_write_ptr(struct Model0_dst_entry *Model0_dst)
{
 unsigned long Model0_p = Model0_dst->Model0__metrics;

 do { if (__builtin_expect(!!(!Model0_p), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/net/dst.h"), "i" (137), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);

 if (Model0_p & 0x1UL)
  return Model0_dst->Model0_ops->Model0_cow_metrics(Model0_dst, Model0_p);
 return ((Model0_u32 *)((Model0_p) & ~0x3UL));
}

/* This may only be invoked before the entry has reached global
 * visibility.
 */
static inline __attribute__((no_instrument_function)) void Model0_dst_init_metrics(struct Model0_dst_entry *Model0_dst,
        const Model0_u32 *Model0_src_metrics,
        bool Model0_read_only)
{
 Model0_dst->Model0__metrics = ((unsigned long) Model0_src_metrics) |
  (Model0_read_only ? 0x1UL : 0);
}

static inline __attribute__((no_instrument_function)) void Model0_dst_copy_metrics(struct Model0_dst_entry *Model0_dest, const struct Model0_dst_entry *Model0_src)
{
 Model0_u32 *Model0_dst_metrics = Model0_dst_metrics_write_ptr(Model0_dest);

 if (Model0_dst_metrics) {
  Model0_u32 *Model0_src_metrics = ((Model0_u32 *)(((Model0_src)->Model0__metrics) & ~0x3UL));

  ({ Model0_size_t Model0___len = ((Model0___RTAX_MAX - 1) * sizeof(Model0_u32)); void *Model0___ret; if (__builtin_constant_p((Model0___RTAX_MAX - 1) * sizeof(Model0_u32)) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_dst_metrics), (Model0_src_metrics), Model0___len); else Model0___ret = __builtin_memcpy((Model0_dst_metrics), (Model0_src_metrics), Model0___len); Model0___ret; });
 }
}

static inline __attribute__((no_instrument_function)) Model0_u32 *Model0_dst_metrics_ptr(struct Model0_dst_entry *Model0_dst)
{
 return ((Model0_u32 *)(((Model0_dst)->Model0__metrics) & ~0x3UL));
}

static inline __attribute__((no_instrument_function)) Model0_u32
Model0_dst_metric_raw(const struct Model0_dst_entry *Model0_dst, const int Model0_metric)
{
 Model0_u32 *Model0_p = ((Model0_u32 *)(((Model0_dst)->Model0__metrics) & ~0x3UL));

 return Model0_p[Model0_metric-1];
}

static inline __attribute__((no_instrument_function)) Model0_u32
Model0_dst_metric(const struct Model0_dst_entry *Model0_dst, const int Model0_metric)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(Model0_metric == Model0_RTAX_HOPLIMIT || Model0_metric == Model0_RTAX_ADVMSS || Model0_metric == Model0_RTAX_MTU); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/net/dst.h", 184); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });


 return Model0_dst_metric_raw(Model0_dst, Model0_metric);
}

static inline __attribute__((no_instrument_function)) Model0_u32
Model0_dst_metric_advmss(const struct Model0_dst_entry *Model0_dst)
{
#if CY_ABSTRACT3 //advmss is from device, which is too low details
     return 1460; //MTU (1500) - TCP header (20) - IP header (20)
#else
 Model0_u32 Model0_advmss = Model0_dst_metric_raw(Model0_dst, Model0_RTAX_ADVMSS);

 if (!Model0_advmss)
  Model0_advmss = Model0_dst->Model0_ops->Model0_default_advmss(Model0_dst);

 return Model0_advmss;
#endif
}

static inline __attribute__((no_instrument_function)) void Model0_dst_metric_set(struct Model0_dst_entry *Model0_dst, int Model0_metric, Model0_u32 Model0_val)
{
 Model0_u32 *Model0_p = Model0_dst_metrics_write_ptr(Model0_dst);

 if (Model0_p)
  Model0_p[Model0_metric-1] = Model0_val;
}

/* Kernel-internal feature bits that are unallocated in user space. */





static inline __attribute__((no_instrument_function)) Model0_u32
Model0_dst_feature(const struct Model0_dst_entry *Model0_dst, Model0_u32 Model0_feature)
{
 return Model0_dst_metric(Model0_dst, Model0_RTAX_FEATURES) & Model0_feature;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_dst_mtu(const struct Model0_dst_entry *Model0_dst)
{
 return Model0_dst->Model0_ops->Model0_mtu(Model0_dst);
}

/* RTT metrics are stored in milliseconds for user ABI, but used as jiffies */
static inline __attribute__((no_instrument_function)) unsigned long Model0_dst_metric_rtt(const struct Model0_dst_entry *Model0_dst, int Model0_metric)
{
 return Model0_msecs_to_jiffies(Model0_dst_metric(Model0_dst, Model0_metric));
}

static inline __attribute__((no_instrument_function)) Model0_u32
Model0_dst_allfrag(const struct Model0_dst_entry *Model0_dst)
{
 int Model0_ret = Model0_dst_feature(Model0_dst, (1 << 3));
 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) int
Model0_dst_metric_locked(const struct Model0_dst_entry *Model0_dst, int Model0_metric)
{
 return Model0_dst_metric(Model0_dst, Model0_RTAX_LOCK) & (1<<Model0_metric);
}

static inline __attribute__((no_instrument_function)) void Model0_dst_hold(struct Model0_dst_entry *Model0_dst)
{
 /*
	 * If your kernel compilation stops here, please check
	 * __pad_to_align_refcnt declaration in struct dst_entry
	 */
 do { bool Model0___cond = !(!(__builtin_offsetof(struct Model0_dst_entry, Model0___refcnt) & 63)); extern void Model0___compiletime_assert_249(void) ; if (Model0___cond) Model0___compiletime_assert_249(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 Model0_atomic_inc(&Model0_dst->Model0___refcnt);
}

static inline __attribute__((no_instrument_function)) void Model0_dst_use(struct Model0_dst_entry *Model0_dst, unsigned long Model0_time)
{
 Model0_dst_hold(Model0_dst);
 Model0_dst->Model0___use++;
 Model0_dst->Model0_lastuse = Model0_time;
}

static inline __attribute__((no_instrument_function)) void Model0_dst_use_noref(struct Model0_dst_entry *Model0_dst, unsigned long Model0_time)
{
 Model0_dst->Model0___use++;
 Model0_dst->Model0_lastuse = Model0_time;
}

static inline __attribute__((no_instrument_function)) struct Model0_dst_entry *Model0_dst_clone(struct Model0_dst_entry *Model0_dst)
{
 if (Model0_dst)
  Model0_atomic_inc(&Model0_dst->Model0___refcnt);
 return Model0_dst;
}

void Model0_dst_release(struct Model0_dst_entry *Model0_dst);

static inline __attribute__((no_instrument_function)) void Model0_refdst_drop(unsigned long Model0_refdst)
{
 if (!(Model0_refdst & 1UL))
  Model0_dst_release((struct Model0_dst_entry *)(Model0_refdst & ~(1UL)));
}

/**
 * skb_dst_drop - drops skb dst
 * @skb: buffer
 *
 * Drops dst reference count if a reference was taken.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_dst_drop(struct Model0_sk_buff *Model0_skb)
{
 #if CY_ABSTRACT1
 #else
 if (Model0_skb->Model0__skb_refdst) {
  Model0_refdst_drop(Model0_skb->Model0__skb_refdst);
  Model0_skb->Model0__skb_refdst = 0UL;
 }
 #endif
}

static inline __attribute__((no_instrument_function)) void Model0___skb_dst_copy(struct Model0_sk_buff *Model0_nskb, unsigned long Model0_refdst)
{
 Model0_nskb->Model0__skb_refdst = Model0_refdst;
 if (!(Model0_nskb->Model0__skb_refdst & 1UL))
  Model0_dst_clone(Model0_skb_dst(Model0_nskb));
}

static inline __attribute__((no_instrument_function)) void Model0_skb_dst_copy(struct Model0_sk_buff *Model0_nskb, const struct Model0_sk_buff *Model0_oskb)
{
 Model0___skb_dst_copy(Model0_nskb, Model0_oskb->Model0__skb_refdst);
}

/**
 * skb_dst_force - makes sure skb dst is refcounted
 * @skb: buffer
 *
 * If dst is not yet refcounted, let's do it
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_dst_force(struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb_dst_is_noref(Model0_skb)) {
  ({ int Model0___ret_warn_on = !!(!Model0_rcu_read_lock_held()); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/net/dst.h", 316); __builtin_expect(!!(Model0___ret_warn_on), 0); });
  Model0_skb->Model0__skb_refdst &= ~1UL;
  Model0_dst_clone(Model0_skb_dst(Model0_skb));
 }
}

/**
 * dst_hold_safe - Take a reference on a dst if possible
 * @dst: pointer to dst entry
 *
 * This helper returns false if it could not safely
 * take a reference on a dst.
 */
static inline __attribute__((no_instrument_function)) bool Model0_dst_hold_safe(struct Model0_dst_entry *Model0_dst)
{
 if (Model0_dst->Model0_flags & 0x0010)
  return Model0_atomic_add_unless((&Model0_dst->Model0___refcnt), 1, 0);
 Model0_dst_hold(Model0_dst);
 return true;
}

/**
 * skb_dst_force_safe - makes sure skb dst is refcounted
 * @skb: buffer
 *
 * If dst is not yet refcounted and not destroyed, grab a ref on it.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_dst_force_safe(struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb_dst_is_noref(Model0_skb)) {
  struct Model0_dst_entry *Model0_dst = Model0_skb_dst(Model0_skb);

  if (!Model0_dst_hold_safe(Model0_dst))
   Model0_dst = ((void *)0);

  Model0_skb->Model0__skb_refdst = (unsigned long)Model0_dst;
 }
}


/**
 *	__skb_tunnel_rx - prepare skb for rx reinsert
 *	@skb: buffer
 *	@dev: tunnel device
 *	@net: netns for packet i/o
 *
 *	After decapsulation, packet is going to re-enter (netif_rx()) our stack,
 *	so make some cleanups. (no accounting done)
 */
static inline __attribute__((no_instrument_function)) void Model0___skb_tunnel_rx(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
       struct Model0_net *Model0_net)
{
 Model0_skb->Model0_dev = Model0_dev;

 /*
	 * Clear hash so that we can recalulate the hash for the
	 * encapsulated packet, unless we have already determine the hash
	 * over the L4 4-tuple.
	 */
 Model0_skb_clear_hash_if_not_l4(Model0_skb);
 Model0_skb_set_queue_mapping(Model0_skb, 0);
 Model0_skb_scrub_packet(Model0_skb, !Model0_net_eq(Model0_net, Model0_dev_net(Model0_dev)));
}

/**
 *	skb_tunnel_rx - prepare skb for rx reinsert
 *	@skb: buffer
 *	@dev: tunnel device
 *
 *	After decapsulation, packet is going to re-enter (netif_rx()) our stack,
 *	so make some cleanups, and perform accounting.
 *	Note: this accounting is not SMP safe.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_tunnel_rx(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
     struct Model0_net *Model0_net)
{
 /* TODO : stats should be SMP safe */
 Model0_dev->Model0_stats.Model0_rx_packets++;
 Model0_dev->Model0_stats.Model0_rx_bytes += Model0_skb->Model0_len;
 Model0___skb_tunnel_rx(Model0_skb, Model0_dev, Model0_net);
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_dst_tclassid(const struct Model0_sk_buff *Model0_skb)
{







 return 0;
}

int Model0_dst_discard_out(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
static inline __attribute__((no_instrument_function)) int Model0_dst_discard(struct Model0_sk_buff *Model0_skb)
{
 return Model0_dst_discard_out(&Model0_init_net, Model0_skb->Model0_sk, Model0_skb);
}
void *Model0_dst_alloc(struct Model0_dst_ops *Model0_ops, struct Model0_net_device *Model0_dev, int Model0_initial_ref,
  int Model0_initial_obsolete, unsigned short Model0_flags);
void Model0_dst_init(struct Model0_dst_entry *Model0_dst, struct Model0_dst_ops *Model0_ops,
       struct Model0_net_device *Model0_dev, int Model0_initial_ref, int Model0_initial_obsolete,
       unsigned short Model0_flags);
void Model0___dst_free(struct Model0_dst_entry *Model0_dst);
struct Model0_dst_entry *Model0_dst_destroy(struct Model0_dst_entry *Model0_dst);

static inline __attribute__((no_instrument_function)) void Model0_dst_free(struct Model0_dst_entry *Model0_dst)
{
 if (Model0_dst->Model0_obsolete > 0)
  return;
 if (!Model0_atomic_read(&Model0_dst->Model0___refcnt)) {
  Model0_dst = Model0_dst_destroy(Model0_dst);
  if (!Model0_dst)
   return;
 }
 Model0___dst_free(Model0_dst);
}

static inline __attribute__((no_instrument_function)) void Model0_dst_rcu_free(struct Model0_callback_head *Model0_head)
{
 struct Model0_dst_entry *Model0_dst = ({ const typeof( ((struct Model0_dst_entry *)0)->Model0_callback_head ) *Model0___mptr = (Model0_head); (struct Model0_dst_entry *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_dst_entry, Model0_callback_head) );});
 Model0_dst_free(Model0_dst);
}

static inline __attribute__((no_instrument_function)) void Model0_dst_confirm(struct Model0_dst_entry *Model0_dst)
{
 Model0_dst->Model0_pending_confirm = 1;
}

static inline __attribute__((no_instrument_function)) int Model0_dst_neigh_output(struct Model0_dst_entry *Model0_dst, struct Model0_neighbour *Model0_n,
       struct Model0_sk_buff *Model0_skb)
{
 const struct Model0_hh_cache *Model0_hh;

 if (Model0_dst->Model0_pending_confirm) {
  unsigned long Model0_now = Model0_jiffies;

  Model0_dst->Model0_pending_confirm = 0;
  /* avoid dirtying neighbour */
  if (Model0_n->Model0_confirmed != Model0_now)
   Model0_n->Model0_confirmed = Model0_now;
 }

 Model0_hh = &Model0_n->Model0_hh;
 if ((Model0_n->Model0_nud_state & (0x80|0x40|0x02)) && Model0_hh->Model0_hh_len)
  return Model0_neigh_hh_output(Model0_hh, Model0_skb);
 else
  return Model0_n->Model0_output(Model0_n, Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour *Model0_dst_neigh_lookup(const struct Model0_dst_entry *Model0_dst, const void *Model0_daddr)
{
 struct Model0_neighbour *Model0_n = Model0_dst->Model0_ops->Model0_neigh_lookup(Model0_dst, ((void *)0), Model0_daddr);
 return Model0_IS_ERR(Model0_n) ? ((void *)0) : Model0_n;
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour *Model0_dst_neigh_lookup_skb(const struct Model0_dst_entry *Model0_dst,
           struct Model0_sk_buff *Model0_skb)
{
 struct Model0_neighbour *Model0_n = Model0_dst->Model0_ops->Model0_neigh_lookup(Model0_dst, Model0_skb, ((void *)0));
 return Model0_IS_ERR(Model0_n) ? ((void *)0) : Model0_n;
}

static inline __attribute__((no_instrument_function)) void Model0_dst_link_failure(struct Model0_sk_buff *Model0_skb)
{
 struct Model0_dst_entry *Model0_dst = Model0_skb_dst(Model0_skb);
 if (Model0_dst && Model0_dst->Model0_ops && Model0_dst->Model0_ops->Model0_link_failure)
  Model0_dst->Model0_ops->Model0_link_failure(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_dst_set_expires(struct Model0_dst_entry *Model0_dst, int Model0_timeout)
{
 unsigned long Model0_expires = Model0_jiffies + Model0_timeout;

 if (Model0_expires == 0)
  Model0_expires = 1;

 if (Model0_dst->Model0_expires == 0 || (({ unsigned long Model0___dummy; typeof(Model0_dst->Model0_expires) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }) && ({ unsigned long Model0___dummy; typeof(Model0_expires) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }) && ((long)((Model0_expires) - (Model0_dst->Model0_expires)) < 0)))
  Model0_dst->Model0_expires = Model0_expires;
}

/* Output packet to network from transport.  */
static inline __attribute__((no_instrument_function)) int Model0_dst_output(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_dst(Model0_skb)->Model0_output(Model0_net, Model0_sk, Model0_skb);
}

/* Input packet from network to transport.  */
static inline __attribute__((no_instrument_function)) int Model0_dst_input(struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_dst(Model0_skb)->Model0_input(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_dst_entry *Model0_dst_check(struct Model0_dst_entry *Model0_dst, Model0_u32 Model0_cookie)
{
 if (Model0_dst->Model0_obsolete)
  Model0_dst = Model0_dst->Model0_ops->Model0_check(Model0_dst, Model0_cookie);
 return Model0_dst;
}

void Model0_dst_subsys_init(void);

/* Flags for xfrm_lookup flags argument. */
enum {
 Model0_XFRM_LOOKUP_ICMP = 1 << 0,
 Model0_XFRM_LOOKUP_QUEUE = 1 << 1,
 Model0_XFRM_LOOKUP_KEEP_DST_REF = 1 << 2,
};

struct Model0_flowi;
struct Model0_dst_entry *Model0_xfrm_lookup(struct Model0_net *Model0_net, struct Model0_dst_entry *Model0_dst_orig,
         const struct Model0_flowi *Model0_fl, const struct Model0_sock *Model0_sk,
         int Model0_flags);

struct Model0_dst_entry *Model0_xfrm_lookup_route(struct Model0_net *Model0_net, struct Model0_dst_entry *Model0_dst_orig,
        const struct Model0_flowi *Model0_fl, const struct Model0_sock *Model0_sk,
        int Model0_flags);

/* skb attached with this dst needs transformation if dst->xfrm is valid */
static inline __attribute__((no_instrument_function)) struct Model0_xfrm_state *Model0_dst_xfrm(const struct Model0_dst_entry *Model0_dst)
{
 return Model0_dst->Model0_xfrm;
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the TCP module.
 *
 * Version:	@(#)tcp.h	1.0.5	05/23/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the TCP protocol.
 *
 * Version:	@(#)tcp.h	1.0.2	04/28/93
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the AF_INET socket handler.
 *
 * Version:	@(#)sock.h	1.0.4	05/13/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Corey Minyard <wf-rch!minyard@relay.EU.net>
 *		Florian La Roche <flla@stud.uni-sb.de>
 *
 * Fixes:
 *		Alan Cox	:	Volatiles in skbuff pointers. See
 *					skbuff comments. May be overdone,
 *					better to prove they can be removed
 *					than the reverse.
 *		Alan Cox	:	Added a zapped field for tcp to note
 *					a socket is reset and must stay shut up
 *		Alan Cox	:	New fields for options
 *	Pauline Middelink	:	identd support
 *		Alan Cox	:	Eliminate low level recv/recvfrom
 *		David S. Miller	:	New socket lookup architecture.
 *              Steve Whitehouse:       Default routines for sock_ops
 *              Arnaldo C. Melo :	removed net_pinfo, tp_pinfo and made
 *              			protinfo be just a void pointer, as the
 *              			protocol specific parts were moved to
 *              			respective headers and ipv4/v6, etc now
 *              			use private slabcaches for its socks
 *              Pedro Hortas	:	New flags field for socket options
 *
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_pagefault_disabled_inc(void)
{
 Model0_get_current()->Model0_pagefault_disabled++;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_pagefault_disabled_dec(void)
{
 Model0_get_current()->Model0_pagefault_disabled--;
 ({ int Model0___ret_warn_on = !!(Model0_get_current()->Model0_pagefault_disabled < 0); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/uaccess.h", 15); __builtin_expect(!!(Model0___ret_warn_on), 0); });
}

/*
 * These routines enable/disable the pagefault handler. If disabled, it will
 * not take any locks and go straight to the fixup table.
 *
 * User access methods will not sleep when called from a pagefault_disabled()
 * environment.
 */
static inline __attribute__((no_instrument_function)) void Model0_pagefault_disable(void)
{
 Model0_pagefault_disabled_inc();
 /*
	 * make sure to have issued the store before a pagefault
	 * can hit.
	 */
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model0_pagefault_enable(void)
{
 /*
	 * make sure to issue those last loads/stores before enabling
	 * the pagefault handler again.
	 */
 __asm__ __volatile__("": : :"memory");
 Model0_pagefault_disabled_dec();
}

/*
 * Is the pagefault handler disabled? If so, user access methods will not sleep.
 */


/*
 * The pagefault handler is in general disabled by pagefault_disable() or
 * when in irq context (via in_atomic()).
 *
 * This function should only be used by the fault handlers. Other users should
 * stick to pagefault_disabled().
 * Please NEVER use preempt_disable() to disable the fault handler. With
 * !CONFIG_PREEMPT_COUNT, this is like a NOP. So the handler won't be disabled.
 * in_atomic() will report different values based on !CONFIG_PREEMPT_COUNT.
 */
/*
 * probe_kernel_read(): safely attempt to read from a location
 * @dst: pointer to the buffer that shall take the data
 * @src: address to read from
 * @size: size of the data chunk
 *
 * Safely read from address @src to the buffer at @dst.  If a kernel fault
 * happens, handle that and return -EFAULT.
 */
extern long Model0_probe_kernel_read(void *Model0_dst, const void *Model0_src, Model0_size_t Model0_size);
extern long Model0___probe_kernel_read(void *Model0_dst, const void *Model0_src, Model0_size_t Model0_size);

/*
 * probe_kernel_write(): safely attempt to write to a location
 * @dst: address to write to
 * @src: pointer to the data that shall be written
 * @size: size of the data chunk
 *
 * Safely write to address @dst from the buffer at @src.  If a kernel fault
 * happens, handle that and return -EFAULT.
 */
extern long __attribute__((no_instrument_function)) Model0_probe_kernel_write(void *Model0_dst, const void *Model0_src, Model0_size_t Model0_size);
extern long __attribute__((no_instrument_function)) Model0___probe_kernel_write(void *Model0_dst, const void *Model0_src, Model0_size_t Model0_size);

extern long Model0_strncpy_from_unsafe(char *Model0_dst, const void *Model0_unsafe_addr, long Model0_count);

/**
 * probe_kernel_address(): safely attempt to read from a location
 * @addr: address to read from
 * @retval: read into this variable
 *
 * Returns 0 on success, or -EFAULT.
 */







struct Model0_page_counter {
 Model0_atomic_long_t Model0_count;
 unsigned long Model0_limit;
 struct Model0_page_counter *Model0_parent;

 /* legacy */
 unsigned long Model0_watermark;
 unsigned long Model0_failcnt;
};







static inline __attribute__((no_instrument_function)) void Model0_page_counter_init(struct Model0_page_counter *Model0_counter,
         struct Model0_page_counter *Model0_parent)
{
 Model0_atomic_long_set(&Model0_counter->Model0_count, 0);
 Model0_counter->Model0_limit = (((long)(~0UL>>1)) / ((1UL) << 12));
 Model0_counter->Model0_parent = Model0_parent;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_page_counter_read(struct Model0_page_counter *Model0_counter)
{
 return Model0_atomic_long_read(&Model0_counter->Model0_count);
}

void Model0_page_counter_cancel(struct Model0_page_counter *Model0_counter, unsigned long Model0_nr_pages);
void Model0_page_counter_charge(struct Model0_page_counter *Model0_counter, unsigned long Model0_nr_pages);
bool Model0_page_counter_try_charge(struct Model0_page_counter *Model0_counter,
        unsigned long Model0_nr_pages,
        struct Model0_page_counter **Model0_fail);
void Model0_page_counter_uncharge(struct Model0_page_counter *Model0_counter, unsigned long Model0_nr_pages);
int Model0_page_counter_limit(struct Model0_page_counter *Model0_counter, unsigned long Model0_limit);
int Model0_page_counter_memparse(const char *Model0_buf, const char *Model0_max,
     unsigned long *Model0_nr_pages);

static inline __attribute__((no_instrument_function)) void Model0_page_counter_reset_watermark(struct Model0_page_counter *Model0_counter)
{
 Model0_counter->Model0_watermark = Model0_page_counter_read(Model0_counter);
}
/* memcontrol.h - Memory Controller
 *
 * Copyright IBM Corporation, 2007
 * Author Balbir Singh <balbir@linux.vnet.ibm.com>
 *
 * Copyright 2007 OpenVZ SWsoft Inc
 * Author: Pavel Emelianov <xemul@openvz.org>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 */
/*
 *  include/linux/eventfd.h
 *
 *  Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>
 *
 */







/*
 * CAREFUL: Check include/uapi/asm-generic/fcntl.h when defining
 * new flags, since they might collide with O_* ones. We want
 * to re-use O_* flags that couldn't possibly have a meaning
 * from eventfd, in order to leave a free define-space for
 * shared O_* flags.
 */







struct Model0_file;



struct Model0_file *Model0_eventfd_file_create(unsigned int Model0_count, int Model0_flags);
struct Model0_eventfd_ctx *Model0_eventfd_ctx_get(struct Model0_eventfd_ctx *Model0_ctx);
void Model0_eventfd_ctx_put(struct Model0_eventfd_ctx *Model0_ctx);
struct Model0_file *Model0_eventfd_fget(int Model0_fd);
struct Model0_eventfd_ctx *Model0_eventfd_ctx_fdget(int Model0_fd);
struct Model0_eventfd_ctx *Model0_eventfd_ctx_fileget(struct Model0_file *Model0_file);
__u64 Model0_eventfd_signal(struct Model0_eventfd_ctx *Model0_ctx, __u64 Model0_n);
Model0_ssize_t Model0_eventfd_ctx_read(struct Model0_eventfd_ctx *Model0_ctx, int Model0_no_wait, __u64 *Model0_cnt);
int Model0_eventfd_ctx_remove_wait_queue(struct Model0_eventfd_ctx *Model0_ctx, Model0_wait_queue_t *Model0_wait,
      __u64 *Model0_cnt);

struct Model0_vmpressure {
 unsigned long Model0_scanned;
 unsigned long Model0_reclaimed;

 unsigned long Model0_tree_scanned;
 unsigned long Model0_tree_reclaimed;
 /* The lock is used to keep the scanned/reclaimed above in sync. */
 struct Model0_spinlock Model0_sr_lock;

 /* The list of vmpressure_event structs. */
 struct Model0_list_head Model0_events;
 /* Have to grab the lock on events traversal or modifications. */
 struct Model0_mutex Model0_events_lock;

 struct Model0_work_struct Model0_work;
};

struct Model0_mem_cgroup;
static inline __attribute__((no_instrument_function)) void Model0_vmpressure(Model0_gfp_t Model0_gfp, struct Model0_mem_cgroup *Model0_memcg, bool Model0_tree,
         unsigned long Model0_scanned, unsigned long Model0_reclaimed) {}
static inline __attribute__((no_instrument_function)) void Model0_vmpressure_prio(Model0_gfp_t Model0_gfp, struct Model0_mem_cgroup *Model0_memcg,
       int Model0_prio) {}


/*
 * include/linux/writeback.h
 */







/*
 * Floating proportions with flexible aging period
 *
 *  Copyright (C) 2011, SUSE, Jan Kara <jack@suse.cz>
 */
/*
 * When maximum proportion of some event type is specified, this is the
 * precision with which we allow limitting. Note that this creates an upper
 * bound on the number of events per period like
 *   ULLONG_MAX >> FPROP_FRAC_SHIFT.
 */



/*
 * ---- Global proportion definitions ----
 */
struct Model0_fprop_global {
 /* Number of events in the current period */
 struct Model0_percpu_counter Model0_events;
 /* Current period */
 unsigned int Model0_period;
 /* Synchronization with period transitions */
 Model0_seqcount_t Model0_sequence;
};

int Model0_fprop_global_init(struct Model0_fprop_global *Model0_p, Model0_gfp_t Model0_gfp);
void Model0_fprop_global_destroy(struct Model0_fprop_global *Model0_p);
bool Model0_fprop_new_period(struct Model0_fprop_global *Model0_p, int Model0_periods);

/*
 *  ---- SINGLE ----
 */
struct Model0_fprop_local_single {
 /* the local events counter */
 unsigned long Model0_events;
 /* Period in which we last updated events */
 unsigned int Model0_period;
 Model0_raw_spinlock_t Model0_lock; /* Protect period and numerator */
};





int Model0_fprop_local_init_single(struct Model0_fprop_local_single *Model0_pl);
void Model0_fprop_local_destroy_single(struct Model0_fprop_local_single *Model0_pl);
void Model0___fprop_inc_single(struct Model0_fprop_global *Model0_p, struct Model0_fprop_local_single *Model0_pl);
void Model0_fprop_fraction_single(struct Model0_fprop_global *Model0_p,
 struct Model0_fprop_local_single *Model0_pl, unsigned long *Model0_numerator,
 unsigned long *Model0_denominator);

static inline __attribute__((no_instrument_function))
void Model0_fprop_inc_single(struct Model0_fprop_global *Model0_p, struct Model0_fprop_local_single *Model0_pl)
{
 unsigned long Model0_flags;

 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_flags = Model0_arch_local_irq_save(); } while (0); } while (0);
 Model0___fprop_inc_single(Model0_p, Model0_pl);
 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_arch_local_irq_restore(Model0_flags); } while (0); } while (0);
}

/*
 * ---- PERCPU ----
 */
struct Model0_fprop_local_percpu {
 /* the local events counter */
 struct Model0_percpu_counter Model0_events;
 /* Period in which we last updated events */
 unsigned int Model0_period;
 Model0_raw_spinlock_t Model0_lock; /* Protect period and numerator */
};

int Model0_fprop_local_init_percpu(struct Model0_fprop_local_percpu *Model0_pl, Model0_gfp_t Model0_gfp);
void Model0_fprop_local_destroy_percpu(struct Model0_fprop_local_percpu *Model0_pl);
void Model0___fprop_inc_percpu(struct Model0_fprop_global *Model0_p, struct Model0_fprop_local_percpu *Model0_pl);
void Model0___fprop_inc_percpu_max(struct Model0_fprop_global *Model0_p, struct Model0_fprop_local_percpu *Model0_pl,
       int Model0_max_frac);
void Model0_fprop_fraction_percpu(struct Model0_fprop_global *Model0_p,
 struct Model0_fprop_local_percpu *Model0_pl, unsigned long *Model0_numerator,
 unsigned long *Model0_denominator);

static inline __attribute__((no_instrument_function))
void Model0_fprop_inc_percpu(struct Model0_fprop_global *Model0_p, struct Model0_fprop_local_percpu *Model0_pl)
{
 unsigned long Model0_flags;

 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_flags = Model0_arch_local_irq_save(); } while (0); } while (0);
 Model0___fprop_inc_percpu(Model0_p, Model0_pl);
 do { do { ({ unsigned long Model0___dummy; typeof(Model0_flags) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }); Model0_arch_local_irq_restore(Model0_flags); } while (0); } while (0);
}
struct Model0_page;
struct Model0_device;
struct Model0_dentry;

/*
 * Bits in bdi_writeback.state
 */
enum Model0_wb_state {
 Model0_WB_registered, /* bdi_register() was done */
 Model0_WB_writeback_running, /* Writeback is in progress */
 Model0_WB_has_dirty_io, /* Dirty inodes on ->b_{dirty|io|more_io} */
};

enum Model0_wb_congested_state {
 Model0_WB_async_congested, /* The async (write) queue is getting full */
 Model0_WB_sync_congested, /* The sync queue is getting full */
};

typedef int (Model0_congested_fn)(void *, int);

enum Model0_wb_stat_item {
 Model0_WB_RECLAIMABLE,
 Model0_WB_WRITEBACK,
 Model0_WB_DIRTIED,
 Model0_WB_WRITTEN,
 Model0_NR_WB_STAT_ITEMS
};



/*
 * For cgroup writeback, multiple wb's may map to the same blkcg.  Those
 * wb's can operate mostly independently but should share the congested
 * state.  To facilitate such sharing, the congested state is tracked using
 * the following struct which is created on demand, indexed by blkcg ID on
 * its bdi, and refcounted.
 */
struct Model0_bdi_writeback_congested {
 unsigned long Model0_state; /* WB_[a]sync_congested flags */
 Model0_atomic_t Model0_refcnt; /* nr of attached wb's and blkg */






};

/*
 * Each wb (bdi_writeback) can perform writeback operations, is measured
 * and throttled, independently.  Without cgroup writeback, each bdi
 * (bdi_writeback) is served by its embedded bdi->wb.
 *
 * On the default hierarchy, blkcg implicitly enables memcg.  This allows
 * using memcg's page ownership for attributing writeback IOs, and every
 * memcg - blkcg combination can be served by its own wb by assigning a
 * dedicated wb to each memcg, which enables isolation across different
 * cgroups and propagation of IO back pressure down from the IO layer upto
 * the tasks which are generating the dirty pages to be written back.
 *
 * A cgroup wb is indexed on its bdi by the ID of the associated memcg,
 * refcounted with the number of inodes attached to it, and pins the memcg
 * and the corresponding blkcg.  As the corresponding blkcg for a memcg may
 * change as blkcg is disabled and enabled higher up in the hierarchy, a wb
 * is tested for blkcg after lookup and removed from index on mismatch so
 * that a new wb for the combination can be created.
 */
struct Model0_bdi_writeback {
 struct Model0_backing_dev_info *Model0_bdi; /* our parent bdi */

 unsigned long Model0_state; /* Always use atomic bitops on this */
 unsigned long Model0_last_old_flush; /* last old data flush */

 struct Model0_list_head Model0_b_dirty; /* dirty inodes */
 struct Model0_list_head Model0_b_io; /* parked for writeback */
 struct Model0_list_head Model0_b_more_io; /* parked for more writeback */
 struct Model0_list_head Model0_b_dirty_time; /* time stamps are dirty */
 Model0_spinlock_t Model0_list_lock; /* protects the b_* lists */

 struct Model0_percpu_counter Model0_stat[Model0_NR_WB_STAT_ITEMS];

 struct Model0_bdi_writeback_congested *Model0_congested;

 unsigned long Model0_bw_time_stamp; /* last time write bw is updated */
 unsigned long Model0_dirtied_stamp;
 unsigned long Model0_written_stamp; /* pages written at bw_time_stamp */
 unsigned long Model0_write_bandwidth; /* the estimated write bandwidth */
 unsigned long Model0_avg_write_bandwidth; /* further smoothed write bw, > 0 */

 /*
	 * The base dirty throttle rate, re-calculated on every 200ms.
	 * All the bdi tasks' dirty rate will be curbed under it.
	 * @dirty_ratelimit tracks the estimated @balanced_dirty_ratelimit
	 * in small steps and is much more smooth/stable than the latter.
	 */
 unsigned long Model0_dirty_ratelimit;
 unsigned long Model0_balanced_dirty_ratelimit;

 struct Model0_fprop_local_percpu Model0_completions;
 int Model0_dirty_exceeded;

 Model0_spinlock_t Model0_work_lock; /* protects work_list & dwork scheduling */
 struct Model0_list_head Model0_work_list;
 struct Model0_delayed_work Model0_dwork; /* work item used for writeback */

 struct Model0_list_head Model0_bdi_node; /* anchored at bdi->wb_list */
};

struct Model0_backing_dev_info {
 struct Model0_list_head Model0_bdi_list;
 unsigned long Model0_ra_pages; /* max readahead in PAGE_SIZE units */
 unsigned int Model0_capabilities; /* Device capabilities */
 Model0_congested_fn *Model0_congested_fn; /* Function pointer if device is md/dm */
 void *Model0_congested_data; /* Pointer to aux data for congested func */

 char *Model0_name;

 unsigned int Model0_min_ratio;
 unsigned int Model0_max_ratio, Model0_max_prop_frac;

 /*
	 * Sum of avg_write_bw of wbs with dirty inodes.  > 0 if there are
	 * any dirty wbs, which is depended upon by bdi_has_dirty().
	 */
 Model0_atomic_long_t Model0_tot_write_bandwidth;

 struct Model0_bdi_writeback Model0_wb; /* the root writeback info for this bdi */
 struct Model0_list_head Model0_wb_list; /* list of all wbs */





 struct Model0_bdi_writeback_congested *Model0_wb_congested;

 Model0_wait_queue_head_t Model0_wb_waitq;

 struct Model0_device *Model0_dev;
 struct Model0_device *Model0_owner;

 struct Model0_timer_list Model0_laptop_mode_wb_timer;


 struct Model0_dentry *Model0_debug_dir;
 struct Model0_dentry *Model0_debug_stats;

};

enum {
 Model0_BLK_RW_ASYNC = 0,
 Model0_BLK_RW_SYNC = 1,
};

void Model0_clear_wb_congested(struct Model0_bdi_writeback_congested *Model0_congested, int Model0_sync);
void Model0_set_wb_congested(struct Model0_bdi_writeback_congested *Model0_congested, int Model0_sync);

static inline __attribute__((no_instrument_function)) void Model0_clear_bdi_congested(struct Model0_backing_dev_info *Model0_bdi, int Model0_sync)
{
 Model0_clear_wb_congested(Model0_bdi->Model0_wb.Model0_congested, Model0_sync);
}

static inline __attribute__((no_instrument_function)) void Model0_set_bdi_congested(struct Model0_backing_dev_info *Model0_bdi, int Model0_sync)
{
 Model0_set_wb_congested(Model0_bdi->Model0_wb.Model0_congested, Model0_sync);
}
static inline __attribute__((no_instrument_function)) bool Model0_wb_tryget(struct Model0_bdi_writeback *Model0_wb)
{
 return true;
}

static inline __attribute__((no_instrument_function)) void Model0_wb_get(struct Model0_bdi_writeback *Model0_wb)
{
}

static inline __attribute__((no_instrument_function)) void Model0_wb_put(struct Model0_bdi_writeback *Model0_wb)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_wb_dying(struct Model0_bdi_writeback *Model0_wb)
{
 return false;
}

extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model0_dirty_throttle_leaks;

/*
 * The 1/4 region under the global dirty thresh is for smooth dirty throttling:
 *
 *	(thresh - thresh/DIRTY_FULL_SCOPE, thresh)
 *
 * Further beyond, all dirtier tasks will enter a loop waiting (possibly long
 * time) for the dirty pages to drop, unless written enough pages.
 *
 * The global dirty threshold is normally equal to the global dirty limit,
 * except when the system suddenly allocates a lot of anonymous memory and
 * knocks down the global dirty threshold quickly, in which case the global
 * dirty limit will follow down slowly to prevent livelocking all dirtier tasks.
 */



struct Model0_backing_dev_info;

/*
 * fs/fs-writeback.c
 */
enum Model0_writeback_sync_modes {
 Model0_WB_SYNC_NONE, /* Don't wait on anything */
 Model0_WB_SYNC_ALL, /* Wait on every mapping */
};

/*
 * why some writeback work was initiated
 */
enum Model0_wb_reason {
 Model0_WB_REASON_BACKGROUND,
 Model0_WB_REASON_TRY_TO_FREE_PAGES,
 Model0_WB_REASON_SYNC,
 Model0_WB_REASON_PERIODIC,
 Model0_WB_REASON_LAPTOP_TIMER,
 Model0_WB_REASON_FREE_MORE_MEM,
 Model0_WB_REASON_FS_FREE_SPACE,
 /*
	 * There is no bdi forker thread any more and works are done
	 * by emergency worker, however, this is TPs userland visible
	 * and we'll be exposing exactly the same information,
	 * so it has a mismatch name.
	 */
 Model0_WB_REASON_FORKER_THREAD,

 Model0_WB_REASON_MAX,
};

/*
 * A control structure which tells the writeback code what to do.  These are
 * always on the stack, and hence need no locking.  They are always initialised
 * in a manner such that unspecified fields are set to zero.
 */
struct Model0_writeback_control {
 long Model0_nr_to_write; /* Write this many pages, and decrement
					   this for each page written */
 long Model0_pages_skipped; /* Pages which were not written */

 /*
	 * For a_ops->writepages(): if start or end are non-zero then this is
	 * a hint that the filesystem need only write out the pages inside that
	 * byterange.  The byte at `end' is included in the writeout request.
	 */
 Model0_loff_t Model0_range_start;
 Model0_loff_t Model0_range_end;

 enum Model0_writeback_sync_modes Model0_sync_mode;

 unsigned Model0_for_kupdate:1; /* A kupdate writeback */
 unsigned Model0_for_background:1; /* A background writeback */
 unsigned Model0_tagged_writepages:1; /* tag-and-write to avoid livelock */
 unsigned Model0_for_reclaim:1; /* Invoked from the page allocator */
 unsigned Model0_range_cyclic:1; /* range_start is cyclic */
 unsigned Model0_for_sync:1; /* sync(2) WB_SYNC_ALL writeback */
};

/*
 * A wb_domain represents a domain that wb's (bdi_writeback's) belong to
 * and are measured against each other in.  There always is one global
 * domain, global_wb_domain, that every wb in the system is a member of.
 * This allows measuring the relative bandwidth of each wb to distribute
 * dirtyable memory accordingly.
 */
struct Model0_wb_domain {
 Model0_spinlock_t Model0_lock;

 /*
	 * Scale the writeback cache size proportional to the relative
	 * writeout speed.
	 *
	 * We do this by keeping a floating proportion between BDIs, based
	 * on page writeback completions [end_page_writeback()]. Those
	 * devices that write out pages fastest will get the larger share,
	 * while the slower will get a smaller share.
	 *
	 * We use page writeout completions because we are interested in
	 * getting rid of dirty pages. Having them written out is the
	 * primary goal.
	 *
	 * We introduce a concept of time, a period over which we measure
	 * these events, because demand can/will vary over time. The length
	 * of this period itself is measured in page writeback completions.
	 */
 struct Model0_fprop_global Model0_completions;
 struct Model0_timer_list Model0_period_timer; /* timer for aging of completions */
 unsigned long Model0_period_time;

 /*
	 * The dirtyable memory and dirty threshold could be suddenly
	 * knocked down by a large amount (eg. on the startup of KVM in a
	 * swapless system). This may throw the system into deep dirty
	 * exceeded state and throttle heavy/light dirtiers alike. To
	 * retain good responsiveness, maintain global_dirty_limit for
	 * tracking slowly down to the knocked down dirty threshold.
	 *
	 * Both fields are protected by ->lock.
	 */
 unsigned long Model0_dirty_limit_tstamp;
 unsigned long Model0_dirty_limit;
};

/**
 * wb_domain_size_changed - memory available to a wb_domain has changed
 * @dom: wb_domain of interest
 *
 * This function should be called when the amount of memory available to
 * @dom has changed.  It resets @dom's dirty limit parameters to prevent
 * the past values which don't match the current configuration from skewing
 * dirty throttling.  Without this, when memory size of a wb_domain is
 * greatly reduced, the dirty throttling logic may allow too many pages to
 * be dirtied leading to consecutive unnecessary OOMs and may get stuck in
 * that situation.
 */
static inline __attribute__((no_instrument_function)) void Model0_wb_domain_size_changed(struct Model0_wb_domain *Model0_dom)
{
 Model0_spin_lock(&Model0_dom->Model0_lock);
 Model0_dom->Model0_dirty_limit_tstamp = Model0_jiffies;
 Model0_dom->Model0_dirty_limit = 0;
 Model0_spin_unlock(&Model0_dom->Model0_lock);
}

/*
 * fs/fs-writeback.c
 */
struct Model0_bdi_writeback;
void Model0_writeback_inodes_sb(struct Model0_super_block *, enum Model0_wb_reason Model0_reason);
void Model0_writeback_inodes_sb_nr(struct Model0_super_block *, unsigned long Model0_nr,
       enum Model0_wb_reason Model0_reason);
bool Model0_try_to_writeback_inodes_sb(struct Model0_super_block *, enum Model0_wb_reason Model0_reason);
bool Model0_try_to_writeback_inodes_sb_nr(struct Model0_super_block *, unsigned long Model0_nr,
       enum Model0_wb_reason Model0_reason);
void Model0_sync_inodes_sb(struct Model0_super_block *);
void Model0_wakeup_flusher_threads(long Model0_nr_pages, enum Model0_wb_reason Model0_reason);
void Model0_inode_wait_for_writeback(struct Model0_inode *Model0_inode);

/* writeback.h requires fs.h; it, too, is not included from here. */
static inline __attribute__((no_instrument_function)) void Model0_wait_on_inode(struct Model0_inode *Model0_inode)
{
 do { Model0__cond_resched(); } while (0);
 Model0_wait_on_bit(&Model0_inode->Model0_i_state, 3, 2);
}
static inline __attribute__((no_instrument_function)) void Model0_inode_attach_wb(struct Model0_inode *Model0_inode, struct Model0_page *Model0_page)
{
}

static inline __attribute__((no_instrument_function)) void Model0_inode_detach_wb(struct Model0_inode *Model0_inode)
{
}

static inline __attribute__((no_instrument_function)) void Model0_wbc_attach_and_unlock_inode(struct Model0_writeback_control *Model0_wbc,
            struct Model0_inode *Model0_inode)

{
 Model0_spin_unlock(&Model0_inode->Model0_i_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_wbc_attach_fdatawrite_inode(struct Model0_writeback_control *Model0_wbc,
            struct Model0_inode *Model0_inode)
{
}

static inline __attribute__((no_instrument_function)) void Model0_wbc_detach_inode(struct Model0_writeback_control *Model0_wbc)
{
}

static inline __attribute__((no_instrument_function)) void Model0_wbc_init_bio(struct Model0_writeback_control *Model0_wbc, struct Model0_bio *Model0_bio)
{
}

static inline __attribute__((no_instrument_function)) void Model0_wbc_account_io(struct Model0_writeback_control *Model0_wbc,
      struct Model0_page *Model0_page, Model0_size_t Model0_bytes)
{
}

static inline __attribute__((no_instrument_function)) void Model0_cgroup_writeback_umount(void)
{
}



/*
 * mm/page-writeback.c
 */

void Model0_laptop_io_completion(struct Model0_backing_dev_info *Model0_info);
void Model0_laptop_sync_completion(void);
void Model0_laptop_mode_sync(struct Model0_work_struct *Model0_work);
void Model0_laptop_mode_timer_fn(unsigned long Model0_data);



void Model0_throttle_vm_writeout(Model0_gfp_t Model0_gfp_mask);
bool Model0_node_dirty_ok(struct Model0_pglist_data *Model0_pgdat);
int Model0_wb_domain_init(struct Model0_wb_domain *Model0_dom, Model0_gfp_t Model0_gfp);




extern struct Model0_wb_domain Model0_global_wb_domain;

/* These are exported to sysctl. */
extern int Model0_dirty_background_ratio;
extern unsigned long Model0_dirty_background_bytes;
extern int Model0_vm_dirty_ratio;
extern unsigned long Model0_vm_dirty_bytes;
extern unsigned int Model0_dirty_writeback_interval;
extern unsigned int Model0_dirty_expire_interval;
extern unsigned int Model0_dirtytime_expire_interval;
extern int Model0_vm_highmem_is_dirtyable;
extern int Model0_block_dump;
extern int Model0_laptop_mode;

extern int Model0_dirty_background_ratio_handler(struct Model0_ctl_table *Model0_table, int Model0_write,
  void *Model0_buffer, Model0_size_t *Model0_lenp,
  Model0_loff_t *Model0_ppos);
extern int Model0_dirty_background_bytes_handler(struct Model0_ctl_table *Model0_table, int Model0_write,
  void *Model0_buffer, Model0_size_t *Model0_lenp,
  Model0_loff_t *Model0_ppos);
extern int Model0_dirty_ratio_handler(struct Model0_ctl_table *Model0_table, int Model0_write,
  void *Model0_buffer, Model0_size_t *Model0_lenp,
  Model0_loff_t *Model0_ppos);
extern int Model0_dirty_bytes_handler(struct Model0_ctl_table *Model0_table, int Model0_write,
  void *Model0_buffer, Model0_size_t *Model0_lenp,
  Model0_loff_t *Model0_ppos);
int Model0_dirtytime_interval_handler(struct Model0_ctl_table *Model0_table, int Model0_write,
          void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);

struct Model0_ctl_table;
int Model0_dirty_writeback_centisecs_handler(struct Model0_ctl_table *, int,
          void *, Model0_size_t *, Model0_loff_t *);

void Model0_global_dirty_limits(unsigned long *Model0_pbackground, unsigned long *Model0_pdirty);
unsigned long Model0_wb_calc_thresh(struct Model0_bdi_writeback *Model0_wb, unsigned long Model0_thresh);

void Model0_wb_update_bandwidth(struct Model0_bdi_writeback *Model0_wb, unsigned long Model0_start_time);
void Model0_page_writeback_init(void);
void Model0_balance_dirty_pages_ratelimited(struct Model0_address_space *Model0_mapping);
bool Model0_wb_over_bg_thresh(struct Model0_bdi_writeback *Model0_wb);

typedef int (*Model0_writepage_t)(struct Model0_page *Model0_page, struct Model0_writeback_control *Model0_wbc,
    void *Model0_data);

int Model0_generic_writepages(struct Model0_address_space *Model0_mapping,
         struct Model0_writeback_control *Model0_wbc);
void Model0_tag_pages_for_writeback(struct Model0_address_space *Model0_mapping,
        unsigned long Model0_start, unsigned long Model0_end);
int Model0_write_cache_pages(struct Model0_address_space *Model0_mapping,
        struct Model0_writeback_control *Model0_wbc, Model0_writepage_t Model0_writepage,
        void *Model0_data);
int Model0_do_writepages(struct Model0_address_space *Model0_mapping, struct Model0_writeback_control *Model0_wbc);
void Model0_writeback_set_ratelimit(void);
void Model0_tag_pages_for_writeback(struct Model0_address_space *Model0_mapping,
        unsigned long Model0_start, unsigned long Model0_end);

void Model0_account_page_redirty(struct Model0_page *Model0_page);

void Model0_sb_mark_inode_writeback(struct Model0_inode *Model0_inode);
void Model0_sb_clear_inode_writeback(struct Model0_inode *Model0_inode);


struct Model0_mem_cgroup;
struct Model0_page;
struct Model0_mm_struct;
struct Model0_kmem_cache;

/*
 * The corresponding mem_cgroup_stat_names is defined in mm/memcontrol.c,
 * These two lists should keep in accord with each other.
 */
enum Model0_mem_cgroup_stat_index {
 /*
	 * For MEM_CONTAINER_TYPE_ALL, usage = pagecache + rss.
	 */
 Model0_MEM_CGROUP_STAT_CACHE, /* # of pages charged as cache */
 Model0_MEM_CGROUP_STAT_RSS, /* # of pages charged as anon rss */
 Model0_MEM_CGROUP_STAT_RSS_HUGE, /* # of pages charged as anon huge */
 Model0_MEM_CGROUP_STAT_FILE_MAPPED, /* # of pages charged as file rss */
 Model0_MEM_CGROUP_STAT_DIRTY, /* # of dirty pages in page cache */
 Model0_MEM_CGROUP_STAT_WRITEBACK, /* # of pages under writeback */
 Model0_MEM_CGROUP_STAT_SWAP, /* # of pages, swapped out */
 Model0_MEM_CGROUP_STAT_NSTATS,
 /* default hierarchy stats */
 Model0_MEMCG_KERNEL_STACK_KB = Model0_MEM_CGROUP_STAT_NSTATS,
 Model0_MEMCG_SLAB_RECLAIMABLE,
 Model0_MEMCG_SLAB_UNRECLAIMABLE,
 Model0_MEMCG_SOCK,
 Model0_MEMCG_NR_STAT,
};

struct Model0_mem_cgroup_reclaim_cookie {
 Model0_pg_data_t *Model0_pgdat;
 int Model0_priority;
 unsigned int Model0_generation;
};

enum Model0_mem_cgroup_events_index {
 Model0_MEM_CGROUP_EVENTS_PGPGIN, /* # of pages paged in */
 Model0_MEM_CGROUP_EVENTS_PGPGOUT, /* # of pages paged out */
 Model0_MEM_CGROUP_EVENTS_PGFAULT, /* # of page-faults */
 Model0_MEM_CGROUP_EVENTS_PGMAJFAULT, /* # of major page-faults */
 Model0_MEM_CGROUP_EVENTS_NSTATS,
 /* default hierarchy events */
 Model0_MEMCG_LOW = Model0_MEM_CGROUP_EVENTS_NSTATS,
 Model0_MEMCG_HIGH,
 Model0_MEMCG_MAX,
 Model0_MEMCG_OOM,
 Model0_MEMCG_NR_EVENTS,
};

/*
 * Per memcg event counter is incremented at every pagein/pageout. With THP,
 * it will be incremated by the number of pages. This counter is used for
 * for trigger some periodic events. This is straightforward and better
 * than using jiffies etc. to handle periodic memcg event.
 */
enum Model0_mem_cgroup_events_target {
 Model0_MEM_CGROUP_TARGET_THRESH,
 Model0_MEM_CGROUP_TARGET_SOFTLIMIT,
 Model0_MEM_CGROUP_TARGET_NUMAINFO,
 Model0_MEM_CGROUP_NTARGETS,
};
struct Model0_mem_cgroup;

static inline __attribute__((no_instrument_function)) bool Model0_mem_cgroup_disabled(void)
{
 return true;
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_events(struct Model0_mem_cgroup *Model0_memcg,
         enum Model0_mem_cgroup_events_index Model0_idx,
         unsigned int Model0_nr)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_mem_cgroup_low(struct Model0_mem_cgroup *Model0_root,
      struct Model0_mem_cgroup *Model0_memcg)
{
 return false;
}

static inline __attribute__((no_instrument_function)) int Model0_mem_cgroup_try_charge(struct Model0_page *Model0_page, struct Model0_mm_struct *Model0_mm,
     Model0_gfp_t Model0_gfp_mask,
     struct Model0_mem_cgroup **Model0_memcgp,
     bool Model0_compound)
{
 *Model0_memcgp = ((void *)0);
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_commit_charge(struct Model0_page *Model0_page,
         struct Model0_mem_cgroup *Model0_memcg,
         bool Model0_lrucare, bool Model0_compound)
{
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_cancel_charge(struct Model0_page *Model0_page,
         struct Model0_mem_cgroup *Model0_memcg,
         bool Model0_compound)
{
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_uncharge(struct Model0_page *Model0_page)
{
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_uncharge_list(struct Model0_list_head *Model0_page_list)
{
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_migrate(struct Model0_page *old, struct Model0_page *Model0_new)
{
}

static inline __attribute__((no_instrument_function)) struct Model0_lruvec *Model0_mem_cgroup_lruvec(struct Model0_pglist_data *Model0_pgdat,
    struct Model0_mem_cgroup *Model0_memcg)
{
 return Model0_node_lruvec(Model0_pgdat);
}

static inline __attribute__((no_instrument_function)) struct Model0_lruvec *Model0_mem_cgroup_page_lruvec(struct Model0_page *Model0_page,
          struct Model0_pglist_data *Model0_pgdat)
{
 return &Model0_pgdat->Model0_lruvec;
}

static inline __attribute__((no_instrument_function)) bool Model0_mm_match_cgroup(struct Model0_mm_struct *Model0_mm,
  struct Model0_mem_cgroup *Model0_memcg)
{
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_task_in_mem_cgroup(struct Model0_task_struct *Model0_task,
          const struct Model0_mem_cgroup *Model0_memcg)
{
 return true;
}

static inline __attribute__((no_instrument_function)) struct Model0_mem_cgroup *
Model0_mem_cgroup_iter(struct Model0_mem_cgroup *Model0_root,
  struct Model0_mem_cgroup *Model0_prev,
  struct Model0_mem_cgroup_reclaim_cookie *Model0_reclaim)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_iter_break(struct Model0_mem_cgroup *Model0_root,
      struct Model0_mem_cgroup *Model0_prev)
{
}

static inline __attribute__((no_instrument_function)) unsigned short Model0_mem_cgroup_id(struct Model0_mem_cgroup *Model0_memcg)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) struct Model0_mem_cgroup *Model0_mem_cgroup_from_id(unsigned short Model0_id)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(Model0_id); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/linux/memcontrol.h", 649); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });
 /* XXX: This should always return root_mem_cgroup */
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model0_mem_cgroup_online(struct Model0_mem_cgroup *Model0_memcg)
{
 return true;
}

static inline __attribute__((no_instrument_function)) unsigned long
Model0_mem_cgroup_get_lru_size(struct Model0_lruvec *Model0_lruvec, enum Model0_lru_list Model0_lru)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) unsigned long
Model0_mem_cgroup_node_nr_lru_pages(struct Model0_mem_cgroup *Model0_memcg,
        int Model0_nid, unsigned int Model0_lru_mask)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void
Model0_mem_cgroup_print_oom_info(struct Model0_mem_cgroup *Model0_memcg, struct Model0_task_struct *Model0_p)
{
}

static inline __attribute__((no_instrument_function)) void Model0_lock_page_memcg(struct Model0_page *Model0_page)
{
}

static inline __attribute__((no_instrument_function)) void Model0_unlock_page_memcg(struct Model0_page *Model0_page)
{
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_handle_over_high(void)
{
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_oom_enable(void)
{
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_oom_disable(void)
{
}

static inline __attribute__((no_instrument_function)) bool Model0_task_in_memcg_oom(struct Model0_task_struct *Model0_p)
{
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model0_mem_cgroup_oom_synchronize(bool Model0_wait)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_inc_page_stat(struct Model0_page *Model0_page,
         enum Model0_mem_cgroup_stat_index Model0_idx)
{
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_dec_page_stat(struct Model0_page *Model0_page,
         enum Model0_mem_cgroup_stat_index Model0_idx)
{
}

static inline __attribute__((no_instrument_function))
unsigned long Model0_mem_cgroup_soft_limit_reclaim(Model0_pg_data_t *Model0_pgdat, int Model0_order,
         Model0_gfp_t Model0_gfp_mask,
         unsigned long *Model0_total_scanned)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_split_huge_fixup(struct Model0_page *Model0_head)
{
}

static inline __attribute__((no_instrument_function))
void Model0_mem_cgroup_count_vm_event(struct Model0_mm_struct *Model0_mm, enum Model0_vm_event_item Model0_idx)
{
}
static inline __attribute__((no_instrument_function)) struct Model0_wb_domain *Model0_mem_cgroup_wb_domain(struct Model0_bdi_writeback *Model0_wb)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_mem_cgroup_wb_stats(struct Model0_bdi_writeback *Model0_wb,
           unsigned long *Model0_pfilepages,
           unsigned long *Model0_pheadroom,
           unsigned long *Model0_pdirty,
           unsigned long *Model0_pwriteback)
{
}



struct Model0_sock;
void Model0_sock_update_memcg(struct Model0_sock *Model0_sk);
void Model0_sock_release_memcg(struct Model0_sock *Model0_sk);
bool Model0_mem_cgroup_charge_skmem(struct Model0_mem_cgroup *Model0_memcg, unsigned int Model0_nr_pages);
void Model0_mem_cgroup_uncharge_skmem(struct Model0_mem_cgroup *Model0_memcg, unsigned int Model0_nr_pages);
static inline __attribute__((no_instrument_function)) bool Model0_mem_cgroup_under_socket_pressure(struct Model0_mem_cgroup *Model0_memcg)
{
 return false;
}


struct Model0_kmem_cache *Model0_memcg_kmem_get_cache(struct Model0_kmem_cache *Model0_cachep);
void Model0_memcg_kmem_put_cache(struct Model0_kmem_cache *Model0_cachep);
int Model0_memcg_kmem_charge_memcg(struct Model0_page *Model0_page, Model0_gfp_t Model0_gfp, int Model0_order,
       struct Model0_mem_cgroup *Model0_memcg);
int Model0_memcg_kmem_charge(struct Model0_page *Model0_page, Model0_gfp_t Model0_gfp, int Model0_order);
void Model0_memcg_kmem_uncharge(struct Model0_page *Model0_page, int Model0_order);
static inline __attribute__((no_instrument_function)) bool Model0_memcg_kmem_enabled(void)
{
 return false;
}

static inline __attribute__((no_instrument_function)) int Model0_memcg_cache_id(struct Model0_mem_cgroup *Model0_memcg)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) void Model0_memcg_get_cache_ids(void)
{
}

static inline __attribute__((no_instrument_function)) void Model0_memcg_put_cache_ids(void)
{
}

static inline __attribute__((no_instrument_function)) void Model0_memcg_kmem_update_page_stat(struct Model0_page *Model0_page,
    enum Model0_mem_cgroup_stat_index Model0_idx, int Model0_val)
{
}




/*
 * Linux Socket Filter Data Structures
 */








enum {
 Model0_TCA_STATS_UNSPEC,
 Model0_TCA_STATS_BASIC,
 Model0_TCA_STATS_RATE_EST,
 Model0_TCA_STATS_QUEUE,
 Model0_TCA_STATS_APP,
 Model0_TCA_STATS_RATE_EST64,
 Model0_TCA_STATS_PAD,
 Model0___TCA_STATS_MAX,
};


/**
 * struct gnet_stats_basic - byte/packet throughput statistics
 * @bytes: number of seen bytes
 * @packets: number of seen packets
 */
struct Model0_gnet_stats_basic {
 __u64 Model0_bytes;
 __u32 Model0_packets;
};
struct Model0_gnet_stats_basic_packed {
 __u64 Model0_bytes;
 __u32 Model0_packets;
} __attribute__ ((packed));

/**
 * struct gnet_stats_rate_est - rate estimator
 * @bps: current byte rate
 * @pps: current packet rate
 */
struct Model0_gnet_stats_rate_est {
 __u32 Model0_bps;
 __u32 Model0_pps;
};

/**
 * struct gnet_stats_rate_est64 - rate estimator
 * @bps: current byte rate
 * @pps: current packet rate
 */
struct Model0_gnet_stats_rate_est64 {
 __u64 Model0_bps;
 __u64 Model0_pps;
};

/**
 * struct gnet_stats_queue - queuing statistics
 * @qlen: queue length
 * @backlog: backlog size of queue
 * @drops: number of dropped packets
 * @requeues: number of requeues
 * @overlimits: number of enqueues over the limit
 */
struct Model0_gnet_stats_queue {
 __u32 Model0_qlen;
 __u32 Model0_backlog;
 __u32 Model0_drops;
 __u32 Model0_requeues;
 __u32 Model0_overlimits;
};

/**
 * struct gnet_estimator - rate estimator configuration
 * @interval: sampling period
 * @ewma_log: the log of measurement window weight
 */
struct Model0_gnet_estimator {
 signed char Model0_interval;
 unsigned char Model0_ewma_log;
};




struct Model0_gnet_stats_basic_cpu {
 struct Model0_gnet_stats_basic_packed Model0_bstats;
 struct Model0_u64_stats_sync Model0_syncp;
};

struct Model0_gnet_dump {
 Model0_spinlock_t * Model0_lock;
 struct Model0_sk_buff * Model0_skb;
 struct Model0_nlattr * Model0_tail;

 /* Backward compatibility */
 int Model0_compat_tc_stats;
 int Model0_compat_xstats;
 int Model0_padattr;
 void * Model0_xstats;
 int Model0_xstats_len;
 struct Model0_tc_stats Model0_tc_stats;
};

int Model0_gnet_stats_start_copy(struct Model0_sk_buff *Model0_skb, int Model0_type, Model0_spinlock_t *Model0_lock,
     struct Model0_gnet_dump *Model0_d, int Model0_padattr);

int Model0_gnet_stats_start_copy_compat(struct Model0_sk_buff *Model0_skb, int Model0_type,
     int Model0_tc_stats_type, int Model0_xstats_type,
     Model0_spinlock_t *Model0_lock, struct Model0_gnet_dump *Model0_d,
     int Model0_padattr);

int Model0_gnet_stats_copy_basic(const Model0_seqcount_t *Model0_running,
     struct Model0_gnet_dump *Model0_d,
     struct Model0_gnet_stats_basic_cpu *Model0_cpu,
     struct Model0_gnet_stats_basic_packed *Model0_b);
void Model0___gnet_stats_copy_basic(const Model0_seqcount_t *Model0_running,
        struct Model0_gnet_stats_basic_packed *Model0_bstats,
        struct Model0_gnet_stats_basic_cpu *Model0_cpu,
        struct Model0_gnet_stats_basic_packed *Model0_b);
int Model0_gnet_stats_copy_rate_est(struct Model0_gnet_dump *Model0_d,
        const struct Model0_gnet_stats_basic_packed *Model0_b,
        struct Model0_gnet_stats_rate_est64 *Model0_r);
int Model0_gnet_stats_copy_queue(struct Model0_gnet_dump *Model0_d,
     struct Model0_gnet_stats_queue *Model0_cpu_q,
     struct Model0_gnet_stats_queue *Model0_q, __u32 Model0_qlen);
int Model0_gnet_stats_copy_app(struct Model0_gnet_dump *Model0_d, void *Model0_st, int Model0_len);

int Model0_gnet_stats_finish_copy(struct Model0_gnet_dump *Model0_d);

int Model0_gen_new_estimator(struct Model0_gnet_stats_basic_packed *Model0_bstats,
        struct Model0_gnet_stats_basic_cpu *Model0_cpu_bstats,
        struct Model0_gnet_stats_rate_est64 *Model0_rate_est,
        Model0_spinlock_t *Model0_stats_lock,
        Model0_seqcount_t *Model0_running, struct Model0_nlattr *Model0_opt);
void Model0_gen_kill_estimator(struct Model0_gnet_stats_basic_packed *Model0_bstats,
   struct Model0_gnet_stats_rate_est64 *Model0_rate_est);
int Model0_gen_replace_estimator(struct Model0_gnet_stats_basic_packed *Model0_bstats,
     struct Model0_gnet_stats_basic_cpu *Model0_cpu_bstats,
     struct Model0_gnet_stats_rate_est64 *Model0_rate_est,
     Model0_spinlock_t *Model0_stats_lock,
     Model0_seqcount_t *Model0_running, struct Model0_nlattr *Model0_opt);
bool Model0_gen_estimator_active(const struct Model0_gnet_stats_basic_packed *Model0_bstats,
     const struct Model0_gnet_stats_rate_est64 *Model0_rate_est);


struct Model0_Qdisc_ops;
struct Model0_qdisc_walker;
struct Model0_tcf_walker;
struct Model0_module;

struct Model0_qdisc_rate_table {
 struct Model0_tc_ratespec Model0_rate;
 Model0_u32 Model0_data[256];
 struct Model0_qdisc_rate_table *Model0_next;
 int Model0_refcnt;
};

enum Model0_qdisc_state_t {
 Model0___QDISC_STATE_SCHED,
 Model0___QDISC_STATE_DEACTIVATED,
};

struct Model0_qdisc_size_table {
 struct Model0_callback_head Model0_rcu;
 struct Model0_list_head Model0_list;
 struct Model0_tc_sizespec Model0_szopts;
 int Model0_refcnt;
 Model0_u16 Model0_data[];
};

struct Model0_Qdisc {
 int (*Model0_enqueue)(struct Model0_sk_buff *Model0_skb,
        struct Model0_Qdisc *Model0_sch,
        struct Model0_sk_buff **Model0_to_free);
 struct Model0_sk_buff * (*Model0_dequeue)(struct Model0_Qdisc *Model0_sch);
 unsigned int Model0_flags;
 Model0_u32 Model0_limit;
 const struct Model0_Qdisc_ops *Model0_ops;
 struct Model0_qdisc_size_table *Model0_stab;
 struct Model0_list_head Model0_list;
 Model0_u32 Model0_handle;
 Model0_u32 Model0_parent;
 void *Model0_u32_node;

 struct Model0_netdev_queue *Model0_dev_queue;

 struct Model0_gnet_stats_rate_est64 Model0_rate_est;
 struct Model0_gnet_stats_basic_cpu *Model0_cpu_bstats;
 struct Model0_gnet_stats_queue *Model0_cpu_qstats;

 /*
	 * For performance sake on SMP, we put highly modified fields at the end
	 */
 struct Model0_sk_buff *Model0_gso_skb __attribute__((__aligned__((1 << (6)))));
 struct Model0_sk_buff_head Model0_q;
 struct Model0_gnet_stats_basic_packed Model0_bstats;
 Model0_seqcount_t Model0_running;
 struct Model0_gnet_stats_queue Model0_qstats;
 unsigned long Model0_state;
 struct Model0_Qdisc *Model0_next_sched;
 struct Model0_sk_buff *Model0_skb_bad_txq;
 struct Model0_callback_head Model0_callback_head;
 int Model0_padded;
 Model0_atomic_t Model0_refcnt;

 Model0_spinlock_t Model0_busylock __attribute__((__aligned__((1 << (6)))));
};

static inline __attribute__((no_instrument_function)) bool Model0_qdisc_is_running(const struct Model0_Qdisc *Model0_qdisc)
{
 return (Model0_raw_read_seqcount(&Model0_qdisc->Model0_running) & 1) ? true : false;
}

static inline __attribute__((no_instrument_function)) bool Model0_qdisc_run_begin(struct Model0_Qdisc *Model0_qdisc)
{
 if (Model0_qdisc_is_running(Model0_qdisc))
  return false;
 /* Variant of write_seqcount_begin() telling lockdep a trylock
	 * was attempted.
	 */
 Model0_raw_write_seqcount_begin(&Model0_qdisc->Model0_running);
 do { } while (0);
 return true;
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_run_end(struct Model0_Qdisc *Model0_qdisc)
{
 Model0_write_seqcount_end(&Model0_qdisc->Model0_running);
}

static inline __attribute__((no_instrument_function)) bool Model0_qdisc_may_bulk(const struct Model0_Qdisc *Model0_qdisc)
{
 return Model0_qdisc->Model0_flags & 0x10;
}

static inline __attribute__((no_instrument_function)) int Model0_qdisc_avail_bulklimit(const struct Model0_netdev_queue *Model0_txq)
{

 /* Non-BQL migrated drivers will return 0, too. */
 return Model0_dql_avail(&Model0_txq->Model0_dql);



}

struct Model0_Qdisc_class_ops {
 /* Child qdisc manipulation */
 struct Model0_netdev_queue * (*Model0_select_queue)(struct Model0_Qdisc *, struct Model0_tcmsg *);
 int (*Model0_graft)(struct Model0_Qdisc *, unsigned long Model0_cl,
     struct Model0_Qdisc *, struct Model0_Qdisc **);
 struct Model0_Qdisc * (*Model0_leaf)(struct Model0_Qdisc *, unsigned long Model0_cl);
 void (*Model0_qlen_notify)(struct Model0_Qdisc *, unsigned long);

 /* Class manipulation routines */
 unsigned long (*Model0_get)(struct Model0_Qdisc *, Model0_u32 Model0_classid);
 void (*Model0_put)(struct Model0_Qdisc *, unsigned long);
 int (*Model0_change)(struct Model0_Qdisc *, Model0_u32, Model0_u32,
     struct Model0_nlattr **, unsigned long *);
 int (*Model0_delete)(struct Model0_Qdisc *, unsigned long);
 void (*Model0_walk)(struct Model0_Qdisc *, struct Model0_qdisc_walker * Model0_arg);

 /* Filter manipulation */
 struct Model0_tcf_proto ** (*Model0_tcf_chain)(struct Model0_Qdisc *, unsigned long);
 bool (*Model0_tcf_cl_offload)(Model0_u32 Model0_classid);
 unsigned long (*Model0_bind_tcf)(struct Model0_Qdisc *, unsigned long,
     Model0_u32 Model0_classid);
 void (*Model0_unbind_tcf)(struct Model0_Qdisc *, unsigned long);

 /* rtnetlink specific */
 int (*Model0_dump)(struct Model0_Qdisc *, unsigned long,
     struct Model0_sk_buff *Model0_skb, struct Model0_tcmsg*);
 int (*Model0_dump_stats)(struct Model0_Qdisc *, unsigned long,
     struct Model0_gnet_dump *);
};

struct Model0_Qdisc_ops {
 struct Model0_Qdisc_ops *Model0_next;
 const struct Model0_Qdisc_class_ops *Model0_cl_ops;
 char Model0_id[16];
 int Model0_priv_size;

 int (*Model0_enqueue)(struct Model0_sk_buff *Model0_skb,
        struct Model0_Qdisc *Model0_sch,
        struct Model0_sk_buff **Model0_to_free);
 struct Model0_sk_buff * (*Model0_dequeue)(struct Model0_Qdisc *);
 struct Model0_sk_buff * (*Model0_peek)(struct Model0_Qdisc *);

 int (*Model0_init)(struct Model0_Qdisc *, struct Model0_nlattr *Model0_arg);
 void (*Model0_reset)(struct Model0_Qdisc *);
 void (*Model0_destroy)(struct Model0_Qdisc *);
 int (*Model0_change)(struct Model0_Qdisc *, struct Model0_nlattr *Model0_arg);
 void (*Model0_attach)(struct Model0_Qdisc *);

 int (*Model0_dump)(struct Model0_Qdisc *, struct Model0_sk_buff *);
 int (*Model0_dump_stats)(struct Model0_Qdisc *, struct Model0_gnet_dump *);

 struct Model0_module *Model0_owner;
};


struct Model0_tcf_result {
 unsigned long Model0_class;
 Model0_u32 Model0_classid;
};

struct Model0_tcf_proto_ops {
 struct Model0_list_head Model0_head;
 char Model0_kind[16];

 int (*Model0_classify)(struct Model0_sk_buff *,
         const struct Model0_tcf_proto *,
         struct Model0_tcf_result *);
 int (*Model0_init)(struct Model0_tcf_proto*);
 bool (*Model0_destroy)(struct Model0_tcf_proto*, bool);

 unsigned long (*Model0_get)(struct Model0_tcf_proto*, Model0_u32 Model0_handle);
 int (*Model0_change)(struct Model0_net *Model0_net, struct Model0_sk_buff *,
     struct Model0_tcf_proto*, unsigned long,
     Model0_u32 Model0_handle, struct Model0_nlattr **,
     unsigned long *, bool);
 int (*Model0_delete)(struct Model0_tcf_proto*, unsigned long);
 void (*Model0_walk)(struct Model0_tcf_proto*, struct Model0_tcf_walker *Model0_arg);

 /* rtnetlink specific */
 int (*Model0_dump)(struct Model0_net*, struct Model0_tcf_proto*, unsigned long,
     struct Model0_sk_buff *Model0_skb, struct Model0_tcmsg*);

 struct Model0_module *Model0_owner;
};

struct Model0_tcf_proto {
 /* Fast access part */
 struct Model0_tcf_proto *Model0_next;
 void *Model0_root;
 int (*Model0_classify)(struct Model0_sk_buff *,
         const struct Model0_tcf_proto *,
         struct Model0_tcf_result *);
 Model0___be16 Model0_protocol;

 /* All the rest */
 Model0_u32 Model0_prio;
 Model0_u32 Model0_classid;
 struct Model0_Qdisc *Model0_q;
 void *Model0_data;
 const struct Model0_tcf_proto_ops *Model0_ops;
 struct Model0_callback_head Model0_rcu;
};

struct Model0_qdisc_skb_cb {
 unsigned int Model0_pkt_len;
 Model0_u16 Model0_slave_dev_queue_mapping;
 Model0_u16 Model0_tc_classid;

 unsigned char Model0_data[20];
};

static inline __attribute__((no_instrument_function)) void Model0_qdisc_cb_private_validate(const struct Model0_sk_buff *Model0_skb, int Model0_sz)
{
 struct Model0_qdisc_skb_cb *Model0_qcb;

 do { bool Model0___cond = !(!(sizeof(Model0_skb->Model0_cb) < __builtin_offsetof(struct Model0_qdisc_skb_cb, Model0_data) + Model0_sz)); extern void Model0___compiletime_assert_245(void) ; if (Model0___cond) Model0___compiletime_assert_245(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!(sizeof(Model0_qcb->Model0_data) < Model0_sz)); extern void Model0___compiletime_assert_246(void) ; if (Model0___cond) Model0___compiletime_assert_246(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
}

static inline __attribute__((no_instrument_function)) int Model0_qdisc_qlen(const struct Model0_Qdisc *Model0_q)
{
 return Model0_q->Model0_q.Model0_qlen;
}

static inline __attribute__((no_instrument_function)) struct Model0_qdisc_skb_cb *Model0_qdisc_skb_cb(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_qdisc_skb_cb *)Model0_skb->Model0_cb;
}

static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_qdisc_lock(struct Model0_Qdisc *Model0_qdisc)
{
 return &Model0_qdisc->Model0_q.Model0_lock;
}

static inline __attribute__((no_instrument_function)) struct Model0_Qdisc *Model0_qdisc_root(const struct Model0_Qdisc *Model0_qdisc)
{
 struct Model0_Qdisc *Model0_q = ({ typeof(*(Model0_qdisc->Model0_dev_queue->Model0_qdisc)) *Model0_________p1 = (typeof(*(Model0_qdisc->Model0_dev_queue->Model0_qdisc)) *)({ typeof((Model0_qdisc->Model0_dev_queue->Model0_qdisc)) Model0__________p1 = ({ union { typeof((Model0_qdisc->Model0_dev_queue->Model0_qdisc)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_qdisc->Model0_dev_queue->Model0_qdisc)), Model0___u.Model0___c, sizeof((Model0_qdisc->Model0_dev_queue->Model0_qdisc))); else Model0___read_once_size_nocheck(&((Model0_qdisc->Model0_dev_queue->Model0_qdisc)), Model0___u.Model0___c, sizeof((Model0_qdisc->Model0_dev_queue->Model0_qdisc))); Model0___u.Model0___val; }); typeof(*((Model0_qdisc->Model0_dev_queue->Model0_qdisc))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_qdisc->Model0_dev_queue->Model0_qdisc)) *)(Model0_________p1)); });

 return Model0_q;
}

static inline __attribute__((no_instrument_function)) struct Model0_Qdisc *Model0_qdisc_root_sleeping(const struct Model0_Qdisc *Model0_qdisc)
{
 return Model0_qdisc->Model0_dev_queue->Model0_qdisc_sleeping;
}

/* The qdisc root lock is a mechanism by which to top level
 * of a qdisc tree can be locked from any qdisc node in the
 * forest.  This allows changing the configuration of some
 * aspect of the qdisc tree while blocking out asynchronous
 * qdisc access in the packet processing paths.
 *
 * It is only legal to do this when the root will not change
 * on us.  Otherwise we'll potentially lock the wrong qdisc
 * root.  This is enforced by holding the RTNL semaphore, which
 * all users of this lock accessor must do.
 */
static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_qdisc_root_lock(const struct Model0_Qdisc *Model0_qdisc)
{
 struct Model0_Qdisc *Model0_root = Model0_qdisc_root(Model0_qdisc);

 do { if (__builtin_expect(!!(!Model0_rtnl_is_locked()), 0)) { Model0_printk("\001" "3" "RTNL: assertion failed at %s (%d)\n", "./include/net/sch_generic.h", 291); Model0_dump_stack(); } } while(0);
 return Model0_qdisc_lock(Model0_root);
}

static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_qdisc_root_sleeping_lock(const struct Model0_Qdisc *Model0_qdisc)
{
 struct Model0_Qdisc *Model0_root = Model0_qdisc_root_sleeping(Model0_qdisc);

 do { if (__builtin_expect(!!(!Model0_rtnl_is_locked()), 0)) { Model0_printk("\001" "3" "RTNL: assertion failed at %s (%d)\n", "./include/net/sch_generic.h", 299); Model0_dump_stack(); } } while(0);
 return Model0_qdisc_lock(Model0_root);
}

static inline __attribute__((no_instrument_function)) Model0_seqcount_t *Model0_qdisc_root_sleeping_running(const struct Model0_Qdisc *Model0_qdisc)
{
 struct Model0_Qdisc *Model0_root = Model0_qdisc_root_sleeping(Model0_qdisc);

 do { if (__builtin_expect(!!(!Model0_rtnl_is_locked()), 0)) { Model0_printk("\001" "3" "RTNL: assertion failed at %s (%d)\n", "./include/net/sch_generic.h", 307); Model0_dump_stack(); } } while(0);
 return &Model0_root->Model0_running;
}

static inline __attribute__((no_instrument_function)) struct Model0_net_device *Model0_qdisc_dev(const struct Model0_Qdisc *Model0_qdisc)
{
 return Model0_qdisc->Model0_dev_queue->Model0_dev;
}

static inline __attribute__((no_instrument_function)) void Model0_sch_tree_lock(const struct Model0_Qdisc *Model0_q)
{
 Model0_spin_lock_bh(Model0_qdisc_root_sleeping_lock(Model0_q));
}

static inline __attribute__((no_instrument_function)) void Model0_sch_tree_unlock(const struct Model0_Qdisc *Model0_q)
{
 Model0_spin_unlock_bh(Model0_qdisc_root_sleeping_lock(Model0_q));
}




extern struct Model0_Qdisc Model0_noop_qdisc;
extern struct Model0_Qdisc_ops Model0_noop_qdisc_ops;
extern struct Model0_Qdisc_ops Model0_pfifo_fast_ops;
extern struct Model0_Qdisc_ops Model0_mq_qdisc_ops;
extern struct Model0_Qdisc_ops Model0_noqueue_qdisc_ops;
extern const struct Model0_Qdisc_ops *Model0_default_qdisc_ops;
static inline __attribute__((no_instrument_function)) const struct Model0_Qdisc_ops *
Model0_get_default_qdisc_ops(const struct Model0_net_device *Model0_dev, int Model0_ntx)
{
 return Model0_ntx < Model0_dev->Model0_real_num_tx_queues ?
   Model0_default_qdisc_ops : &Model0_pfifo_fast_ops;
}

struct Model0_Qdisc_class_common {
 Model0_u32 Model0_classid;
 struct Model0_hlist_node Model0_hnode;
};

struct Model0_Qdisc_class_hash {
 struct Model0_hlist_head *Model0_hash;
 unsigned int Model0_hashsize;
 unsigned int Model0_hashmask;
 unsigned int Model0_hashelems;
};

static inline __attribute__((no_instrument_function)) unsigned int Model0_qdisc_class_hash(Model0_u32 Model0_id, Model0_u32 Model0_mask)
{
 Model0_id ^= Model0_id >> 8;
 Model0_id ^= Model0_id >> 4;
 return Model0_id & Model0_mask;
}

static inline __attribute__((no_instrument_function)) struct Model0_Qdisc_class_common *
Model0_qdisc_class_find(const struct Model0_Qdisc_class_hash *Model0_hash, Model0_u32 Model0_id)
{
 struct Model0_Qdisc_class_common *Model0_cl;
 unsigned int Model0_h;

 Model0_h = Model0_qdisc_class_hash(Model0_id, Model0_hash->Model0_hashmask);
 for (Model0_cl = ({ typeof((&Model0_hash->Model0_hash[Model0_h])->Model0_first) Model0_____ptr = ((&Model0_hash->Model0_hash[Model0_h])->Model0_first); Model0_____ptr ? ({ const typeof( ((typeof(*(Model0_cl)) *)0)->Model0_hnode ) *Model0___mptr = (Model0_____ptr); (typeof(*(Model0_cl)) *)( (char *)Model0___mptr - __builtin_offsetof(typeof(*(Model0_cl)), Model0_hnode) );}) : ((void *)0); }); Model0_cl; Model0_cl = ({ typeof((Model0_cl)->Model0_hnode.Model0_next) Model0_____ptr = ((Model0_cl)->Model0_hnode.Model0_next); Model0_____ptr ? ({ const typeof( ((typeof(*(Model0_cl)) *)0)->Model0_hnode ) *Model0___mptr = (Model0_____ptr); (typeof(*(Model0_cl)) *)( (char *)Model0___mptr - __builtin_offsetof(typeof(*(Model0_cl)), Model0_hnode) );}) : ((void *)0); })) {
  if (Model0_cl->Model0_classid == Model0_id)
   return Model0_cl;
 }
 return ((void *)0);
}

int Model0_qdisc_class_hash_init(struct Model0_Qdisc_class_hash *);
void Model0_qdisc_class_hash_insert(struct Model0_Qdisc_class_hash *,
        struct Model0_Qdisc_class_common *);
void Model0_qdisc_class_hash_remove(struct Model0_Qdisc_class_hash *,
        struct Model0_Qdisc_class_common *);
void Model0_qdisc_class_hash_grow(struct Model0_Qdisc *, struct Model0_Qdisc_class_hash *);
void Model0_qdisc_class_hash_destroy(struct Model0_Qdisc_class_hash *);

void Model0_dev_init_scheduler(struct Model0_net_device *Model0_dev);
void Model0_dev_shutdown(struct Model0_net_device *Model0_dev);
void Model0_dev_activate(struct Model0_net_device *Model0_dev);
void Model0_dev_deactivate(struct Model0_net_device *Model0_dev);
void Model0_dev_deactivate_many(struct Model0_list_head *Model0_head);
struct Model0_Qdisc *Model0_dev_graft_qdisc(struct Model0_netdev_queue *Model0_dev_queue,
         struct Model0_Qdisc *Model0_qdisc);
void Model0_qdisc_reset(struct Model0_Qdisc *Model0_qdisc);
void Model0_qdisc_destroy(struct Model0_Qdisc *Model0_qdisc);
void Model0_qdisc_tree_reduce_backlog(struct Model0_Qdisc *Model0_qdisc, unsigned int Model0_n,
          unsigned int Model0_len);
struct Model0_Qdisc *Model0_qdisc_alloc(struct Model0_netdev_queue *Model0_dev_queue,
     const struct Model0_Qdisc_ops *Model0_ops);
struct Model0_Qdisc *Model0_qdisc_create_dflt(struct Model0_netdev_queue *Model0_dev_queue,
    const struct Model0_Qdisc_ops *Model0_ops, Model0_u32 Model0_parentid);
void Model0___qdisc_calculate_pkt_len(struct Model0_sk_buff *Model0_skb,
          const struct Model0_qdisc_size_table *Model0_stab);
bool Model0_tcf_destroy(struct Model0_tcf_proto *Model0_tp, bool Model0_force);
void Model0_tcf_destroy_chain(struct Model0_tcf_proto **Model0_fl);
int Model0_skb_do_redirect(struct Model0_sk_buff *);

static inline __attribute__((no_instrument_function)) bool Model0_skb_at_tc_ingress(const struct Model0_sk_buff *Model0_skb)
{

 return ((((Model0_skb->Model0_tc_verd)) & ((((((((1))<<(2))-1)) << ((((12)))))))) >> ((((12))))) & 0x1;



}

/* Reset all TX qdiscs greater then index of a device.  */
static inline __attribute__((no_instrument_function)) void Model0_qdisc_reset_all_tx_gt(struct Model0_net_device *Model0_dev, unsigned int Model0_i)
{
 struct Model0_Qdisc *Model0_qdisc;

 for (; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  Model0_qdisc = ({ do { } while (0); ; ((typeof(*(Model0_netdev_get_tx_queue(Model0_dev, Model0_i)->Model0_qdisc)) *)((Model0_netdev_get_tx_queue(Model0_dev, Model0_i)->Model0_qdisc))); });
  if (Model0_qdisc) {
   Model0_spin_lock_bh(Model0_qdisc_lock(Model0_qdisc));
   Model0_qdisc_reset(Model0_qdisc);
   Model0_spin_unlock_bh(Model0_qdisc_lock(Model0_qdisc));
  }
 }
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_reset_all_tx(struct Model0_net_device *Model0_dev)
{
 Model0_qdisc_reset_all_tx_gt(Model0_dev, 0);
}

/* Are all TX queues of the device empty?  */
static inline __attribute__((no_instrument_function)) bool Model0_qdisc_all_tx_empty(const struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;

 Model0_rcu_read_lock();
 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_i);
  const struct Model0_Qdisc *Model0_q = ({ typeof(*(Model0_txq->Model0_qdisc)) *Model0_________p1 = (typeof(*(Model0_txq->Model0_qdisc)) *)({ typeof((Model0_txq->Model0_qdisc)) Model0__________p1 = ({ union { typeof((Model0_txq->Model0_qdisc)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_txq->Model0_qdisc)), Model0___u.Model0___c, sizeof((Model0_txq->Model0_qdisc))); else Model0___read_once_size_nocheck(&((Model0_txq->Model0_qdisc)), Model0___u.Model0___c, sizeof((Model0_txq->Model0_qdisc))); Model0___u.Model0___val; }); typeof(*((Model0_txq->Model0_qdisc))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_txq->Model0_qdisc)) *)(Model0_________p1)); });

  if (Model0_q->Model0_q.Model0_qlen) {
   Model0_rcu_read_unlock();
   return false;
  }
 }
 Model0_rcu_read_unlock();
 return true;
}

/* Are any of the TX qdiscs changing?  */
static inline __attribute__((no_instrument_function)) bool Model0_qdisc_tx_changing(const struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;

 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_i);
  if (({ typeof(*(Model0_txq->Model0_qdisc)) *Model0__________p1 = (typeof(*(Model0_txq->Model0_qdisc)) *)({ union { typeof((Model0_txq->Model0_qdisc)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_txq->Model0_qdisc)), Model0___u.Model0___c, sizeof((Model0_txq->Model0_qdisc))); else Model0___read_once_size_nocheck(&((Model0_txq->Model0_qdisc)), Model0___u.Model0___c, sizeof((Model0_txq->Model0_qdisc))); Model0___u.Model0___val; }); ; ((typeof(*(Model0_txq->Model0_qdisc)) *)(Model0__________p1)); }) != Model0_txq->Model0_qdisc_sleeping)
   return true;
 }
 return false;
}

/* Is the device using the noop qdisc on all queues?  */
static inline __attribute__((no_instrument_function)) bool Model0_qdisc_tx_is_noop(const struct Model0_net_device *Model0_dev)
{
 unsigned int Model0_i;

 for (Model0_i = 0; Model0_i < Model0_dev->Model0_num_tx_queues; Model0_i++) {
  struct Model0_netdev_queue *Model0_txq = Model0_netdev_get_tx_queue(Model0_dev, Model0_i);
  if (({ typeof(*(Model0_txq->Model0_qdisc)) *Model0__________p1 = (typeof(*(Model0_txq->Model0_qdisc)) *)({ union { typeof((Model0_txq->Model0_qdisc)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_txq->Model0_qdisc)), Model0___u.Model0___c, sizeof((Model0_txq->Model0_qdisc))); else Model0___read_once_size_nocheck(&((Model0_txq->Model0_qdisc)), Model0___u.Model0___c, sizeof((Model0_txq->Model0_qdisc))); Model0___u.Model0___val; }); ; ((typeof(*(Model0_txq->Model0_qdisc)) *)(Model0__________p1)); }) != &Model0_noop_qdisc)
   return false;
 }
 return true;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_qdisc_pkt_len(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_qdisc_skb_cb(Model0_skb)->Model0_pkt_len;
}

/* additional qdisc xmit flags (NET_XMIT_MASK in linux/netdevice.h) */
enum Model0_net_xmit_qdisc_t {
 Model0___NET_XMIT_STOLEN = 0x00010000,
 Model0___NET_XMIT_BYPASS = 0x00020000,
};







static inline __attribute__((no_instrument_function)) void Model0_qdisc_calculate_pkt_len(struct Model0_sk_buff *Model0_skb,
        const struct Model0_Qdisc *Model0_sch)
{

 struct Model0_qdisc_size_table *Model0_stab = ({ typeof(*(Model0_sch->Model0_stab)) *Model0_________p1 = (typeof(*(Model0_sch->Model0_stab)) *)({ typeof((Model0_sch->Model0_stab)) Model0__________p1 = ({ union { typeof((Model0_sch->Model0_stab)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_sch->Model0_stab)), Model0___u.Model0___c, sizeof((Model0_sch->Model0_stab))); else Model0___read_once_size_nocheck(&((Model0_sch->Model0_stab)), Model0___u.Model0___c, sizeof((Model0_sch->Model0_stab))); Model0___u.Model0___val; }); typeof(*((Model0_sch->Model0_stab))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_sch->Model0_stab)) *)(Model0_________p1)); });

 if (Model0_stab)
  Model0___qdisc_calculate_pkt_len(Model0_skb, Model0_stab);

}

static inline __attribute__((no_instrument_function)) int Model0_qdisc_enqueue(struct Model0_sk_buff *Model0_skb, struct Model0_Qdisc *Model0_sch,
    struct Model0_sk_buff **Model0_to_free)
{
 Model0_qdisc_calculate_pkt_len(Model0_skb, Model0_sch);
 return Model0_sch->Model0_enqueue(Model0_skb, Model0_sch, Model0_to_free);
}

static inline __attribute__((no_instrument_function)) bool Model0_qdisc_is_percpu_stats(const struct Model0_Qdisc *Model0_q)
{
 return Model0_q->Model0_flags & 0x20;
}

static inline __attribute__((no_instrument_function)) void Model0__bstats_update(struct Model0_gnet_stats_basic_packed *Model0_bstats,
      __u64 Model0_bytes, __u32 Model0_packets)
{
 Model0_bstats->Model0_bytes += Model0_bytes;
 Model0_bstats->Model0_packets += Model0_packets;
}

static inline __attribute__((no_instrument_function)) void Model0_bstats_update(struct Model0_gnet_stats_basic_packed *Model0_bstats,
     const struct Model0_sk_buff *Model0_skb)
{
 Model0__bstats_update(Model0_bstats,
         Model0_qdisc_pkt_len(Model0_skb),
         Model0_skb_is_gso(Model0_skb) ? ((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_gso_segs : 1);
}

static inline __attribute__((no_instrument_function)) void Model0__bstats_cpu_update(struct Model0_gnet_stats_basic_cpu *Model0_bstats,
          __u64 Model0_bytes, __u32 Model0_packets)
{
 Model0_u64_stats_update_begin(&Model0_bstats->Model0_syncp);
 Model0__bstats_update(&Model0_bstats->Model0_bstats, Model0_bytes, Model0_packets);
 Model0_u64_stats_update_end(&Model0_bstats->Model0_syncp);
}

static inline __attribute__((no_instrument_function)) void Model0_bstats_cpu_update(struct Model0_gnet_stats_basic_cpu *Model0_bstats,
         const struct Model0_sk_buff *Model0_skb)
{
 Model0_u64_stats_update_begin(&Model0_bstats->Model0_syncp);
 Model0_bstats_update(&Model0_bstats->Model0_bstats, Model0_skb);
 Model0_u64_stats_update_end(&Model0_bstats->Model0_syncp);
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_bstats_cpu_update(struct Model0_Qdisc *Model0_sch,
        const struct Model0_sk_buff *Model0_skb)
{
 Model0_bstats_cpu_update(({ do { const void *Model0___vpp_verify = (typeof((Model0_sch->Model0_cpu_bstats) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0_tcp_ptr__; asm volatile("add " "%%""gs"":" "%" "1" ", %0" : "=r" (Model0_tcp_ptr__) : "m" (Model0_this_cpu_off), "0" (Model0_sch->Model0_cpu_bstats)); (typeof(*(Model0_sch->Model0_cpu_bstats)) *)Model0_tcp_ptr__; }); }), Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_bstats_update(struct Model0_Qdisc *Model0_sch,
           const struct Model0_sk_buff *Model0_skb)
{
 Model0_bstats_update(&Model0_sch->Model0_bstats, Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_qstats_backlog_dec(struct Model0_Qdisc *Model0_sch,
         const struct Model0_sk_buff *Model0_skb)
{
 Model0_sch->Model0_qstats.Model0_backlog -= Model0_qdisc_pkt_len(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_qstats_backlog_inc(struct Model0_Qdisc *Model0_sch,
         const struct Model0_sk_buff *Model0_skb)
{
 Model0_sch->Model0_qstats.Model0_backlog += Model0_qdisc_pkt_len(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0___qdisc_qstats_drop(struct Model0_Qdisc *Model0_sch, int Model0_count)
{
 Model0_sch->Model0_qstats.Model0_drops += Model0_count;
}

static inline __attribute__((no_instrument_function)) void Model0_qstats_drop_inc(struct Model0_gnet_stats_queue *Model0_qstats)
{
 Model0_qstats->Model0_drops++;
}

static inline __attribute__((no_instrument_function)) void Model0_qstats_overlimit_inc(struct Model0_gnet_stats_queue *Model0_qstats)
{
 Model0_qstats->Model0_overlimits++;
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_qstats_drop(struct Model0_Qdisc *Model0_sch)
{
 Model0_qstats_drop_inc(&Model0_sch->Model0_qstats);
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_qstats_cpu_drop(struct Model0_Qdisc *Model0_sch)
{
 Model0_qstats_drop_inc(({ do { const void *Model0___vpp_verify = (typeof((Model0_sch->Model0_cpu_qstats) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); ({ unsigned long Model0_tcp_ptr__; asm volatile("add " "%%""gs"":" "%" "1" ", %0" : "=r" (Model0_tcp_ptr__) : "m" (Model0_this_cpu_off), "0" (Model0_sch->Model0_cpu_qstats)); (typeof(*(Model0_sch->Model0_cpu_qstats)) *)Model0_tcp_ptr__; }); }));
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_qstats_overlimit(struct Model0_Qdisc *Model0_sch)
{
 Model0_sch->Model0_qstats.Model0_overlimits++;
}

static inline __attribute__((no_instrument_function)) int Model0___qdisc_enqueue_tail(struct Model0_sk_buff *Model0_skb, struct Model0_Qdisc *Model0_sch,
           struct Model0_sk_buff_head *Model0_list)
{
 Model0___skb_queue_tail(Model0_list, Model0_skb);
 Model0_qdisc_qstats_backlog_inc(Model0_sch, Model0_skb);

 return 0x00;
}

static inline __attribute__((no_instrument_function)) int Model0_qdisc_enqueue_tail(struct Model0_sk_buff *Model0_skb, struct Model0_Qdisc *Model0_sch)
{
 return Model0___qdisc_enqueue_tail(Model0_skb, Model0_sch, &Model0_sch->Model0_q);
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0___qdisc_dequeue_head(struct Model0_Qdisc *Model0_sch,
         struct Model0_sk_buff_head *Model0_list)
{
 struct Model0_sk_buff *Model0_skb = Model0___skb_dequeue(Model0_list);

 if (__builtin_expect(!!(Model0_skb != ((void *)0)), 1)) {
  Model0_qdisc_qstats_backlog_dec(Model0_sch, Model0_skb);
  Model0_qdisc_bstats_update(Model0_sch, Model0_skb);
 }

 return Model0_skb;
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_qdisc_dequeue_head(struct Model0_Qdisc *Model0_sch)
{
 return Model0___qdisc_dequeue_head(Model0_sch, &Model0_sch->Model0_q);
}

/* Instead of calling kfree_skb() while root qdisc lock is held,
 * queue the skb for future freeing at end of __dev_xmit_skb()
 */
static inline __attribute__((no_instrument_function)) void Model0___qdisc_drop(struct Model0_sk_buff *Model0_skb, struct Model0_sk_buff **Model0_to_free)
{
 Model0_skb->Model0_next = *Model0_to_free;
 *Model0_to_free = Model0_skb;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0___qdisc_queue_drop_head(struct Model0_Qdisc *Model0_sch,
         struct Model0_sk_buff_head *Model0_list,
         struct Model0_sk_buff **Model0_to_free)
{
 struct Model0_sk_buff *Model0_skb = Model0___skb_dequeue(Model0_list);

 if (__builtin_expect(!!(Model0_skb != ((void *)0)), 1)) {
  unsigned int Model0_len = Model0_qdisc_pkt_len(Model0_skb);

  Model0_qdisc_qstats_backlog_dec(Model0_sch, Model0_skb);
  Model0___qdisc_drop(Model0_skb, Model0_to_free);
  return Model0_len;
 }

 return 0;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_qdisc_queue_drop_head(struct Model0_Qdisc *Model0_sch,
       struct Model0_sk_buff **Model0_to_free)
{
 return Model0___qdisc_queue_drop_head(Model0_sch, &Model0_sch->Model0_q, Model0_to_free);
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_qdisc_peek_head(struct Model0_Qdisc *Model0_sch)
{
 return Model0_skb_peek(&Model0_sch->Model0_q);
}

/* generic pseudo peek method for non-work-conserving qdisc */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_qdisc_peek_dequeued(struct Model0_Qdisc *Model0_sch)
{
 /* we can reuse ->gso_skb because peek isn't called for root qdiscs */
 if (!Model0_sch->Model0_gso_skb) {
  Model0_sch->Model0_gso_skb = Model0_sch->Model0_dequeue(Model0_sch);
  if (Model0_sch->Model0_gso_skb) {
   /* it's still part of the queue */
   Model0_qdisc_qstats_backlog_inc(Model0_sch, Model0_sch->Model0_gso_skb);
   Model0_sch->Model0_q.Model0_qlen++;
  }
 }

 return Model0_sch->Model0_gso_skb;
}

/* use instead of qdisc->dequeue() for all qdiscs queried with ->peek() */
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_qdisc_dequeue_peeked(struct Model0_Qdisc *Model0_sch)
{
 struct Model0_sk_buff *Model0_skb = Model0_sch->Model0_gso_skb;

 if (Model0_skb) {
  Model0_sch->Model0_gso_skb = ((void *)0);
  Model0_qdisc_qstats_backlog_dec(Model0_sch, Model0_skb);
  Model0_sch->Model0_q.Model0_qlen--;
 } else {
  Model0_skb = Model0_sch->Model0_dequeue(Model0_sch);
 }

 return Model0_skb;
}

static inline __attribute__((no_instrument_function)) void Model0___qdisc_reset_queue(struct Model0_sk_buff_head *Model0_list)
{
 /*
	 * We do not know the backlog in bytes of this list, it
	 * is up to the caller to correct it
	 */
 if (!Model0_skb_queue_empty(Model0_list)) {
  Model0_rtnl_kfree_skbs(Model0_list->Model0_next, Model0_list->Model0_prev);
  Model0___skb_queue_head_init(Model0_list);
 }
}

static inline __attribute__((no_instrument_function)) void Model0_qdisc_reset_queue(struct Model0_Qdisc *Model0_sch)
{
 Model0___qdisc_reset_queue(&Model0_sch->Model0_q);
 Model0_sch->Model0_qstats.Model0_backlog = 0;
}

static inline __attribute__((no_instrument_function)) struct Model0_Qdisc *Model0_qdisc_replace(struct Model0_Qdisc *Model0_sch, struct Model0_Qdisc *Model0_new,
       struct Model0_Qdisc **Model0_pold)
{
 struct Model0_Qdisc *old;

 Model0_sch_tree_lock(Model0_sch);
 old = *Model0_pold;
 *Model0_pold = Model0_new;
 if (old != ((void *)0)) {
  Model0_qdisc_tree_reduce_backlog(old, old->Model0_q.Model0_qlen, old->Model0_qstats.Model0_backlog);
  Model0_qdisc_reset(old);
 }
 Model0_sch_tree_unlock(Model0_sch);

 return old;
}

static inline __attribute__((no_instrument_function)) void Model0_rtnl_qdisc_drop(struct Model0_sk_buff *Model0_skb, struct Model0_Qdisc *Model0_sch)
{
 Model0_rtnl_kfree_skbs(Model0_skb, Model0_skb);
 Model0_qdisc_qstats_drop(Model0_sch);
}


static inline __attribute__((no_instrument_function)) int Model0_qdisc_drop(struct Model0_sk_buff *Model0_skb, struct Model0_Qdisc *Model0_sch,
        struct Model0_sk_buff **Model0_to_free)
{
 Model0___qdisc_drop(Model0_skb, Model0_to_free);
 Model0_qdisc_qstats_drop(Model0_sch);

 return 0x01;
}

/* Length to Time (L2T) lookup in a qdisc_rate_table, to determine how
   long it will take to send a packet given its size.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_qdisc_l2t(struct Model0_qdisc_rate_table* Model0_rtab, unsigned int Model0_pktlen)
{
 int Model0_slot = Model0_pktlen + Model0_rtab->Model0_rate.Model0_cell_align + Model0_rtab->Model0_rate.Model0_overhead;
 if (Model0_slot < 0)
  Model0_slot = 0;
 Model0_slot >>= Model0_rtab->Model0_rate.Model0_cell_log;
 if (Model0_slot > 255)
  return Model0_rtab->Model0_data[255]*(Model0_slot >> 8) + Model0_rtab->Model0_data[Model0_slot & 0xFF];
 return Model0_rtab->Model0_data[Model0_slot];
}

struct Model0_psched_ratecfg {
 Model0_u64 Model0_rate_bytes_ps; /* bytes per second */
 Model0_u32 Model0_mult;
 Model0_u16 Model0_overhead;
 Model0_u8 Model0_linklayer;
 Model0_u8 Model0_shift;
};

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_psched_l2t_ns(const struct Model0_psched_ratecfg *Model0_r,
    unsigned int Model0_len)
{
 Model0_len += Model0_r->Model0_overhead;

 if (__builtin_expect(!!(Model0_r->Model0_linklayer == Model0_TC_LINKLAYER_ATM), 0))
  return ((Model0_u64)((((Model0_len) + (48) - 1) / (48))*53) * Model0_r->Model0_mult) >> Model0_r->Model0_shift;

 return ((Model0_u64)Model0_len * Model0_r->Model0_mult) >> Model0_r->Model0_shift;
}

void Model0_psched_ratecfg_precompute(struct Model0_psched_ratecfg *Model0_r,
          const struct Model0_tc_ratespec *Model0_conf,
          Model0_u64 Model0_rate64);

static inline __attribute__((no_instrument_function)) void Model0_psched_ratecfg_getrate(struct Model0_tc_ratespec *Model0_res,
       const struct Model0_psched_ratecfg *Model0_r)
{
 memset(Model0_res, 0, sizeof(*Model0_res));

 /* legacy struct tc_ratespec has a 32bit @rate field
	 * Qdisc using 64bit rate should add new attributes
	 * in order to maintain compatibility.
	 */
 Model0_res->Model0_rate = ({ Model0_u64 Model0___min1 = (Model0_r->Model0_rate_bytes_ps); Model0_u64 Model0___min2 = (~0U); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; });

 Model0_res->Model0_overhead = Model0_r->Model0_overhead;
 Model0_res->Model0_linklayer = (Model0_r->Model0_linklayer & 0x0F);
}




/* Caches aren't brain-dead on the intel. */




/* Keep includes the same across arches.  */


/*
 * The cache doesn't need to be flushed when TLB entries change when
 * the cache is mapped to physical memory, not virtual memory
 */



/*
 * The set_memory_* API can be used to change various attributes of a virtual
 * address range. The attributes include:
 * Cachability   : UnCached, WriteCombining, WriteThrough, WriteBack
 * Executability : eXeutable, NoteXecutable
 * Read/Write    : ReadOnly, ReadWrite
 * Presence      : NotPresent
 *
 * Within a category, the attributes are mutually exclusive.
 *
 * The implementation of this API will take care of various aspects that
 * are associated with changing such attributes, such as:
 * - Flushing TLBs
 * - Flushing CPU caches
 * - Making sure aliases of the memory behind the mapping don't violate
 *   coherency rules as defined by the CPU in the system.
 *
 * What this API does not do:
 * - Provide exclusion between various callers - including callers that
 *   operation on other mappings of the same physical page
 * - Restore default attributes when a page is freed
 * - Guarantee that mappings other than the requested one are
 *   in any state, other than that these do not violate rules for
 *   the CPU you have. Do not depend on any effects on other mappings,
 *   CPUs other than the one you have may have more relaxed rules.
 * The caller is required to take care of these.
 */

int Model0__set_memory_uc(unsigned long Model0_addr, int Model0_numpages);
int Model0__set_memory_wc(unsigned long Model0_addr, int Model0_numpages);
int Model0__set_memory_wt(unsigned long Model0_addr, int Model0_numpages);
int Model0__set_memory_wb(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_uc(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_wc(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_wt(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_wb(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_x(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_nx(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_ro(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_rw(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_np(unsigned long Model0_addr, int Model0_numpages);
int Model0_set_memory_4k(unsigned long Model0_addr, int Model0_numpages);

int Model0_set_memory_array_uc(unsigned long *Model0_addr, int Model0_addrinarray);
int Model0_set_memory_array_wc(unsigned long *Model0_addr, int Model0_addrinarray);
int Model0_set_memory_array_wt(unsigned long *Model0_addr, int Model0_addrinarray);
int Model0_set_memory_array_wb(unsigned long *Model0_addr, int Model0_addrinarray);

int Model0_set_pages_array_uc(struct Model0_page **Model0_pages, int Model0_addrinarray);
int Model0_set_pages_array_wc(struct Model0_page **Model0_pages, int Model0_addrinarray);
int Model0_set_pages_array_wt(struct Model0_page **Model0_pages, int Model0_addrinarray);
int Model0_set_pages_array_wb(struct Model0_page **Model0_pages, int Model0_addrinarray);

/*
 * For legacy compatibility with the old APIs, a few functions
 * are provided that work on a "struct page".
 * These functions operate ONLY on the 1:1 kernel mapping of the
 * memory that the struct page represents, and internally just
 * call the set_memory_* function. See the description of the
 * set_memory_* function for more details on conventions.
 *
 * These APIs should be considered *deprecated* and are likely going to
 * be removed in the future.
 * The reason for this is the implicit operation on the 1:1 mapping only,
 * making this not a generally useful API.
 *
 * Specifically, many users of the old APIs had a virtual address,
 * called virt_to_page() or vmalloc_to_page() on that address to
 * get a struct page* that the old API required.
 * To convert these cases, use set_memory_*() on the original
 * virtual address, do not use these functions.
 */

int Model0_set_pages_uc(struct Model0_page *Model0_page, int Model0_numpages);
int Model0_set_pages_wb(struct Model0_page *Model0_page, int Model0_numpages);
int Model0_set_pages_x(struct Model0_page *Model0_page, int Model0_numpages);
int Model0_set_pages_nx(struct Model0_page *Model0_page, int Model0_numpages);
int Model0_set_pages_ro(struct Model0_page *Model0_page, int Model0_numpages);
int Model0_set_pages_rw(struct Model0_page *Model0_page, int Model0_numpages);


void Model0_clflush_cache_range(void *Model0_addr, unsigned int Model0_size);



extern const int Model0_rodata_test_data;
extern int Model0_kernel_set_to_readonly;
void Model0_set_kernel_text_rw(void);
void Model0_set_kernel_text_ro(void);




static inline __attribute__((no_instrument_function)) int Model0_rodata_test(void)
{
 return 0;
}

/*
 * Linux Socket Filter Data Structures
 */










/* Instruction classes */
/* ld/ldx fields */
/* alu/jmp fields */

/*
 * Current version of the filter code architecture.
 */



/*
 *	Try and keep these values and structures similar to BSD, especially
 *	the BPF code definitions which need to match so you can share filters
 */

struct Model0_sock_filter { /* Filter block */
 Model0___u16 Model0_code; /* Actual filter code */
 __u8 Model0_jt; /* Jump true */
 __u8 Model0_jf; /* Jump false */
 __u32 Model0_k; /* Generic multiuse field */
};

struct Model0_sock_fprog { /* Required for SO_ATTACH_FILTER. */
 unsigned short Model0_len; /* Number of filter blocks */
 struct Model0_sock_filter *Model0_filter;
};

/* ret - BPF_K and BPF_X also apply */



/* misc */




/*
 * Macros for filter block array initializers.
 */







/*
 * Number of scratch memory words for: BPF_ST and BPF_STX
 */


/* RATIONALE. Negative offsets are invalid in BPF.
   We use them to reference ancillary data.
   Unlike introduction new instructions, it does not break
   existing compilers/optimizers.
 */
/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of version 2 of the GNU General Public
 * License as published by the Free Software Foundation.
 */






/* Extended instruction set based on top of classic BPF */

/* instruction classes */


/* ld/ldx fields */



/* alu/jmp fields */



/* change endianness of a register */
/* Register numbers */
enum {
 Model0_BPF_REG_0 = 0,
 Model0_BPF_REG_1,
 Model0_BPF_REG_2,
 Model0_BPF_REG_3,
 Model0_BPF_REG_4,
 Model0_BPF_REG_5,
 Model0_BPF_REG_6,
 Model0_BPF_REG_7,
 Model0_BPF_REG_8,
 Model0_BPF_REG_9,
 Model0_BPF_REG_10,
 Model0___MAX_BPF_REG,
};

/* BPF has 10 general purpose 64-bit registers and stack frame. */


struct Model0_bpf_insn {
 __u8 Model0_code; /* opcode */
 __u8 Model0_dst_reg:4; /* dest register */
 __u8 Model0_src_reg:4; /* source register */
 Model0___s16 Model0_off; /* signed offset */
 Model0___s32 Model0_imm; /* signed immediate constant */
};

/* BPF syscall commands, see bpf(2) man-page for details. */
enum Model0_bpf_cmd {
 Model0_BPF_MAP_CREATE,
 Model0_BPF_MAP_LOOKUP_ELEM,
 Model0_BPF_MAP_UPDATE_ELEM,
 Model0_BPF_MAP_DELETE_ELEM,
 Model0_BPF_MAP_GET_NEXT_KEY,
 Model0_BPF_PROG_LOAD,
 Model0_BPF_OBJ_PIN,
 Model0_BPF_OBJ_GET,
};

enum Model0_bpf_map_type {
 Model0_BPF_MAP_TYPE_UNSPEC,
 Model0_BPF_MAP_TYPE_HASH,
 Model0_BPF_MAP_TYPE_ARRAY,
 Model0_BPF_MAP_TYPE_PROG_ARRAY,
 Model0_BPF_MAP_TYPE_PERF_EVENT_ARRAY,
 Model0_BPF_MAP_TYPE_PERCPU_HASH,
 Model0_BPF_MAP_TYPE_PERCPU_ARRAY,
 Model0_BPF_MAP_TYPE_STACK_TRACE,
 Model0_BPF_MAP_TYPE_CGROUP_ARRAY,
};

enum Model0_bpf_prog_type {
 Model0_BPF_PROG_TYPE_UNSPEC,
 Model0_BPF_PROG_TYPE_SOCKET_FILTER,
 Model0_BPF_PROG_TYPE_KPROBE,
 Model0_BPF_PROG_TYPE_SCHED_CLS,
 Model0_BPF_PROG_TYPE_SCHED_ACT,
 Model0_BPF_PROG_TYPE_TRACEPOINT,
 Model0_BPF_PROG_TYPE_XDP,
};



/* flags for BPF_MAP_UPDATE_ELEM command */






union Model0_bpf_attr {
 struct { /* anonymous struct used by BPF_MAP_CREATE command */
  __u32 Model0_map_type; /* one of enum bpf_map_type */
  __u32 Model0_key_size; /* size of key in bytes */
  __u32 Model0_value_size; /* size of value in bytes */
  __u32 Model0_max_entries; /* max number of entries in a map */
  __u32 Model0_map_flags; /* prealloc or not */
 };

 struct { /* anonymous struct used by BPF_MAP_*_ELEM commands */
  __u32 Model0_map_fd;
  __u64 __attribute__((aligned(8))) Model0_key;
  union {
   __u64 __attribute__((aligned(8))) Model0_value;
   __u64 __attribute__((aligned(8))) Model0_next_key;
  };
  __u64 Model0_flags;
 };

 struct { /* anonymous struct used by BPF_PROG_LOAD command */
  __u32 Model0_prog_type; /* one of enum bpf_prog_type */
  __u32 Model0_insn_cnt;
  __u64 __attribute__((aligned(8))) Model0_insns;
  __u64 __attribute__((aligned(8))) Model0_license;
  __u32 Model0_log_level; /* verbosity level of verifier */
  __u32 Model0_log_size; /* size of user buffer */
  __u64 __attribute__((aligned(8))) Model0_log_buf; /* user supplied buffer */
  __u32 Model0_kern_version; /* checked when prog_type=kprobe */
 };

 struct { /* anonymous struct used by BPF_OBJ_* commands */
  __u64 __attribute__((aligned(8))) Model0_pathname;
  __u32 Model0_bpf_fd;
 };
} __attribute__((aligned(8)));

/* integer value in 'imm' field of BPF_CALL instruction selects which helper
 * function eBPF program intends to call
 */
enum Model0_bpf_func_id {
 Model0_BPF_FUNC_unspec,
 Model0_BPF_FUNC_map_lookup_elem, /* void *map_lookup_elem(&map, &key) */
 Model0_BPF_FUNC_map_update_elem, /* int map_update_elem(&map, &key, &value, flags) */
 Model0_BPF_FUNC_map_delete_elem, /* int map_delete_elem(&map, &key) */
 Model0_BPF_FUNC_probe_read, /* int bpf_probe_read(void *dst, int size, void *src) */
 Model0_BPF_FUNC_ktime_get_ns, /* u64 bpf_ktime_get_ns(void) */
 Model0_BPF_FUNC_trace_printk, /* int bpf_trace_printk(const char *fmt, int fmt_size, ...) */
 Model0_BPF_FUNC_get_prandom_u32, /* u32 prandom_u32(void) */
 Model0_BPF_FUNC_get_smp_processor_id, /* u32 raw_smp_processor_id(void) */

 /**
	 * skb_store_bytes(skb, offset, from, len, flags) - store bytes into packet
	 * @skb: pointer to skb
	 * @offset: offset within packet from skb->mac_header
	 * @from: pointer where to copy bytes from
	 * @len: number of bytes to store into packet
	 * @flags: bit 0 - if true, recompute skb->csum
	 *         other bits - reserved
	 * Return: 0 on success
	 */
 Model0_BPF_FUNC_skb_store_bytes,

 /**
	 * l3_csum_replace(skb, offset, from, to, flags) - recompute IP checksum
	 * @skb: pointer to skb
	 * @offset: offset within packet where IP checksum is located
	 * @from: old value of header field
	 * @to: new value of header field
	 * @flags: bits 0-3 - size of header field
	 *         other bits - reserved
	 * Return: 0 on success
	 */
 Model0_BPF_FUNC_l3_csum_replace,

 /**
	 * l4_csum_replace(skb, offset, from, to, flags) - recompute TCP/UDP checksum
	 * @skb: pointer to skb
	 * @offset: offset within packet where TCP/UDP checksum is located
	 * @from: old value of header field
	 * @to: new value of header field
	 * @flags: bits 0-3 - size of header field
	 *         bit 4 - is pseudo header
	 *         other bits - reserved
	 * Return: 0 on success
	 */
 Model0_BPF_FUNC_l4_csum_replace,

 /**
	 * bpf_tail_call(ctx, prog_array_map, index) - jump into another BPF program
	 * @ctx: context pointer passed to next program
	 * @prog_array_map: pointer to map which type is BPF_MAP_TYPE_PROG_ARRAY
	 * @index: index inside array that selects specific program to run
	 * Return: 0 on success
	 */
 Model0_BPF_FUNC_tail_call,

 /**
	 * bpf_clone_redirect(skb, ifindex, flags) - redirect to another netdev
	 * @skb: pointer to skb
	 * @ifindex: ifindex of the net device
	 * @flags: bit 0 - if set, redirect to ingress instead of egress
	 *         other bits - reserved
	 * Return: 0 on success
	 */
 Model0_BPF_FUNC_clone_redirect,

 /**
	 * u64 bpf_get_current_pid_tgid(void)
	 * Return: current->tgid << 32 | current->pid
	 */
 Model0_BPF_FUNC_get_current_pid_tgid,

 /**
	 * u64 bpf_get_current_uid_gid(void)
	 * Return: current_gid << 32 | current_uid
	 */
 Model0_BPF_FUNC_get_current_uid_gid,

 /**
	 * bpf_get_current_comm(char *buf, int size_of_buf)
	 * stores current->comm into buf
	 * Return: 0 on success
	 */
 Model0_BPF_FUNC_get_current_comm,

 /**
	 * bpf_get_cgroup_classid(skb) - retrieve a proc's classid
	 * @skb: pointer to skb
	 * Return: classid if != 0
	 */
 Model0_BPF_FUNC_get_cgroup_classid,
 Model0_BPF_FUNC_skb_vlan_push, /* bpf_skb_vlan_push(skb, vlan_proto, vlan_tci) */
 Model0_BPF_FUNC_skb_vlan_pop, /* bpf_skb_vlan_pop(skb) */

 /**
	 * bpf_skb_[gs]et_tunnel_key(skb, key, size, flags)
	 * retrieve or populate tunnel metadata
	 * @skb: pointer to skb
	 * @key: pointer to 'struct bpf_tunnel_key'
	 * @size: size of 'struct bpf_tunnel_key'
	 * @flags: room for future extensions
	 * Retrun: 0 on success
	 */
 Model0_BPF_FUNC_skb_get_tunnel_key,
 Model0_BPF_FUNC_skb_set_tunnel_key,
 Model0_BPF_FUNC_perf_event_read, /* u64 bpf_perf_event_read(&map, index) */
 /**
	 * bpf_redirect(ifindex, flags) - redirect to another netdev
	 * @ifindex: ifindex of the net device
	 * @flags: bit 0 - if set, redirect to ingress instead of egress
	 *         other bits - reserved
	 * Return: TC_ACT_REDIRECT
	 */
 Model0_BPF_FUNC_redirect,

 /**
	 * bpf_get_route_realm(skb) - retrieve a dst's tclassid
	 * @skb: pointer to skb
	 * Return: realm if != 0
	 */
 Model0_BPF_FUNC_get_route_realm,

 /**
	 * bpf_perf_event_output(ctx, map, index, data, size) - output perf raw sample
	 * @ctx: struct pt_regs*
	 * @map: pointer to perf_event_array map
	 * @index: index of event in the map
	 * @data: data on stack to be output as raw data
	 * @size: size of data
	 * Return: 0 on success
	 */
 Model0_BPF_FUNC_perf_event_output,
 Model0_BPF_FUNC_skb_load_bytes,

 /**
	 * bpf_get_stackid(ctx, map, flags) - walk user or kernel stack and return id
	 * @ctx: struct pt_regs*
	 * @map: pointer to stack_trace map
	 * @flags: bits 0-7 - numer of stack frames to skip
	 *         bit 8 - collect user stack instead of kernel
	 *         bit 9 - compare stacks by hash only
	 *         bit 10 - if two different stacks hash into the same stackid
	 *                  discard old
	 *         other bits - reserved
	 * Return: >= 0 stackid on success or negative error
	 */
 Model0_BPF_FUNC_get_stackid,

 /**
	 * bpf_csum_diff(from, from_size, to, to_size, seed) - calculate csum diff
	 * @from: raw from buffer
	 * @from_size: length of from buffer
	 * @to: raw to buffer
	 * @to_size: length of to buffer
	 * @seed: optional seed
	 * Return: csum result
	 */
 Model0_BPF_FUNC_csum_diff,

 /**
	 * bpf_skb_[gs]et_tunnel_opt(skb, opt, size)
	 * retrieve or populate tunnel options metadata
	 * @skb: pointer to skb
	 * @opt: pointer to raw tunnel option data
	 * @size: size of @opt
	 * Return: 0 on success for set, option size for get
	 */
 Model0_BPF_FUNC_skb_get_tunnel_opt,
 Model0_BPF_FUNC_skb_set_tunnel_opt,

 /**
	 * bpf_skb_change_proto(skb, proto, flags)
	 * Change protocol of the skb. Currently supported is
	 * v4 -> v6, v6 -> v4 transitions. The helper will also
	 * resize the skb. eBPF program is expected to fill the
	 * new headers via skb_store_bytes and lX_csum_replace.
	 * @skb: pointer to skb
	 * @proto: new skb->protocol type
	 * @flags: reserved
	 * Return: 0 on success or negative error
	 */
 Model0_BPF_FUNC_skb_change_proto,

 /**
	 * bpf_skb_change_type(skb, type)
	 * Change packet type of skb.
	 * @skb: pointer to skb
	 * @type: new skb->pkt_type type
	 * Return: 0 on success or negative error
	 */
 Model0_BPF_FUNC_skb_change_type,

 /**
	 * bpf_skb_under_cgroup(skb, map, index) - Check cgroup2 membership of skb
	 * @skb: pointer to skb
	 * @map: pointer to bpf_map in BPF_MAP_TYPE_CGROUP_ARRAY type
	 * @index: index of the cgroup in the bpf_map
	 * Return:
	 *   == 0 skb failed the cgroup2 descendant test
	 *   == 1 skb succeeded the cgroup2 descendant test
	 *    < 0 error
	 */
 Model0_BPF_FUNC_skb_under_cgroup,

 /**
	 * bpf_get_hash_recalc(skb)
	 * Retrieve and possibly recalculate skb->hash.
	 * @skb: pointer to skb
	 * Return: hash
	 */
 Model0_BPF_FUNC_get_hash_recalc,

 /**
	 * u64 bpf_get_current_task(void)
	 * Returns current task_struct
	 * Return: current
	 */
 Model0_BPF_FUNC_get_current_task,

 /**
	 * bpf_probe_write_user(void *dst, void *src, int len)
	 * safely attempt to write to a location
	 * @dst: destination address in userspace
	 * @src: source address on stack
	 * @len: number of bytes to copy
	 * Return: 0 on success or negative error
	 */
 Model0_BPF_FUNC_probe_write_user,

 Model0___BPF_FUNC_MAX_ID,
};

/* All flags used by eBPF helper functions, placed here. */

/* BPF_FUNC_skb_store_bytes flags. */



/* BPF_FUNC_l3_csum_replace and BPF_FUNC_l4_csum_replace flags.
 * First 4 bits are for passing the header field size.
 */


/* BPF_FUNC_l4_csum_replace flags. */



/* BPF_FUNC_clone_redirect and BPF_FUNC_redirect flags. */


/* BPF_FUNC_skb_set_tunnel_key and BPF_FUNC_skb_get_tunnel_key flags. */


/* BPF_FUNC_get_stackid flags. */





/* BPF_FUNC_skb_set_tunnel_key flags. */



/* BPF_FUNC_perf_event_output and BPF_FUNC_perf_event_read flags. */


/* BPF_FUNC_perf_event_output for sk_buff input context. */


/* user accessible mirror of in-kernel sk_buff.
 * new fields can only be added to the end of this structure
 */
struct Model0___sk_buff {
 __u32 Model0_len;
 __u32 Model0_pkt_type;
 __u32 Model0_mark;
 __u32 Model0_queue_mapping;
 __u32 Model0_protocol;
 __u32 Model0_vlan_present;
 __u32 Model0_vlan_tci;
 __u32 Model0_vlan_proto;
 __u32 Model0_priority;
 __u32 Model0_ingress_ifindex;
 __u32 Model0_ifindex;
 __u32 Model0_tc_index;
 __u32 Model0_cb[5];
 __u32 Model0_hash;
 __u32 Model0_tc_classid;
 __u32 Model0_data;
 __u32 Model0_data_end;
};

struct Model0_bpf_tunnel_key {
 __u32 Model0_tunnel_id;
 union {
  __u32 Model0_remote_ipv4;
  __u32 Model0_remote_ipv6[4];
 };
 __u8 Model0_tunnel_tos;
 __u8 Model0_tunnel_ttl;
 Model0___u16 Model0_tunnel_ext;
 __u32 Model0_tunnel_label;
};

/* User return codes for XDP prog type.
 * A valid XDP program must return one of these defined values. All other
 * return codes are reserved for future use. Unknown return codes will result
 * in packet drop.
 */
enum Model0_xdp_action {
 Model0_XDP_ABORTED = 0,
 Model0_XDP_DROP,
 Model0_XDP_PASS,
 Model0_XDP_TX,
};

/* user accessible metadata for XDP packet hook
 * new fields must be added to the end of this structure
 */
struct Model0_xdp_md {
 __u32 Model0_data;
 __u32 Model0_data_end;
};

struct Model0_sk_buff;
struct Model0_sock;
struct Model0_seccomp_data;
struct Model0_bpf_prog_aux;

/* ArgX, context and stack frame pointer register positions. Note,
 * Arg1, Arg2, Arg3, etc are used as argument mappings of function
 * calls in BPF_CALL instruction.
 */
/* Additional register mappings for converted user programs. */




/* Kernel hidden auxiliary/helper register for hardening step.
 * Only used by eBPF JITs. It's nothing more than a temporary
 * register that JITs use internally, only that here it's part
 * of eBPF instructions that have been rewritten for blinding
 * constants. See JIT pre-step in bpf_jit_blind_constants().
 */



/* BPF program can access up to 512 bytes of stack space. */


/* Helper macros for filter block array initializers. */

/* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
/* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
/* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
/* Short form of mov, dst_reg = src_reg */
/* Short form of mov, dst_reg = imm32 */
/* BPF_LD_IMM64 macro encodes single 'load 64-bit immediate' insn */
/* pseudo BPF_LD_IMM64 insn used to refer to process-local map_fd */



/* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
/* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
/* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
/* Memory load, dst_reg = *(uint *) (src_reg + off16) */
/* Memory store, *(uint *) (dst_reg + off16) = src_reg */
/* Atomic memory add, *(uint *)(dst_reg + off16) += src_reg */
/* Memory store, *(uint *) (dst_reg + off16) = imm32 */
/* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
/* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
/* Function call */
/* Raw code statement block */
/* Program exit */
/* Internal classic blocks for direct assignment */
/* A struct sock_filter is architecture independent. */
struct Model0_compat_sock_fprog {
 Model0_u16 Model0_len;
 Model0_compat_uptr_t Model0_filter; /* struct sock_filter * */
};


struct Model0_sock_fprog_kern {
 Model0_u16 Model0_len;
 struct Model0_sock_filter *Model0_filter;
};

struct Model0_bpf_binary_header {
 unsigned int Model0_pages;
 Model0_u8 Model0_image[];
};

struct Model0_bpf_prog {
 Model0_u16 Model0_pages; /* Number of allocated pages */
                               ;
 Model0_u16 Model0_jited:1, /* Is our filter JIT'ed? */
    Model0_gpl_compatible:1, /* Is filter GPL compatible? */
    Model0_cb_access:1, /* Is control block accessed? */
    Model0_dst_needed:1; /* Do we need dst entry? */
                             ;
 Model0_u32 Model0_len; /* Number of filter blocks */
 enum Model0_bpf_prog_type Model0_type; /* Type of BPF program */
 struct Model0_bpf_prog_aux *Model0_aux; /* Auxiliary fields */
 struct Model0_sock_fprog_kern *Model0_orig_prog; /* Original BPF program */
 unsigned int (*Model0_bpf_func)(const struct Model0_sk_buff *Model0_skb,
         const struct Model0_bpf_insn *Model0_filter);
 /* Instructions for interpreter */
 union {
  struct Model0_sock_filter Model0_insns[0];
  struct Model0_bpf_insn Model0_insnsi[0];
 };
};

struct Model0_sk_filter {
 Model0_atomic_t Model0_refcnt;
 struct Model0_callback_head Model0_rcu;
 struct Model0_bpf_prog *Model0_prog;
};





struct Model0_bpf_skb_data_end {
 struct Model0_qdisc_skb_cb Model0_qdisc_cb;
 void *Model0_data_end;
};

struct Model0_xdp_buff {
 void *Model0_data;
 void *Model0_data_end;
};

/* compute the linear packet data range [data, data_end) which
 * will be accessed by cls_bpf and act_bpf programs
 */
static inline __attribute__((no_instrument_function)) void Model0_bpf_compute_data_end(struct Model0_sk_buff *Model0_skb)
{
 struct Model0_bpf_skb_data_end *Model0_cb = (struct Model0_bpf_skb_data_end *)Model0_skb->Model0_cb;

 do { bool Model0___cond = !(!(sizeof(*Model0_cb) > (sizeof(((struct Model0_sk_buff*)0)->Model0_cb)))); extern void Model0___compiletime_assert_383(void) ; if (Model0___cond) Model0___compiletime_assert_383(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 Model0_cb->Model0_data_end = Model0_skb->Model0_data + Model0_skb_headlen(Model0_skb);
}

static inline __attribute__((no_instrument_function)) Model0_u8 *Model0_bpf_skb_cb(struct Model0_sk_buff *Model0_skb)
{
 /* eBPF programs may read/write skb->cb[] area to transfer meta
	 * data between tail calls. Since this also needs to work with
	 * tc, that scratch memory is mapped to qdisc_skb_cb's data area.
	 *
	 * In some socket filter cases, the cb unfortunately needs to be
	 * saved/restored so that protocol specific skb->cb[] data won't
	 * be lost. In any case, due to unpriviledged eBPF programs
	 * attached to sockets, we need to clear the bpf_skb_cb() area
	 * to not leak previous contents to user space.
	 */
 do { bool Model0___cond = !(!((sizeof(((struct Model0___sk_buff*)0)->Model0_cb)) != 20)); extern void Model0___compiletime_assert_399(void) ; if (Model0___cond) Model0___compiletime_assert_399(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 do { bool Model0___cond = !(!((sizeof(((struct Model0___sk_buff*)0)->Model0_cb)) != (sizeof(((struct Model0_qdisc_skb_cb*)0)->Model0_data)))); extern void Model0___compiletime_assert_401(void) ; if (Model0___cond) Model0___compiletime_assert_401(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);


 return Model0_qdisc_skb_cb(Model0_skb)->Model0_data;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_bpf_prog_run_save_cb(const struct Model0_bpf_prog *Model0_prog,
           struct Model0_sk_buff *Model0_skb)
{
 Model0_u8 *Model0_cb_data = Model0_bpf_skb_cb(Model0_skb);
 Model0_u8 Model0_cb_saved[20];
 Model0_u32 Model0_res;

 if (__builtin_expect(!!(Model0_prog->Model0_cb_access), 0)) {
  ({ Model0_size_t Model0___len = (sizeof(Model0_cb_saved)); void *Model0___ret; if (__builtin_constant_p(sizeof(Model0_cb_saved)) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_cb_saved), (Model0_cb_data), Model0___len); else Model0___ret = __builtin_memcpy((Model0_cb_saved), (Model0_cb_data), Model0___len); Model0___ret; });
  memset(Model0_cb_data, 0, sizeof(Model0_cb_saved));
 }

 Model0_res = (*Model0_prog->Model0_bpf_func)(Model0_skb, Model0_prog->Model0_insnsi);

 if (__builtin_expect(!!(Model0_prog->Model0_cb_access), 0))
  ({ Model0_size_t Model0___len = (sizeof(Model0_cb_saved)); void *Model0___ret; if (__builtin_constant_p(sizeof(Model0_cb_saved)) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_cb_data), (Model0_cb_saved), Model0___len); else Model0___ret = __builtin_memcpy((Model0_cb_data), (Model0_cb_saved), Model0___len); Model0___ret; });

 return Model0_res;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_bpf_prog_run_clear_cb(const struct Model0_bpf_prog *Model0_prog,
     struct Model0_sk_buff *Model0_skb)
{
 Model0_u8 *Model0_cb_data = Model0_bpf_skb_cb(Model0_skb);

 if (__builtin_expect(!!(Model0_prog->Model0_cb_access), 0))
  memset(Model0_cb_data, 0, 20);

 return (*Model0_prog->Model0_bpf_func)(Model0_skb, Model0_prog->Model0_insnsi);
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_bpf_prog_run_xdp(const struct Model0_bpf_prog *Model0_prog,
       struct Model0_xdp_buff *Model0_xdp)
{
 Model0_u32 Model0_ret;

 Model0_rcu_read_lock();
 Model0_ret = (*Model0_prog->Model0_bpf_func)((void *)Model0_xdp, Model0_prog->Model0_insnsi);
 Model0_rcu_read_unlock();

 return Model0_ret;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_bpf_prog_size(unsigned int Model0_proglen)
{
 return ({ typeof(sizeof(struct Model0_bpf_prog)) Model0__max1 = (sizeof(struct Model0_bpf_prog)); typeof(__builtin_offsetof(struct Model0_bpf_prog, Model0_insns[Model0_proglen])) Model0__max2 = (__builtin_offsetof(struct Model0_bpf_prog, Model0_insns[Model0_proglen])); (void) (&Model0__max1 == &Model0__max2); Model0__max1 > Model0__max2 ? Model0__max1 : Model0__max2; });

}

static inline __attribute__((no_instrument_function)) bool Model0_bpf_prog_was_classic(const struct Model0_bpf_prog *Model0_prog)
{
 /* When classic BPF programs have been loaded and the arch
	 * does not have a classic BPF JIT (anymore), they have been
	 * converted via bpf_migrate_filter() to eBPF and thus always
	 * have an unspec program type.
	 */
 return Model0_prog->Model0_type == Model0_BPF_PROG_TYPE_UNSPEC;
}
static inline __attribute__((no_instrument_function)) void Model0_bpf_prog_lock_ro(struct Model0_bpf_prog *Model0_fp)
{
}

static inline __attribute__((no_instrument_function)) void Model0_bpf_prog_unlock_ro(struct Model0_bpf_prog *Model0_fp)
{
}


int Model0_sk_filter_trim_cap(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, unsigned int Model0_cap);
static inline __attribute__((no_instrument_function)) int Model0_sk_filter(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 return Model0_sk_filter_trim_cap(Model0_sk, Model0_skb, 1);
}

struct Model0_bpf_prog *Model0_bpf_prog_select_runtime(struct Model0_bpf_prog *Model0_fp, int *err);
void Model0_bpf_prog_free(struct Model0_bpf_prog *Model0_fp);

struct Model0_bpf_prog *Model0_bpf_prog_alloc(unsigned int Model0_size, Model0_gfp_t Model0_gfp_extra_flags);
struct Model0_bpf_prog *Model0_bpf_prog_realloc(struct Model0_bpf_prog *Model0_fp_old, unsigned int Model0_size,
      Model0_gfp_t Model0_gfp_extra_flags);
void Model0___bpf_prog_free(struct Model0_bpf_prog *Model0_fp);

static inline __attribute__((no_instrument_function)) void Model0_bpf_prog_unlock_free(struct Model0_bpf_prog *Model0_fp)
{
 Model0_bpf_prog_unlock_ro(Model0_fp);
 Model0___bpf_prog_free(Model0_fp);
}

typedef int (*Model0_bpf_aux_classic_check_t)(struct Model0_sock_filter *Model0_filter,
           unsigned int Model0_flen);

int Model0_bpf_prog_create(struct Model0_bpf_prog **Model0_pfp, struct Model0_sock_fprog_kern *Model0_fprog);
int Model0_bpf_prog_create_from_user(struct Model0_bpf_prog **Model0_pfp, struct Model0_sock_fprog *Model0_fprog,
         Model0_bpf_aux_classic_check_t Model0_trans, bool Model0_save_orig);
void Model0_bpf_prog_destroy(struct Model0_bpf_prog *Model0_fp);

int Model0_sk_attach_filter(struct Model0_sock_fprog *Model0_fprog, struct Model0_sock *Model0_sk);
int Model0_sk_attach_bpf(Model0_u32 Model0_ufd, struct Model0_sock *Model0_sk);
int Model0_sk_reuseport_attach_filter(struct Model0_sock_fprog *Model0_fprog, struct Model0_sock *Model0_sk);
int Model0_sk_reuseport_attach_bpf(Model0_u32 Model0_ufd, struct Model0_sock *Model0_sk);
int Model0_sk_detach_filter(struct Model0_sock *Model0_sk);
int Model0_sk_get_filter(struct Model0_sock *Model0_sk, struct Model0_sock_filter *Model0_filter,
    unsigned int Model0_len);

bool Model0_sk_filter_charge(struct Model0_sock *Model0_sk, struct Model0_sk_filter *Model0_fp);
void Model0_sk_filter_uncharge(struct Model0_sock *Model0_sk, struct Model0_sk_filter *Model0_fp);

Model0_u64 Model0___bpf_call_base(Model0_u64 Model0_r1, Model0_u64 Model0_r2, Model0_u64 Model0_r3, Model0_u64 Model0_r4, Model0_u64 Model0_r5);

struct Model0_bpf_prog *Model0_bpf_int_jit_compile(struct Model0_bpf_prog *Model0_prog);
bool Model0_bpf_helper_changes_skb_data(void *func);

struct Model0_bpf_prog *Model0_bpf_patch_insn_single(struct Model0_bpf_prog *Model0_prog, Model0_u32 Model0_off,
           const struct Model0_bpf_insn *Model0_patch, Model0_u32 Model0_len);
void Model0_bpf_warn_invalid_xdp_action(Model0_u32 Model0_act);
static inline __attribute__((no_instrument_function)) void Model0_bpf_jit_compile(struct Model0_bpf_prog *Model0_fp)
{
}

static inline __attribute__((no_instrument_function)) void Model0_bpf_jit_free(struct Model0_bpf_prog *Model0_fp)
{
 Model0_bpf_prog_unlock_free(Model0_fp);
}




static inline __attribute__((no_instrument_function)) bool Model0_bpf_needs_clear_a(const struct Model0_sock_filter *Model0_first)
{
 switch (Model0_first->Model0_code) {
 case 0x06 | 0x00:
 case 0x00 | 0x00 | 0x80:
  return false;

 case 0x00 | 0x00 | 0x20:
 case 0x00 | 0x08 | 0x20:
 case 0x00 | 0x10 | 0x20:
  if (Model0_first->Model0_k == (-0x1000) + 40)
   return true;
  return false;

 default:
  return true;
 }
}

static inline __attribute__((no_instrument_function)) Model0_u16 Model0_bpf_anc_helper(const struct Model0_sock_filter *Model0_ftest)
{
 do { if (__builtin_expect(!!(Model0_ftest->Model0_code & (1UL << (15))), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/filter.h"), "i" (624), "i" (sizeof(struct Model0_bug_entry))); do { } while (1); } while (0); } while (0);

 switch (Model0_ftest->Model0_code) {
 case 0x00 | 0x00 | 0x20:
 case 0x00 | 0x08 | 0x20:
 case 0x00 | 0x10 | 0x20:


  switch (Model0_ftest->Model0_k) {
  case (-0x1000) + 0: return (1UL << (15)) | 0;
  case (-0x1000) + 4: return (1UL << (15)) | 4;
  case (-0x1000) + 8: return (1UL << (15)) | 8;
  case (-0x1000) + 12: return (1UL << (15)) | 12;
  case (-0x1000) + 16: return (1UL << (15)) | 16;
  case (-0x1000) + 20: return (1UL << (15)) | 20;
  case (-0x1000) + 24: return (1UL << (15)) | 24;
  case (-0x1000) + 28: return (1UL << (15)) | 28;
  case (-0x1000) + 32: return (1UL << (15)) | 32;
  case (-0x1000) + 36: return (1UL << (15)) | 36;
  case (-0x1000) + 40: return (1UL << (15)) | 40;
  case (-0x1000) + 44: return (1UL << (15)) | 44;
  case (-0x1000) + 48: return (1UL << (15)) | 48;
  case (-0x1000) + 52: return (1UL << (15)) | 52;
  case (-0x1000) + 56: return (1UL << (15)) | 56;
  case (-0x1000) + 60: return (1UL << (15)) | 60;
  }
  /* Fallthrough. */
 default:
  return Model0_ftest->Model0_code;
 }
}

void *Model0_bpf_internal_load_pointer_neg_helper(const struct Model0_sk_buff *Model0_skb,
        int Model0_k, unsigned int Model0_size);

static inline __attribute__((no_instrument_function)) void *Model0_bpf_load_pointer(const struct Model0_sk_buff *Model0_skb, int Model0_k,
         unsigned int Model0_size, void *Model0_buffer)
{
 if (Model0_k >= 0)
  return Model0_skb_header_pointer(Model0_skb, Model0_k, Model0_size, Model0_buffer);

 return Model0_bpf_internal_load_pointer_neg_helper(Model0_skb, Model0_k, Model0_size);
}

static inline __attribute__((no_instrument_function)) int Model0_bpf_tell_extensions(void)
{
 return 64;
}





/*
 * RCU-protected list version
 */



/**
 * hlist_nulls_del_init_rcu - deletes entry from hash list with re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_nulls_unhashed() on the node return true after this. It is
 * useful for RCU based read lockfree traversal if the writer side
 * must know if the list entry is still hashed or already unhashed.
 *
 * In particular, it means that we can not poison the forward pointers
 * that may still be used for walking the hash list and we can only
 * zero the pprev pointer so list_unhashed() will return true after
 * this.
 *
 * The caller must take whatever precautions are necessary (such as
 * holding appropriate locks) to avoid racing with another
 * list-mutation primitive, such as hlist_nulls_add_head_rcu() or
 * hlist_nulls_del_rcu(), running on this same list.  However, it is
 * perfectly legal to run concurrently with the _rcu list-traversal
 * primitives, such as hlist_nulls_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_nulls_del_init_rcu(struct Model0_hlist_nulls_node *Model0_n)
{
 if (!Model0_hlist_nulls_unhashed(Model0_n)) {
  Model0___hlist_nulls_del(Model0_n);
  Model0_n->Model0_pprev = ((void *)0);
 }
}







/**
 * hlist_nulls_del_rcu - deletes entry from hash list without re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_nulls_unhashed() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the hash list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
 * or hlist_nulls_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_nulls_for_each_entry().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_nulls_del_rcu(struct Model0_hlist_nulls_node *Model0_n)
{
 Model0___hlist_nulls_del(Model0_n);
 Model0_n->Model0_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_nulls_add_head_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist_nulls,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
 * or hlist_nulls_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_nulls_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_nulls_add_head_rcu(struct Model0_hlist_nulls_node *Model0_n,
     struct Model0_hlist_nulls_head *Model0_h)
{
 struct Model0_hlist_nulls_node *Model0_first = Model0_h->Model0_first;

 Model0_n->Model0_next = Model0_first;
 Model0_n->Model0_pprev = &Model0_h->Model0_first;
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_n); if (__builtin_constant_p(Model0_n) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))))) ((typeof((*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first)))), Model0___u.Model0___c, sizeof(((*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))) == sizeof(char) || sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))) == sizeof(short) || sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))) == sizeof(int) || sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))) == sizeof(long))); extern void Model0___compiletime_assert_97(void) ; if (Model0___cond) Model0___compiletime_assert_97(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first)))) ((typeof(*((typeof((*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first)))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 if (!Model0_is_a_nulls(Model0_first))
  Model0_first->Model0_pprev = &Model0_n->Model0_next;
}

/**
 * hlist_nulls_add_tail_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the end of the specified hlist_nulls,
 * while permitting racing traversals.  NOTE: tail insertion requires
 * list traversal.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
 * or hlist_nulls_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_nulls_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model0_hlist_nulls_add_tail_rcu(struct Model0_hlist_nulls_node *Model0_n,
     struct Model0_hlist_nulls_head *Model0_h)
{
 struct Model0_hlist_nulls_node *Model0_i, *Model0_last = ((void *)0);

 for (Model0_i = (*((struct Model0_hlist_nulls_node **)&(Model0_h)->Model0_first)); !Model0_is_a_nulls(Model0_i);
      Model0_i = (*((struct Model0_hlist_nulls_node **)&(Model0_i)->Model0_next)))
  Model0_last = Model0_i;

 if (Model0_last) {
  Model0_n->Model0_next = Model0_last->Model0_next;
  Model0_n->Model0_pprev = &Model0_last->Model0_next;
  ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_n); if (__builtin_constant_p(Model0_n) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next)))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(((*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))))) ((typeof((*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))))(Model0__r_a_p__v)) }; Model0___write_once_size(&(((*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next)))), Model0___u.Model0___c, sizeof(((*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))) == sizeof(char) || sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))) == sizeof(short) || sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))) == sizeof(int) || sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))) == sizeof(long))); extern void Model0___compiletime_assert_134(void) ; if (Model0___cond) Model0___compiletime_assert_134(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next)))) ((typeof(*((typeof((*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))))Model0__r_a_p__v)) *)((typeof((*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&(*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next))), Model0___u.Model0___c, sizeof(*&(*((struct Model0_hlist_nulls_node **)&(Model0_last)->Model0_next)))); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 } else {
  Model0_hlist_nulls_add_head_rcu(Model0_n, Model0_h);
 }
}

/**
 * hlist_nulls_for_each_entry_rcu - iterate over rcu list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_nulls_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_nulls_node within the struct.
 *
 * The barrier() is needed to make sure compiler doesn't cache first element [1],
 * as this loop can be restarted [2]
 * [1] Documentation/atomic_ops.txt around line 114
 * [2] Documentation/RCU/rculist_nulls.txt around line 146
 */



/* These are specified by iBCS2 */







/* The rest seem to be more-or-less nonstandard. Check them! */
struct Model0_pollfd {
 int Model0_fd;
 short Model0_events;
 short Model0_revents;
};

extern struct Model0_ctl_table Model0_epoll_table[]; /* for sysctl */
/* ~832 bytes of stack space used max in sys_select/sys_poll before allocating
   additional memory. */
struct Model0_poll_table_struct;

/* 
 * structures and helpers for f_op->poll implementations
 */
typedef void (*Model0_poll_queue_proc)(struct Model0_file *, Model0_wait_queue_head_t *, struct Model0_poll_table_struct *);

/*
 * Do not touch the structure directly, use the access functions
 * poll_does_not_wait() and poll_requested_events() instead.
 */
typedef struct Model0_poll_table_struct {
 Model0_poll_queue_proc Model0__qproc;
 unsigned long Model0__key;
} Model0_poll_table;

static inline __attribute__((no_instrument_function)) void Model0_poll_wait(struct Model0_file * Model0_filp, Model0_wait_queue_head_t * Model0_wait_address, Model0_poll_table *Model0_p)
{
 if (Model0_p && Model0_p->Model0__qproc && Model0_wait_address)
  Model0_p->Model0__qproc(Model0_filp, Model0_wait_address, Model0_p);
}

/*
 * Return true if it is guaranteed that poll will not wait. This is the case
 * if the poll() of another file descriptor in the set got an event, so there
 * is no need for waiting.
 */
static inline __attribute__((no_instrument_function)) bool Model0_poll_does_not_wait(const Model0_poll_table *Model0_p)
{
 return Model0_p == ((void *)0) || Model0_p->Model0__qproc == ((void *)0);
}

/*
 * Return the set of events that the application wants to poll for.
 * This is useful for drivers that need to know whether a DMA transfer has
 * to be started implicitly on poll(). You typically only want to do that
 * if the application is actually polling for POLLIN and/or POLLOUT.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_poll_requested_events(const Model0_poll_table *Model0_p)
{
 return Model0_p ? Model0_p->Model0__key : ~0UL;
}

static inline __attribute__((no_instrument_function)) void Model0_init_poll_funcptr(Model0_poll_table *Model0_pt, Model0_poll_queue_proc Model0_qproc)
{
 Model0_pt->Model0__qproc = Model0_qproc;
 Model0_pt->Model0__key = ~0UL; /* all events enabled */
}

struct Model0_poll_table_entry {
 struct Model0_file *Model0_filp;
 unsigned long Model0_key;
 Model0_wait_queue_t Model0_wait;
 Model0_wait_queue_head_t *Model0_wait_address;
};

/*
 * Structures and helpers for select/poll syscall
 */
struct Model0_poll_wqueues {
 Model0_poll_table Model0_pt;
 struct Model0_poll_table_page *Model0_table;
 struct Model0_task_struct *Model0_polling_task;
 int Model0_triggered;
 int error;
 int Model0_inline_index;
 struct Model0_poll_table_entry Model0_inline_entries[((832 - 256) / sizeof(struct Model0_poll_table_entry))];
};

extern void Model0_poll_initwait(struct Model0_poll_wqueues *Model0_pwq);
extern void Model0_poll_freewait(struct Model0_poll_wqueues *Model0_pwq);
extern int Model0_poll_schedule_timeout(struct Model0_poll_wqueues *Model0_pwq, int Model0_state,
     Model0_ktime_t *Model0_expires, unsigned long Model0_slack);
extern Model0_u64 Model0_select_estimate_accuracy(struct Model0_timespec *Model0_tv);


static inline __attribute__((no_instrument_function)) int Model0_poll_schedule(struct Model0_poll_wqueues *Model0_pwq, int Model0_state)
{
 return Model0_poll_schedule_timeout(Model0_pwq, Model0_state, ((void *)0), 0);
}

/*
 * Scalable version of the fd_set.
 */

typedef struct {
 unsigned long *Model0_in, *Model0_out, *Model0_ex;
 unsigned long *Model0_res_in, *Model0_res_out, *Model0_res_ex;
} Model0_fd_set_bits;

/*
 * How many longwords for "nr" bits?
 */




/*
 * We do a VERIFY_WRITE here even though we are only reading this time:
 * we'll write to it eventually..
 *
 * Use "unsigned long" accesses to let user-mode fd_set's be long-aligned.
 */
static inline __attribute__((no_instrument_function))
int Model0_get_fd_set(unsigned long Model0_nr, void *Model0_ufdset, unsigned long *Model0_fdset)
{
 Model0_nr = ((((Model0_nr)+(8*sizeof(long))-1)/(8*sizeof(long)))*sizeof(long));
 if (Model0_ufdset)
  return Model0_copy_from_user(Model0_fdset, Model0_ufdset, Model0_nr) ? -14 : 0;

 memset(Model0_fdset, 0, Model0_nr);
 return 0;
}

static inline __attribute__((no_instrument_function)) unsigned long __attribute__((warn_unused_result))
Model0_set_fd_set(unsigned long Model0_nr, void *Model0_ufdset, unsigned long *Model0_fdset)
{
 if (Model0_ufdset)
  return Model0___copy_to_user(Model0_ufdset, Model0_fdset, ((((Model0_nr)+(8*sizeof(long))-1)/(8*sizeof(long)))*sizeof(long)));
 return 0;
}

static inline __attribute__((no_instrument_function))
void Model0_zero_fd_set(unsigned long Model0_nr, unsigned long *Model0_fdset)
{
 memset(Model0_fdset, 0, ((((Model0_nr)+(8*sizeof(long))-1)/(8*sizeof(long)))*sizeof(long)));
}



extern int Model0_do_select(int Model0_n, Model0_fd_set_bits *Model0_fds, struct Model0_timespec *Model0_end_time);
extern int Model0_do_sys_poll(struct Model0_pollfd * Model0_ufds, unsigned int Model0_nfds,
         struct Model0_timespec *Model0_end_time);
extern int Model0_core_sys_select(int Model0_n, Model0_fd_set *Model0_inp, Model0_fd_set *Model0_outp,
      Model0_fd_set *Model0_exp, struct Model0_timespec *Model0_end_time);

extern int Model0_poll_select_set_timeout(struct Model0_timespec *Model0_to, Model0_time64_t Model0_sec,
       long Model0_nsec);




/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the TCP protocol sk_state field.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */



enum {
 Model0_TCP_ESTABLISHED = 1,
 Model0_TCP_SYN_SENT,
 Model0_TCP_SYN_RECV,
 Model0_TCP_FIN_WAIT1,
 Model0_TCP_FIN_WAIT2,
 Model0_TCP_TIME_WAIT,
 Model0_TCP_CLOSE,
 Model0_TCP_CLOSE_WAIT,
 Model0_TCP_LAST_ACK,
 Model0_TCP_LISTEN,
 Model0_TCP_CLOSING, /* Now a valid state */
 Model0_TCP_NEW_SYN_RECV,

 Model0_TCP_MAX_STATES /* Leave at the end! */
};





enum {
 Model0_TCPF_ESTABLISHED = (1 << 1),
 Model0_TCPF_SYN_SENT = (1 << 2),
 Model0_TCPF_SYN_RECV = (1 << 3),
 Model0_TCPF_FIN_WAIT1 = (1 << 4),
 Model0_TCPF_FIN_WAIT2 = (1 << 5),
 Model0_TCPF_TIME_WAIT = (1 << 6),
 Model0_TCPF_CLOSE = (1 << 7),
 Model0_TCPF_CLOSE_WAIT = (1 << 8),
 Model0_TCPF_LAST_ACK = (1 << 9),
 Model0_TCPF_LISTEN = (1 << 10),
 Model0_TCPF_CLOSING = (1 << 11),
 Model0_TCPF_NEW_SYN_RECV = (1 << 12),
};
/*
 * Userspace API for hardware time stamping of network packets
 *
 * Copyright (C) 2008,2009 Intel Corporation
 * Author: Patrick Ohly <patrick.ohly@intel.com>
 *
 */






/* SO_TIMESTAMPING gets an integer bit field comprised of these values */
enum {
 Model0_SOF_TIMESTAMPING_TX_HARDWARE = (1<<0),
 Model0_SOF_TIMESTAMPING_TX_SOFTWARE = (1<<1),
 Model0_SOF_TIMESTAMPING_RX_HARDWARE = (1<<2),
 Model0_SOF_TIMESTAMPING_RX_SOFTWARE = (1<<3),
 Model0_SOF_TIMESTAMPING_SOFTWARE = (1<<4),
 Model0_SOF_TIMESTAMPING_SYS_HARDWARE = (1<<5),
 Model0_SOF_TIMESTAMPING_RAW_HARDWARE = (1<<6),
 Model0_SOF_TIMESTAMPING_OPT_ID = (1<<7),
 Model0_SOF_TIMESTAMPING_TX_SCHED = (1<<8),
 Model0_SOF_TIMESTAMPING_TX_ACK = (1<<9),
 Model0_SOF_TIMESTAMPING_OPT_CMSG = (1<<10),
 Model0_SOF_TIMESTAMPING_OPT_TSONLY = (1<<11),

 Model0_SOF_TIMESTAMPING_LAST = Model0_SOF_TIMESTAMPING_OPT_TSONLY,
 Model0_SOF_TIMESTAMPING_MASK = (Model0_SOF_TIMESTAMPING_LAST - 1) |
     Model0_SOF_TIMESTAMPING_LAST
};

/*
 * SO_TIMESTAMPING flags are either for recording a packet timestamp or for
 * reporting the timestamp to user space.
 * Recording flags can be set both via socket options and control messages.
 */





/**
 * struct hwtstamp_config - %SIOCGHWTSTAMP and %SIOCSHWTSTAMP parameter
 *
 * @flags:	no flags defined right now, must be zero for %SIOCSHWTSTAMP
 * @tx_type:	one of HWTSTAMP_TX_*
 * @rx_filter:	one of HWTSTAMP_FILTER_*
 *
 * %SIOCGHWTSTAMP and %SIOCSHWTSTAMP expect a &struct ifreq with a
 * ifr_data pointer to this structure.  For %SIOCSHWTSTAMP, if the
 * driver or hardware does not support the requested @rx_filter value,
 * the driver may use a more general filter mode.  In this case
 * @rx_filter will indicate the actual mode on return.
 */
struct Model0_hwtstamp_config {
 int Model0_flags;
 int Model0_tx_type;
 int Model0_rx_filter;
};

/* possible values for hwtstamp_config->tx_type */
enum Model0_hwtstamp_tx_types {
 /*
	 * No outgoing packet will need hardware time stamping;
	 * should a packet arrive which asks for it, no hardware
	 * time stamping will be done.
	 */
 Model0_HWTSTAMP_TX_OFF,

 /*
	 * Enables hardware time stamping for outgoing packets;
	 * the sender of the packet decides which are to be
	 * time stamped by setting %SOF_TIMESTAMPING_TX_SOFTWARE
	 * before sending the packet.
	 */
 Model0_HWTSTAMP_TX_ON,

 /*
	 * Enables time stamping for outgoing packets just as
	 * HWTSTAMP_TX_ON does, but also enables time stamp insertion
	 * directly into Sync packets. In this case, transmitted Sync
	 * packets will not received a time stamp via the socket error
	 * queue.
	 */
 Model0_HWTSTAMP_TX_ONESTEP_SYNC,
};

/* possible values for hwtstamp_config->rx_filter */
enum Model0_hwtstamp_rx_filters {
 /* time stamp no incoming packet at all */
 Model0_HWTSTAMP_FILTER_NONE,

 /* time stamp any incoming packet */
 Model0_HWTSTAMP_FILTER_ALL,

 /* return value: time stamp all packets requested plus some others */
 Model0_HWTSTAMP_FILTER_SOME,

 /* PTP v1, UDP, any kind of event packet */
 Model0_HWTSTAMP_FILTER_PTP_V1_L4_EVENT,
 /* PTP v1, UDP, Sync packet */
 Model0_HWTSTAMP_FILTER_PTP_V1_L4_SYNC,
 /* PTP v1, UDP, Delay_req packet */
 Model0_HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ,
 /* PTP v2, UDP, any kind of event packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_L4_EVENT,
 /* PTP v2, UDP, Sync packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_L4_SYNC,
 /* PTP v2, UDP, Delay_req packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ,

 /* 802.AS1, Ethernet, any kind of event packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_L2_EVENT,
 /* 802.AS1, Ethernet, Sync packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_L2_SYNC,
 /* 802.AS1, Ethernet, Delay_req packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ,

 /* PTP v2/802.AS1, any layer, any kind of event packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_EVENT,
 /* PTP v2/802.AS1, any layer, Sync packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_SYNC,
 /* PTP v2/802.AS1, any layer, Delay_req packet */
 Model0_HWTSTAMP_FILTER_PTP_V2_DELAY_REQ,
};

/*
 * This structure really needs to be cleaned up.
 * Most of it is for TCP, and not used by any of
 * the other protocols.
 */

/* Define this to get the SOCK_DBG debugging facility. */
/* This is the per-socket lock.  The spinlock provides a synchronization
 * between user contexts and software interrupt processing, whereas the
 * mini-semaphore synchronizes multiple users amongst themselves.
 */
typedef struct {
 Model0_spinlock_t Model0_slock;
 int Model0_owned;
 Model0_wait_queue_head_t Model0_wq;
 /*
	 * We express the mutex-alike socket_lock semantics
	 * to the lock validator by explicitly managing
	 * the slock as a lock variant (in addition to
	 * the slock itself):
	 */



} Model0_socket_lock_t;

struct Model0_sock;
struct Model0_proto;
struct Model0_net;

typedef __u32 Model0___portpair;
typedef __u64 Model0___addrpair;

/**
 *	struct sock_common - minimal network layer representation of sockets
 *	@skc_daddr: Foreign IPv4 addr
 *	@skc_rcv_saddr: Bound local IPv4 addr
 *	@skc_hash: hash value used with various protocol lookup tables
 *	@skc_u16hashes: two u16 hash values used by UDP lookup tables
 *	@skc_dport: placeholder for inet_dport/tw_dport
 *	@skc_num: placeholder for inet_num/tw_num
 *	@skc_family: network address family
 *	@skc_state: Connection state
 *	@skc_reuse: %SO_REUSEADDR setting
 *	@skc_reuseport: %SO_REUSEPORT setting
 *	@skc_bound_dev_if: bound device index if != 0
 *	@skc_bind_node: bind hash linkage for various protocol lookup tables
 *	@skc_portaddr_node: second hash linkage for UDP/UDP-Lite protocol
 *	@skc_prot: protocol handlers inside a network family
 *	@skc_net: reference to the network namespace of this socket
 *	@skc_node: main hash linkage for various protocol lookup tables
 *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
 *	@skc_tx_queue_mapping: tx queue number for this connection
 *	@skc_flags: place holder for sk_flags
 *		%SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
 *		%SO_OOBINLINE settings, %SO_TIMESTAMPING settings
 *	@skc_incoming_cpu: record/match cpu processing incoming packets
 *	@skc_refcnt: reference count
 *
 *	This is the minimal network layer representation of sockets, the header
 *	for struct sock and struct inet_timewait_sock.
 */
struct Model0_sock_common {
 /* skc_daddr and skc_rcv_saddr must be grouped on a 8 bytes aligned
	 * address on 64bit arches : cf INET_MATCH()
	 */
 union {
  Model0___addrpair Model0_skc_addrpair;
  struct {
   Model0___be32 Model0_skc_daddr;
   Model0___be32 Model0_skc_rcv_saddr;
  };
 };
 union {
  unsigned int Model0_skc_hash;
  Model0___u16 Model0_skc_u16hashes[2];
 };
 /* skc_dport && skc_num must be grouped as well */
 union {
  Model0___portpair Model0_skc_portpair;
  struct {
   Model0___be16 Model0_skc_dport;
   Model0___u16 Model0_skc_num;
  };
 };

 unsigned short Model0_skc_family;
 volatile unsigned char Model0_skc_state;
 unsigned char Model0_skc_reuse:4;
 unsigned char Model0_skc_reuseport:1;
 unsigned char Model0_skc_ipv6only:1;
 unsigned char Model0_skc_net_refcnt:1;
 int Model0_skc_bound_dev_if;
 union {
  struct Model0_hlist_node Model0_skc_bind_node;
  struct Model0_hlist_node Model0_skc_portaddr_node;
 };
 struct Model0_proto *Model0_skc_prot;
 Model0_possible_net_t Model0_skc_net;


 struct Model0_in6_addr Model0_skc_v6_daddr;
 struct Model0_in6_addr Model0_skc_v6_rcv_saddr;


 Model0_atomic64_t Model0_skc_cookie;

 /* following fields are padding to force
	 * offset(struct sock, sk_refcnt) == 128 on 64bit arches
	 * assuming IPV6 is enabled. We use this padding differently
	 * for different kind of 'sockets'
	 */
 union {
  unsigned long Model0_skc_flags;
  struct Model0_sock *Model0_skc_listener; /* request_sock */
  struct Model0_inet_timewait_death_row *Model0_skc_tw_dr; /* inet_timewait_sock */
 };
 /*
	 * fields between dontcopy_begin/dontcopy_end
	 * are not copied in sock_copy()
	 */
 /* private: */
 int Model0_skc_dontcopy_begin[0];
 /* public: */
 union {
  struct Model0_hlist_node Model0_skc_node;
  struct Model0_hlist_nulls_node Model0_skc_nulls_node;
 };
 int Model0_skc_tx_queue_mapping;
 union {
  int Model0_skc_incoming_cpu;
  Model0_u32 Model0_skc_rcv_wnd;
  Model0_u32 Model0_skc_tw_rcv_nxt; /* struct tcp_timewait_sock  */
 };

 Model0_atomic_t Model0_skc_refcnt;
 /* private: */
 int Model0_skc_dontcopy_end[0];
 union {
  Model0_u32 Model0_skc_rxhash;
  Model0_u32 Model0_skc_window_clamp;
  Model0_u32 Model0_skc_tw_snd_nxt; /* struct tcp_timewait_sock */
 };
 /* public: */
};

/**
  *	struct sock - network layer representation of sockets
  *	@__sk_common: shared layout with inet_timewait_sock
  *	@sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
  *	@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
  *	@sk_lock:	synchronizer
  *	@sk_rcvbuf: size of receive buffer in bytes
  *	@sk_wq: sock wait queue and async head
  *	@sk_rx_dst: receive input route used by early demux
  *	@sk_dst_cache: destination cache
  *	@sk_policy: flow policy
  *	@sk_receive_queue: incoming packets
  *	@sk_wmem_alloc: transmit queue bytes committed
  *	@sk_write_queue: Packet sending queue
  *	@sk_omem_alloc: "o" is "option" or "other"
  *	@sk_wmem_queued: persistent queue size
  *	@sk_forward_alloc: space allocated forward
  *	@sk_napi_id: id of the last napi context to receive data for sk
  *	@sk_ll_usec: usecs to busypoll when there is no data
  *	@sk_allocation: allocation mode
  *	@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
  *	@sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)
  *	@sk_sndbuf: size of send buffer in bytes
  *	@sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets
  *	@sk_no_check_rx: allow zero checksum in RX packets
  *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
  *	@sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
  *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
  *	@sk_gso_max_size: Maximum GSO segment size to build
  *	@sk_gso_max_segs: Maximum number of GSO segments
  *	@sk_lingertime: %SO_LINGER l_linger setting
  *	@sk_backlog: always used with the per-socket spinlock held
  *	@sk_callback_lock: used with the callbacks in the end of this struct
  *	@sk_error_queue: rarely used
  *	@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt,
  *			  IPV6_ADDRFORM for instance)
  *	@sk_err: last error
  *	@sk_err_soft: errors that don't cause failure but are the cause of a
  *		      persistent failure not just 'timed out'
  *	@sk_drops: raw/udp drops counter
  *	@sk_ack_backlog: current listen backlog
  *	@sk_max_ack_backlog: listen backlog set in listen()
  *	@sk_priority: %SO_PRIORITY setting
  *	@sk_type: socket type (%SOCK_STREAM, etc)
  *	@sk_protocol: which protocol this socket belongs in this network family
  *	@sk_peer_pid: &struct pid for this socket's peer
  *	@sk_peer_cred: %SO_PEERCRED setting
  *	@sk_rcvlowat: %SO_RCVLOWAT setting
  *	@sk_rcvtimeo: %SO_RCVTIMEO setting
  *	@sk_sndtimeo: %SO_SNDTIMEO setting
  *	@sk_txhash: computed flow hash for use on transmit
  *	@sk_filter: socket filtering instructions
  *	@sk_timer: sock cleanup timer
  *	@sk_stamp: time stamp of last packet received
  *	@sk_tsflags: SO_TIMESTAMPING socket options
  *	@sk_tskey: counter to disambiguate concurrent tstamp requests
  *	@sk_socket: Identd and reporting IO signals
  *	@sk_user_data: RPC layer private data
  *	@sk_frag: cached page frag
  *	@sk_peek_off: current peek_offset value
  *	@sk_send_head: front of stuff to transmit
  *	@sk_security: used by security modules
  *	@sk_mark: generic packet mark
  *	@sk_cgrp_data: cgroup data for this cgroup
  *	@sk_memcg: this socket's memory cgroup association
  *	@sk_write_pending: a write to stream socket waits to start
  *	@sk_state_change: callback to indicate change in the state of the sock
  *	@sk_data_ready: callback to indicate there is data to be processed
  *	@sk_write_space: callback to indicate there is bf sending space available
  *	@sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)
  *	@sk_backlog_rcv: callback to process the backlog
  *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
  *	@sk_reuseport_cb: reuseport group container
 */
struct Model0_sock {
 /*
	 * Now struct inet_timewait_sock also uses sock_common, so please just
	 * don't add nothing before this first member (__sk_common) --acme
	 */
 struct Model0_sock_common Model0___sk_common;
 Model0_socket_lock_t Model0_sk_lock;
 struct Model0_sk_buff_head Model0_sk_receive_queue;
 /*
	 * The backlog queue is special, it is always used with
	 * the per-socket spinlock held and requires low latency
	 * access. Therefore we special case it's implementation.
	 * Note : rmem_alloc is in this structure to fill a hole
	 * on 64bit arches, not because its logically part of
	 * backlog.
	 */
 struct {
  Model0_atomic_t Model0_rmem_alloc;
  int Model0_len;
  struct Model0_sk_buff *Model0_head;
  struct Model0_sk_buff *Model0_tail;
 } Model0_sk_backlog;

 int Model0_sk_forward_alloc;

 __u32 Model0_sk_txhash;

 unsigned int Model0_sk_napi_id;
 unsigned int Model0_sk_ll_usec;

 Model0_atomic_t Model0_sk_drops;
 int Model0_sk_rcvbuf;

 struct Model0_sk_filter *Model0_sk_filter;
 union {
  struct Model0_socket_wq *Model0_sk_wq;
  struct Model0_socket_wq *Model0_sk_wq_raw;
 };

 struct Model0_xfrm_policy *Model0_sk_policy[2];

 struct Model0_dst_entry *Model0_sk_rx_dst;
 struct Model0_dst_entry *Model0_sk_dst_cache;
 /* Note: 32bit hole on 64bit arches */
 Model0_atomic_t Model0_sk_wmem_alloc;
 Model0_atomic_t Model0_sk_omem_alloc;
 int Model0_sk_sndbuf;
 struct Model0_sk_buff_head Model0_sk_write_queue;

 /*
	 * Because of non atomicity rules, all
	 * changes are protected by socket lock.
	 */
                                ;
 unsigned int Model0_sk_padding : 2,
    Model0_sk_no_check_tx : 1,
    Model0_sk_no_check_rx : 1,
    Model0_sk_userlocks : 4,
    Model0_sk_protocol : 8,
    Model0_sk_type : 16;

                              ;

 int Model0_sk_wmem_queued;
 Model0_gfp_t Model0_sk_allocation;
 Model0_u32 Model0_sk_pacing_rate; /* bytes per second */
 Model0_u32 Model0_sk_max_pacing_rate;
 Model0_netdev_features_t Model0_sk_route_caps;
 Model0_netdev_features_t Model0_sk_route_nocaps;
 int Model0_sk_gso_type;
 unsigned int Model0_sk_gso_max_size;
 Model0_u16 Model0_sk_gso_max_segs;
 int Model0_sk_rcvlowat;
 unsigned long Model0_sk_lingertime;
 struct Model0_sk_buff_head Model0_sk_error_queue;
 struct Model0_proto *Model0_sk_prot_creator;
 Model0_rwlock_t Model0_sk_callback_lock;
 int Model0_sk_err,
    Model0_sk_err_soft;
 Model0_u32 Model0_sk_ack_backlog;
 Model0_u32 Model0_sk_max_ack_backlog;
 __u32 Model0_sk_priority;
 __u32 Model0_sk_mark;
 struct Model0_pid *Model0_sk_peer_pid;
 const struct Model0_cred *Model0_sk_peer_cred;
 long Model0_sk_rcvtimeo;
 long Model0_sk_sndtimeo;
 struct Model0_timer_list Model0_sk_timer;
 Model0_ktime_t Model0_sk_stamp;
 Model0_u16 Model0_sk_tsflags;
 Model0_u8 Model0_sk_shutdown;
 Model0_u32 Model0_sk_tskey;
 struct Model0_socket *Model0_sk_socket;
 void *Model0_sk_user_data;
 struct Model0_page_frag Model0_sk_frag;
 struct Model0_sk_buff *Model0_sk_send_head;
 Model0___s32 Model0_sk_peek_off;
 int Model0_sk_write_pending;

 void *Model0_sk_security;

 struct Model0_sock_cgroup_data Model0_sk_cgrp_data;
 struct Model0_mem_cgroup *Model0_sk_memcg;
 void (*Model0_sk_state_change)(struct Model0_sock *Model0_sk);
 void (*Model0_sk_data_ready)(struct Model0_sock *Model0_sk);
 void (*Model0_sk_write_space)(struct Model0_sock *Model0_sk);
 void (*Model0_sk_error_report)(struct Model0_sock *Model0_sk);
 int (*Model0_sk_backlog_rcv)(struct Model0_sock *Model0_sk,
        struct Model0_sk_buff *Model0_skb);
 void (*Model0_sk_destruct)(struct Model0_sock *Model0_sk);
 struct Model0_sock_reuseport *Model0_sk_reuseport_cb;
 struct Model0_callback_head Model0_sk_rcu;
};






/*
 * SK_CAN_REUSE and SK_NO_REUSE on a socket mean that the socket is OK
 * or not whether his port will be reused by someone else. SK_FORCE_REUSE
 * on a socket means that the socket will reuse everybody else's port
 * without looking at the other's sk_reuse value.
 */





int Model0_sk_set_peek_off(struct Model0_sock *Model0_sk, int Model0_val);

static inline __attribute__((no_instrument_function)) int Model0_sk_peek_offset(struct Model0_sock *Model0_sk, int Model0_flags)
{
 if (__builtin_expect(!!(Model0_flags & 2), 0)) {
  Model0_s32 Model0_off = ({ union { typeof(Model0_sk->Model0_sk_peek_off) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_sk->Model0_sk_peek_off), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_peek_off)); else Model0___read_once_size_nocheck(&(Model0_sk->Model0_sk_peek_off), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_peek_off)); Model0___u.Model0___val; });
  if (Model0_off >= 0)
   return Model0_off;
 }

 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_sk_peek_offset_bwd(struct Model0_sock *Model0_sk, int Model0_val)
{
 Model0_s32 Model0_off = ({ union { typeof(Model0_sk->Model0_sk_peek_off) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_sk->Model0_sk_peek_off), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_peek_off)); else Model0___read_once_size_nocheck(&(Model0_sk->Model0_sk_peek_off), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_peek_off)); Model0___u.Model0___val; });

 if (__builtin_expect(!!(Model0_off >= 0), 0)) {
  Model0_off = ({ Model0_s32 Model0___max1 = (Model0_off - Model0_val); Model0_s32 Model0___max2 = (0); Model0___max1 > Model0___max2 ? Model0___max1: Model0___max2; });
  ({ union { typeof(Model0_sk->Model0_sk_peek_off) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(Model0_sk->Model0_sk_peek_off)) (Model0_off) }; Model0___write_once_size(&(Model0_sk->Model0_sk_peek_off), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_peek_off)); Model0___u.Model0___val; });
 }
}

static inline __attribute__((no_instrument_function)) void Model0_sk_peek_offset_fwd(struct Model0_sock *Model0_sk, int Model0_val)
{
 Model0_sk_peek_offset_bwd(Model0_sk, -Model0_val);
}

/*
 * Hashed lists helper routines
 */
static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_sk_entry(const struct Model0_hlist_node *Model0_node)
{
 return ({ const typeof( ((struct Model0_sock *)0)->Model0___sk_common.Model0_skc_node ) *Model0___mptr = (Model0_node); (struct Model0_sock *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_sock, Model0___sk_common.Model0_skc_node) );});
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0___sk_head(const struct Model0_hlist_head *Model0_head)
{
 return ({ const typeof( ((struct Model0_sock *)0)->Model0___sk_common.Model0_skc_node ) *Model0___mptr = (Model0_head->Model0_first); (struct Model0_sock *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_sock, Model0___sk_common.Model0_skc_node) );});
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_sk_head(const struct Model0_hlist_head *Model0_head)
{
 return Model0_hlist_empty(Model0_head) ? ((void *)0) : Model0___sk_head(Model0_head);
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0___sk_nulls_head(const struct Model0_hlist_nulls_head *Model0_head)
{
 return ({ const typeof( ((struct Model0_sock *)0)->Model0___sk_common.Model0_skc_nulls_node ) *Model0___mptr = (Model0_head->Model0_first); (struct Model0_sock *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_sock, Model0___sk_common.Model0_skc_nulls_node) );});
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_sk_nulls_head(const struct Model0_hlist_nulls_head *Model0_head)
{
 return Model0_hlist_nulls_empty(Model0_head) ? ((void *)0) : Model0___sk_nulls_head(Model0_head);
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_sk_next(const struct Model0_sock *Model0_sk)
{
 return Model0_sk->Model0___sk_common.Model0_skc_node.Model0_next ?
  ({ const typeof( ((struct Model0_sock *)0)->Model0___sk_common.Model0_skc_node ) *Model0___mptr = (Model0_sk->Model0___sk_common.Model0_skc_node.Model0_next); (struct Model0_sock *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_sock, Model0___sk_common.Model0_skc_node) );}) : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_sk_nulls_next(const struct Model0_sock *Model0_sk)
{
 return (!Model0_is_a_nulls(Model0_sk->Model0___sk_common.Model0_skc_nulls_node.Model0_next)) ?
  ({ const typeof( ((struct Model0_sock *)0)->Model0___sk_common.Model0_skc_nulls_node ) *Model0___mptr = (Model0_sk->Model0___sk_common.Model0_skc_nulls_node.Model0_next); (struct Model0_sock *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_sock, Model0___sk_common.Model0_skc_nulls_node) );}) :

  ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_unhashed(const struct Model0_sock *Model0_sk)
{
 return Model0_hlist_unhashed(&Model0_sk->Model0___sk_common.Model0_skc_node);
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_hashed(const struct Model0_sock *Model0_sk)
{
 return !Model0_sk_unhashed(Model0_sk);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_node_init(struct Model0_hlist_node *Model0_node)
{
 Model0_node->Model0_pprev = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_nulls_node_init(struct Model0_hlist_nulls_node *Model0_node)
{
 Model0_node->Model0_pprev = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0___sk_del_node(struct Model0_sock *Model0_sk)
{
 Model0___hlist_del(&Model0_sk->Model0___sk_common.Model0_skc_node);
}

/* NB: equivalent to hlist_del_init_rcu */
static inline __attribute__((no_instrument_function)) bool Model0___sk_del_node_init(struct Model0_sock *Model0_sk)
{
 if (Model0_sk_hashed(Model0_sk)) {
  Model0___sk_del_node(Model0_sk);
  Model0_sk_node_init(&Model0_sk->Model0___sk_common.Model0_skc_node);
  return true;
 }
 return false;
}

/* Grab socket reference count. This operation is valid only
   when sk is ALREADY grabbed f.e. it is found in hash table
   or a list and the lookup is made under lock preventing hash table
   modifications.
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_sock_hold(struct Model0_sock *Model0_sk)
{
 Model0_atomic_inc(&Model0_sk->Model0___sk_common.Model0_skc_refcnt);
}

/* Ungrab socket in the context, which assumes that socket refcnt
   cannot hit zero, f.e. it is true in context of any socketcall.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0___sock_put(struct Model0_sock *Model0_sk)
{
 Model0_atomic_dec(&Model0_sk->Model0___sk_common.Model0_skc_refcnt);
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_del_node_init(struct Model0_sock *Model0_sk)
{
 bool Model0_rc = Model0___sk_del_node_init(Model0_sk);

 if (Model0_rc) {
  /* paranoid for a while -acme */
  ({ int Model0___ret_warn_on = !!(Model0_atomic_read(&Model0_sk->Model0___sk_common.Model0_skc_refcnt) == 1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/net/sock.h", 598); __builtin_expect(!!(Model0___ret_warn_on), 0); });
  Model0___sock_put(Model0_sk);
 }
 return Model0_rc;
}


static inline __attribute__((no_instrument_function)) bool Model0___sk_nulls_del_node_init_rcu(struct Model0_sock *Model0_sk)
{
 if (Model0_sk_hashed(Model0_sk)) {
  Model0_hlist_nulls_del_init_rcu(&Model0_sk->Model0___sk_common.Model0_skc_nulls_node);
  return true;
 }
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_nulls_del_node_init_rcu(struct Model0_sock *Model0_sk)
{
 bool Model0_rc = Model0___sk_nulls_del_node_init_rcu(Model0_sk);

 if (Model0_rc) {
  /* paranoid for a while -acme */
  ({ int Model0___ret_warn_on = !!(Model0_atomic_read(&Model0_sk->Model0___sk_common.Model0_skc_refcnt) == 1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/net/sock.h", 620); __builtin_expect(!!(Model0___ret_warn_on), 0); });
  Model0___sock_put(Model0_sk);
 }
 return Model0_rc;
}

static inline __attribute__((no_instrument_function)) void Model0___sk_add_node(struct Model0_sock *Model0_sk, struct Model0_hlist_head *Model0_list)
{
 Model0_hlist_add_head(&Model0_sk->Model0___sk_common.Model0_skc_node, Model0_list);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_add_node(struct Model0_sock *Model0_sk, struct Model0_hlist_head *Model0_list)
{
 Model0_sock_hold(Model0_sk);
 Model0___sk_add_node(Model0_sk, Model0_list);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_add_node_rcu(struct Model0_sock *Model0_sk, struct Model0_hlist_head *Model0_list)
{
 Model0_sock_hold(Model0_sk);
 if (1 && Model0_sk->Model0___sk_common.Model0_skc_reuseport &&
     Model0_sk->Model0___sk_common.Model0_skc_family == 10)
  Model0_hlist_add_tail_rcu(&Model0_sk->Model0___sk_common.Model0_skc_node, Model0_list);
 else
  Model0_hlist_add_head_rcu(&Model0_sk->Model0___sk_common.Model0_skc_node, Model0_list);
}

static inline __attribute__((no_instrument_function)) void Model0___sk_nulls_add_node_rcu(struct Model0_sock *Model0_sk, struct Model0_hlist_nulls_head *Model0_list)
{
 if (1 && Model0_sk->Model0___sk_common.Model0_skc_reuseport &&
     Model0_sk->Model0___sk_common.Model0_skc_family == 10)
  Model0_hlist_nulls_add_tail_rcu(&Model0_sk->Model0___sk_common.Model0_skc_nulls_node, Model0_list);
 else
  Model0_hlist_nulls_add_head_rcu(&Model0_sk->Model0___sk_common.Model0_skc_nulls_node, Model0_list);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_nulls_add_node_rcu(struct Model0_sock *Model0_sk, struct Model0_hlist_nulls_head *Model0_list)
{
 Model0_sock_hold(Model0_sk);
 Model0___sk_nulls_add_node_rcu(Model0_sk, Model0_list);
}

static inline __attribute__((no_instrument_function)) void Model0___sk_del_bind_node(struct Model0_sock *Model0_sk)
{
 Model0___hlist_del(&Model0_sk->Model0___sk_common.Model0_skc_bind_node);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_add_bind_node(struct Model0_sock *Model0_sk,
     struct Model0_hlist_head *Model0_list)
{
 Model0_hlist_add_head(&Model0_sk->Model0___sk_common.Model0_skc_bind_node, Model0_list);
}
/**
 * sk_for_each_entry_offset_rcu - iterate over a list at a given struct offset
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @head:	the head for your list.
 * @offset:	offset of hlist_node within the struct.
 *
 */






static inline __attribute__((no_instrument_function)) struct Model0_user_namespace *Model0_sk_user_ns(struct Model0_sock *Model0_sk)
{
 /* Careful only use this in a context where these parameters
	 * can not change and must all be valid, such as recvmsg from
	 * userspace.
	 */
 return Model0_sk->Model0_sk_socket->Model0_file->Model0_f_cred->Model0_user_ns;
}

/* Sock flags */
enum Model0_sock_flags {
 Model0_SOCK_DEAD,
 Model0_SOCK_DONE,
 Model0_SOCK_URGINLINE,
 Model0_SOCK_KEEPOPEN,
 Model0_SOCK_LINGER,
 Model0_SOCK_DESTROY,
 Model0_SOCK_BROADCAST,
 Model0_SOCK_TIMESTAMP,
 Model0_SOCK_ZAPPED,
 Model0_SOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */
 Model0_SOCK_DBG, /* %SO_DEBUG setting */
 Model0_SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
 Model0_SOCK_RCVTSTAMPNS, /* %SO_TIMESTAMPNS setting */
 Model0_SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
 Model0_SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
 Model0_SOCK_MEMALLOC, /* VM depends on this socket for swapping */
 Model0_SOCK_TIMESTAMPING_RX_SOFTWARE, /* %SOF_TIMESTAMPING_RX_SOFTWARE */
 Model0_SOCK_FASYNC, /* fasync() active */
 Model0_SOCK_RXQ_OVFL,
 Model0_SOCK_ZEROCOPY, /* buffers from userspace */
 Model0_SOCK_WIFI_STATUS, /* push wifi status to userspace */
 Model0_SOCK_NOFCS, /* Tell NIC not to do the Ethernet FCS.
		     * Will use last 4 bytes of packet sent from
		     * user-space instead.
		     */
 Model0_SOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */
 Model0_SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
 Model0_SOCK_RCU_FREE, /* wait rcu grace period in sk_destruct() */
};



static inline __attribute__((no_instrument_function)) void Model0_sock_copy_flags(struct Model0_sock *Model0_nsk, struct Model0_sock *Model0_osk)
{
 Model0_nsk->Model0___sk_common.Model0_skc_flags = Model0_osk->Model0___sk_common.Model0_skc_flags;
}

static inline __attribute__((no_instrument_function)) void Model0_sock_set_flag(struct Model0_sock *Model0_sk, enum Model0_sock_flags Model0_flag)
{
 Model0___set_bit(Model0_flag, &Model0_sk->Model0___sk_common.Model0_skc_flags);
}

static inline __attribute__((no_instrument_function)) void Model0_sock_reset_flag(struct Model0_sock *Model0_sk, enum Model0_sock_flags Model0_flag)
{
 Model0___clear_bit(Model0_flag, &Model0_sk->Model0___sk_common.Model0_skc_flags);
}

static inline __attribute__((no_instrument_function)) bool Model0_sock_flag(const struct Model0_sock *Model0_sk, enum Model0_sock_flags Model0_flag)
{
 return (__builtin_constant_p((Model0_flag)) ? Model0_constant_test_bit((Model0_flag), (&Model0_sk->Model0___sk_common.Model0_skc_flags)) : Model0_variable_test_bit((Model0_flag), (&Model0_sk->Model0___sk_common.Model0_skc_flags)));
}


extern struct Model0_static_key Model0_memalloc_socks;
static inline __attribute__((no_instrument_function)) int Model0_sk_memalloc_socks(void)
{
 return Model0_static_key_false(&Model0_memalloc_socks);
}
static inline __attribute__((no_instrument_function)) Model0_gfp_t Model0_sk_gfp_mask(const struct Model0_sock *Model0_sk, Model0_gfp_t Model0_gfp_mask)
{
 return Model0_gfp_mask | (Model0_sk->Model0_sk_allocation & (( Model0_gfp_t)0x2000u));
}

static inline __attribute__((no_instrument_function)) void Model0_sk_acceptq_removed(struct Model0_sock *Model0_sk)
{
 Model0_sk->Model0_sk_ack_backlog--;
}

static inline __attribute__((no_instrument_function)) void Model0_sk_acceptq_added(struct Model0_sock *Model0_sk)
{
 Model0_sk->Model0_sk_ack_backlog++;
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_acceptq_is_full(const struct Model0_sock *Model0_sk)
{
 return Model0_sk->Model0_sk_ack_backlog > Model0_sk->Model0_sk_max_ack_backlog;
}

/*
 * Compute minimal free write space needed to queue new packets.
 */
static inline __attribute__((no_instrument_function)) int Model0_sk_stream_min_wspace(const struct Model0_sock *Model0_sk)
{
 return Model0_sk->Model0_sk_wmem_queued >> 1;
}

static inline __attribute__((no_instrument_function)) int Model0_sk_stream_wspace(const struct Model0_sock *Model0_sk)
{
 return Model0_sk->Model0_sk_sndbuf - Model0_sk->Model0_sk_wmem_queued;
}

void Model0_sk_stream_write_space(struct Model0_sock *Model0_sk);

/* OOB backlog add */
static inline __attribute__((no_instrument_function)) void Model0___sk_add_backlog(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 /* dont let skb dst not refcounted, we are going to leave rcu lock */
 Model0_skb_dst_force_safe(Model0_skb);

 if (!Model0_sk->Model0_sk_backlog.Model0_tail)
  Model0_sk->Model0_sk_backlog.Model0_head = Model0_skb;
 else
  Model0_sk->Model0_sk_backlog.Model0_tail->Model0_next = Model0_skb;

 Model0_sk->Model0_sk_backlog.Model0_tail = Model0_skb;
 Model0_skb->Model0_next = ((void *)0);
}

/*
 * Take into account size of receive queue and backlog queue
 * Do not take into account this skb truesize,
 * to allow even a single big packet to come.
 */
static inline __attribute__((no_instrument_function)) bool Model0_sk_rcvqueues_full(const struct Model0_sock *Model0_sk, unsigned int Model0_limit)
{
 unsigned int Model0_qsize = Model0_sk->Model0_sk_backlog.Model0_len + Model0_atomic_read(&Model0_sk->Model0_sk_backlog.Model0_rmem_alloc);

 return Model0_qsize > Model0_limit;
}

/* The per-socket spinlock must be held here. */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result)) int Model0_sk_add_backlog(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
           unsigned int Model0_limit)
{
 if (Model0_sk_rcvqueues_full(Model0_sk, Model0_limit))
  return -105;

 /*
	 * If the skb was allocated from pfmemalloc reserves, only
	 * allow SOCK_MEMALLOC sockets to use it as this socket is
	 * helping free memory
	 */
 if (Model0_skb_pfmemalloc(Model0_skb) && !Model0_sock_flag(Model0_sk, Model0_SOCK_MEMALLOC))
  return -12;

 Model0___sk_add_backlog(Model0_sk, Model0_skb);
 Model0_sk->Model0_sk_backlog.Model0_len += Model0_skb->Model0_truesize;
 return 0;
}

int Model0___sk_backlog_rcv(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) int Model0_sk_backlog_rcv(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 if (Model0_sk_memalloc_socks() && Model0_skb_pfmemalloc(Model0_skb))
  return Model0___sk_backlog_rcv(Model0_sk, Model0_skb);

 #if CY_ABSTRACT7
  return Model0_tcp_v4_do_rcv(Model0_sk, Model0_skb);
 #else
 return Model0_sk->Model0_sk_backlog_rcv(Model0_sk, Model0_skb);
 #endif
}

static inline __attribute__((no_instrument_function)) void Model0_sk_incoming_cpu_update(struct Model0_sock *Model0_sk)
{
#if CY_ABSTRACT6
    Model0_sk->Model0___sk_common.Model0_skc_incoming_cpu = 0; //raw_smp_processor_id() = 0; https://elixir.bootlin.com/linux/v4.8/source/include/linux/smp.h#L130
#else
 Model0_sk->Model0___sk_common.Model0_skc_incoming_cpu = (({ typeof(Model0_cpu_number) Model0_pscr_ret__; do { const void *Model0___vpp_verify = (typeof((&(Model0_cpu_number)) + 0))((void *)0); (void)Model0___vpp_verify; } while (0); switch(sizeof(Model0_cpu_number)) { case 1: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 2: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 4: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; case 8: Model0_pscr_ret__ = ({ typeof(Model0_cpu_number) Model0_pfo_ret__; switch (sizeof(Model0_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model0_pfo_ret__) : "m" (Model0_cpu_number)); break; default: Model0___bad_percpu_size(); } Model0_pfo_ret__; }); break; default: Model0___bad_size_call_parameter(); break; } Model0_pscr_ret__; }));
#endif
}

static inline __attribute__((no_instrument_function)) void Model0_sock_rps_record_flow_hash(__u32 Model0_hash)
{

 struct Model0_rps_sock_flow_table *Model0_sock_flow_table;

 Model0_rcu_read_lock();
 Model0_sock_flow_table = ({ typeof(*(Model0_rps_sock_flow_table)) *Model0_________p1 = (typeof(*(Model0_rps_sock_flow_table)) *)({ typeof((Model0_rps_sock_flow_table)) Model0__________p1 = ({ union { typeof((Model0_rps_sock_flow_table)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_rps_sock_flow_table)), Model0___u.Model0___c, sizeof((Model0_rps_sock_flow_table))); else Model0___read_once_size_nocheck(&((Model0_rps_sock_flow_table)), Model0___u.Model0___c, sizeof((Model0_rps_sock_flow_table))); Model0___u.Model0___val; }); typeof(*((Model0_rps_sock_flow_table))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_rps_sock_flow_table)) *)(Model0_________p1)); });
 Model0_rps_record_sock_flow(Model0_sock_flow_table, Model0_hash);
 Model0_rcu_read_unlock();

}

static inline __attribute__((no_instrument_function)) void Model0_sock_rps_record_flow(const struct Model0_sock *Model0_sk)
{

 Model0_sock_rps_record_flow_hash(Model0_sk->Model0___sk_common.Model0_skc_rxhash);

}

static inline __attribute__((no_instrument_function)) void Model0_sock_rps_save_rxhash(struct Model0_sock *Model0_sk,
     const struct Model0_sk_buff *Model0_skb)
{

 if (__builtin_expect(!!(Model0_sk->Model0___sk_common.Model0_skc_rxhash != Model0_skb->Model0_hash), 0))
  Model0_sk->Model0___sk_common.Model0_skc_rxhash = Model0_skb->Model0_hash;

}

static inline __attribute__((no_instrument_function)) void Model0_sock_rps_reset_rxhash(struct Model0_sock *Model0_sk)
{

 Model0_sk->Model0___sk_common.Model0_skc_rxhash = 0;

}
int Model0_sk_stream_wait_connect(struct Model0_sock *Model0_sk, long *Model0_timeo_p);
int Model0_sk_stream_wait_memory(struct Model0_sock *Model0_sk, long *Model0_timeo_p);
void Model0_sk_stream_wait_close(struct Model0_sock *Model0_sk, long Model0_timeo_p);
int Model0_sk_stream_error(struct Model0_sock *Model0_sk, int Model0_flags, int err);
void Model0_sk_stream_kill_queues(struct Model0_sock *Model0_sk);
void Model0_sk_set_memalloc(struct Model0_sock *Model0_sk);
void Model0_sk_clear_memalloc(struct Model0_sock *Model0_sk);

void Model0___sk_flush_backlog(struct Model0_sock *Model0_sk);

static inline __attribute__((no_instrument_function)) bool Model0_sk_flush_backlog(struct Model0_sock *Model0_sk)
{
 if (__builtin_expect(!!(({ union { typeof(Model0_sk->Model0_sk_backlog.Model0_tail) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_sk->Model0_sk_backlog.Model0_tail), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_backlog.Model0_tail)); else Model0___read_once_size_nocheck(&(Model0_sk->Model0_sk_backlog.Model0_tail), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_backlog.Model0_tail)); Model0___u.Model0___val; })), 0)) {
  Model0___sk_flush_backlog(Model0_sk);
  return true;
 }
 return false;
}

int Model0_sk_wait_data(struct Model0_sock *Model0_sk, long *Model0_timeo, const struct Model0_sk_buff *Model0_skb);

struct Model0_request_sock_ops;
struct Model0_timewait_sock_ops;
struct Model0_inet_hashinfo;
struct Model0_raw_hashinfo;
struct Model0_module;

/*
 * caches using SLAB_DESTROY_BY_RCU should let .next pointer from nulls nodes
 * un-modified. Special care is taken when initializing object to zero.
 */
static inline __attribute__((no_instrument_function)) void Model0_sk_prot_clear_nulls(struct Model0_sock *Model0_sk, int Model0_size)
{
 if (__builtin_offsetof(struct Model0_sock, Model0___sk_common.Model0_skc_node.Model0_next) != 0)
  memset(Model0_sk, 0, __builtin_offsetof(struct Model0_sock, Model0___sk_common.Model0_skc_node.Model0_next));
 memset(&Model0_sk->Model0___sk_common.Model0_skc_node.Model0_pprev, 0,
        Model0_size - __builtin_offsetof(struct Model0_sock, Model0___sk_common.Model0_skc_node.Model0_pprev));
}

/* Networking protocol blocks we attach to sockets.
 * socket layer -> transport layer interface
 */
struct Model0_proto {
 void (*Model0_close)(struct Model0_sock *Model0_sk,
     long Model0_timeout);
 int (*Model0_connect)(struct Model0_sock *Model0_sk,
     struct Model0_sockaddr *Model0_uaddr,
     int Model0_addr_len);
 int (*Model0_disconnect)(struct Model0_sock *Model0_sk, int Model0_flags);

 struct Model0_sock * (*Model0_accept)(struct Model0_sock *Model0_sk, int Model0_flags, int *err);

 int (*Model0_ioctl)(struct Model0_sock *Model0_sk, int Model0_cmd,
      unsigned long Model0_arg);
 int (*Model0_init)(struct Model0_sock *Model0_sk);
 void (*Model0_destroy)(struct Model0_sock *Model0_sk);
 void (*Model0_shutdown)(struct Model0_sock *Model0_sk, int Model0_how);
 int (*Model0_setsockopt)(struct Model0_sock *Model0_sk, int Model0_level,
     int Model0_optname, char *Model0_optval,
     unsigned int Model0_optlen);
 int (*Model0_getsockopt)(struct Model0_sock *Model0_sk, int Model0_level,
     int Model0_optname, char *Model0_optval,
     int *Model0_option);

 int (*Model0_compat_setsockopt)(struct Model0_sock *Model0_sk,
     int Model0_level,
     int Model0_optname, char *Model0_optval,
     unsigned int Model0_optlen);
 int (*Model0_compat_getsockopt)(struct Model0_sock *Model0_sk,
     int Model0_level,
     int Model0_optname, char *Model0_optval,
     int *Model0_option);
 int (*Model0_compat_ioctl)(struct Model0_sock *Model0_sk,
     unsigned int Model0_cmd, unsigned long Model0_arg);

 int (*Model0_sendmsg)(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg,
        Model0_size_t Model0_len);
 int (*Model0_recvmsg)(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg,
        Model0_size_t Model0_len, int Model0_noblock, int Model0_flags,
        int *Model0_addr_len);
 int (*Model0_sendpage)(struct Model0_sock *Model0_sk, struct Model0_page *Model0_page,
     int Model0_offset, Model0_size_t Model0_size, int Model0_flags);
 int (*Model0_bind)(struct Model0_sock *Model0_sk,
     struct Model0_sockaddr *Model0_uaddr, int Model0_addr_len);

 int (*Model0_backlog_rcv) (struct Model0_sock *Model0_sk,
      struct Model0_sk_buff *Model0_skb);

 void (*Model0_release_cb)(struct Model0_sock *Model0_sk);

 /* Keeping track of sk's, looking them up, and port selection methods. */
 int (*Model0_hash)(struct Model0_sock *Model0_sk);
 void (*Model0_unhash)(struct Model0_sock *Model0_sk);
 void (*Model0_rehash)(struct Model0_sock *Model0_sk);
 int (*Model0_get_port)(struct Model0_sock *Model0_sk, unsigned short Model0_snum);
 void (*Model0_clear_sk)(struct Model0_sock *Model0_sk, int Model0_size);

 /* Keeping track of sockets in use */

 unsigned int Model0_inuse_idx;


 bool (*Model0_stream_memory_free)(const struct Model0_sock *Model0_sk);
 /* Memory pressure */
 void (*Model0_enter_memory_pressure)(struct Model0_sock *Model0_sk);
 Model0_atomic_long_t *Model0_memory_allocated; /* Current allocated memory. */
 struct Model0_percpu_counter *Model0_sockets_allocated; /* Current number of sockets. */
 /*
	 * Pressure flag: try to collapse.
	 * Technical note: it is used by multiple contexts non atomically.
	 * All the __sk_mem_schedule() is of this nature: accounting
	 * is strict, actions are advisory and have some latency.
	 */
 int *Model0_memory_pressure;
 long *Model0_sysctl_mem;
 int *Model0_sysctl_wmem;
 int *Model0_sysctl_rmem;
 int Model0_max_header;
 bool Model0_no_autobind;

 struct Model0_kmem_cache *Model0_slab;
 unsigned int Model0_obj_size;
 int Model0_slab_flags;

 struct Model0_percpu_counter *Model0_orphan_count;

 struct Model0_request_sock_ops *Model0_rsk_prot;
 struct Model0_timewait_sock_ops *Model0_twsk_prot;

 union {
  struct Model0_inet_hashinfo *Model0_hashinfo;
  struct Model0_udp_table *Model0_udp_table;
  struct Model0_raw_hashinfo *Model0_raw_hash;
 } Model0_h;

 struct Model0_module *Model0_owner;

 char Model0_name[32];

 struct Model0_list_head Model0_node;



 int (*Model0_diag_destroy)(struct Model0_sock *Model0_sk, int err);
};

int Model0_proto_register(struct Model0_proto *Model0_prot, int Model0_alloc_slab);
void Model0_proto_unregister(struct Model0_proto *Model0_prot);
static inline __attribute__((no_instrument_function)) bool Model0_sk_stream_memory_free(const struct Model0_sock *Model0_sk)
{
 if (Model0_sk->Model0_sk_wmem_queued >= Model0_sk->Model0_sk_sndbuf)
  return false;

 return Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_stream_memory_free ?
  Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_stream_memory_free(Model0_sk) : true;
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_stream_is_writeable(const struct Model0_sock *Model0_sk)
{
 return Model0_sk_stream_wspace(Model0_sk) >= Model0_sk_stream_min_wspace(Model0_sk) &&
        Model0_sk_stream_memory_free(Model0_sk);
}


static inline __attribute__((no_instrument_function)) bool Model0_sk_has_memory_pressure(const struct Model0_sock *Model0_sk)
{
 return Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_memory_pressure != ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_under_memory_pressure(const struct Model0_sock *Model0_sk)
{
 if (!Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_memory_pressure)
  return false;

 if (0 && Model0_sk->Model0_sk_memcg &&
     Model0_mem_cgroup_under_socket_pressure(Model0_sk->Model0_sk_memcg))
  return true;

 return !!*Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_memory_pressure;
}

static inline __attribute__((no_instrument_function)) void Model0_sk_leave_memory_pressure(struct Model0_sock *Model0_sk)
{
 int *Model0_memory_pressure = Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_memory_pressure;

 if (!Model0_memory_pressure)
  return;

 if (*Model0_memory_pressure)
  *Model0_memory_pressure = 0;
}

#if CY_ABSTRACT7
void Model0_tcp_enter_memory_pressure(struct Model0_sock *Model0_sk);
#endif
static inline __attribute__((no_instrument_function)) void Model0_sk_enter_memory_pressure(struct Model0_sock *Model0_sk)
{
 #if CY_ABSTRACT7
  Model0_tcp_enter_memory_pressure(Model0_sk);
 #else
 if (!Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_enter_memory_pressure)
  return;

 Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_enter_memory_pressure(Model0_sk);
 #endif
}

static inline __attribute__((no_instrument_function)) long Model0_sk_prot_mem_limits(const struct Model0_sock *Model0_sk, int Model0_index)
{
 return Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_sysctl_mem[Model0_index];
}

static inline __attribute__((no_instrument_function)) long
Model0_sk_memory_allocated(const struct Model0_sock *Model0_sk)
{
 return Model0_atomic_long_read(Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_memory_allocated);
}

static inline __attribute__((no_instrument_function)) long
Model0_sk_memory_allocated_add(struct Model0_sock *Model0_sk, int Model0_amt)
{
 return Model0_atomic_long_add_return(Model0_amt, Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_memory_allocated);
}

static inline __attribute__((no_instrument_function)) void
Model0_sk_memory_allocated_sub(struct Model0_sock *Model0_sk, int Model0_amt)
{
 Model0_atomic_long_sub(Model0_amt, Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_memory_allocated);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_sockets_allocated_dec(struct Model0_sock *Model0_sk)
{
 Model0_percpu_counter_dec(Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_sockets_allocated);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_sockets_allocated_inc(struct Model0_sock *Model0_sk)
{
 Model0_percpu_counter_inc(Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_sockets_allocated);
}

static inline __attribute__((no_instrument_function)) int
Model0_sk_sockets_allocated_read_positive(struct Model0_sock *Model0_sk)
{
 return Model0_percpu_counter_read_positive(Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_sockets_allocated);
}

static inline __attribute__((no_instrument_function)) int
Model0_proto_sockets_allocated_sum_positive(struct Model0_proto *Model0_prot)
{
 return Model0_percpu_counter_sum_positive(Model0_prot->Model0_sockets_allocated);
}

static inline __attribute__((no_instrument_function)) long
Model0_proto_memory_allocated(struct Model0_proto *Model0_prot)
{
 return Model0_atomic_long_read(Model0_prot->Model0_memory_allocated);
}

static inline __attribute__((no_instrument_function)) bool
Model0_proto_memory_pressure(struct Model0_proto *Model0_prot)
{
 if (!Model0_prot->Model0_memory_pressure)
  return false;
 return !!*Model0_prot->Model0_memory_pressure;
}



/* Called with local bh disabled */
void Model0_sock_prot_inuse_add(struct Model0_net *Model0_net, struct Model0_proto *Model0_prot, int Model0_inc);
int Model0_sock_prot_inuse_get(struct Model0_net *Model0_net, struct Model0_proto *Model0_proto);
/* With per-bucket locks this operation is not-atomic, so that
 * this version is not worse.
 */
static inline __attribute__((no_instrument_function)) int Model0___sk_prot_rehash(struct Model0_sock *Model0_sk)
{
 Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_unhash(Model0_sk);
 return Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_hash(Model0_sk);
}

void Model0_sk_prot_clear_portaddr_nulls(struct Model0_sock *Model0_sk, int Model0_size);

/* About 10 seconds */


/* Sockets 0-1023 can't be bound to unless you are superuser */
struct Model0_socket_alloc {
 struct Model0_socket Model0_socket;
 struct Model0_inode Model0_vfs_inode;
};

static inline __attribute__((no_instrument_function)) struct Model0_socket *Model0_SOCKET_I(struct Model0_inode *Model0_inode)
{
 return &({ const typeof( ((struct Model0_socket_alloc *)0)->Model0_vfs_inode ) *Model0___mptr = (Model0_inode); (struct Model0_socket_alloc *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_socket_alloc, Model0_vfs_inode) );})->Model0_socket;
}

static inline __attribute__((no_instrument_function)) struct Model0_inode *Model0_SOCK_INODE(struct Model0_socket *Model0_socket)
{
 return &({ const typeof( ((struct Model0_socket_alloc *)0)->Model0_socket ) *Model0___mptr = (Model0_socket); (struct Model0_socket_alloc *)( (char *)Model0___mptr - __builtin_offsetof(struct Model0_socket_alloc, Model0_socket) );})->Model0_vfs_inode;
}

/*
 * Functions for memory accounting
 */
int Model0___sk_mem_schedule(struct Model0_sock *Model0_sk, int Model0_size, int Model0_kind);
void Model0___sk_mem_reclaim(struct Model0_sock *Model0_sk, int Model0_amount);






static inline __attribute__((no_instrument_function)) int Model0_sk_mem_pages(int Model0_amt)
{
 return (Model0_amt + ((int)((1UL) << 12)) - 1) >> ( __builtin_constant_p(((int)((1UL) << 12))) ? ( (((int)((1UL) << 12))) < 1 ? Model0_____ilog2_NaN() : (((int)((1UL) << 12))) & (1ULL << 63) ? 63 : (((int)((1UL) << 12))) & (1ULL << 62) ? 62 : (((int)((1UL) << 12))) & (1ULL << 61) ? 61 : (((int)((1UL) << 12))) & (1ULL << 60) ? 60 : (((int)((1UL) << 12))) & (1ULL << 59) ? 59 : (((int)((1UL) << 12))) & (1ULL << 58) ? 58 : (((int)((1UL) << 12))) & (1ULL << 57) ? 57 : (((int)((1UL) << 12))) & (1ULL << 56) ? 56 : (((int)((1UL) << 12))) & (1ULL << 55) ? 55 : (((int)((1UL) << 12))) & (1ULL << 54) ? 54 : (((int)((1UL) << 12))) & (1ULL << 53) ? 53 : (((int)((1UL) << 12))) & (1ULL << 52) ? 52 : (((int)((1UL) << 12))) & (1ULL << 51) ? 51 : (((int)((1UL) << 12))) & (1ULL << 50) ? 50 : (((int)((1UL) << 12))) & (1ULL << 49) ? 49 : (((int)((1UL) << 12))) & (1ULL << 48) ? 48 : (((int)((1UL) << 12))) & (1ULL << 47) ? 47 : (((int)((1UL) << 12))) & (1ULL << 46) ? 46 : (((int)((1UL) << 12))) & (1ULL << 45) ? 45 : (((int)((1UL) << 12))) & (1ULL << 44) ? 44 : (((int)((1UL) << 12))) & (1ULL << 43) ? 43 : (((int)((1UL) << 12))) & (1ULL << 42) ? 42 : (((int)((1UL) << 12))) & (1ULL << 41) ? 41 : (((int)((1UL) << 12))) & (1ULL << 40) ? 40 : (((int)((1UL) << 12))) & (1ULL << 39) ? 39 : (((int)((1UL) << 12))) & (1ULL << 38) ? 38 : (((int)((1UL) << 12))) & (1ULL << 37) ? 37 : (((int)((1UL) << 12))) & (1ULL << 36) ? 36 : (((int)((1UL) << 12))) & (1ULL << 35) ? 35 : (((int)((1UL) << 12))) & (1ULL << 34) ? 34 : (((int)((1UL) << 12))) & (1ULL << 33) ? 33 : (((int)((1UL) << 12))) & (1ULL << 32) ? 32 : (((int)((1UL) << 12))) & (1ULL << 31) ? 31 : (((int)((1UL) << 12))) & (1ULL << 30) ? 30 : (((int)((1UL) << 12))) & (1ULL << 29) ? 29 : (((int)((1UL) << 12))) & (1ULL << 28) ? 28 : (((int)((1UL) << 12))) & (1ULL << 27) ? 27 : (((int)((1UL) << 12))) & (1ULL << 26) ? 26 : (((int)((1UL) << 12))) & (1ULL << 25) ? 25 : (((int)((1UL) << 12))) & (1ULL << 24) ? 24 : (((int)((1UL) << 12))) & (1ULL << 23) ? 23 : (((int)((1UL) << 12))) & (1ULL << 22) ? 22 : (((int)((1UL) << 12))) & (1ULL << 21) ? 21 : (((int)((1UL) << 12))) & (1ULL << 20) ? 20 : (((int)((1UL) << 12))) & (1ULL << 19) ? 19 : (((int)((1UL) << 12))) & (1ULL << 18) ? 18 : (((int)((1UL) << 12))) & (1ULL << 17) ? 17 : (((int)((1UL) << 12))) & (1ULL << 16) ? 16 : (((int)((1UL) << 12))) & (1ULL << 15) ? 15 : (((int)((1UL) << 12))) & (1ULL << 14) ? 14 : (((int)((1UL) << 12))) & (1ULL << 13) ? 13 : (((int)((1UL) << 12))) & (1ULL << 12) ? 12 : (((int)((1UL) << 12))) & (1ULL << 11) ? 11 : (((int)((1UL) << 12))) & (1ULL << 10) ? 10 : (((int)((1UL) << 12))) & (1ULL << 9) ? 9 : (((int)((1UL) << 12))) & (1ULL << 8) ? 8 : (((int)((1UL) << 12))) & (1ULL << 7) ? 7 : (((int)((1UL) << 12))) & (1ULL << 6) ? 6 : (((int)((1UL) << 12))) & (1ULL << 5) ? 5 : (((int)((1UL) << 12))) & (1ULL << 4) ? 4 : (((int)((1UL) << 12))) & (1ULL << 3) ? 3 : (((int)((1UL) << 12))) & (1ULL << 2) ? 2 : (((int)((1UL) << 12))) & (1ULL << 1) ? 1 : (((int)((1UL) << 12))) & (1ULL << 0) ? 0 : Model0_____ilog2_NaN() ) : (sizeof(((int)((1UL) << 12))) <= 4) ? Model0___ilog2_u32(((int)((1UL) << 12))) : Model0___ilog2_u64(((int)((1UL) << 12))) );
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_has_account(struct Model0_sock *Model0_sk)
{
 /* return true if protocol supports memory accounting */
 return !!Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_memory_allocated;
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_wmem_schedule(struct Model0_sock *Model0_sk, int Model0_size)
{
 if (!Model0_sk_has_account(Model0_sk))
  return true;
 return Model0_size <= Model0_sk->Model0_sk_forward_alloc ||
  Model0___sk_mem_schedule(Model0_sk, Model0_size, 0);
}

static inline __attribute__((no_instrument_function)) bool
Model0_sk_rmem_schedule(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, int Model0_size)
{
 if (!Model0_sk_has_account(Model0_sk))
  return true;
 return Model0_size<= Model0_sk->Model0_sk_forward_alloc ||
  Model0___sk_mem_schedule(Model0_sk, Model0_size, 1) ||
  Model0_skb_pfmemalloc(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_mem_reclaim(struct Model0_sock *Model0_sk)
{
 if (!Model0_sk_has_account(Model0_sk))
  return;
 if (Model0_sk->Model0_sk_forward_alloc >= ((int)((1UL) << 12)))
  Model0___sk_mem_reclaim(Model0_sk, Model0_sk->Model0_sk_forward_alloc);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_mem_reclaim_partial(struct Model0_sock *Model0_sk)
{
 if (!Model0_sk_has_account(Model0_sk))
  return;
 if (Model0_sk->Model0_sk_forward_alloc > ((int)((1UL) << 12)))
  Model0___sk_mem_reclaim(Model0_sk, Model0_sk->Model0_sk_forward_alloc - 1);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_mem_charge(struct Model0_sock *Model0_sk, int Model0_size)
{
 if (!Model0_sk_has_account(Model0_sk))
  return;
 Model0_sk->Model0_sk_forward_alloc -= Model0_size;
}

static inline __attribute__((no_instrument_function)) void Model0_sk_mem_uncharge(struct Model0_sock *Model0_sk, int Model0_size)
{
 if (!Model0_sk_has_account(Model0_sk))
  return;
 Model0_sk->Model0_sk_forward_alloc += Model0_size;

 /* Avoid a possible overflow.
	 * TCP send queues can make this happen, if sk_mem_reclaim()
	 * is not called and more than 2 GBytes are released at once.
	 *
	 * If we reach 2 MBytes, reclaim 1 MBytes right now, there is
	 * no need to hold that much forward allocation anyway.
	 */
 if (__builtin_expect(!!(Model0_sk->Model0_sk_forward_alloc >= 1 << 21), 0))
  Model0___sk_mem_reclaim(Model0_sk, 1 << 20);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_wmem_free_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 Model0_sock_set_flag(Model0_sk, Model0_SOCK_QUEUE_SHRUNK);
 Model0_sk->Model0_sk_wmem_queued -= Model0_skb->Model0_truesize;
 Model0_sk_mem_uncharge(Model0_sk, Model0_skb->Model0_truesize);
 Model0___kfree_skb(Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_sock_release_ownership(struct Model0_sock *Model0_sk)
{
 if (Model0_sk->Model0_sk_lock.Model0_owned) {
  Model0_sk->Model0_sk_lock.Model0_owned = 0;

  /* The sk_lock has mutex_unlock() semantics: */
  do { } while (0);
 }
}

/*
 * Macro so as to not evaluate some arguments when
 * lockdep is not enabled.
 *
 * Mark both the sk_lock and the sk_lock.slock as a
 * per-address-family lock class.
 */
void Model0_lock_sock_nested(struct Model0_sock *Model0_sk, int Model0_subclass);

static inline __attribute__((no_instrument_function)) void Model0_lock_sock(struct Model0_sock *Model0_sk)
{
 Model0_lock_sock_nested(Model0_sk, 0);
}

void Model0_release_sock(struct Model0_sock *Model0_sk);

/* BH context may only use the following locking interface. */






bool Model0_lock_sock_fast(struct Model0_sock *Model0_sk);
/**
 * unlock_sock_fast - complement of lock_sock_fast
 * @sk: socket
 * @slow: slow mode
 *
 * fast unlock socket for user context.
 * If slow mode is on, we call regular release_sock()
 */
static inline __attribute__((no_instrument_function)) void Model0_unlock_sock_fast(struct Model0_sock *Model0_sk, bool Model0_slow)
{
 if (Model0_slow)
  Model0_release_sock(Model0_sk);
 else
  Model0_spin_unlock_bh(&Model0_sk->Model0_sk_lock.Model0_slock);
}

/* Used by processes to "lock" a socket state, so that
 * interrupts and bottom half handlers won't change it
 * from under us. It essentially blocks any incoming
 * packets, so that we won't get any new data or any
 * packets that change the state of the socket.
 *
 * While locked, BH processing will add new packets to
 * the backlog queue.  This queue is processed by the
 * owner of the socket lock right before it is released.
 *
 * Since ~2.3.5 it is also exclusive sleep lock serializing
 * accesses from user process context.
 */

static inline __attribute__((no_instrument_function)) void Model0_sock_owned_by_me(const struct Model0_sock *Model0_sk)
{



}

static inline __attribute__((no_instrument_function)) bool Model0_sock_owned_by_user(const struct Model0_sock *Model0_sk)
{
 Model0_sock_owned_by_me(Model0_sk);
 return Model0_sk->Model0_sk_lock.Model0_owned;
}

/* no reclassification while locks are held */
static inline __attribute__((no_instrument_function)) bool Model0_sock_allow_reclassification(const struct Model0_sock *Model0_csk)
{
 struct Model0_sock *Model0_sk = (struct Model0_sock *)Model0_csk;

 return !Model0_sk->Model0_sk_lock.Model0_owned && !Model0_spin_is_locked(&Model0_sk->Model0_sk_lock.Model0_slock);
}

struct Model0_sock *Model0_sk_alloc(struct Model0_net *Model0_net, int Model0_family, Model0_gfp_t Model0_priority,
        struct Model0_proto *Model0_prot, int Model0_kern);
void Model0_sk_free(struct Model0_sock *Model0_sk);
void Model0_sk_destruct(struct Model0_sock *Model0_sk);
struct Model0_sock *Model0_sk_clone_lock(const struct Model0_sock *Model0_sk, const Model0_gfp_t Model0_priority);

struct Model0_sk_buff *Model0_sock_wmalloc(struct Model0_sock *Model0_sk, unsigned long Model0_size, int Model0_force,
        Model0_gfp_t Model0_priority);
void Model0___sock_wfree(struct Model0_sk_buff *Model0_skb);
void Model0_sock_wfree(struct Model0_sk_buff *Model0_skb);
void Model0_skb_orphan_partial(struct Model0_sk_buff *Model0_skb);
void Model0_sock_rfree(struct Model0_sk_buff *Model0_skb);
void Model0_sock_efree(struct Model0_sk_buff *Model0_skb);

void Model0_sock_edemux(struct Model0_sk_buff *Model0_skb);




int Model0_sock_setsockopt(struct Model0_socket *Model0_sock, int Model0_level, int Model0_op,
      char *Model0_optval, unsigned int Model0_optlen);

int Model0_sock_getsockopt(struct Model0_socket *Model0_sock, int Model0_level, int Model0_op,
      char *Model0_optval, int *Model0_optlen);
struct Model0_sk_buff *Model0_sock_alloc_send_skb(struct Model0_sock *Model0_sk, unsigned long Model0_size,
        int Model0_noblock, int *Model0_errcode);
struct Model0_sk_buff *Model0_sock_alloc_send_pskb(struct Model0_sock *Model0_sk, unsigned long Model0_header_len,
         unsigned long Model0_data_len, int Model0_noblock,
         int *Model0_errcode, int Model0_max_page_order);
void *Model0_sock_kmalloc(struct Model0_sock *Model0_sk, int Model0_size, Model0_gfp_t Model0_priority);
void Model0_sock_kfree_s(struct Model0_sock *Model0_sk, void *Model0_mem, int Model0_size);
void Model0_sock_kzfree_s(struct Model0_sock *Model0_sk, void *Model0_mem, int Model0_size);
void Model0_sk_send_sigurg(struct Model0_sock *Model0_sk);

struct Model0_sockcm_cookie {
 Model0_u32 Model0_mark;
 Model0_u16 Model0_tsflags;
};

int Model0___sock_cmsg_send(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, struct Model0_cmsghdr *Model0_cmsg,
       struct Model0_sockcm_cookie *Model0_sockc);
int Model0_sock_cmsg_send(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg,
     struct Model0_sockcm_cookie *Model0_sockc);

/*
 * Functions to fill in entries in struct proto_ops when a protocol
 * does not implement a particular function.
 */
int Model0_sock_no_bind(struct Model0_socket *, struct Model0_sockaddr *, int);
int Model0_sock_no_connect(struct Model0_socket *, struct Model0_sockaddr *, int, int);
int Model0_sock_no_socketpair(struct Model0_socket *, struct Model0_socket *);
int Model0_sock_no_accept(struct Model0_socket *, struct Model0_socket *, int);
int Model0_sock_no_getname(struct Model0_socket *, struct Model0_sockaddr *, int *, int);
unsigned int Model0_sock_no_poll(struct Model0_file *, struct Model0_socket *,
     struct Model0_poll_table_struct *);
int Model0_sock_no_ioctl(struct Model0_socket *, unsigned int, unsigned long);
int Model0_sock_no_listen(struct Model0_socket *, int);
int Model0_sock_no_shutdown(struct Model0_socket *, int);
int Model0_sock_no_getsockopt(struct Model0_socket *, int , int, char *, int *);
int Model0_sock_no_setsockopt(struct Model0_socket *, int, int, char *, unsigned int);
int Model0_sock_no_sendmsg(struct Model0_socket *, struct Model0_msghdr *, Model0_size_t);
int Model0_sock_no_recvmsg(struct Model0_socket *, struct Model0_msghdr *, Model0_size_t, int);
int Model0_sock_no_mmap(struct Model0_file *Model0_file, struct Model0_socket *Model0_sock,
   struct Model0_vm_area_struct *Model0_vma);
Model0_ssize_t Model0_sock_no_sendpage(struct Model0_socket *Model0_sock, struct Model0_page *Model0_page, int Model0_offset,
    Model0_size_t Model0_size, int Model0_flags);

/*
 * Functions to fill in entries in struct proto_ops when a protocol
 * uses the inet style.
 */
int Model0_sock_common_getsockopt(struct Model0_socket *Model0_sock, int Model0_level, int Model0_optname,
      char *Model0_optval, int *Model0_optlen);
int Model0_sock_common_recvmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, Model0_size_t Model0_size,
   int Model0_flags);
int Model0_sock_common_setsockopt(struct Model0_socket *Model0_sock, int Model0_level, int Model0_optname,
      char *Model0_optval, unsigned int Model0_optlen);
int Model0_compat_sock_common_getsockopt(struct Model0_socket *Model0_sock, int Model0_level,
  int Model0_optname, char *Model0_optval, int *Model0_optlen);
int Model0_compat_sock_common_setsockopt(struct Model0_socket *Model0_sock, int Model0_level,
  int Model0_optname, char *Model0_optval, unsigned int Model0_optlen);

void Model0_sk_common_release(struct Model0_sock *Model0_sk);

/*
 *	Default socket callbacks and setup code
 */

/* Initialise core socket variables */
void Model0_sock_init_data(struct Model0_socket *Model0_sock, struct Model0_sock *Model0_sk);

/*
 * Socket reference counting postulates.
 *
 * * Each user of socket SHOULD hold a reference count.
 * * Each access point to socket (an hash table bucket, reference from a list,
 *   running timer, skb in flight MUST hold a reference count.
 * * When reference count hits 0, it means it will never increase back.
 * * When reference count hits 0, it means that no references from
 *   outside exist to this socket and current process on current CPU
 *   is last user and may/should destroy this socket.
 * * sk_free is called from any context: process, BH, IRQ. When
 *   it is called, socket has no references from outside -> sk_free
 *   may release descendant resources allocated by the socket, but
 *   to the time when it is called, socket is NOT referenced by any
 *   hash tables, lists etc.
 * * Packets, delivered from outside (from network or from another process)
 *   and enqueued on receive/error queues SHOULD NOT grab reference count,
 *   when they sit in queue. Otherwise, packets will leak to hole, when
 *   socket is looked up by one cpu and unhasing is made by another CPU.
 *   It is true for udp/raw, netlink (leak to receive and error queues), tcp
 *   (leak to backlog). Packet socket does all the processing inside
 *   BR_NETPROTO_LOCK, so that it has not this race condition. UNIX sockets
 *   use separate SMP lock, so that they are prone too.
 */

/* Ungrab socket and destroy it, if it was the last reference. */
static inline __attribute__((no_instrument_function)) void Model0_sock_put(struct Model0_sock *Model0_sk)
{
 if (Model0_atomic_dec_and_test(&Model0_sk->Model0___sk_common.Model0_skc_refcnt))
  Model0_sk_free(Model0_sk);
}
/* Generic version of sock_put(), dealing with all sockets
 * (TCP_TIMEWAIT, TCP_NEW_SYN_RECV, ESTABLISHED...)
 */
void Model0_sock_gen_put(struct Model0_sock *Model0_sk);

int Model0___sk_receive_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, const int Model0_nested,
       unsigned int Model0_trim_cap);
static inline __attribute__((no_instrument_function)) int Model0_sk_receive_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
     const int Model0_nested)
{
 return Model0___sk_receive_skb(Model0_sk, Model0_skb, Model0_nested, 1);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_tx_queue_set(struct Model0_sock *Model0_sk, int Model0_tx_queue)
{
 Model0_sk->Model0___sk_common.Model0_skc_tx_queue_mapping = Model0_tx_queue;
}

static inline __attribute__((no_instrument_function)) void Model0_sk_tx_queue_clear(struct Model0_sock *Model0_sk)
{
 Model0_sk->Model0___sk_common.Model0_skc_tx_queue_mapping = -1;
}

static inline __attribute__((no_instrument_function)) int Model0_sk_tx_queue_get(const struct Model0_sock *Model0_sk)
{
 return Model0_sk ? Model0_sk->Model0___sk_common.Model0_skc_tx_queue_mapping : -1;
}

static inline __attribute__((no_instrument_function)) void Model0_sk_set_socket(struct Model0_sock *Model0_sk, struct Model0_socket *Model0_sock)
{
 Model0_sk_tx_queue_clear(Model0_sk);
 Model0_sk->Model0_sk_socket = Model0_sock;
}

static inline __attribute__((no_instrument_function)) Model0_wait_queue_head_t *Model0_sk_sleep(struct Model0_sock *Model0_sk)
{
 do { bool Model0___cond = !(!(__builtin_offsetof(struct Model0_socket_wq, Model0_wait) != 0)); extern void Model0___compiletime_assert_1620(void) ; if (Model0___cond) Model0___compiletime_assert_1620(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 return &({ typeof(Model0_sk->Model0_sk_wq) Model0_________p1 = ({ typeof(Model0_sk->Model0_sk_wq) Model0__________p1 = ({ union { typeof(Model0_sk->Model0_sk_wq) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_sk->Model0_sk_wq), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_wq)); else Model0___read_once_size_nocheck(&(Model0_sk->Model0_sk_wq), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_wq)); Model0___u.Model0___val; }); typeof(*(Model0_sk->Model0_sk_wq)) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); ((typeof(*Model0_sk->Model0_sk_wq) *)(Model0_________p1)); })->Model0_wait;
}
/* Detach socket from process context.
 * Announce socket dead, detach it from wait queue and inode.
 * Note that parent inode held reference count on this struct sock,
 * we do not release it in this function, because protocol
 * probably wants some additional cleanups or even continuing
 * to work with this socket (TCP).
 */
static inline __attribute__((no_instrument_function)) void Model0_sock_orphan(struct Model0_sock *Model0_sk)
{
 Model0__raw_write_lock_bh(&Model0_sk->Model0_sk_callback_lock);
 Model0_sock_set_flag(Model0_sk, Model0_SOCK_DEAD);
 Model0_sk_set_socket(Model0_sk, ((void *)0));
 Model0_sk->Model0_sk_wq = ((void *)0);
 Model0__raw_write_unlock_bh(&Model0_sk->Model0_sk_callback_lock);
}

static inline __attribute__((no_instrument_function)) void Model0_sock_graft(struct Model0_sock *Model0_sk, struct Model0_socket *Model0_parent)
{
 Model0__raw_write_lock_bh(&Model0_sk->Model0_sk_callback_lock);
 Model0_sk->Model0_sk_wq = Model0_parent->Model0_wq;
 Model0_parent->Model0_sk = Model0_sk;
 Model0_sk_set_socket(Model0_sk, Model0_parent);
 Model0_security_sock_graft(Model0_sk, Model0_parent);
 Model0__raw_write_unlock_bh(&Model0_sk->Model0_sk_callback_lock);
}

Model0_kuid_t Model0_sock_i_uid(struct Model0_sock *Model0_sk);
unsigned long Model0_sock_i_ino(struct Model0_sock *Model0_sk);

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_net_tx_rndhash(void)
{
 Model0_u32 Model0_v = Model0_prandom_u32();

 return Model0_v ?: 1;
}

static inline __attribute__((no_instrument_function)) void Model0_sk_set_txhash(struct Model0_sock *Model0_sk)
{
 Model0_sk->Model0_sk_txhash = Model0_net_tx_rndhash();
}

static inline __attribute__((no_instrument_function)) void Model0_sk_rethink_txhash(struct Model0_sock *Model0_sk)
{
 if (Model0_sk->Model0_sk_txhash)
  Model0_sk_set_txhash(Model0_sk);
}

static inline __attribute__((no_instrument_function)) struct Model0_dst_entry *
Model0___sk_dst_get(struct Model0_sock *Model0_sk)
{
 return ({ typeof(*(Model0_sk->Model0_sk_dst_cache)) *Model0_________p1 = (typeof(*(Model0_sk->Model0_sk_dst_cache)) *)({ typeof((Model0_sk->Model0_sk_dst_cache)) Model0__________p1 = ({ union { typeof((Model0_sk->Model0_sk_dst_cache)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_sk->Model0_sk_dst_cache)), Model0___u.Model0___c, sizeof((Model0_sk->Model0_sk_dst_cache))); else Model0___read_once_size_nocheck(&((Model0_sk->Model0_sk_dst_cache)), Model0___u.Model0___c, sizeof((Model0_sk->Model0_sk_dst_cache))); Model0___u.Model0___val; }); typeof(*((Model0_sk->Model0_sk_dst_cache))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_sk->Model0_sk_dst_cache)) *)(Model0_________p1)); });

}

static inline __attribute__((no_instrument_function)) struct Model0_dst_entry *
Model0_sk_dst_get(struct Model0_sock *Model0_sk)
{
 struct Model0_dst_entry *Model0_dst;

 Model0_rcu_read_lock();
 Model0_dst = ({ typeof(*(Model0_sk->Model0_sk_dst_cache)) *Model0_________p1 = (typeof(*(Model0_sk->Model0_sk_dst_cache)) *)({ typeof((Model0_sk->Model0_sk_dst_cache)) Model0__________p1 = ({ union { typeof((Model0_sk->Model0_sk_dst_cache)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_sk->Model0_sk_dst_cache)), Model0___u.Model0___c, sizeof((Model0_sk->Model0_sk_dst_cache))); else Model0___read_once_size_nocheck(&((Model0_sk->Model0_sk_dst_cache)), Model0___u.Model0___c, sizeof((Model0_sk->Model0_sk_dst_cache))); Model0___u.Model0___val; }); typeof(*((Model0_sk->Model0_sk_dst_cache))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_sk->Model0_sk_dst_cache)) *)(Model0_________p1)); });
 if (Model0_dst && !Model0_atomic_add_unless((&Model0_dst->Model0___refcnt), 1, 0))
  Model0_dst = ((void *)0);
 Model0_rcu_read_unlock();
 return Model0_dst;
}

static inline __attribute__((no_instrument_function)) void Model0_dst_negative_advice(struct Model0_sock *Model0_sk)
{
 struct Model0_dst_entry *Model0_ndst, *Model0_dst = Model0___sk_dst_get(Model0_sk);

 Model0_sk_rethink_txhash(Model0_sk);

 if (Model0_dst && Model0_dst->Model0_ops->Model0_negative_advice) {
  Model0_ndst = Model0_dst->Model0_ops->Model0_negative_advice(Model0_dst);

  if (Model0_ndst != Model0_dst) {
   ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_ndst); if (__builtin_constant_p(Model0_ndst) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof((Model0_sk->Model0_sk_dst_cache)) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof((Model0_sk->Model0_sk_dst_cache))) ((typeof(Model0_sk->Model0_sk_dst_cache))(Model0__r_a_p__v)) }; Model0___write_once_size(&((Model0_sk->Model0_sk_dst_cache)), Model0___u.Model0___c, sizeof((Model0_sk->Model0_sk_dst_cache))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&Model0_sk->Model0_sk_dst_cache) == sizeof(char) || sizeof(*&Model0_sk->Model0_sk_dst_cache) == sizeof(short) || sizeof(*&Model0_sk->Model0_sk_dst_cache) == sizeof(int) || sizeof(*&Model0_sk->Model0_sk_dst_cache) == sizeof(long))); extern void Model0___compiletime_assert_1700(void) ; if (Model0___cond) Model0___compiletime_assert_1700(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&Model0_sk->Model0_sk_dst_cache) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&Model0_sk->Model0_sk_dst_cache)) ((typeof(*((typeof(Model0_sk->Model0_sk_dst_cache))Model0__r_a_p__v)) *)((typeof(Model0_sk->Model0_sk_dst_cache))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&Model0_sk->Model0_sk_dst_cache), Model0___u.Model0___c, sizeof(*&Model0_sk->Model0_sk_dst_cache)); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
   Model0_sk_tx_queue_clear(Model0_sk);
  }
 }
}

static inline __attribute__((no_instrument_function)) void
Model0___sk_dst_set(struct Model0_sock *Model0_sk, struct Model0_dst_entry *Model0_dst)
{
 struct Model0_dst_entry *Model0_old_dst;

 Model0_sk_tx_queue_clear(Model0_sk);
 /*
	 * This can be called while sk is owned by the caller only,
	 * with no state that can be checked in a rcu_dereference_check() cond
	 */
 Model0_old_dst = ({ typeof(Model0_sk->Model0_sk_dst_cache) Model0_________p1 = ({ typeof(Model0_sk->Model0_sk_dst_cache) Model0__________p1 = ({ union { typeof(Model0_sk->Model0_sk_dst_cache) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_sk->Model0_sk_dst_cache), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_dst_cache)); else Model0___read_once_size_nocheck(&(Model0_sk->Model0_sk_dst_cache), Model0___u.Model0___c, sizeof(Model0_sk->Model0_sk_dst_cache)); Model0___u.Model0___val; }); typeof(*(Model0_sk->Model0_sk_dst_cache)) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); ((typeof(*Model0_sk->Model0_sk_dst_cache) *)(Model0_________p1)); });
 ({ Model0_uintptr_t Model0__r_a_p__v = (Model0_uintptr_t)(Model0_dst); if (__builtin_constant_p(Model0_dst) && (Model0__r_a_p__v) == (Model0_uintptr_t)((void *)0)) ({ union { typeof((Model0_sk->Model0_sk_dst_cache)) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof((Model0_sk->Model0_sk_dst_cache))) ((typeof(Model0_sk->Model0_sk_dst_cache))(Model0__r_a_p__v)) }; Model0___write_once_size(&((Model0_sk->Model0_sk_dst_cache)), Model0___u.Model0___c, sizeof((Model0_sk->Model0_sk_dst_cache))); Model0___u.Model0___val; }); else do { do { bool Model0___cond = !((sizeof(*&Model0_sk->Model0_sk_dst_cache) == sizeof(char) || sizeof(*&Model0_sk->Model0_sk_dst_cache) == sizeof(short) || sizeof(*&Model0_sk->Model0_sk_dst_cache) == sizeof(int) || sizeof(*&Model0_sk->Model0_sk_dst_cache) == sizeof(long))); extern void Model0___compiletime_assert_1717(void) ; if (Model0___cond) Model0___compiletime_assert_1717(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&Model0_sk->Model0_sk_dst_cache) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&Model0_sk->Model0_sk_dst_cache)) ((typeof(*((typeof(Model0_sk->Model0_sk_dst_cache))Model0__r_a_p__v)) *)((typeof(Model0_sk->Model0_sk_dst_cache))Model0__r_a_p__v)) }; Model0___write_once_size(&(*&Model0_sk->Model0_sk_dst_cache), Model0___u.Model0___c, sizeof(*&Model0_sk->Model0_sk_dst_cache)); Model0___u.Model0___val; }); } while (0); Model0__r_a_p__v; });
 Model0_dst_release(Model0_old_dst);
}

static inline __attribute__((no_instrument_function)) void
Model0_sk_dst_set(struct Model0_sock *Model0_sk, struct Model0_dst_entry *Model0_dst)
{
 struct Model0_dst_entry *Model0_old_dst;

 Model0_sk_tx_queue_clear(Model0_sk);
 Model0_old_dst = ({ __typeof__ (*((( struct Model0_dst_entry **)&Model0_sk->Model0_sk_dst_cache))) Model0___ret = ((Model0_dst)); switch (sizeof(*((( struct Model0_dst_entry **)&Model0_sk->Model0_sk_dst_cache)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*((( struct Model0_dst_entry **)&Model0_sk->Model0_sk_dst_cache))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*((( struct Model0_dst_entry **)&Model0_sk->Model0_sk_dst_cache))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*((( struct Model0_dst_entry **)&Model0_sk->Model0_sk_dst_cache))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*((( struct Model0_dst_entry **)&Model0_sk->Model0_sk_dst_cache))) : : "memory", "cc"); break; default: Model0___xchg_wrong_size(); } Model0___ret; });
 Model0_dst_release(Model0_old_dst);
}

static inline __attribute__((no_instrument_function)) void
Model0___sk_dst_reset(struct Model0_sock *Model0_sk)
{
 Model0___sk_dst_set(Model0_sk, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void
Model0_sk_dst_reset(struct Model0_sock *Model0_sk)
{
 Model0_sk_dst_set(Model0_sk, ((void *)0));
}

struct Model0_dst_entry *Model0___sk_dst_check(struct Model0_sock *Model0_sk, Model0_u32 Model0_cookie);

struct Model0_dst_entry *Model0_sk_dst_check(struct Model0_sock *Model0_sk, Model0_u32 Model0_cookie);

bool Model0_sk_mc_loop(struct Model0_sock *Model0_sk);

static inline __attribute__((no_instrument_function)) bool Model0_sk_can_gso(const struct Model0_sock *Model0_sk)
{
 return Model0_net_gso_ok(Model0_sk->Model0_sk_route_caps, Model0_sk->Model0_sk_gso_type);
}

void Model0_sk_setup_caps(struct Model0_sock *Model0_sk, struct Model0_dst_entry *Model0_dst);

static inline __attribute__((no_instrument_function)) void Model0_sk_nocaps_add(struct Model0_sock *Model0_sk, Model0_netdev_features_t Model0_flags)
{
 Model0_sk->Model0_sk_route_nocaps |= Model0_flags;
 Model0_sk->Model0_sk_route_caps &= ~Model0_flags;
}

static inline __attribute__((no_instrument_function)) bool Model0_sk_check_csum_caps(struct Model0_sock *Model0_sk)
{
 return (Model0_sk->Model0_sk_route_caps & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_HW_CSUM_BIT))) ||
        (Model0_sk->Model0___sk_common.Model0_skc_family == 2 &&
  (Model0_sk->Model0_sk_route_caps & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_IP_CSUM_BIT)))) ||
        (Model0_sk->Model0___sk_common.Model0_skc_family == 10 &&
  (Model0_sk->Model0_sk_route_caps & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_IPV6_CSUM_BIT))));
}

static inline __attribute__((no_instrument_function)) int Model0_skb_do_copy_data_nocache(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
        struct Model0_iov_iter *Model0_from, char *Model0_to,
        int Model0_copy, int Model0_offset)
{
 if (Model0_skb->Model0_ip_summed == 0) {
  Model0___wsum Model0_csum = 0;
  if (Model0_csum_and_copy_from_iter(Model0_to, Model0_copy, &Model0_csum, Model0_from) != Model0_copy)
   return -14;
  Model0_skb->Model0_csum = Model0_csum_block_add(Model0_skb->Model0_csum, Model0_csum, Model0_offset);
 } else if (Model0_sk->Model0_sk_route_caps & ((Model0_netdev_features_t)1 << (Model0_NETIF_F_NOCACHE_COPY_BIT))) {
  if (Model0_copy_from_iter_nocache(Model0_to, Model0_copy, Model0_from) != Model0_copy)
   return -14;
 } else if (Model0_copy_from_iter(Model0_to, Model0_copy, Model0_from) != Model0_copy)
  return -14;

 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_add_data_nocache(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
           struct Model0_iov_iter *Model0_from, int Model0_copy)
{
 int err, Model0_offset = Model0_skb->Model0_len;

 err = Model0_skb_do_copy_data_nocache(Model0_sk, Model0_skb, Model0_from, Model0_skb_put(Model0_skb, Model0_copy),
           Model0_copy, Model0_offset);
 if (err)
  Model0___skb_trim(Model0_skb, Model0_offset);

 return err;
}

static inline __attribute__((no_instrument_function)) int Model0_skb_copy_to_page_nocache(struct Model0_sock *Model0_sk, struct Model0_iov_iter *Model0_from,
        struct Model0_sk_buff *Model0_skb,
        struct Model0_page *Model0_page,
        int Model0_off, int Model0_copy)
{
 int err;

 err = Model0_skb_do_copy_data_nocache(Model0_sk, Model0_skb, Model0_from, Model0_lowmem_page_address(Model0_page) + Model0_off,
           Model0_copy, Model0_skb->Model0_len);
 if (err)
  return err;

 Model0_skb->Model0_len += Model0_copy;
 Model0_skb->Model0_data_len += Model0_copy;
 Model0_skb->Model0_truesize += Model0_copy;
 Model0_sk->Model0_sk_wmem_queued += Model0_copy;
 Model0_sk_mem_charge(Model0_sk, Model0_copy);
 return 0;
}

/**
 * sk_wmem_alloc_get - returns write allocations
 * @sk: socket
 *
 * Returns sk_wmem_alloc minus initial offset of one
 */
static inline __attribute__((no_instrument_function)) int Model0_sk_wmem_alloc_get(const struct Model0_sock *Model0_sk)
{
 return Model0_atomic_read(&Model0_sk->Model0_sk_wmem_alloc) - 1;
}

/**
 * sk_rmem_alloc_get - returns read allocations
 * @sk: socket
 *
 * Returns sk_rmem_alloc
 */
static inline __attribute__((no_instrument_function)) int Model0_sk_rmem_alloc_get(const struct Model0_sock *Model0_sk)
{
 return Model0_atomic_read(&Model0_sk->Model0_sk_backlog.Model0_rmem_alloc);
}

/**
 * sk_has_allocations - check if allocations are outstanding
 * @sk: socket
 *
 * Returns true if socket has write or read allocations
 */
static inline __attribute__((no_instrument_function)) bool Model0_sk_has_allocations(const struct Model0_sock *Model0_sk)
{
 return Model0_sk_wmem_alloc_get(Model0_sk) || Model0_sk_rmem_alloc_get(Model0_sk);
}

/**
 * skwq_has_sleeper - check if there are any waiting processes
 * @wq: struct socket_wq
 *
 * Returns true if socket_wq has waiting processes
 *
 * The purpose of the skwq_has_sleeper and sock_poll_wait is to wrap the memory
 * barrier call. They were added due to the race found within the tcp code.
 *
 * Consider following tcp code paths:
 *
 * CPU1                  CPU2
 *
 * sys_select            receive packet
 *   ...                 ...
 *   __add_wait_queue    update tp->rcv_nxt
 *   ...                 ...
 *   tp->rcv_nxt check   sock_def_readable
 *   ...                 {
 *   schedule               rcu_read_lock();
 *                          wq = rcu_dereference(sk->sk_wq);
 *                          if (wq && waitqueue_active(&wq->wait))
 *                              wake_up_interruptible(&wq->wait)
 *                          ...
 *                       }
 *
 * The race for tcp fires when the __add_wait_queue changes done by CPU1 stay
 * in its cache, and so does the tp->rcv_nxt update on CPU2 side.  The CPU1
 * could then endup calling schedule and sleep forever if there are no more
 * data on the socket.
 *
 */
static inline __attribute__((no_instrument_function)) bool Model0_skwq_has_sleeper(struct Model0_socket_wq *Model0_wq)
{
 return Model0_wq && Model0_wq_has_sleeper(&Model0_wq->Model0_wait);
}

/**
 * sock_poll_wait - place memory barrier behind the poll_wait call.
 * @filp:           file
 * @wait_address:   socket wait queue
 * @p:              poll_table
 *
 * See the comments in the wq_has_sleeper function.
 */
static inline __attribute__((no_instrument_function)) void Model0_sock_poll_wait(struct Model0_file *Model0_filp,
  Model0_wait_queue_head_t *Model0_wait_address, Model0_poll_table *Model0_p)
{
 if (!Model0_poll_does_not_wait(Model0_p) && Model0_wait_address) {
  Model0_poll_wait(Model0_filp, Model0_wait_address, Model0_p);
  /* We need to be sure we are in sync with the
		 * socket flags modification.
		 *
		 * This memory barrier is paired in the wq_has_sleeper.
		 */
  asm volatile("mfence":::"memory");
 }
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_hash_from_sk(struct Model0_sk_buff *Model0_skb, struct Model0_sock *Model0_sk)
{
 if (Model0_sk->Model0_sk_txhash) {
  Model0_skb->Model0_l4_hash = 1;
  Model0_skb->Model0_hash = Model0_sk->Model0_sk_txhash;
 }
}

void Model0_skb_set_owner_w(struct Model0_sk_buff *Model0_skb, struct Model0_sock *Model0_sk);

/*
 *	Queue a received datagram if it will fit. Stream and sequenced
 *	protocols can't normally use this as they need to fit buffers in
 *	and play with them.
 *
 *	Inlined as it's very short and called for pretty much every
 *	packet ever received.
 */
static inline __attribute__((no_instrument_function)) void Model0_skb_set_owner_r(struct Model0_sk_buff *Model0_skb, struct Model0_sock *Model0_sk)
{
 Model0_skb_orphan(Model0_skb);
 Model0_skb->Model0_sk = Model0_sk;
 Model0_skb->Model0_destructor = Model0_sock_rfree;
 Model0_atomic_add(Model0_skb->Model0_truesize, &Model0_sk->Model0_sk_backlog.Model0_rmem_alloc);
 Model0_sk_mem_charge(Model0_sk, Model0_skb->Model0_truesize);
}

void Model0_sk_reset_timer(struct Model0_sock *Model0_sk, struct Model0_timer_list *Model0_timer,
      unsigned long Model0_expires);

void Model0_sk_stop_timer(struct Model0_sock *Model0_sk, struct Model0_timer_list *Model0_timer);

int Model0___sock_queue_rcv_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_sock_queue_rcv_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

int Model0_sock_queue_err_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
struct Model0_sk_buff *Model0_sock_dequeue_err_skb(struct Model0_sock *Model0_sk);

/*
 *	Recover an error report and clear atomically
 */

static inline __attribute__((no_instrument_function)) int Model0_sock_error(struct Model0_sock *Model0_sk)
{
 int err;
 if (__builtin_expect(!!(!Model0_sk->Model0_sk_err), 1))
  return 0;
 err = ({ __typeof__ (*((&Model0_sk->Model0_sk_err))) Model0___ret = ((0)); switch (sizeof(*((&Model0_sk->Model0_sk_err)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model0___ret), "+m" (*((&Model0_sk->Model0_sk_err))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_sk->Model0_sk_err))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_sk->Model0_sk_err))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model0___ret), "+m" (*((&Model0_sk->Model0_sk_err))) : : "memory", "cc"); break; default: Model0___xchg_wrong_size(); } Model0___ret; });
 return -err;
}

static inline __attribute__((no_instrument_function)) unsigned long Model0_sock_wspace(struct Model0_sock *Model0_sk)
{
 int Model0_amt = 0;

 if (!(Model0_sk->Model0_sk_shutdown & 2)) {
  Model0_amt = Model0_sk->Model0_sk_sndbuf - Model0_atomic_read(&Model0_sk->Model0_sk_wmem_alloc);
  if (Model0_amt < 0)
   Model0_amt = 0;
 }
 return Model0_amt;
}

/* Note:
 *  We use sk->sk_wq_raw, from contexts knowing this
 *  pointer is not NULL and cannot disappear/change.
 */
static inline __attribute__((no_instrument_function)) void Model0_sk_set_bit(int Model0_nr, struct Model0_sock *Model0_sk)
{
 if ((Model0_nr == 0 || Model0_nr == 1) &&
     !Model0_sock_flag(Model0_sk, Model0_SOCK_FASYNC))
  return;

 Model0_set_bit(Model0_nr, &Model0_sk->Model0_sk_wq_raw->Model0_flags);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_clear_bit(int Model0_nr, struct Model0_sock *Model0_sk)
{
 if ((Model0_nr == 0 || Model0_nr == 1) &&
     !Model0_sock_flag(Model0_sk, Model0_SOCK_FASYNC))
  return;

 Model0_clear_bit(Model0_nr, &Model0_sk->Model0_sk_wq_raw->Model0_flags);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_wake_async(const struct Model0_sock *Model0_sk, int Model0_how, int Model0_band)
{
 if (Model0_sock_flag(Model0_sk, Model0_SOCK_FASYNC)) {
  Model0_rcu_read_lock();
  Model0_sock_wake_async(({ typeof(*(Model0_sk->Model0_sk_wq)) *Model0_________p1 = (typeof(*(Model0_sk->Model0_sk_wq)) *)({ typeof((Model0_sk->Model0_sk_wq)) Model0__________p1 = ({ union { typeof((Model0_sk->Model0_sk_wq)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_sk->Model0_sk_wq)), Model0___u.Model0___c, sizeof((Model0_sk->Model0_sk_wq))); else Model0___read_once_size_nocheck(&((Model0_sk->Model0_sk_wq)), Model0___u.Model0___c, sizeof((Model0_sk->Model0_sk_wq))); Model0___u.Model0___val; }); typeof(*((Model0_sk->Model0_sk_wq))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_sk->Model0_sk_wq)) *)(Model0_________p1)); }), Model0_how, Model0_band);
  Model0_rcu_read_unlock();
 }
}

/* Since sk_{r,w}mem_alloc sums skb->truesize, even a small frame might
 * need sizeof(sk_buff) + MTU + padding, unless net driver perform copybreak.
 * Note: for send buffers, TCP works better if we can build two skbs at
 * minimum.
 */





static inline __attribute__((no_instrument_function)) void Model0_sk_stream_moderate_sndbuf(struct Model0_sock *Model0_sk)
{
 if (!(Model0_sk->Model0_sk_userlocks & 1)) {
  Model0_sk->Model0_sk_sndbuf = ({ typeof(Model0_sk->Model0_sk_sndbuf) Model0__min1 = (Model0_sk->Model0_sk_sndbuf); typeof(Model0_sk->Model0_sk_wmem_queued >> 1) Model0__min2 = (Model0_sk->Model0_sk_wmem_queued >> 1); (void) (&Model0__min1 == &Model0__min2); Model0__min1 < Model0__min2 ? Model0__min1 : Model0__min2; });
  Model0_sk->Model0_sk_sndbuf = ({ Model0_u32 Model0___max1 = (Model0_sk->Model0_sk_sndbuf); Model0_u32 Model0___max2 = (((2048 + ((((sizeof(struct Model0_sk_buff))) + ((typeof((sizeof(struct Model0_sk_buff))))(((1 << (6)))) - 1)) & ~((typeof((sizeof(struct Model0_sk_buff))))(((1 << (6)))) - 1))) * 2)); Model0___max1 > Model0___max2 ? Model0___max1: Model0___max2; });
 }
}

struct Model0_sk_buff *Model0_sk_stream_alloc_skb(struct Model0_sock *Model0_sk, int Model0_size, Model0_gfp_t Model0_gfp,
        bool Model0_force_schedule);

/**
 * sk_page_frag - return an appropriate page_frag
 * @sk: socket
 *
 * If socket allocation mode allows current thread to sleep, it means its
 * safe to use the per task page_frag instead of the per socket one.
 */
static inline __attribute__((no_instrument_function)) struct Model0_page_frag *Model0_sk_page_frag(struct Model0_sock *Model0_sk)
{
 if (Model0_gfpflags_allow_blocking(Model0_sk->Model0_sk_allocation))
  return &Model0_get_current()->Model0_task_frag;

 return &Model0_sk->Model0_sk_frag;
}

bool Model0_sk_page_frag_refill(struct Model0_sock *Model0_sk, struct Model0_page_frag *Model0_pfrag);

/*
 *	Default write policy as shown to user space via poll/select/SIGIO
 */
static inline __attribute__((no_instrument_function)) bool Model0_sock_writeable(const struct Model0_sock *Model0_sk)
{
 return Model0_atomic_read(&Model0_sk->Model0_sk_wmem_alloc) < (Model0_sk->Model0_sk_sndbuf >> 1);
}

static inline __attribute__((no_instrument_function)) Model0_gfp_t Model0_gfp_any(void)
{
 return ((Model0_preempt_count() & (((1UL << (8))-1) << (0 + 8)))) ? ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)) : ((( Model0_gfp_t)(0x400000u|0x2000000u)) | (( Model0_gfp_t)0x40u) | (( Model0_gfp_t)0x80u));
}

static inline __attribute__((no_instrument_function)) long Model0_sock_rcvtimeo(const struct Model0_sock *Model0_sk, bool Model0_noblock)
{
 return Model0_noblock ? 0 : Model0_sk->Model0_sk_rcvtimeo;
}

static inline __attribute__((no_instrument_function)) long Model0_sock_sndtimeo(const struct Model0_sock *Model0_sk, bool Model0_noblock)
{
 return Model0_noblock ? 0 : Model0_sk->Model0_sk_sndtimeo;
}

static inline __attribute__((no_instrument_function)) int Model0_sock_rcvlowat(const struct Model0_sock *Model0_sk, int Model0_waitall, int Model0_len)
{
 return (Model0_waitall ? Model0_len : ({ int Model0___min1 = (Model0_sk->Model0_sk_rcvlowat); int Model0___min2 = (Model0_len); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; })) ? : 1;
}

/* Alas, with timeout socket operations are not restartable.
 * Compare this to poll().
 */
static inline __attribute__((no_instrument_function)) int Model0_sock_intr_errno(long Model0_timeo)
{
 return Model0_timeo == ((long)(~0UL>>1)) ? -512 : -4;
}

struct Model0_sock_skb_cb {
 Model0_u32 Model0_dropcount;
};

/* Store sock_skb_cb at the end of skb->cb[] so protocol families
 * using skb->cb[] would keep using it directly and utilize its
 * alignement guarantee.
 */
static inline __attribute__((no_instrument_function)) void
Model0_sock_skb_set_dropcount(const struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 ((struct Model0_sock_skb_cb *)((Model0_skb)->Model0_cb + (((sizeof(((struct Model0_sk_buff*)0)->Model0_cb)) - sizeof(struct Model0_sock_skb_cb)))))->Model0_dropcount = Model0_atomic_read(&Model0_sk->Model0_sk_drops);
}

static inline __attribute__((no_instrument_function)) void Model0_sk_drops_add(struct Model0_sock *Model0_sk, const struct Model0_sk_buff *Model0_skb)
{
 int Model0_segs = ({ Model0_u16 Model0___max1 = (1); Model0_u16 Model0___max2 = (((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_gso_segs); Model0___max1 > Model0___max2 ? Model0___max1: Model0___max2; });

 Model0_atomic_add(Model0_segs, &Model0_sk->Model0_sk_drops);
}

void Model0___sock_recv_timestamp(struct Model0_msghdr *Model0_msg, struct Model0_sock *Model0_sk,
      struct Model0_sk_buff *Model0_skb);
void Model0___sock_recv_wifi_status(struct Model0_msghdr *Model0_msg, struct Model0_sock *Model0_sk,
        struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) void
Model0_sock_recv_timestamp(struct Model0_msghdr *Model0_msg, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 Model0_ktime_t Model0_kt = Model0_skb->Model0_tstamp;
 struct Model0_skb_shared_hwtstamps *Model0_hwtstamps = Model0_skb_hwtstamps(Model0_skb);

 /*
	 * generate control messages if
	 * - receive time stamping in software requested
	 * - software time stamp available and wanted
	 * - hardware time stamps available and wanted
	 */
 if (Model0_sock_flag(Model0_sk, Model0_SOCK_RCVTSTAMP) ||
     (Model0_sk->Model0_sk_tsflags & Model0_SOF_TIMESTAMPING_RX_SOFTWARE) ||
     (Model0_kt.Model0_tv64 && Model0_sk->Model0_sk_tsflags & Model0_SOF_TIMESTAMPING_SOFTWARE) ||
     (Model0_hwtstamps->Model0_hwtstamp.Model0_tv64 &&
      (Model0_sk->Model0_sk_tsflags & Model0_SOF_TIMESTAMPING_RAW_HARDWARE)))
  Model0___sock_recv_timestamp(Model0_msg, Model0_sk, Model0_skb);
 else
  Model0_sk->Model0_sk_stamp = Model0_kt;

 if (Model0_sock_flag(Model0_sk, Model0_SOCK_WIFI_STATUS) && Model0_skb->Model0_wifi_acked_valid)
  Model0___sock_recv_wifi_status(Model0_msg, Model0_sk, Model0_skb);
}

void Model0___sock_recv_ts_and_drops(struct Model0_msghdr *Model0_msg, struct Model0_sock *Model0_sk,
         struct Model0_sk_buff *Model0_skb);

static inline __attribute__((no_instrument_function)) void Model0_sock_recv_ts_and_drops(struct Model0_msghdr *Model0_msg, struct Model0_sock *Model0_sk,
       struct Model0_sk_buff *Model0_skb)
{





 if (Model0_sk->Model0___sk_common.Model0_skc_flags & ((1UL << Model0_SOCK_RXQ_OVFL) | (1UL << Model0_SOCK_RCVTSTAMP)) || Model0_sk->Model0_sk_tsflags & (Model0_SOF_TIMESTAMPING_SOFTWARE | Model0_SOF_TIMESTAMPING_RAW_HARDWARE))
  Model0___sock_recv_ts_and_drops(Model0_msg, Model0_sk, Model0_skb);
 else
  Model0_sk->Model0_sk_stamp = Model0_skb->Model0_tstamp;
}

void Model0___sock_tx_timestamp(Model0___u16 Model0_tsflags, __u8 *Model0_tx_flags);

/**
 * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
 * @sk:		socket sending this packet
 * @tsflags:	timestamping flags to use
 * @tx_flags:	completed with instructions for time stamping
 *
 * Note : callers should take care of initial *tx_flags value (usually 0)
 */
static inline __attribute__((no_instrument_function)) void Model0_sock_tx_timestamp(const struct Model0_sock *Model0_sk, Model0___u16 Model0_tsflags,
         __u8 *Model0_tx_flags)
{
 if (__builtin_expect(!!(Model0_tsflags), 0))
  Model0___sock_tx_timestamp(Model0_tsflags, Model0_tx_flags);
 if (__builtin_expect(!!(Model0_sock_flag(Model0_sk, Model0_SOCK_WIFI_STATUS)), 0))
  *Model0_tx_flags |= Model0_SKBTX_WIFI_STATUS;
}

/**
 * sk_eat_skb - Release a skb if it is no longer needed
 * @sk: socket to eat this skb from
 * @skb: socket buffer to eat
 *
 * This routine must be called with interrupts disabled or with the socket
 * locked so that the sk_buff queue operation is ok.
*/
static inline __attribute__((no_instrument_function)) void Model0_sk_eat_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 Model0___skb_unlink(Model0_skb, &Model0_sk->Model0_sk_receive_queue);
 Model0___kfree_skb(Model0_skb);
}

static inline __attribute__((no_instrument_function))
struct Model0_net *Model0_sock_net(const struct Model0_sock *Model0_sk)
{
 return Model0_read_pnet(&Model0_sk->Model0___sk_common.Model0_skc_net);
}

static inline __attribute__((no_instrument_function))
void Model0_sock_net_set(struct Model0_sock *Model0_sk, struct Model0_net *Model0_net)
{
 Model0_write_pnet(&Model0_sk->Model0___sk_common.Model0_skc_net, Model0_net);
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_skb_steal_sock(struct Model0_sk_buff *Model0_skb)
{
 if (Model0_skb->Model0_sk) {
  struct Model0_sock *Model0_sk = Model0_skb->Model0_sk;

  Model0_skb->Model0_destructor = ((void *)0);
  Model0_skb->Model0_sk = ((void *)0);
  return Model0_sk;
 }
 return ((void *)0);
}

/* This helper checks if a socket is a full socket,
 * ie _not_ a timewait or request socket.
 */
static inline __attribute__((no_instrument_function)) bool Model0_sk_fullsock(const struct Model0_sock *Model0_sk)
{
 return (1 << Model0_sk->Model0___sk_common.Model0_skc_state) & ~(Model0_TCPF_TIME_WAIT | Model0_TCPF_NEW_SYN_RECV);
}

/* This helper checks if a socket is a LISTEN or NEW_SYN_RECV
 * SYNACK messages can be attached to either ones (depending on SYNCOOKIE)
 */
static inline __attribute__((no_instrument_function)) bool Model0_sk_listener(const struct Model0_sock *Model0_sk)
{
 return (1 << Model0_sk->Model0___sk_common.Model0_skc_state) & (Model0_TCPF_LISTEN | Model0_TCPF_NEW_SYN_RECV);
}

/**
 * sk_state_load - read sk->sk_state for lockless contexts
 * @sk: socket pointer
 *
 * Paired with sk_state_store(). Used in places we do not hold socket lock :
 * tcp_diag_get_info(), tcp_get_info(), tcp_poll(), get_tcp4_sock() ...
 */
static inline __attribute__((no_instrument_function)) int Model0_sk_state_load(const struct Model0_sock *Model0_sk)
{
 return ({ typeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) Model0____p1 = ({ union { typeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(*&Model0_sk->Model0___sk_common.Model0_skc_state), Model0___u.Model0___c, sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state)); else Model0___read_once_size_nocheck(&(*&Model0_sk->Model0___sk_common.Model0_skc_state), Model0___u.Model0___c, sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state)); Model0___u.Model0___val; }); do { bool Model0___cond = !((sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) == sizeof(char) || sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) == sizeof(short) || sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) == sizeof(int) || sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) == sizeof(long))); extern void Model0___compiletime_assert_2241(void) ; if (Model0___cond) Model0___compiletime_assert_2241(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); Model0____p1; });
}

/**
 * sk_state_store - update sk->sk_state
 * @sk: socket pointer
 * @newstate: new state
 *
 * Paired with sk_state_load(). Should be used in contexts where
 * state change might impact lockless readers.
 */
static inline __attribute__((no_instrument_function)) void Model0_sk_state_store(struct Model0_sock *Model0_sk, int Model0_newstate)
{
 do { do { bool Model0___cond = !((sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) == sizeof(char) || sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) == sizeof(short) || sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) == sizeof(int) || sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) == sizeof(long))); extern void Model0___compiletime_assert_2254(void) ; if (Model0___cond) Model0___compiletime_assert_2254(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&Model0_sk->Model0___sk_common.Model0_skc_state) Model0___val; char Model0___c[1]; } Model0___u = { .Model0___val = ( typeof(*&Model0_sk->Model0___sk_common.Model0_skc_state)) (Model0_newstate) }; Model0___write_once_size(&(*&Model0_sk->Model0___sk_common.Model0_skc_state), Model0___u.Model0___c, sizeof(*&Model0_sk->Model0___sk_common.Model0_skc_state)); Model0___u.Model0___val; }); } while (0);
}

void Model0_sock_enable_timestamp(struct Model0_sock *Model0_sk, int Model0_flag);
int Model0_sock_get_timestamp(struct Model0_sock *, struct Model0_timeval *);
int Model0_sock_get_timestampns(struct Model0_sock *, struct Model0_timespec *);
int Model0_sock_recv_errqueue(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, int Model0_len, int Model0_level,
         int Model0_type);

bool Model0_sk_ns_capable(const struct Model0_sock *Model0_sk,
     struct Model0_user_namespace *Model0_user_ns, int Model0_cap);
bool Model0_sk_capable(const struct Model0_sock *Model0_sk, int Model0_cap);
bool Model0_sk_net_capable(const struct Model0_sock *Model0_sk, int Model0_cap);

extern __u32 Model0_sysctl_wmem_max;
extern __u32 Model0_sysctl_rmem_max;

extern int Model0_sysctl_tstamp_allow_data;
extern int Model0_sysctl_optmem_max;

extern __u32 Model0_sysctl_wmem_default;
extern __u32 Model0_sysctl_rmem_default;
/*
 * NET		Generic infrastructure for INET connection oriented protocols.
 *
 *		Definitions for inet_connection_sock 
 *
 * Authors:	Many people, see the TCP sources
 *
 * 		From code originally in TCP
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for inet_sock
 *
 * Authors:	Many, reorganised here by
 * 		Arnaldo Carvalho de Melo <acme@mandriva.com>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */











/* jhash.h: Jenkins hash support.
 *
 * Copyright (C) 2006. Bob Jenkins (bob_jenkins@burtleburtle.net)
 *
 * http://burtleburtle.net/bob/hash/
 *
 * These are the credits from Bob's sources:
 *
 * lookup3.c, by Bob Jenkins, May 2006, Public Domain.
 *
 * These are functions for producing 32-bit hashes for hash table lookup.
 * hashword(), hashlittle(), hashlittle2(), hashbig(), mix(), and final()
 * are externally useful functions.  Routines to test the hash are included
 * if SELF_TEST is defined.  You can use this free for any purpose.  It's in
 * the public domain.  It has no warranty.
 *
 * Copyright (C) 2009-2010 Jozsef Kadlecsik (kadlec@blackhole.kfki.hu)
 *
 * I've modified Bob's hash to be useful in the Linux kernel, and
 * any bugs present are my fault.
 * Jozsef
 */







struct Model0___una_u16 { Model0_u16 Model0_x; } __attribute__((packed));
struct Model0___una_u32 { Model0_u32 Model0_x; } __attribute__((packed));
struct Model0___una_u64 { Model0_u64 Model0_x; } __attribute__((packed));

static inline __attribute__((no_instrument_function)) Model0_u16 Model0___get_unaligned_cpu16(const void *Model0_p)
{
 const struct Model0___una_u16 *Model0_ptr = (const struct Model0___una_u16 *)Model0_p;
 return Model0_ptr->Model0_x;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0___get_unaligned_cpu32(const void *Model0_p)
{
 const struct Model0___una_u32 *Model0_ptr = (const struct Model0___una_u32 *)Model0_p;
 return Model0_ptr->Model0_x;
}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0___get_unaligned_cpu64(const void *Model0_p)
{
 const struct Model0___una_u64 *Model0_ptr = (const struct Model0___una_u64 *)Model0_p;
 return Model0_ptr->Model0_x;
}

static inline __attribute__((no_instrument_function)) void Model0___put_unaligned_cpu16(Model0_u16 Model0_val, void *Model0_p)
{
 struct Model0___una_u16 *Model0_ptr = (struct Model0___una_u16 *)Model0_p;
 Model0_ptr->Model0_x = Model0_val;
}

static inline __attribute__((no_instrument_function)) void Model0___put_unaligned_cpu32(Model0_u32 Model0_val, void *Model0_p)
{
 struct Model0___una_u32 *Model0_ptr = (struct Model0___una_u32 *)Model0_p;
 Model0_ptr->Model0_x = Model0_val;
}

static inline __attribute__((no_instrument_function)) void Model0___put_unaligned_cpu64(Model0_u64 Model0_val, void *Model0_p)
{
 struct Model0___una_u64 *Model0_ptr = (struct Model0___una_u64 *)Model0_p;
 Model0_ptr->Model0_x = Model0_val;
}

/* Best hash sizes are of power of two */

/* Mask the hash value, i.e (value & jhash_mask(n)) instead of (value % n) */


/* __jhash_mix -- mix 3 32-bit values reversibly. */
/* __jhash_final - final mixing of 3 32-bit values (a,b,c) into c */
/* An arbitrary initial parameter */


/* jhash - hash an arbitrary key
 * @k: sequence of bytes as key
 * @length: the length of the key
 * @initval: the previous hash, or an arbitray value
 *
 * The generic version, hashes an arbitrary sequence of bytes.
 * No alignment or length assumptions are made about the input key.
 *
 * Returns the hash value of the key. The result depends on endianness.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_jhash(const void *Model0_key, Model0_u32 Model0_length, Model0_u32 Model0_initval)
{
 Model0_u32 Model0_a, Model0_b, Model0_c;
 const Model0_u8 *Model0_k = Model0_key;

 /* Set up the internal state */
 Model0_a = Model0_b = Model0_c = 0xdeadbeef + Model0_length + Model0_initval;

 /* All but the last block: affect some 32 bits of (a,b,c) */
 while (Model0_length > 12) {
  Model0_a += Model0___get_unaligned_cpu32(Model0_k);
  Model0_b += Model0___get_unaligned_cpu32(Model0_k + 4);
  Model0_c += Model0___get_unaligned_cpu32(Model0_k + 8);
  { Model0_a -= Model0_c; Model0_a ^= Model0_rol32(Model0_c, 4); Model0_c += Model0_b; Model0_b -= Model0_a; Model0_b ^= Model0_rol32(Model0_a, 6); Model0_a += Model0_c; Model0_c -= Model0_b; Model0_c ^= Model0_rol32(Model0_b, 8); Model0_b += Model0_a; Model0_a -= Model0_c; Model0_a ^= Model0_rol32(Model0_c, 16); Model0_c += Model0_b; Model0_b -= Model0_a; Model0_b ^= Model0_rol32(Model0_a, 19); Model0_a += Model0_c; Model0_c -= Model0_b; Model0_c ^= Model0_rol32(Model0_b, 4); Model0_b += Model0_a; };
  Model0_length -= 12;
  Model0_k += 12;
 }
 /* Last block: affect all 32 bits of (c) */
 /* All the case statements fall through */
 switch (Model0_length) {
 case 12: Model0_c += (Model0_u32)Model0_k[11]<<24;
 case 11: Model0_c += (Model0_u32)Model0_k[10]<<16;
 case 10: Model0_c += (Model0_u32)Model0_k[9]<<8;
 case 9: Model0_c += Model0_k[8];
 case 8: Model0_b += (Model0_u32)Model0_k[7]<<24;
 case 7: Model0_b += (Model0_u32)Model0_k[6]<<16;
 case 6: Model0_b += (Model0_u32)Model0_k[5]<<8;
 case 5: Model0_b += Model0_k[4];
 case 4: Model0_a += (Model0_u32)Model0_k[3]<<24;
 case 3: Model0_a += (Model0_u32)Model0_k[2]<<16;
 case 2: Model0_a += (Model0_u32)Model0_k[1]<<8;
 case 1: Model0_a += Model0_k[0];
   { Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 14); Model0_a ^= Model0_c; Model0_a -= Model0_rol32(Model0_c, 11); Model0_b ^= Model0_a; Model0_b -= Model0_rol32(Model0_a, 25); Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 16); Model0_a ^= Model0_c; Model0_a -= Model0_rol32(Model0_c, 4); Model0_b ^= Model0_a; Model0_b -= Model0_rol32(Model0_a, 14); Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 24); };
 case 0: /* Nothing left to add */
  break;
 }

 return Model0_c;
}

/* jhash2 - hash an array of u32's
 * @k: the key which must be an array of u32's
 * @length: the number of u32's in the key
 * @initval: the previous hash, or an arbitray value
 *
 * Returns the hash value of the key.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_jhash2(const Model0_u32 *Model0_k, Model0_u32 Model0_length, Model0_u32 Model0_initval)
{
 Model0_u32 Model0_a, Model0_b, Model0_c;

 /* Set up the internal state */
 Model0_a = Model0_b = Model0_c = 0xdeadbeef + (Model0_length<<2) + Model0_initval;

 /* Handle most of the key */
 while (Model0_length > 3) {
  Model0_a += Model0_k[0];
  Model0_b += Model0_k[1];
  Model0_c += Model0_k[2];
  { Model0_a -= Model0_c; Model0_a ^= Model0_rol32(Model0_c, 4); Model0_c += Model0_b; Model0_b -= Model0_a; Model0_b ^= Model0_rol32(Model0_a, 6); Model0_a += Model0_c; Model0_c -= Model0_b; Model0_c ^= Model0_rol32(Model0_b, 8); Model0_b += Model0_a; Model0_a -= Model0_c; Model0_a ^= Model0_rol32(Model0_c, 16); Model0_c += Model0_b; Model0_b -= Model0_a; Model0_b ^= Model0_rol32(Model0_a, 19); Model0_a += Model0_c; Model0_c -= Model0_b; Model0_c ^= Model0_rol32(Model0_b, 4); Model0_b += Model0_a; };
  Model0_length -= 3;
  Model0_k += 3;
 }

 /* Handle the last 3 u32's: all the case statements fall through */
 switch (Model0_length) {
 case 3: Model0_c += Model0_k[2];
 case 2: Model0_b += Model0_k[1];
 case 1: Model0_a += Model0_k[0];
  { Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 14); Model0_a ^= Model0_c; Model0_a -= Model0_rol32(Model0_c, 11); Model0_b ^= Model0_a; Model0_b -= Model0_rol32(Model0_a, 25); Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 16); Model0_a ^= Model0_c; Model0_a -= Model0_rol32(Model0_c, 4); Model0_b ^= Model0_a; Model0_b -= Model0_rol32(Model0_a, 14); Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 24); };
 case 0: /* Nothing left to add */
  break;
 }

 return Model0_c;
}


/* __jhash_nwords - hash exactly 3, 2 or 1 word(s) */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0___jhash_nwords(Model0_u32 Model0_a, Model0_u32 Model0_b, Model0_u32 Model0_c, Model0_u32 Model0_initval)
{
 Model0_a += Model0_initval;
 Model0_b += Model0_initval;
 Model0_c += Model0_initval;

 { Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 14); Model0_a ^= Model0_c; Model0_a -= Model0_rol32(Model0_c, 11); Model0_b ^= Model0_a; Model0_b -= Model0_rol32(Model0_a, 25); Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 16); Model0_a ^= Model0_c; Model0_a -= Model0_rol32(Model0_c, 4); Model0_b ^= Model0_a; Model0_b -= Model0_rol32(Model0_a, 14); Model0_c ^= Model0_b; Model0_c -= Model0_rol32(Model0_b, 24); };

 return Model0_c;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_jhash_3words(Model0_u32 Model0_a, Model0_u32 Model0_b, Model0_u32 Model0_c, Model0_u32 Model0_initval)
{
 return Model0___jhash_nwords(Model0_a, Model0_b, Model0_c, Model0_initval + 0xdeadbeef + (3 << 2));
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_jhash_2words(Model0_u32 Model0_a, Model0_u32 Model0_b, Model0_u32 Model0_initval)
{
 return Model0___jhash_nwords(Model0_a, Model0_b, 0, Model0_initval + 0xdeadbeef + (2 << 2));
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_jhash_1word(Model0_u32 Model0_a, Model0_u32 Model0_initval)
{
 return Model0___jhash_nwords(Model0_a, 0, 0, Model0_initval + 0xdeadbeef + (1 << 2));
}




/*
 * NET		Generic infrastructure for Network protocols.
 *
 *		Definitions for request_sock 
 *
 * Authors:	Arnaldo Carvalho de Melo <acme@conectiva.com.br>
 *
 * 		From code originally in include/net/tcp.h
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
struct Model0_request_sock;
struct Model0_sk_buff;
struct Model0_dst_entry;
struct Model0_proto;

struct Model0_request_sock_ops {
 int Model0_family;
 int Model0_obj_size;
 struct Model0_kmem_cache *Model0_slab;
 char *Model0_slab_name;
 int (*Model0_rtx_syn_ack)(const struct Model0_sock *Model0_sk,
           struct Model0_request_sock *Model0_req);
 void (*Model0_send_ack)(const struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
        struct Model0_request_sock *Model0_req);
 void (*Model0_send_reset)(const struct Model0_sock *Model0_sk,
          struct Model0_sk_buff *Model0_skb);
 void (*Model0_destructor)(struct Model0_request_sock *Model0_req);
 void (*Model0_syn_ack_timeout)(const struct Model0_request_sock *Model0_req);
};

int Model0_inet_rtx_syn_ack(const struct Model0_sock *Model0_parent, struct Model0_request_sock *Model0_req);

/* struct request_sock - mini sock to represent a connection request
 */
struct Model0_request_sock {
 struct Model0_sock_common Model0___req_common;






 struct Model0_request_sock *Model0_dl_next;
 Model0_u16 Model0_mss;
 Model0_u8 Model0_num_retrans; /* number of retransmits */
 Model0_u8 Model0_cookie_ts:1; /* syncookie: encode tcpopts in timestamp */
 Model0_u8 Model0_num_timeout:7; /* number of timeouts */
 Model0_u32 Model0_ts_recent;
 struct Model0_timer_list Model0_rsk_timer;
 const struct Model0_request_sock_ops *Model0_rsk_ops;
 struct Model0_sock *Model0_sk;
 Model0_u32 *Model0_saved_syn;
 Model0_u32 Model0_secid;
 Model0_u32 Model0_peer_secid;
};

static inline __attribute__((no_instrument_function)) struct Model0_request_sock *Model0_inet_reqsk(const struct Model0_sock *Model0_sk)
{
 return (struct Model0_request_sock *)Model0_sk;
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_req_to_sk(struct Model0_request_sock *Model0_req)
{
 return (struct Model0_sock *)Model0_req;
}
#if CY_ABSTRACT1
void *Model0_cy_kmem_cache_alloc(int Model0_memory_type);
extern bool Model0_Src_sk;
#endif
extern struct Model0_tcp_sock Model0_Server_L;
static inline __attribute__((no_instrument_function)) struct Model0_request_sock *
Model0_reqsk_alloc(const struct Model0_request_sock_ops *Model0_ops, struct Model0_sock *Model0_sk_listener,
     bool Model0_attach_listener)
{
 struct Model0_request_sock *Model0_req;

#if CY_ABSTRACT1
 if (Model0_Src_sk){ //from attacke
    Model0_req = Model0_cy_kmem_cache_alloc(1); //Model0_Req_A
 }
 else{
    Model0_req = Model0_cy_kmem_cache_alloc(2); //Model0_Req
 }
#else
 Model0_req = Model0_kmem_cache_alloc(Model0_ops->Model0_slab, ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)) | (( Model0_gfp_t)0x200u));
#endif
 if (!Model0_req)
  return ((void *)0);
 Model0_req->Model0___req_common.Model0_skc_listener = ((void *)0);
 if (Model0_attach_listener) {
  if (__builtin_expect(!!(!Model0_atomic_add_unless((&Model0_sk_listener->Model0___sk_common.Model0_skc_refcnt), 1, 0)), 0)) {
   Model0_kmem_cache_free(Model0_ops->Model0_slab, Model0_req);
   return ((void *)0);
  }
  Model0_req->Model0___req_common.Model0_skc_listener = Model0_sk_listener;
 }
 Model0_req->Model0_rsk_ops = Model0_ops;
 Model0_req_to_sk(Model0_req)->Model0___sk_common.Model0_skc_prot = Model0_sk_listener->Model0___sk_common.Model0_skc_prot;
 Model0_sk_node_init(&Model0_req_to_sk(Model0_req)->Model0___sk_common.Model0_skc_node);
 Model0_sk_tx_queue_clear(Model0_req_to_sk(Model0_req));
 Model0_req->Model0_saved_syn = ((void *)0);
 Model0_atomic_set(&Model0_req->Model0___req_common.Model0_skc_refcnt, 0);

 return Model0_req;
}

#if CY_ABSTRACT7
static void Model0_tcp_v4_reqsk_destructor(struct Model0_request_sock *Model0_req);
#endif

static inline __attribute__((no_instrument_function)) void Model0_reqsk_free(struct Model0_request_sock *Model0_req)
{
 /* temporary debugging */
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model0___warned; int Model0___ret_warn_once = !!(Model0_atomic_read(&Model0_req->Model0___req_common.Model0_skc_refcnt) != 0); if (__builtin_expect(!!(Model0___ret_warn_once && !Model0___warned), 0)) { Model0___warned = true; ({ int Model0___ret_warn_on = !!(1); if (__builtin_expect(!!(Model0___ret_warn_on), 0)) Model0_warn_slowpath_null("./include/net/request_sock.h", 111); __builtin_expect(!!(Model0___ret_warn_on), 0); }); } __builtin_expect(!!(Model0___ret_warn_once), 0); });
#if CY_ABSTRACT7
 Model0_tcp_v4_reqsk_destructor(Model0_req);
#else
 Model0_req->Model0_rsk_ops->Model0_destructor(Model0_req);
#endif
 if (Model0_req->Model0___req_common.Model0_skc_listener)
  Model0_sock_put(Model0_req->Model0___req_common.Model0_skc_listener);
 Model0_kfree(Model0_req->Model0_saved_syn);
 Model0_kmem_cache_free(Model0_req->Model0_rsk_ops->Model0_slab, Model0_req);
}

static inline __attribute__((no_instrument_function)) void Model0_reqsk_put(struct Model0_request_sock *Model0_req)
{
 if (Model0_atomic_dec_and_test(&Model0_req->Model0___req_common.Model0_skc_refcnt))
  Model0_reqsk_free(Model0_req);
}

extern int Model0_sysctl_max_syn_backlog;

/*
 * For a TCP Fast Open listener -
 *	lock - protects the access to all the reqsk, which is co-owned by
 *		the listener and the child socket.
 *	qlen - pending TFO requests (still in TCP_SYN_RECV).
 *	max_qlen - max TFO reqs allowed before TFO is disabled.
 *
 *	XXX (TFO) - ideally these fields can be made as part of "listen_sock"
 *	structure above. But there is some implementation difficulty due to
 *	listen_sock being part of request_sock_queue hence will be freed when
 *	a listener is stopped. But TFO related fields may continue to be
 *	accessed even after a listener is closed, until its sk_refcnt drops
 *	to 0 implying no more outstanding TFO reqs. One solution is to keep
 *	listen_opt around until	sk_refcnt drops to 0. But there is some other
 *	complexity that needs to be resolved. E.g., a listener can be disabled
 *	temporarily through shutdown()->tcp_disconnect(), and re-enabled later.
 */
struct Model0_fastopen_queue {
 struct Model0_request_sock *Model0_rskq_rst_head; /* Keep track of past TFO */
 struct Model0_request_sock *Model0_rskq_rst_tail; /* requests that caused RST.
						 * This is part of the defense
						 * against spoofing attack.
						 */
 Model0_spinlock_t Model0_lock;
 int Model0_qlen; /* # of pending (TCP_SYN_RECV) reqs */
 int Model0_max_qlen; /* != 0 iff TFO is currently enabled */
};

/** struct request_sock_queue - queue of request_socks
 *
 * @rskq_accept_head - FIFO head of established children
 * @rskq_accept_tail - FIFO tail of established children
 * @rskq_defer_accept - User waits for some data after accept()
 *
 */
struct Model0_request_sock_queue {
 Model0_spinlock_t Model0_rskq_lock;
 Model0_u8 Model0_rskq_defer_accept;

 Model0_u32 Model0_synflood_warned;
 Model0_atomic_t Model0_qlen;
 Model0_atomic_t Model0_young;

 struct Model0_request_sock *Model0_rskq_accept_head;
 struct Model0_request_sock *Model0_rskq_accept_tail;
 struct Model0_fastopen_queue Model0_fastopenq; /* Check max_qlen != 0 to determine
					     * if TFO is enabled.
					     */
};

void Model0_reqsk_queue_alloc(struct Model0_request_sock_queue *Model0_queue);

void Model0_reqsk_fastopen_remove(struct Model0_sock *Model0_sk, struct Model0_request_sock *Model0_req,
      bool Model0_reset);

static inline __attribute__((no_instrument_function)) bool Model0_reqsk_queue_empty(const struct Model0_request_sock_queue *Model0_queue)
{
 return Model0_queue->Model0_rskq_accept_head == ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_request_sock *Model0_reqsk_queue_remove(struct Model0_request_sock_queue *Model0_queue,
            struct Model0_sock *Model0_parent)
{
 struct Model0_request_sock *Model0_req;

 Model0_spin_lock_bh(&Model0_queue->Model0_rskq_lock);
 Model0_req = Model0_queue->Model0_rskq_accept_head;
 if (Model0_req) {
  Model0_sk_acceptq_removed(Model0_parent);
  Model0_queue->Model0_rskq_accept_head = Model0_req->Model0_dl_next;
  if (Model0_queue->Model0_rskq_accept_head == ((void *)0))
   Model0_queue->Model0_rskq_accept_tail = ((void *)0);
 }
 Model0_spin_unlock_bh(&Model0_queue->Model0_rskq_lock);
 return Model0_req;
}

static inline __attribute__((no_instrument_function)) void Model0_reqsk_queue_removed(struct Model0_request_sock_queue *Model0_queue,
           const struct Model0_request_sock *Model0_req)
{
 if (Model0_req->Model0_num_timeout == 0)
  Model0_atomic_dec(&Model0_queue->Model0_young);
 Model0_atomic_dec(&Model0_queue->Model0_qlen);
}

static inline __attribute__((no_instrument_function)) void Model0_reqsk_queue_added(struct Model0_request_sock_queue *Model0_queue)
{
 Model0_atomic_inc(&Model0_queue->Model0_young);
 Model0_atomic_inc(&Model0_queue->Model0_qlen);
}

static inline __attribute__((no_instrument_function)) int Model0_reqsk_queue_len(const struct Model0_request_sock_queue *Model0_queue)
{
 return Model0_atomic_read(&Model0_queue->Model0_qlen);
}

static inline __attribute__((no_instrument_function)) int Model0_reqsk_queue_len_young(const struct Model0_request_sock_queue *Model0_queue)
{
 return Model0_atomic_read(&Model0_queue->Model0_young);
}





struct Model0_net;

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_net_hash_mix(const struct Model0_net *Model0_net)
{

 /*
	 * shift this right to eliminate bits, that are
	 * always zeroed
	 */

 return (Model0_u32)(((unsigned long)Model0_net) >> (6));



}

/*
 * include/net/l3mdev.h - L3 master device API
 * Copyright (c) 2015 Cumulus Networks
 * Copyright (c) 2015 David Ahern <dsa@cumulusnetworks.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 */
















/* rule is permanent, and cannot be deleted */







/* try to find source address in routing lookups */


struct Model0_fib_rule_hdr {
 __u8 Model0_family;
 __u8 Model0_dst_len;
 __u8 Model0_src_len;
 __u8 Model0_tos;

 __u8 Model0_table;
 __u8 Model0_res1; /* reserved */
 __u8 Model0_res2; /* reserved */
 __u8 Model0_action;

 __u32 Model0_flags;
};

enum {
 Model0_FRA_UNSPEC,
 Model0_FRA_DST, /* destination address */
 Model0_FRA_SRC, /* source address */
 Model0_FRA_IIFNAME, /* interface name */

 Model0_FRA_GOTO, /* target to jump to (FR_ACT_GOTO) */
 Model0_FRA_UNUSED2,
 Model0_FRA_PRIORITY, /* priority/preference */
 Model0_FRA_UNUSED3,
 Model0_FRA_UNUSED4,
 Model0_FRA_UNUSED5,
 Model0_FRA_FWMARK, /* mark */
 Model0_FRA_FLOW, /* flow/class id */
 Model0_FRA_TUN_ID,
 Model0_FRA_SUPPRESS_IFGROUP,
 Model0_FRA_SUPPRESS_PREFIXLEN,
 Model0_FRA_TABLE, /* Extended table id */
 Model0_FRA_FWMASK, /* mask for netfilter mark */
 Model0_FRA_OIFNAME,
 Model0_FRA_PAD,
 Model0_FRA_L3MDEV, /* iif or oif is l3mdev goto its table */
 Model0___FRA_MAX
};



enum {
 Model0_FR_ACT_UNSPEC,
 Model0_FR_ACT_TO_TBL, /* Pass to fixed table */
 Model0_FR_ACT_GOTO, /* Jump to another rule */
 Model0_FR_ACT_NOP, /* No operation */
 Model0_FR_ACT_RES3,
 Model0_FR_ACT_RES4,
 Model0_FR_ACT_BLACKHOLE, /* Drop without notification */
 Model0_FR_ACT_UNREACHABLE, /* Drop with ENETUNREACH */
 Model0_FR_ACT_PROHIBIT, /* Drop with EACCES */
 Model0___FR_ACT_MAX,
};



struct Model0_fib_rule {
 struct Model0_list_head Model0_list;
 int Model0_iifindex;
 int Model0_oifindex;
 Model0_u32 Model0_mark;
 Model0_u32 Model0_mark_mask;
 Model0_u32 Model0_flags;
 Model0_u32 Model0_table;
 Model0_u8 Model0_action;
 Model0_u8 Model0_l3mdev;
 /* 2 bytes hole, try to use */
 Model0_u32 Model0_target;
 Model0___be64 Model0_tun_id;
 struct Model0_fib_rule *Model0_ctarget;
 struct Model0_net *Model0_fr_net;

 Model0_atomic_t Model0_refcnt;
 Model0_u32 Model0_pref;
 int Model0_suppress_ifgroup;
 int Model0_suppress_prefixlen;
 char Model0_iifname[16];
 char Model0_oifname[16];
 struct Model0_callback_head Model0_rcu;
};

struct Model0_fib_lookup_arg {
 void *Model0_lookup_ptr;
 void *Model0_result;
 struct Model0_fib_rule *Model0_rule;
 Model0_u32 Model0_table;
 int Model0_flags;


};

struct Model0_fib_rules_ops {
 int Model0_family;
 struct Model0_list_head Model0_list;
 int Model0_rule_size;
 int Model0_addr_size;
 int Model0_unresolved_rules;
 int Model0_nr_goto_rules;

 int (*Model0_action)(struct Model0_fib_rule *,
       struct Model0_flowi *, int,
       struct Model0_fib_lookup_arg *);
 bool (*Model0_suppress)(struct Model0_fib_rule *,
         struct Model0_fib_lookup_arg *);
 int (*Model0_match)(struct Model0_fib_rule *,
      struct Model0_flowi *, int);
 int (*Model0_configure)(struct Model0_fib_rule *,
          struct Model0_sk_buff *,
          struct Model0_fib_rule_hdr *,
          struct Model0_nlattr **);
 int (*Model0_delete)(struct Model0_fib_rule *);
 int (*Model0_compare)(struct Model0_fib_rule *,
        struct Model0_fib_rule_hdr *,
        struct Model0_nlattr **);
 int (*Model0_fill)(struct Model0_fib_rule *, struct Model0_sk_buff *,
     struct Model0_fib_rule_hdr *);
 Model0_size_t (*Model0_nlmsg_payload)(struct Model0_fib_rule *);

 /* Called after modifications to the rules set, must flush
	 * the route cache if one exists. */
 void (*Model0_flush_cache)(struct Model0_fib_rules_ops *Model0_ops);

 int Model0_nlgroup;
 const struct Model0_nla_policy *Model0_policy;
 struct Model0_list_head Model0_rules_list;
 struct Model0_module *Model0_owner;
 struct Model0_net *Model0_fro_net;
 struct Model0_callback_head Model0_rcu;
};
static inline __attribute__((no_instrument_function)) void Model0_fib_rule_get(struct Model0_fib_rule *Model0_rule)
{
 Model0_atomic_inc(&Model0_rule->Model0_refcnt);
}

static inline __attribute__((no_instrument_function)) void Model0_fib_rule_put(struct Model0_fib_rule *Model0_rule)
{
 if (Model0_atomic_dec_and_test(&Model0_rule->Model0_refcnt))
  do { do { bool Model0___cond = !(!(!((__builtin_offsetof(typeof(*(Model0_rule)), Model0_rcu)) < 4096))); extern void Model0___compiletime_assert_105(void) ; if (Model0___cond) Model0___compiletime_assert_105(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); Model0_kfree_call_rcu(&((Model0_rule)->Model0_rcu), (Model0_rcu_callback_t)(unsigned long)(__builtin_offsetof(typeof(*(Model0_rule)), Model0_rcu))); } while (0);
}
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_fib_rule_get_table(struct Model0_fib_rule *Model0_rule,
         struct Model0_fib_lookup_arg *Model0_arg)
{
 return Model0_rule->Model0_table;
}


static inline __attribute__((no_instrument_function)) Model0_u32 Model0_frh_get_table(struct Model0_fib_rule_hdr *Model0_frh, struct Model0_nlattr **Model0_nla)
{
 if (Model0_nla[Model0_FRA_TABLE])
  return Model0_nla_get_u32(Model0_nla[Model0_FRA_TABLE]);
 return Model0_frh->Model0_table;
}

struct Model0_fib_rules_ops *Model0_fib_rules_register(const struct Model0_fib_rules_ops *,
      struct Model0_net *);
void Model0_fib_rules_unregister(struct Model0_fib_rules_ops *);

int Model0_fib_rules_lookup(struct Model0_fib_rules_ops *, struct Model0_flowi *, int Model0_flags,
       struct Model0_fib_lookup_arg *);
int Model0_fib_default_rule_add(struct Model0_fib_rules_ops *, Model0_u32 Model0_pref, Model0_u32 Model0_table,
    Model0_u32 Model0_flags);

int Model0_fib_nl_newrule(struct Model0_sk_buff *Model0_skb, struct Model0_nlmsghdr *Model0_nlh);
int Model0_fib_nl_delrule(struct Model0_sk_buff *Model0_skb, struct Model0_nlmsghdr *Model0_nlh);

/**
 * struct l3mdev_ops - l3mdev operations
 *
 * @l3mdev_fib_table: Get FIB table id to use for lookups
 *
 * @l3mdev_get_rtable: Get cached IPv4 rtable (dst_entry) for device
 *
 * @l3mdev_get_saddr: Get source address for a flow
 *
 * @l3mdev_get_rt6_dst: Get cached IPv6 rt6_info (dst_entry) for device
 */

struct Model0_l3mdev_ops {
 Model0_u32 (*Model0_l3mdev_fib_table)(const struct Model0_net_device *Model0_dev);
 struct Model0_sk_buff * (*Model0_l3mdev_l3_rcv)(struct Model0_net_device *Model0_dev,
       struct Model0_sk_buff *Model0_skb, Model0_u16 Model0_proto);

 /* IPv4 ops */
 struct Model0_rtable * (*Model0_l3mdev_get_rtable)(const struct Model0_net_device *Model0_dev,
          const struct Model0_flowi4 *Model0_fl4);
 int (*Model0_l3mdev_get_saddr)(struct Model0_net_device *Model0_dev,
         struct Model0_flowi4 *Model0_fl4);

 /* IPv6 ops */
 struct Model0_dst_entry * (*Model0_l3mdev_get_rt6_dst)(const struct Model0_net_device *Model0_dev,
       struct Model0_flowi6 *Model0_fl6);
 int (*Model0_l3mdev_get_saddr6)(struct Model0_net_device *Model0_dev,
      const struct Model0_sock *Model0_sk,
      struct Model0_flowi6 *Model0_fl6);
};
static inline __attribute__((no_instrument_function)) int Model0_l3mdev_master_ifindex_rcu(const struct Model0_net_device *Model0_dev)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model0_l3mdev_master_ifindex(struct Model0_net_device *Model0_dev)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_l3mdev_master_ifindex_by_index(struct Model0_net *Model0_net, int Model0_ifindex)
{
 return 0;
}

static inline __attribute__((no_instrument_function))
const struct Model0_net_device *Model0_l3mdev_master_dev_rcu(const struct Model0_net_device *Model0_dev)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_l3mdev_fib_oif_rcu(struct Model0_net_device *Model0_dev)
{
 return Model0_dev ? Model0_dev->Model0_ifindex : 0;
}
static inline __attribute__((no_instrument_function)) int Model0_l3mdev_fib_oif(struct Model0_net_device *Model0_dev)
{
 return Model0_dev ? Model0_dev->Model0_ifindex : 0;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_l3mdev_fib_table_rcu(const struct Model0_net_device *Model0_dev)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_l3mdev_fib_table(const struct Model0_net_device *Model0_dev)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_l3mdev_fib_table_by_index(struct Model0_net *Model0_net, int Model0_ifindex)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0_l3mdev_get_rtable(const struct Model0_net_device *Model0_dev,
            const struct Model0_flowi4 *Model0_fl4)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model0_netif_index_is_l3_master(struct Model0_net *Model0_net, int Model0_ifindex)
{
 return false;
}

static inline __attribute__((no_instrument_function)) int Model0_l3mdev_get_saddr(struct Model0_net *Model0_net, int Model0_ifindex,
       struct Model0_flowi4 *Model0_fl4)
{
 return 0;
}

static inline __attribute__((no_instrument_function))
struct Model0_dst_entry *Model0_l3mdev_get_rt6_dst(struct Model0_net *Model0_net, struct Model0_flowi6 *Model0_fl6)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_l3mdev_get_saddr6(struct Model0_net *Model0_net, const struct Model0_sock *Model0_sk,
        struct Model0_flowi6 *Model0_fl6)
{
 return 0;
}

static inline __attribute__((no_instrument_function))
struct Model0_sk_buff *Model0_l3mdev_ip_rcv(struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb;
}

static inline __attribute__((no_instrument_function))
struct Model0_sk_buff *Model0_l3mdev_ip6_rcv(struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb;
}

static inline __attribute__((no_instrument_function))
int Model0_l3mdev_fib_rule_match(struct Model0_net *Model0_net, struct Model0_flowi *Model0_fl,
     struct Model0_fib_lookup_arg *Model0_arg)
{
 return 1;
}

/** struct ip_options - IP Options
 *
 * @faddr - Saved first hop address
 * @nexthop - Saved nexthop address in LSRR and SSRR
 * @is_strictroute - Strict source route
 * @srr_is_hit - Packet destination addr was our one
 * @is_changed - IP checksum more not valid
 * @rr_needaddr - Need to record addr of outgoing dev
 * @ts_needtime - Need to record timestamp
 * @ts_needaddr - Need to record addr of outgoing dev
 */
struct Model0_ip_options {
 Model0___be32 Model0_faddr;
 Model0___be32 Model0_nexthop;
 unsigned char Model0_optlen;
 unsigned char Model0_srr;
 unsigned char Model0_rr;
 unsigned char Model0_ts;
 unsigned char Model0_is_strictroute:1,
   Model0_srr_is_hit:1,
   Model0_is_changed:1,
   Model0_rr_needaddr:1,
   Model0_ts_needtime:1,
   Model0_ts_needaddr:1;
 unsigned char Model0_router_alert;
 unsigned char Model0_cipso;
 unsigned char Model0___pad2;
 unsigned char Model0___data[0];
};

struct Model0_ip_options_rcu {
 struct Model0_callback_head Model0_rcu;
 struct Model0_ip_options Model0_opt;
};

struct Model0_ip_options_data {
 struct Model0_ip_options_rcu Model0_opt;
 char Model0_data[40];
};

struct Model0_inet_request_sock {
 struct Model0_request_sock Model0_req;
                                ;
 Model0_u16 Model0_snd_wscale : 4,
    Model0_rcv_wscale : 4,
    Model0_tstamp_ok : 1,
    Model0_sack_ok : 1,
    Model0_wscale_ok : 1,
    Model0_ecn_ok : 1,
    Model0_acked : 1,
    Model0_no_srccheck: 1;
                              ;
 Model0_u32 Model0_ir_mark;
 union {
  struct Model0_ip_options_rcu *Model0_opt;

  struct {
   struct Model0_ipv6_txoptions *Model0_ipv6_opt;
   struct Model0_sk_buff *Model0_pktopts;
  };

 };
};

static inline __attribute__((no_instrument_function)) struct Model0_inet_request_sock *Model0_inet_rsk(const struct Model0_request_sock *Model0_sk)
{
 return (struct Model0_inet_request_sock *)Model0_sk;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_inet_request_mark(const struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 if (!Model0_sk->Model0_sk_mark && Model0_sock_net(Model0_sk)->Model0_ipv4.Model0_sysctl_tcp_fwmark_accept)
  return Model0_skb->Model0_mark;

 return Model0_sk->Model0_sk_mark;
}

static inline __attribute__((no_instrument_function)) int Model0_inet_request_bound_dev_if(const struct Model0_sock *Model0_sk,
         struct Model0_sk_buff *Model0_skb)
{







 return Model0_sk->Model0___sk_common.Model0_skc_bound_dev_if;
}

struct Model0_inet_cork {
 unsigned int Model0_flags;
 Model0___be32 Model0_addr;
 struct Model0_ip_options *Model0_opt;
 unsigned int Model0_fragsize;
 int Model0_length; /* Total length of all frames */
 struct Model0_dst_entry *Model0_dst;
 Model0_u8 Model0_tx_flags;
 __u8 Model0_ttl;
 Model0___s16 Model0_tos;
 char Model0_priority;
};

struct Model0_inet_cork_full {
 struct Model0_inet_cork Model0_base;
 struct Model0_flowi Model0_fl;
};

struct Model0_ip_mc_socklist;
struct Model0_ipv6_pinfo;
struct Model0_rtable;

/** struct inet_sock - representation of INET sockets
 *
 * @sk - ancestor class
 * @pinet6 - pointer to IPv6 control block
 * @inet_daddr - Foreign IPv4 addr
 * @inet_rcv_saddr - Bound local IPv4 addr
 * @inet_dport - Destination port
 * @inet_num - Local port
 * @inet_saddr - Sending source
 * @uc_ttl - Unicast TTL
 * @inet_sport - Source port
 * @inet_id - ID counter for DF pkts
 * @tos - TOS
 * @mc_ttl - Multicasting TTL
 * @is_icsk - is this an inet_connection_sock?
 * @uc_index - Unicast outgoing device index
 * @mc_index - Multicast device index
 * @mc_list - Group array
 * @cork - info to build ip hdr on each ip frag while socket is corked
 */
struct Model0_inet_sock {
 /* sk and pinet6 has to be the first two members of inet_sock */
 struct Model0_sock Model0_sk;

 struct Model0_ipv6_pinfo *Model0_pinet6;

 /* Socket demultiplex comparisons on incoming packets. */





 Model0___be32 Model0_inet_saddr;
 Model0___s16 Model0_uc_ttl;
 Model0___u16 Model0_cmsg_flags;
 Model0___be16 Model0_inet_sport;
 Model0___u16 Model0_inet_id;

 struct Model0_ip_options_rcu *Model0_inet_opt;
 int Model0_rx_dst_ifindex;
 __u8 Model0_tos;
 __u8 Model0_min_ttl;
 __u8 Model0_mc_ttl;
 __u8 Model0_pmtudisc;
 __u8 Model0_recverr:1,
    Model0_is_icsk:1,
    Model0_freebind:1,
    Model0_hdrincl:1,
    Model0_mc_loop:1,
    Model0_transparent:1,
    Model0_mc_all:1,
    Model0_nodefrag:1;
 __u8 Model0_bind_address_no_port:1;
 __u8 Model0_rcv_tos;
 __u8 Model0_convert_csum;
 int Model0_uc_index;
 int Model0_mc_index;
 Model0___be32 Model0_mc_addr;
 struct Model0_ip_mc_socklist *Model0_mc_list;
 struct Model0_inet_cork_full Model0_cork;
};




/* cmsg flags for inet */
/**
 * sk_to_full_sk - Access to a full socket
 * @sk: pointer to a socket
 *
 * SYNACK messages might be attached to request sockets.
 * Some places want to reach the listener in this case.
 */
static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_sk_to_full_sk(struct Model0_sock *Model0_sk)
{

 if (Model0_sk && Model0_sk->Model0___sk_common.Model0_skc_state == Model0_TCP_NEW_SYN_RECV)
  Model0_sk = Model0_inet_reqsk(Model0_sk)->Model0___req_common.Model0_skc_listener;

 return Model0_sk;
}

/* sk_to_full_sk() variant with a const argument */
static inline __attribute__((no_instrument_function)) const struct Model0_sock *Model0_sk_const_to_full_sk(const struct Model0_sock *Model0_sk)
{

 if (Model0_sk && Model0_sk->Model0___sk_common.Model0_skc_state == Model0_TCP_NEW_SYN_RECV)
  Model0_sk = ((const struct Model0_request_sock *)Model0_sk)->Model0___req_common.Model0_skc_listener;

 return Model0_sk;
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_skb_to_full_sk(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_sk_to_full_sk(Model0_skb->Model0_sk);
}

static inline __attribute__((no_instrument_function)) struct Model0_inet_sock *Model0_inet_sk(const struct Model0_sock *Model0_sk)
{
 return (struct Model0_inet_sock *)Model0_sk;
}

static inline __attribute__((no_instrument_function)) void Model0___inet_sk_copy_descendant(struct Model0_sock *Model0_sk_to,
          const struct Model0_sock *Model0_sk_from,
          const int Model0_ancestor_size)
{
 ({ Model0_size_t Model0___len = (Model0_sk_from->Model0___sk_common.Model0_skc_prot->Model0_obj_size - Model0_ancestor_size); void *Model0___ret; if (__builtin_constant_p(Model0_sk_from->Model0___sk_common.Model0_skc_prot->Model0_obj_size - Model0_ancestor_size) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_inet_sk(Model0_sk_to) + 1), (Model0_inet_sk(Model0_sk_from) + 1), Model0___len); else Model0___ret = __builtin_memcpy((Model0_inet_sk(Model0_sk_to) + 1), (Model0_inet_sk(Model0_sk_from) + 1), Model0___len); Model0___ret; });

}
int Model0_inet_sk_rebuild_header(struct Model0_sock *Model0_sk);

static inline __attribute__((no_instrument_function)) unsigned int Model0___inet_ehashfn(const Model0___be32 Model0_laddr,
       const Model0___u16 Model0_lport,
       const Model0___be32 Model0_faddr,
       const Model0___be16 Model0_fport,
       Model0_u32 Model0_initval)
{
 return Model0_jhash_3words(( __u32) Model0_laddr,
       ( __u32) Model0_faddr,
       ((__u32) Model0_lport) << 16 | ( __u32)Model0_fport,
       Model0_initval);
}

struct Model0_request_sock *Model0_inet_reqsk_alloc(const struct Model0_request_sock_ops *Model0_ops,
          struct Model0_sock *Model0_sk_listener,
          bool Model0_attach_listener);

static inline __attribute__((no_instrument_function)) __u8 Model0_inet_sk_flowi_flags(const struct Model0_sock *Model0_sk)
{
 __u8 Model0_flags = 0;

 if (Model0_inet_sk(Model0_sk)->Model0_transparent || Model0_inet_sk(Model0_sk)->Model0_hdrincl)
  Model0_flags |= 0x01;
 return Model0_flags;
}

static inline __attribute__((no_instrument_function)) void Model0_inet_inc_convert_csum(struct Model0_sock *Model0_sk)
{
 Model0_inet_sk(Model0_sk)->Model0_convert_csum++;
}

static inline __attribute__((no_instrument_function)) void Model0_inet_dec_convert_csum(struct Model0_sock *Model0_sk)
{
 if (Model0_inet_sk(Model0_sk)->Model0_convert_csum > 0)
  Model0_inet_sk(Model0_sk)->Model0_convert_csum--;
}

static inline __attribute__((no_instrument_function)) bool Model0_inet_get_convert_csum(struct Model0_sock *Model0_sk)
{
 return !!Model0_inet_sk(Model0_sk)->Model0_convert_csum;
}




/* Cancel timers, when they are not required. */


struct Model0_inet_bind_bucket;
struct Model0_tcp_congestion_ops;

/*
 * Pointers to address related TCP functions
 * (i.e. things that depend on the address family)
 */
struct Model0_inet_connection_sock_af_ops {
 int (*Model0_queue_xmit)(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, struct Model0_flowi *Model0_fl);
 void (*Model0_send_check)(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
 int (*Model0_rebuild_header)(struct Model0_sock *Model0_sk);
 void (*Model0_sk_rx_dst_set)(struct Model0_sock *Model0_sk, const struct Model0_sk_buff *Model0_skb);
 int (*Model0_conn_request)(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
 struct Model0_sock *(*Model0_syn_recv_sock)(const struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
          struct Model0_request_sock *Model0_req,
          struct Model0_dst_entry *Model0_dst,
          struct Model0_request_sock *Model0_req_unhash,
          bool *Model0_own_req);
 Model0_u16 Model0_net_header_len;
 Model0_u16 Model0_net_frag_header_len;
 Model0_u16 Model0_sockaddr_len;
 int (*Model0_setsockopt)(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
      char *Model0_optval, unsigned int Model0_optlen);
 int (*Model0_getsockopt)(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
      char *Model0_optval, int *Model0_optlen);

 int (*Model0_compat_setsockopt)(struct Model0_sock *Model0_sk,
    int Model0_level, int Model0_optname,
    char *Model0_optval, unsigned int Model0_optlen);
 int (*Model0_compat_getsockopt)(struct Model0_sock *Model0_sk,
    int Model0_level, int Model0_optname,
    char *Model0_optval, int *Model0_optlen);

 void (*Model0_addr2sockaddr)(struct Model0_sock *Model0_sk, struct Model0_sockaddr *);
 int (*Model0_bind_conflict)(const struct Model0_sock *Model0_sk,
         const struct Model0_inet_bind_bucket *Model0_tb, bool Model0_relax);
 void (*Model0_mtu_reduced)(struct Model0_sock *Model0_sk);
};

/** inet_connection_sock - INET connection oriented sock
 *
 * @icsk_accept_queue:	   FIFO of established children 
 * @icsk_bind_hash:	   Bind node
 * @icsk_timeout:	   Timeout
 * @icsk_retransmit_timer: Resend (no ack)
 * @icsk_rto:		   Retransmit timeout
 * @icsk_pmtu_cookie	   Last pmtu seen by socket
 * @icsk_ca_ops		   Pluggable congestion control hook
 * @icsk_af_ops		   Operations which are AF_INET{4,6} specific
 * @icsk_ca_state:	   Congestion control state
 * @icsk_retransmits:	   Number of unrecovered [RTO] timeouts
 * @icsk_pending:	   Scheduled timer event
 * @icsk_backoff:	   Backoff
 * @icsk_syn_retries:      Number of allowed SYN (or equivalent) retries
 * @icsk_probes_out:	   unanswered 0 window probes
 * @icsk_ext_hdr_len:	   Network protocol overhead (IP/IPv6 options)
 * @icsk_ack:		   Delayed ACK control data
 * @icsk_mtup;		   MTU probing control data
 */
struct Model0_inet_connection_sock {
 /* inet_sock has to be the first member! */
 struct Model0_inet_sock Model0_icsk_inet;
 struct Model0_request_sock_queue Model0_icsk_accept_queue;
 struct Model0_inet_bind_bucket *Model0_icsk_bind_hash;
 unsigned long Model0_icsk_timeout;
  struct Model0_timer_list Model0_icsk_retransmit_timer;
  struct Model0_timer_list Model0_icsk_delack_timer;
 __u32 Model0_icsk_rto;
 __u32 Model0_icsk_pmtu_cookie;
 const struct Model0_tcp_congestion_ops *Model0_icsk_ca_ops;
 const struct Model0_inet_connection_sock_af_ops *Model0_icsk_af_ops;
 unsigned int (*Model0_icsk_sync_mss)(struct Model0_sock *Model0_sk, Model0_u32 Model0_pmtu);
 __u8 Model0_icsk_ca_state:6,
      Model0_icsk_ca_setsockopt:1,
      Model0_icsk_ca_dst_locked:1;
 __u8 Model0_icsk_retransmits;
 __u8 Model0_icsk_pending;
 __u8 Model0_icsk_backoff;
 __u8 Model0_icsk_syn_retries;
 __u8 Model0_icsk_probes_out;
 Model0___u16 Model0_icsk_ext_hdr_len;
 struct {
  __u8 Model0_pending; /* ACK is pending			   */
  __u8 Model0_quick; /* Scheduled number of quick acks	   */
  __u8 Model0_pingpong; /* The session is interactive		   */
  __u8 Model0_blocked; /* Delayed ACK was blocked by socket lock */
  __u32 Model0_ato; /* Predicted tick of soft clock	   */
  unsigned long Model0_timeout; /* Currently scheduled timeout		   */
  __u32 Model0_lrcvtime; /* timestamp of last received data packet */
  Model0___u16 Model0_last_seg_size; /* Size of last incoming segment	   */
  Model0___u16 Model0_rcv_mss; /* MSS used for delayed ACK decisions	   */
 } Model0_icsk_ack;
 struct {
  int Model0_enabled;

  /* Range of MTUs to search */
  int Model0_search_high;
  int Model0_search_low;

  /* Information on the current probe. */
  int Model0_probe_size;

  Model0_u32 Model0_probe_timestamp;
 } Model0_icsk_mtup;
 Model0_u32 Model0_icsk_user_timeout;

 Model0_u64 Model0_icsk_ca_priv[64 / sizeof(Model0_u64)];

};







static inline __attribute__((no_instrument_function)) struct Model0_inet_connection_sock *Model0_inet_csk(const struct Model0_sock *Model0_sk)
{
 return (struct Model0_inet_connection_sock *)Model0_sk;
}

static inline __attribute__((no_instrument_function)) void *Model0_inet_csk_ca(const struct Model0_sock *Model0_sk)
{
 return (void *)Model0_inet_csk(Model0_sk)->Model0_icsk_ca_priv;
}

struct Model0_sock *Model0_inet_csk_clone_lock(const struct Model0_sock *Model0_sk,
     const struct Model0_request_sock *Model0_req,
     const Model0_gfp_t Model0_priority);

enum Model0_inet_csk_ack_state_t {
 Model0_ICSK_ACK_SCHED = 1,
 Model0_ICSK_ACK_TIMER = 2,
 Model0_ICSK_ACK_PUSHED = 4,
 Model0_ICSK_ACK_PUSHED2 = 8
};

void Model0_inet_csk_init_xmit_timers(struct Model0_sock *Model0_sk,
          void (*Model0_retransmit_handler)(unsigned long),
          void (*Model0_delack_handler)(unsigned long),
          void (*Model0_keepalive_handler)(unsigned long));
void Model0_inet_csk_clear_xmit_timers(struct Model0_sock *Model0_sk);

static inline __attribute__((no_instrument_function)) void Model0_inet_csk_schedule_ack(struct Model0_sock *Model0_sk)
{
 Model0_inet_csk(Model0_sk)->Model0_icsk_ack.Model0_pending |= Model0_ICSK_ACK_SCHED;
}

static inline __attribute__((no_instrument_function)) int Model0_inet_csk_ack_scheduled(const struct Model0_sock *Model0_sk)
{
 return Model0_inet_csk(Model0_sk)->Model0_icsk_ack.Model0_pending & Model0_ICSK_ACK_SCHED;
}

static inline __attribute__((no_instrument_function)) void Model0_inet_csk_delack_init(struct Model0_sock *Model0_sk)
{
 memset(&Model0_inet_csk(Model0_sk)->Model0_icsk_ack, 0, sizeof(Model0_inet_csk(Model0_sk)->Model0_icsk_ack));
}

void Model0_inet_csk_delete_keepalive_timer(struct Model0_sock *Model0_sk);
void Model0_inet_csk_reset_keepalive_timer(struct Model0_sock *Model0_sk, unsigned long Model0_timeout);


extern const char Model0_inet_csk_timer_bug_msg[];


static inline __attribute__((no_instrument_function)) void Model0_inet_csk_clear_xmit_timer(struct Model0_sock *Model0_sk, const int Model0_what)
{
 struct Model0_inet_connection_sock *Model0_icsk = Model0_inet_csk(Model0_sk);

 if (Model0_what == 1 || Model0_what == 3) {
  Model0_icsk->Model0_icsk_pending = 0;



 } else if (Model0_what == 2) {
  Model0_icsk->Model0_icsk_ack.Model0_blocked = Model0_icsk->Model0_icsk_ack.Model0_pending = 0;



 }

 else {
  ({ do { if (0) Model0_printk("\001" "7" "TCP: " "%s", Model0_inet_csk_timer_bug_msg); } while (0); 0; });
 }

}

/*
 *	Reset the retransmission timer
 */
static inline __attribute__((no_instrument_function)) void Model0_inet_csk_reset_xmit_timer(struct Model0_sock *Model0_sk, const int Model0_what,
          unsigned long Model0_when,
          const unsigned long Model0_max_when)
{
 struct Model0_inet_connection_sock *Model0_icsk = Model0_inet_csk(Model0_sk);

 if (Model0_when > Model0_max_when) {

  ({ do { if (0) Model0_printk("\001" "7" "TCP: " "reset_xmit_timer: sk=%p %d when=0x%lx, caller=%p\n", Model0_sk, Model0_what, Model0_when, Model0_current_text_addr()); } while (0); 0; });


  Model0_when = Model0_max_when;
 }

 if (Model0_what == 1 || Model0_what == 3 ||
     Model0_what == 4 || Model0_what == 5) {
  Model0_icsk->Model0_icsk_pending = Model0_what;
  Model0_icsk->Model0_icsk_timeout = Model0_jiffies + Model0_when;
  Model0_sk_reset_timer(Model0_sk, &Model0_icsk->Model0_icsk_retransmit_timer, Model0_icsk->Model0_icsk_timeout);
 } else if (Model0_what == 2) {
  Model0_icsk->Model0_icsk_ack.Model0_pending |= Model0_ICSK_ACK_TIMER;
  Model0_icsk->Model0_icsk_ack.Model0_timeout = Model0_jiffies + Model0_when;
  Model0_sk_reset_timer(Model0_sk, &Model0_icsk->Model0_icsk_delack_timer, Model0_icsk->Model0_icsk_ack.Model0_timeout);
 }

 else {
  ({ do { if (0) Model0_printk("\001" "7" "TCP: " "%s", Model0_inet_csk_timer_bug_msg); } while (0); 0; });
 }

}

static inline __attribute__((no_instrument_function)) unsigned long
Model0_inet_csk_rto_backoff(const struct Model0_inet_connection_sock *Model0_icsk,
       unsigned long Model0_max_when)
{
        Model0_u64 Model0_when = (Model0_u64)Model0_icsk->Model0_icsk_rto << Model0_icsk->Model0_icsk_backoff;

        return (unsigned long)({ Model0_u64 Model0___min1 = (Model0_when); Model0_u64 Model0___min2 = (Model0_max_when); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; });
}

struct Model0_sock *Model0_inet_csk_accept(struct Model0_sock *Model0_sk, int Model0_flags, int *err);

int Model0_inet_csk_bind_conflict(const struct Model0_sock *Model0_sk,
      const struct Model0_inet_bind_bucket *Model0_tb, bool Model0_relax);
int Model0_inet_csk_get_port(struct Model0_sock *Model0_sk, unsigned short Model0_snum);

struct Model0_dst_entry *Model0_inet_csk_route_req(const struct Model0_sock *Model0_sk, struct Model0_flowi4 *Model0_fl4,
         const struct Model0_request_sock *Model0_req);
struct Model0_dst_entry *Model0_inet_csk_route_child_sock(const struct Model0_sock *Model0_sk,
         struct Model0_sock *Model0_newsk,
         const struct Model0_request_sock *Model0_req);

struct Model0_sock *Model0_inet_csk_reqsk_queue_add(struct Model0_sock *Model0_sk,
          struct Model0_request_sock *Model0_req,
          struct Model0_sock *Model0_child);
void Model0_inet_csk_reqsk_queue_hash_add(struct Model0_sock *Model0_sk, struct Model0_request_sock *Model0_req,
       unsigned long Model0_timeout);
struct Model0_sock *Model0_inet_csk_complete_hashdance(struct Model0_sock *Model0_sk, struct Model0_sock *Model0_child,
      struct Model0_request_sock *Model0_req,
      bool Model0_own_req);

static inline __attribute__((no_instrument_function)) void Model0_inet_csk_reqsk_queue_added(struct Model0_sock *Model0_sk)
{
 Model0_reqsk_queue_added(&Model0_inet_csk(Model0_sk)->Model0_icsk_accept_queue);
}

static inline __attribute__((no_instrument_function)) int Model0_inet_csk_reqsk_queue_len(const struct Model0_sock *Model0_sk)
{
 return Model0_reqsk_queue_len(&Model0_inet_csk(Model0_sk)->Model0_icsk_accept_queue);
}

static inline __attribute__((no_instrument_function)) int Model0_inet_csk_reqsk_queue_young(const struct Model0_sock *Model0_sk)
{
 return Model0_reqsk_queue_len_young(&Model0_inet_csk(Model0_sk)->Model0_icsk_accept_queue);
}

static inline __attribute__((no_instrument_function)) int Model0_inet_csk_reqsk_queue_is_full(const struct Model0_sock *Model0_sk)
{
 return Model0_inet_csk_reqsk_queue_len(Model0_sk) >= Model0_sk->Model0_sk_max_ack_backlog;
}

void Model0_inet_csk_reqsk_queue_drop(struct Model0_sock *Model0_sk, struct Model0_request_sock *Model0_req);
void Model0_inet_csk_reqsk_queue_drop_and_put(struct Model0_sock *Model0_sk, struct Model0_request_sock *Model0_req);

void Model0_inet_csk_destroy_sock(struct Model0_sock *Model0_sk);
void Model0_inet_csk_prepare_forced_close(struct Model0_sock *Model0_sk);

/*
 * LISTEN is a special case for poll..
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_inet_csk_listen_poll(const struct Model0_sock *Model0_sk)
{
 return !Model0_reqsk_queue_empty(&Model0_inet_csk(Model0_sk)->Model0_icsk_accept_queue) ?
   (0x0001 | 0x0040) : 0;
}

int Model0_inet_csk_listen_start(struct Model0_sock *Model0_sk, int Model0_backlog);
void Model0_inet_csk_listen_stop(struct Model0_sock *Model0_sk);

void Model0_inet_csk_addr2sockaddr(struct Model0_sock *Model0_sk, struct Model0_sockaddr *Model0_uaddr);

int Model0_inet_csk_compat_getsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
          char *Model0_optval, int *Model0_optlen);
int Model0_inet_csk_compat_setsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
          char *Model0_optval, unsigned int Model0_optlen);

struct Model0_dst_entry *Model0_inet_csk_update_pmtu(struct Model0_sock *Model0_sk, Model0_u32 Model0_mtu);
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for a generic INET TIMEWAIT sock
 *
 *		From code originally in net/tcp.h
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
/*
 * NET		Generic infrastructure for Network protocols.
 *
 * Authors:	Arnaldo Carvalho de Melo <acme@conectiva.com.br>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







struct Model0_timewait_sock_ops {
 struct Model0_kmem_cache *Model0_twsk_slab;
 char *Model0_twsk_slab_name;
 unsigned int Model0_twsk_obj_size;
 int (*Model0_twsk_unique)(struct Model0_sock *Model0_sk,
           struct Model0_sock *Model0_sktw, void *Model0_twp);
 void (*Model0_twsk_destructor)(struct Model0_sock *Model0_sk);
};

static inline __attribute__((no_instrument_function)) int Model0_twsk_unique(struct Model0_sock *Model0_sk, struct Model0_sock *Model0_sktw, void *Model0_twp)
{
 if (Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_twsk_prot->Model0_twsk_unique != ((void *)0))
  return Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_twsk_prot->Model0_twsk_unique(Model0_sk, Model0_sktw, Model0_twp);
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_twsk_destructor(struct Model0_sock *Model0_sk)
{
 if (Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_twsk_prot->Model0_twsk_destructor != ((void *)0))
  Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_twsk_prot->Model0_twsk_destructor(Model0_sk);
}



struct Model0_inet_hashinfo;

struct Model0_inet_timewait_death_row {
 Model0_atomic_t Model0_tw_count;

 struct Model0_inet_hashinfo *Model0_hashinfo __attribute__((__aligned__((1 << (6)))));
 int Model0_sysctl_tw_recycle;
 int Model0_sysctl_max_tw_buckets;
};

struct Model0_inet_bind_bucket;

/*
 * This is a TIME_WAIT sock. It works around the memory consumption
 * problems of sockets in such a state on heavily loaded servers, but
 * without violating the protocol specification.
 */
struct Model0_inet_timewait_sock {
 /*
	 * Now struct sock also uses sock_common, so please just
	 * don't add nothing before this first member (__tw_common) --acme
	 */
 struct Model0_sock_common Model0___tw_common;
 int Model0_tw_timeout;
 volatile unsigned char Model0_tw_substate;
 unsigned char Model0_tw_rcv_wscale;

 /* Socket demultiplex comparisons on incoming packets. */
 /* these three are in inet_sock */
 Model0___be16 Model0_tw_sport;
                                ;
 /* And these are ours. */
 unsigned int Model0_tw_kill : 1,
    Model0_tw_transparent : 1,
    Model0_tw_flowlabel : 20,
    Model0_tw_pad : 2, /* 2 bits hole */
    Model0_tw_tos : 8;
                              ;
 struct Model0_timer_list Model0_tw_timer;
 struct Model0_inet_bind_bucket *Model0_tw_tb;
};


static inline __attribute__((no_instrument_function)) struct Model0_inet_timewait_sock *Model0_inet_twsk(const struct Model0_sock *Model0_sk)
{
 return (struct Model0_inet_timewait_sock *)Model0_sk;
}

void Model0_inet_twsk_free(struct Model0_inet_timewait_sock *Model0_tw);
void Model0_inet_twsk_put(struct Model0_inet_timewait_sock *Model0_tw);

void Model0_inet_twsk_bind_unhash(struct Model0_inet_timewait_sock *Model0_tw,
      struct Model0_inet_hashinfo *Model0_hashinfo);

struct Model0_inet_timewait_sock *Model0_inet_twsk_alloc(const struct Model0_sock *Model0_sk,
        struct Model0_inet_timewait_death_row *Model0_dr,
        const int Model0_state);

void Model0___inet_twsk_hashdance(struct Model0_inet_timewait_sock *Model0_tw, struct Model0_sock *Model0_sk,
      struct Model0_inet_hashinfo *Model0_hashinfo);

void Model0___inet_twsk_schedule(struct Model0_inet_timewait_sock *Model0_tw, int Model0_timeo,
     bool Model0_rearm);

static inline __attribute__((no_instrument_function)) void Model0_inet_twsk_schedule(struct Model0_inet_timewait_sock *Model0_tw, int Model0_timeo)
{
 Model0___inet_twsk_schedule(Model0_tw, Model0_timeo, false);
}

static inline __attribute__((no_instrument_function)) void Model0_inet_twsk_reschedule(struct Model0_inet_timewait_sock *Model0_tw, int Model0_timeo)
{
 Model0___inet_twsk_schedule(Model0_tw, Model0_timeo, true);
}

void Model0_inet_twsk_deschedule_put(struct Model0_inet_timewait_sock *Model0_tw);

void Model0_inet_twsk_purge(struct Model0_inet_hashinfo *Model0_hashinfo,
       struct Model0_inet_timewait_death_row *Model0_twdr, int Model0_family);

static inline __attribute__((no_instrument_function))
struct Model0_net *Model0_twsk_net(const struct Model0_inet_timewait_sock *Model0_twsk)
{
 return Model0_read_pnet(&Model0_twsk->Model0___tw_common.Model0_skc_net);
}

static inline __attribute__((no_instrument_function))
void Model0_twsk_net_set(struct Model0_inet_timewait_sock *Model0_twsk, struct Model0_net *Model0_net)
{
 Model0_write_pnet(&Model0_twsk->Model0___tw_common.Model0_skc_net, Model0_net);
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the TCP protocol.
 *
 * Version:	@(#)tcp.h	1.0.2	04/28/93
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







struct Model0_tcphdr {
 Model0___be16 Model0_source;
 Model0___be16 Model0_dest;
 Model0___be32 Model0_seq;
 Model0___be32 Model0_ack_seq;

 Model0___u16 Model0_res1:4,
  Model0_doff:4,
  Model0_fin:1,
  Model0_syn:1,
  Model0_rst:1,
  Model0_psh:1,
  Model0_ack:1,
  Model0_urg:1,
  Model0_ece:1,
  Model0_cwr:1;
 Model0___be16 Model0_window;
 Model0___sum16 Model0_check;
 Model0___be16 Model0_urg_ptr;
};

/*
 *	The union cast uses a gcc extension to avoid aliasing problems
 *  (union is compatible to any of its members)
 *  This means this part of the code is -fstrict-aliasing safe now.
 */
union Model0_tcp_word_hdr {
 struct Model0_tcphdr Model0_hdr;
 Model0___be32 Model0_words[5];
};



enum {
 Model0_TCP_FLAG_CWR = (( Model0___be32)((__u32)( (((__u32)((0x00800000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00800000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00800000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00800000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_FLAG_ECE = (( Model0___be32)((__u32)( (((__u32)((0x00400000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00400000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00400000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00400000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_FLAG_URG = (( Model0___be32)((__u32)( (((__u32)((0x00200000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00200000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00200000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00200000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_FLAG_ACK = (( Model0___be32)((__u32)( (((__u32)((0x00100000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00100000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00100000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00100000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_FLAG_PSH = (( Model0___be32)((__u32)( (((__u32)((0x00080000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00080000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00080000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00080000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_FLAG_RST = (( Model0___be32)((__u32)( (((__u32)((0x00040000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00040000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00040000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00040000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_FLAG_SYN = (( Model0___be32)((__u32)( (((__u32)((0x00020000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00020000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00020000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00020000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_FLAG_FIN = (( Model0___be32)((__u32)( (((__u32)((0x00010000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00010000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00010000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00010000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_RESERVED_BITS = (( Model0___be32)((__u32)( (((__u32)((0x0F000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0F000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0F000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0F000000)) & (__u32)0xff000000UL) >> 24)))),
 Model0_TCP_DATA_OFFSET = (( Model0___be32)((__u32)( (((__u32)((0xF0000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xF0000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xF0000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xF0000000)) & (__u32)0xff000000UL) >> 24))))
};

/*
 * TCP general constants
 */



/* TCP socket options */
struct Model0_tcp_repair_opt {
 __u32 Model0_opt_code;
 __u32 Model0_opt_val;
};

struct Model0_tcp_repair_window {
 __u32 Model0_snd_wl1;
 __u32 Model0_snd_wnd;
 __u32 Model0_max_window;

 __u32 Model0_rcv_wnd;
 __u32 Model0_rcv_wup;
};

enum {
 Model0_TCP_NO_QUEUE,
 Model0_TCP_RECV_QUEUE,
 Model0_TCP_SEND_QUEUE,
 Model0_TCP_QUEUES_NR,
};

/* for TCP_INFO socket option */







enum Model0_tcp_ca_state {
 Model0_TCP_CA_Open = 0,

 Model0_TCP_CA_Disorder = 1,

 Model0_TCP_CA_CWR = 2,

 Model0_TCP_CA_Recovery = 3,

 Model0_TCP_CA_Loss = 4

};

struct Model0_tcp_info {
 __u8 Model0_tcpi_state;
 __u8 Model0_tcpi_ca_state;
 __u8 Model0_tcpi_retransmits;
 __u8 Model0_tcpi_probes;
 __u8 Model0_tcpi_backoff;
 __u8 Model0_tcpi_options;
 __u8 Model0_tcpi_snd_wscale : 4, Model0_tcpi_rcv_wscale : 4;

 __u32 Model0_tcpi_rto;
 __u32 Model0_tcpi_ato;
 __u32 Model0_tcpi_snd_mss;
 __u32 Model0_tcpi_rcv_mss;

 __u32 Model0_tcpi_unacked;
 __u32 Model0_tcpi_sacked;
 __u32 Model0_tcpi_lost;
 __u32 Model0_tcpi_retrans;
 __u32 Model0_tcpi_fackets;

 /* Times. */
 __u32 Model0_tcpi_last_data_sent;
 __u32 Model0_tcpi_last_ack_sent; /* Not remembered, sorry. */
 __u32 Model0_tcpi_last_data_recv;
 __u32 Model0_tcpi_last_ack_recv;

 /* Metrics. */
 __u32 Model0_tcpi_pmtu;
 __u32 Model0_tcpi_rcv_ssthresh;
 __u32 Model0_tcpi_rtt;
 __u32 Model0_tcpi_rttvar;
 __u32 Model0_tcpi_snd_ssthresh;
 __u32 Model0_tcpi_snd_cwnd;
 __u32 Model0_tcpi_advmss;
 __u32 Model0_tcpi_reordering;

 __u32 Model0_tcpi_rcv_rtt;
 __u32 Model0_tcpi_rcv_space;

 __u32 Model0_tcpi_total_retrans;

 __u64 Model0_tcpi_pacing_rate;
 __u64 Model0_tcpi_max_pacing_rate;
 __u64 Model0_tcpi_bytes_acked; /* RFC4898 tcpEStatsAppHCThruOctetsAcked */
 __u64 Model0_tcpi_bytes_received; /* RFC4898 tcpEStatsAppHCThruOctetsReceived */
 __u32 Model0_tcpi_segs_out; /* RFC4898 tcpEStatsPerfSegsOut */
 __u32 Model0_tcpi_segs_in; /* RFC4898 tcpEStatsPerfSegsIn */

 __u32 Model0_tcpi_notsent_bytes;
 __u32 Model0_tcpi_min_rtt;
 __u32 Model0_tcpi_data_segs_in; /* RFC4898 tcpEStatsDataSegsIn */
 __u32 Model0_tcpi_data_segs_out; /* RFC4898 tcpEStatsDataSegsOut */
};

/* for TCP_MD5SIG socket option */


struct Model0_tcp_md5sig {
 struct Model0___kernel_sockaddr_storage Model0_tcpm_addr; /* address associated */
 Model0___u16 Model0___tcpm_pad1; /* zero */
 Model0___u16 Model0_tcpm_keylen; /* key length */
 __u32 Model0___tcpm_pad2; /* zero */
 __u8 Model0_tcpm_key[80]; /* key (binary) */
};

static inline __attribute__((no_instrument_function)) struct Model0_tcphdr *Model0_tcp_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_tcphdr *)Model0_skb_transport_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) unsigned int Model0___tcp_hdrlen(const struct Model0_tcphdr *Model0_th)
{
 return Model0_th->Model0_doff * 4;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_tcp_hdrlen(const struct Model0_sk_buff *Model0_skb)
{
 return Model0___tcp_hdrlen(Model0_tcp_hdr(Model0_skb));
}

static inline __attribute__((no_instrument_function)) struct Model0_tcphdr *Model0_inner_tcp_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_tcphdr *)Model0_skb_inner_transport_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_inner_tcp_hdrlen(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_inner_tcp_hdr(Model0_skb)->Model0_doff * 4;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_tcp_optlen(const struct Model0_sk_buff *Model0_skb)
{
 return (Model0_tcp_hdr(Model0_skb)->Model0_doff - 5) * 4;
}

/* TCP Fast Open */




/* TCP Fast Open Cookie as stored in memory */
struct Model0_tcp_fastopen_cookie {
 Model0_s8 Model0_len;
 Model0_u8 Model0_val[16];
 bool Model0_exp; /* In RFC6994 experimental option format */
};

/* This defines a selective acknowledgement block. */
struct Model0_tcp_sack_block_wire {
 Model0___be32 Model0_start_seq;
 Model0___be32 Model0_end_seq;
};

struct Model0_tcp_sack_block {
 Model0_u32 Model0_start_seq;
 Model0_u32 Model0_end_seq;
};

/*These are used to set the sack_ok field in struct tcp_options_received */




struct Model0_tcp_options_received {
/*	PAWS/RTTM data	*/
 long Model0_ts_recent_stamp;/* Time we stored ts_recent (for aging) */
 Model0_u32 Model0_ts_recent; /* Time stamp to echo next		*/
 Model0_u32 Model0_rcv_tsval; /* Time stamp value             	*/
 Model0_u32 Model0_rcv_tsecr; /* Time stamp echo reply        	*/
 Model0_u16 Model0_saw_tstamp : 1, /* Saw TIMESTAMP on last packet		*/
  Model0_tstamp_ok : 1, /* TIMESTAMP seen on SYN packet		*/
  Model0_dsack : 1, /* D-SACK is scheduled			*/
  Model0_wscale_ok : 1, /* Wscale seen on SYN packet		*/
  Model0_sack_ok : 4, /* SACK seen on SYN packet		*/
  Model0_snd_wscale : 4, /* Window scaling received from sender	*/
  Model0_rcv_wscale : 4; /* Window scaling to send to receiver	*/
 Model0_u8 Model0_num_sacks; /* Number of SACK blocks		*/
 Model0_u16 Model0_user_mss; /* mss requested by user in ioctl	*/
 Model0_u16 Model0_mss_clamp; /* Maximal mss, negotiated at connection setup */
};

static inline __attribute__((no_instrument_function)) void Model0_tcp_clear_options(struct Model0_tcp_options_received *Model0_rx_opt)
{
 Model0_rx_opt->Model0_tstamp_ok = Model0_rx_opt->Model0_sack_ok = 0;
 Model0_rx_opt->Model0_wscale_ok = Model0_rx_opt->Model0_snd_wscale = 0;
}

/* This is the max number of SACKS that we'll generate and process. It's safe
 * to increase this, although since:
 *   size = TCPOLEN_SACK_BASE_ALIGNED (4) + n * TCPOLEN_SACK_PERBLOCK (8)
 * only four options will fit in a standard TCP header */


struct Model0_tcp_request_sock_ops;

struct Model0_tcp_request_sock {
 struct Model0_inet_request_sock Model0_req;
 const struct Model0_tcp_request_sock_ops *Model0_af_specific;
 struct Model0_skb_mstamp Model0_snt_synack; /* first SYNACK sent time */
 bool Model0_tfo_listener;
 Model0_u32 Model0_txhash;
 Model0_u32 Model0_rcv_isn;
 Model0_u32 Model0_snt_isn;
 Model0_u32 Model0_last_oow_ack_time; /* last SYNACK */
 Model0_u32 Model0_rcv_nxt; /* the ack # by SYNACK. For
						  * FastOpen it's the seq#
						  * after data-in-SYN.
						  */
};

static inline __attribute__((no_instrument_function)) struct Model0_tcp_request_sock *Model0_tcp_rsk(const struct Model0_request_sock *Model0_req)
{
 return (struct Model0_tcp_request_sock *)Model0_req;
}

struct Model0_tcp_sock {
 /* inet_connection_sock has to be the first member of tcp_sock */
 struct Model0_inet_connection_sock Model0_inet_conn;
 Model0_u16 Model0_tcp_header_len; /* Bytes of tcp header to send		*/
 Model0_u16 Model0_gso_segs; /* Max number of segs per GSO packet	*/

/*
 *	Header prediction flags
 *	0x5?10 << 16 + snd_wnd in net byte order
 */
 Model0___be32 Model0_pred_flags;

/*
 *	RFC793 variables by their proper names. This means you can
 *	read the code and the spec side by side (and laugh ...)
 *	See RFC793 and RFC1122. The RFC writes these in capitals.
 */
 Model0_u64 Model0_bytes_received; /* RFC4898 tcpEStatsAppHCThruOctetsReceived
				 * sum(delta(rcv_nxt)), or how many bytes
				 * were acked.
				 */
 Model0_u32 Model0_segs_in; /* RFC4898 tcpEStatsPerfSegsIn
				 * total number of segments in.
				 */
 Model0_u32 Model0_data_segs_in; /* RFC4898 tcpEStatsPerfDataSegsIn
				 * total number of data segments in.
				 */
  Model0_u32 Model0_rcv_nxt; /* What we want to receive next 	*/
 Model0_u32 Model0_copied_seq; /* Head of yet unread data		*/
 Model0_u32 Model0_rcv_wup; /* rcv_nxt on last window update sent	*/
  Model0_u32 Model0_snd_nxt; /* Next sequence we send		*/
 Model0_u32 Model0_segs_out; /* RFC4898 tcpEStatsPerfSegsOut
				 * The total number of segments sent.
				 */
 Model0_u32 Model0_data_segs_out; /* RFC4898 tcpEStatsPerfDataSegsOut
				 * total number of data segments sent.
				 */
 Model0_u64 Model0_bytes_acked; /* RFC4898 tcpEStatsAppHCThruOctetsAcked
				 * sum(delta(snd_una)), or how many bytes
				 * were acked.
				 */
 struct Model0_u64_stats_sync Model0_syncp; /* protects 64bit vars (cf tcp_get_info()) */

  Model0_u32 Model0_snd_una; /* First byte we want an ack for	*/
  Model0_u32 Model0_snd_sml; /* Last byte of the most recently transmitted small packet */
 Model0_u32 Model0_rcv_tstamp; /* timestamp of last received ACK (for keepalives) */
 Model0_u32 Model0_lsndtime; /* timestamp of last sent data packet (for restart window) */
 Model0_u32 Model0_last_oow_ack_time; /* timestamp of last out-of-window ACK */

 Model0_u32 Model0_tsoffset; /* timestamp offset */

 struct Model0_list_head Model0_tsq_node; /* anchor in tsq_tasklet.head list */
 unsigned long Model0_tsq_flags;

 /* Data for direct copy to user */
 struct {
  struct Model0_sk_buff_head Model0_prequeue;
  struct Model0_task_struct *Model0_task;
  struct Model0_msghdr *Model0_msg;
  int Model0_memory;
  int Model0_len;
 } Model0_ucopy;

 Model0_u32 Model0_snd_wl1; /* Sequence for window update		*/
 Model0_u32 Model0_snd_wnd; /* The window we expect to receive	*/
 Model0_u32 Model0_max_window; /* Maximal window ever seen from peer	*/
 Model0_u32 Model0_mss_cache; /* Cached effective mss, not including SACKS */

 Model0_u32 Model0_window_clamp; /* Maximal window to advertise		*/
 Model0_u32 Model0_rcv_ssthresh; /* Current window clamp			*/

 /* Information of the most recently (s)acked skb */
 struct Model0_tcp_rack {
  struct Model0_skb_mstamp Model0_mstamp; /* (Re)sent time of the skb */
  Model0_u8 Model0_advanced; /* mstamp advanced since last lost marking */
  Model0_u8 Model0_reord; /* reordering detected */
 } Model0_rack;
 Model0_u16 Model0_advmss; /* Advertised MSS			*/
 Model0_u8 unused;
 Model0_u8 Model0_nonagle : 4,/* Disable Nagle algorithm?             */
  Model0_thin_lto : 1,/* Use linear timeouts for thin streams */
  Model0_thin_dupack : 1,/* Fast retransmit on first dupack      */
  Model0_repair : 1,
  Model0_frto : 1;/* F-RTO (RFC5682) activated in CA_Loss */
 Model0_u8 Model0_repair_queue;
 Model0_u8 Model0_do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
  Model0_syn_data:1, /* SYN includes data */
  Model0_syn_fastopen:1, /* SYN includes Fast Open option */
  Model0_syn_fastopen_exp:1,/* SYN includes Fast Open exp. option */
  Model0_syn_data_acked:1,/* data in SYN is acked by SYN-ACK */
  Model0_save_syn:1, /* Save headers of SYN packet */
  Model0_is_cwnd_limited:1;/* forward progress limited by snd_cwnd? */
 Model0_u32 Model0_tlp_high_seq; /* snd_nxt at the time of TLP retransmit. */

/* RTT measurement */
 Model0_u32 Model0_srtt_us; /* smoothed round trip time << 3 in usecs */
 Model0_u32 Model0_mdev_us; /* medium deviation			*/
 Model0_u32 Model0_mdev_max_us; /* maximal mdev for the last rtt period	*/
 Model0_u32 Model0_rttvar_us; /* smoothed mdev_max			*/
 Model0_u32 Model0_rtt_seq; /* sequence number to update rttvar	*/
 struct Model0_rtt_meas {
  Model0_u32 Model0_rtt, Model0_ts; /* RTT in usec and sampling time in jiffies. */
 } Model0_rtt_min[3];

 Model0_u32 Model0_packets_out; /* Packets which are "in flight"	*/
 Model0_u32 Model0_retrans_out; /* Retransmitted packets out		*/
 Model0_u32 Model0_max_packets_out; /* max packets_out in last window */
 Model0_u32 Model0_max_packets_seq; /* right edge of max_packets_out flight */

 Model0_u16 Model0_urg_data; /* Saved octet of OOB data and control flags */
 Model0_u8 Model0_ecn_flags; /* ECN status bits.			*/
 Model0_u8 Model0_keepalive_probes; /* num of allowed keep alive probes	*/
 Model0_u32 Model0_reordering; /* Packet reordering metric.		*/
 Model0_u32 Model0_snd_up; /* Urgent pointer		*/

/*
 *      Options received (usually on last packet, some only on SYN packets).
 */
 struct Model0_tcp_options_received Model0_rx_opt;

/*
 *	Slow start and congestion control (see also Nagle, and Karn & Partridge)
 */
  Model0_u32 Model0_snd_ssthresh; /* Slow start size threshold		*/
  Model0_u32 Model0_snd_cwnd; /* Sending congestion window		*/
 Model0_u32 Model0_snd_cwnd_cnt; /* Linear increase counter		*/
 Model0_u32 Model0_snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
 Model0_u32 Model0_snd_cwnd_used;
 Model0_u32 Model0_snd_cwnd_stamp;
 Model0_u32 Model0_prior_cwnd; /* Congestion window at start of Recovery. */
 Model0_u32 Model0_prr_delivered; /* Number of newly delivered packets to
				 * receiver in Recovery. */
 Model0_u32 Model0_prr_out; /* Total number of pkts sent during Recovery. */
 Model0_u32 Model0_delivered; /* Total data packets delivered incl. rexmits */

  Model0_u32 Model0_rcv_wnd; /* Current receiver window		*/
 Model0_u32 Model0_write_seq; /* Tail(+1) of data held in tcp send buffer */
 Model0_u32 Model0_notsent_lowat; /* TCP_NOTSENT_LOWAT */
 Model0_u32 Model0_pushed_seq; /* Last pushed seq, required to talk to windows */
 Model0_u32 Model0_lost_out; /* Lost packets			*/
 Model0_u32 Model0_sacked_out; /* SACK'd packets			*/
 Model0_u32 Model0_fackets_out; /* FACK'd packets			*/

 /* from STCP, retrans queue hinting */
 struct Model0_sk_buff* Model0_lost_skb_hint;
 struct Model0_sk_buff *Model0_retransmit_skb_hint;

 /* OOO segments go in this list. Note that socket lock must be held,
	 * as we do not use sk_buff_head lock.
	 */
 struct Model0_sk_buff_head Model0_out_of_order_queue;

 /* SACKs data, these 2 need to be together (see tcp_options_write) */
 struct Model0_tcp_sack_block Model0_duplicate_sack[1]; /* D-SACK block */
 struct Model0_tcp_sack_block Model0_selective_acks[4]; /* The SACKS themselves*/

 struct Model0_tcp_sack_block Model0_recv_sack_cache[4];

 struct Model0_sk_buff *Model0_highest_sack; /* skb just after the highest
					 * skb with SACKed bit set
					 * (validity guaranteed only if
					 * sacked_out > 0)
					 */

 int Model0_lost_cnt_hint;
 Model0_u32 Model0_retransmit_high; /* L-bits may be on up to this seqno */

 Model0_u32 Model0_prior_ssthresh; /* ssthresh saved at recovery start	*/
 Model0_u32 Model0_high_seq; /* snd_nxt at onset of congestion	*/

 Model0_u32 Model0_retrans_stamp; /* Timestamp of the last retransmit,
				 * also used in SYN-SENT to remember stamp of
				 * the first SYN. */
 Model0_u32 Model0_undo_marker; /* snd_una upon a new recovery episode. */
 int Model0_undo_retrans; /* number of undoable retransmissions. */
 Model0_u32 Model0_total_retrans; /* Total retransmits for entire connection */

 Model0_u32 Model0_urg_seq; /* Seq of received urgent pointer */
 unsigned int Model0_keepalive_time; /* time before keep alive takes place */
 unsigned int Model0_keepalive_intvl; /* time interval between keep alive probes */

 int Model0_linger2;

/* Receiver side RTT estimation */
 struct {
  Model0_u32 Model0_rtt;
  Model0_u32 Model0_seq;
  Model0_u32 Model0_time;
 } Model0_rcv_rtt_est;

/* Receiver queue space */
 struct {
  int Model0_space;
  Model0_u32 Model0_seq;
  Model0_u32 Model0_time;
 } Model0_rcvq_space;

/* TCP-specific MTU probe information. */
 struct {
  Model0_u32 Model0_probe_seq_start;
  Model0_u32 Model0_probe_seq_end;
 } Model0_mtu_probe;
 Model0_u32 Model0_mtu_info; /* We received an ICMP_FRAG_NEEDED / ICMPV6_PKT_TOOBIG
			   * while socket was owned by user.
			   */


/* TCP AF-Specific parts; only used by MD5 Signature support so far */
 const struct Model0_tcp_sock_af_ops *Model0_af_specific;

/* TCP MD5 Signature Option information */
 struct Model0_tcp_md5sig_info *Model0_md5sig_info;


/* TCP fastopen related information */
 struct Model0_tcp_fastopen_request *Model0_fastopen_req;
 /* fastopen_rsk points to request_sock that resulted in this big
	 * socket. Used to retransmit SYNACKs etc.
	 */
 struct Model0_request_sock *Model0_fastopen_rsk;
 Model0_u32 *Model0_saved_syn;
};

enum Model0_tsq_flags {
 Model0_TSQ_THROTTLED,
 Model0_TSQ_QUEUED,
 Model0_TCP_TSQ_DEFERRED, /* tcp_tasklet_func() found socket was owned */
 Model0_TCP_WRITE_TIMER_DEFERRED, /* tcp_write_timer() found socket was owned */
 Model0_TCP_DELACK_TIMER_DEFERRED, /* tcp_delack_timer() found socket was owned */
 Model0_TCP_MTU_REDUCED_DEFERRED, /* tcp_v{4|6}_err() could not call
				    * tcp_v{4|6}_mtu_reduced()
				    */
};

static inline __attribute__((no_instrument_function)) struct Model0_tcp_sock *Model0_tcp_sk(const struct Model0_sock *Model0_sk)
{
 return (struct Model0_tcp_sock *)Model0_sk;
}

struct Model0_tcp_timewait_sock {
 struct Model0_inet_timewait_sock Model0_tw_sk;


 Model0_u32 Model0_tw_rcv_wnd;
 Model0_u32 Model0_tw_ts_offset;
 Model0_u32 Model0_tw_ts_recent;

 /* The time we sent the last out-of-window ACK: */
 Model0_u32 Model0_tw_last_oow_ack_time;

 long Model0_tw_ts_recent_stamp;

 struct Model0_tcp_md5sig_key *Model0_tw_md5_key;

};

static inline __attribute__((no_instrument_function)) struct Model0_tcp_timewait_sock *Model0_tcp_twsk(const struct Model0_sock *Model0_sk)
{
 return (struct Model0_tcp_timewait_sock *)Model0_sk;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_passive_fastopen(const struct Model0_sock *Model0_sk)
{
 return (Model0_sk->Model0___sk_common.Model0_skc_state == Model0_TCP_SYN_RECV &&
  Model0_tcp_sk(Model0_sk)->Model0_fastopen_rsk != ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model0_fastopen_queue_tune(struct Model0_sock *Model0_sk, int Model0_backlog)
{
 struct Model0_request_sock_queue *Model0_queue = &Model0_inet_csk(Model0_sk)->Model0_icsk_accept_queue;
 int Model0_somaxconn = ({ union { typeof(Model0_sock_net(Model0_sk)->Model0_core.Model0_sysctl_somaxconn) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_sock_net(Model0_sk)->Model0_core.Model0_sysctl_somaxconn), Model0___u.Model0___c, sizeof(Model0_sock_net(Model0_sk)->Model0_core.Model0_sysctl_somaxconn)); else Model0___read_once_size_nocheck(&(Model0_sock_net(Model0_sk)->Model0_core.Model0_sysctl_somaxconn), Model0___u.Model0___c, sizeof(Model0_sock_net(Model0_sk)->Model0_core.Model0_sysctl_somaxconn)); Model0___u.Model0___val; });

 Model0_queue->Model0_fastopenq.Model0_max_qlen = ({ unsigned int Model0___min1 = (Model0_backlog); unsigned int Model0___min2 = (Model0_somaxconn); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; });
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_move_syn(struct Model0_tcp_sock *Model0_tp,
    struct Model0_request_sock *Model0_req)
{
 Model0_tp->Model0_saved_syn = Model0_req->Model0_saved_syn;
 Model0_req->Model0_saved_syn = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_saved_syn_free(struct Model0_tcp_sock *Model0_tp)
{
 Model0_kfree(Model0_tp->Model0_saved_syn);
 Model0_tp->Model0_saved_syn = ((void *)0);
}





void Model0_sha_init(__u32 *Model0_buf);
void Model0_sha_transform(__u32 *Model0_digest, const char *Model0_data, __u32 *Model0_W);




void Model0_md5_transform(__u32 *Model0_hash, __u32 const *Model0_in);

__u32 Model0_half_md4_transform(__u32 Model0_buf[4], __u32 const Model0_in[8]);





/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the BSD Socket
 *		interface as the means of communication with the user level.
 *
 * Authors:	Lotsa people, from code originally in tcp
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */







/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the IP protocol.
 *
 * Version:	@(#)ip.h	1.0.2	04/28/93
 *
 * Authors:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the IP protocol.
 *
 * Version:	@(#)ip.h	1.0.2	04/28/93
 *
 * Authors:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
/* IP options */
struct Model0_iphdr {

 __u8 Model0_ihl:4,
  Model0_version:4;






 __u8 Model0_tos;
 Model0___be16 Model0_tot_len;
 Model0___be16 Model0_id;
 Model0___be16 Model0_frag_off;
 __u8 Model0_ttl;
 __u8 Model0_protocol;
 Model0___sum16 Model0_check;
 Model0___be32 Model0_saddr;
 Model0___be32 Model0_daddr;
 /*The options start here. */
};


struct Model0_ip_auth_hdr {
 __u8 Model0_nexthdr;
 __u8 Model0_hdrlen; /* This one is measured in 32 bit units! */
 Model0___be16 Model0_reserved;
 Model0___be32 Model0_spi;
 Model0___be32 Model0_seq_no; /* Sequence number */
 __u8 Model0_auth_data[0]; /* Variable len but >=4. Mind the 64 bit alignment! */
};

struct Model0_ip_esp_hdr {
 Model0___be32 Model0_spi;
 Model0___be32 Model0_seq_no; /* Sequence number */
 __u8 Model0_enc_data[0]; /* Variable len but >=8. Mind the 64 bit alignment! */
};

struct Model0_ip_comp_hdr {
 __u8 Model0_nexthdr;
 __u8 Model0_flags;
 Model0___be16 Model0_cpi;
};

struct Model0_ip_beet_phdr {
 __u8 Model0_nexthdr;
 __u8 Model0_hdrlen;
 __u8 Model0_padlen;
 __u8 Model0_reserved;
};

/* index values for the variables in ipv4_devconf */
enum
{
 Model0_IPV4_DEVCONF_FORWARDING=1,
 Model0_IPV4_DEVCONF_MC_FORWARDING,
 Model0_IPV4_DEVCONF_PROXY_ARP,
 Model0_IPV4_DEVCONF_ACCEPT_REDIRECTS,
 Model0_IPV4_DEVCONF_SECURE_REDIRECTS,
 Model0_IPV4_DEVCONF_SEND_REDIRECTS,
 Model0_IPV4_DEVCONF_SHARED_MEDIA,
 Model0_IPV4_DEVCONF_RP_FILTER,
 Model0_IPV4_DEVCONF_ACCEPT_SOURCE_ROUTE,
 Model0_IPV4_DEVCONF_BOOTP_RELAY,
 Model0_IPV4_DEVCONF_LOG_MARTIANS,
 Model0_IPV4_DEVCONF_TAG,
 Model0_IPV4_DEVCONF_ARPFILTER,
 Model0_IPV4_DEVCONF_MEDIUM_ID,
 Model0_IPV4_DEVCONF_NOXFRM,
 Model0_IPV4_DEVCONF_NOPOLICY,
 Model0_IPV4_DEVCONF_FORCE_IGMP_VERSION,
 Model0_IPV4_DEVCONF_ARP_ANNOUNCE,
 Model0_IPV4_DEVCONF_ARP_IGNORE,
 Model0_IPV4_DEVCONF_PROMOTE_SECONDARIES,
 Model0_IPV4_DEVCONF_ARP_ACCEPT,
 Model0_IPV4_DEVCONF_ARP_NOTIFY,
 Model0_IPV4_DEVCONF_ACCEPT_LOCAL,
 Model0_IPV4_DEVCONF_SRC_VMARK,
 Model0_IPV4_DEVCONF_PROXY_ARP_PVLAN,
 Model0_IPV4_DEVCONF_ROUTE_LOCALNET,
 Model0_IPV4_DEVCONF_IGMPV2_UNSOLICITED_REPORT_INTERVAL,
 Model0_IPV4_DEVCONF_IGMPV3_UNSOLICITED_REPORT_INTERVAL,
 Model0_IPV4_DEVCONF_IGNORE_ROUTES_WITH_LINKDOWN,
 Model0_IPV4_DEVCONF_DROP_UNICAST_IN_L2_MULTICAST,
 Model0_IPV4_DEVCONF_DROP_GRATUITOUS_ARP,
 Model0___IPV4_DEVCONF_MAX
};

static inline __attribute__((no_instrument_function)) struct Model0_iphdr *Model0_ip_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_iphdr *)Model0_skb_network_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_iphdr *Model0_inner_ip_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_iphdr *)Model0_skb_inner_network_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_iphdr *Model0_ipip_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_iphdr *)Model0_skb_transport_header(Model0_skb);
}











/* The latest drafts declared increase in minimal mtu up to 1280. */



/*
 *	Advanced API
 *	source interface/address selection, source routing, etc...
 *	*under construction*
 */


struct Model0_in6_pktinfo {
 struct Model0_in6_addr Model0_ipi6_addr;
 int Model0_ipi6_ifindex;
};



struct Model0_ip6_mtuinfo {
 struct Model0_sockaddr_in6 Model0_ip6m_addr;
 __u32 Model0_ip6m_mtu;
};


struct Model0_in6_ifreq {
 struct Model0_in6_addr Model0_ifr6_addr;
 __u32 Model0_ifr6_prefixlen;
 int Model0_ifr6_ifindex;
};





/*
 *	routing header
 */
struct Model0_ipv6_rt_hdr {
 __u8 Model0_nexthdr;
 __u8 Model0_hdrlen;
 __u8 Model0_type;
 __u8 Model0_segments_left;

 /*
	 *	type specific data
	 *	variable length field
	 */
};


struct Model0_ipv6_opt_hdr {
 __u8 Model0_nexthdr;
 __u8 Model0_hdrlen;
 /* 
	 * TLV encoded option data follows.
	 */
} __attribute__((packed)); /* required for some archs */




/* Router Alert option values (RFC2711) */


/*
 *	routing header type 0 (used in cmsghdr struct)
 */

struct Model0_rt0_hdr {
 struct Model0_ipv6_rt_hdr Model0_rt_hdr;
 __u32 Model0_reserved;
 struct Model0_in6_addr Model0_addr[0];


};

/*
 *	routing header type 2
 */

struct Model0_rt2_hdr {
 struct Model0_ipv6_rt_hdr Model0_rt_hdr;
 __u32 Model0_reserved;
 struct Model0_in6_addr Model0_addr;


};

/*
 *	home address option in destination options header
 */

struct Model0_ipv6_destopt_hao {
 __u8 Model0_type;
 __u8 Model0_length;
 struct Model0_in6_addr Model0_addr;
} __attribute__((packed));

/*
 *	IPv6 fixed header
 *
 *	BEWARE, it is incorrect. The first 4 bits of flow_lbl
 *	are glued to priority now, forming "class".
 */

struct Model0_ipv6hdr {

 __u8 Model0_priority:4,
    Model0_version:4;






 __u8 Model0_flow_lbl[3];

 Model0___be16 Model0_payload_len;
 __u8 Model0_nexthdr;
 __u8 Model0_hop_limit;

 struct Model0_in6_addr Model0_saddr;
 struct Model0_in6_addr Model0_daddr;
};


/* index values for the variables in ipv6_devconf */
enum {
 Model0_DEVCONF_FORWARDING = 0,
 Model0_DEVCONF_HOPLIMIT,
 Model0_DEVCONF_MTU6,
 Model0_DEVCONF_ACCEPT_RA,
 Model0_DEVCONF_ACCEPT_REDIRECTS,
 Model0_DEVCONF_AUTOCONF,
 Model0_DEVCONF_DAD_TRANSMITS,
 Model0_DEVCONF_RTR_SOLICITS,
 Model0_DEVCONF_RTR_SOLICIT_INTERVAL,
 Model0_DEVCONF_RTR_SOLICIT_DELAY,
 Model0_DEVCONF_USE_TEMPADDR,
 Model0_DEVCONF_TEMP_VALID_LFT,
 Model0_DEVCONF_TEMP_PREFERED_LFT,
 Model0_DEVCONF_REGEN_MAX_RETRY,
 Model0_DEVCONF_MAX_DESYNC_FACTOR,
 Model0_DEVCONF_MAX_ADDRESSES,
 Model0_DEVCONF_FORCE_MLD_VERSION,
 Model0_DEVCONF_ACCEPT_RA_DEFRTR,
 Model0_DEVCONF_ACCEPT_RA_PINFO,
 Model0_DEVCONF_ACCEPT_RA_RTR_PREF,
 Model0_DEVCONF_RTR_PROBE_INTERVAL,
 Model0_DEVCONF_ACCEPT_RA_RT_INFO_MAX_PLEN,
 Model0_DEVCONF_PROXY_NDP,
 Model0_DEVCONF_OPTIMISTIC_DAD,
 Model0_DEVCONF_ACCEPT_SOURCE_ROUTE,
 Model0_DEVCONF_MC_FORWARDING,
 Model0_DEVCONF_DISABLE_IPV6,
 Model0_DEVCONF_ACCEPT_DAD,
 Model0_DEVCONF_FORCE_TLLAO,
 Model0_DEVCONF_NDISC_NOTIFY,
 Model0_DEVCONF_MLDV1_UNSOLICITED_REPORT_INTERVAL,
 Model0_DEVCONF_MLDV2_UNSOLICITED_REPORT_INTERVAL,
 Model0_DEVCONF_SUPPRESS_FRAG_NDISC,
 Model0_DEVCONF_ACCEPT_RA_FROM_LOCAL,
 Model0_DEVCONF_USE_OPTIMISTIC,
 Model0_DEVCONF_ACCEPT_RA_MTU,
 Model0_DEVCONF_STABLE_SECRET,
 Model0_DEVCONF_USE_OIF_ADDRS_ONLY,
 Model0_DEVCONF_ACCEPT_RA_MIN_HOP_LIMIT,
 Model0_DEVCONF_IGNORE_ROUTES_WITH_LINKDOWN,
 Model0_DEVCONF_DROP_UNICAST_IN_L2_MULTICAST,
 Model0_DEVCONF_DROP_UNSOLICITED_NA,
 Model0_DEVCONF_KEEP_ADDR_ON_DOWN,
 Model0_DEVCONF_MAX
};



/*
 * This structure contains configuration options per IPv6 link.
 */
struct Model0_ipv6_devconf {
 Model0___s32 Model0_forwarding;
 Model0___s32 Model0_hop_limit;
 Model0___s32 Model0_mtu6;
 Model0___s32 Model0_accept_ra;
 Model0___s32 Model0_accept_redirects;
 Model0___s32 Model0_autoconf;
 Model0___s32 Model0_dad_transmits;
 Model0___s32 Model0_rtr_solicits;
 Model0___s32 Model0_rtr_solicit_interval;
 Model0___s32 Model0_rtr_solicit_delay;
 Model0___s32 Model0_force_mld_version;
 Model0___s32 Model0_mldv1_unsolicited_report_interval;
 Model0___s32 Model0_mldv2_unsolicited_report_interval;
 Model0___s32 Model0_use_tempaddr;
 Model0___s32 Model0_temp_valid_lft;
 Model0___s32 Model0_temp_prefered_lft;
 Model0___s32 Model0_regen_max_retry;
 Model0___s32 Model0_max_desync_factor;
 Model0___s32 Model0_max_addresses;
 Model0___s32 Model0_accept_ra_defrtr;
 Model0___s32 Model0_accept_ra_min_hop_limit;
 Model0___s32 Model0_accept_ra_pinfo;
 Model0___s32 Model0_ignore_routes_with_linkdown;







 Model0___s32 Model0_proxy_ndp;
 Model0___s32 Model0_accept_source_route;
 Model0___s32 Model0_accept_ra_from_local;







 Model0___s32 Model0_disable_ipv6;
 Model0___s32 Model0_drop_unicast_in_l2_multicast;
 Model0___s32 Model0_accept_dad;
 Model0___s32 Model0_force_tllao;
 Model0___s32 Model0_ndisc_notify;
 Model0___s32 Model0_suppress_frag_ndisc;
 Model0___s32 Model0_accept_ra_mtu;
 Model0___s32 Model0_drop_unsolicited_na;
 struct Model0_ipv6_stable_secret {
  bool Model0_initialized;
  struct Model0_in6_addr Model0_secret;
 } Model0_stable_secret;
 Model0___s32 Model0_use_oif_addrs_only;
 Model0___s32 Model0_keep_addr_on_down;

 struct Model0_ctl_table_header *Model0_sysctl_header;
};

struct Model0_ipv6_params {
 Model0___s32 Model0_disable_ipv6;
 Model0___s32 Model0_autoconf;
};
extern struct Model0_ipv6_params Model0_ipv6_defaults;











struct Model0_icmp6hdr {

 __u8 Model0_icmp6_type;
 __u8 Model0_icmp6_code;
 Model0___sum16 Model0_icmp6_cksum;


 union {
  Model0___be32 Model0_un_data32[1];
  Model0___be16 Model0_un_data16[2];
  __u8 Model0_un_data8[4];

  struct Model0_icmpv6_echo {
   Model0___be16 identifier;
   Model0___be16 Model0_sequence;
  } Model0_u_echo;

                struct Model0_icmpv6_nd_advt {

                        __u32 Model0_reserved:5,
                          Model0_override:1,
                          Model0_solicited:1,
                          Model0_router:1,
     Model0_reserved2:24;
                } Model0_u_nd_advt;

                struct Model0_icmpv6_nd_ra {
   __u8 Model0_hop_limit;

   __u8 Model0_reserved:3,
     Model0_router_pref:2,
     Model0_home_agent:1,
     Model0_other:1,
     Model0_managed:1;
   Model0___be16 Model0_rt_lifetime;
                } Model0_u_nd_ra;

 } Model0_icmp6_dataun;
};
/*
 *	Codes for Destination Unreachable
 */
/*
 *	Codes for Time Exceeded
 */



/*
 *	Codes for Parameter Problem
 */




/*
 *	constants for (set|get)sockopt
 */



/*
 *	ICMPV6 filter
 */






struct Model0_icmp6_filter {
 __u32 Model0_data[8];
};

/*
 *	Definitions for MLDv2
 */

static inline __attribute__((no_instrument_function)) struct Model0_icmp6hdr *Model0_icmp6_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_icmp6hdr *)Model0_skb_transport_header(Model0_skb);
}




extern void Model0_icmpv6_send(struct Model0_sk_buff *Model0_skb, Model0_u8 Model0_type, Model0_u8 Model0_code, __u32 Model0_info);

typedef void Model0_ip6_icmp_send_t(struct Model0_sk_buff *Model0_skb, Model0_u8 Model0_type, Model0_u8 Model0_code, __u32 Model0_info,
        const struct Model0_in6_addr *Model0_force_saddr);
extern int Model0_inet6_register_icmp_sender(Model0_ip6_icmp_send_t *Model0_fn);
extern int Model0_inet6_unregister_icmp_sender(Model0_ip6_icmp_send_t *Model0_fn);
int Model0_ip6_err_gen_icmpv6_unreach(struct Model0_sk_buff *Model0_skb, int Model0_nhs, int Model0_type,
          unsigned int Model0_data_len);
extern int Model0_icmpv6_init(void);
extern int Model0_icmpv6_err_convert(Model0_u8 Model0_type, Model0_u8 Model0_code,
          int *err);
extern void Model0_icmpv6_cleanup(void);
extern void Model0_icmpv6_param_prob(struct Model0_sk_buff *Model0_skb,
         Model0_u8 Model0_code, int Model0_pos);

struct Model0_flowi6;
struct Model0_in6_addr;
extern void Model0_icmpv6_flow_init(struct Model0_sock *Model0_sk,
        struct Model0_flowi6 *Model0_fl6,
        Model0_u8 Model0_type,
        const struct Model0_in6_addr *Model0_saddr,
        const struct Model0_in6_addr *Model0_daddr,
        int Model0_oif);

/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the UDP protocol.
 *
 * Version:	@(#)udp.h	1.0.2	04/28/93
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the UDP protocol.
 *
 * Version:	@(#)udp.h	1.0.2	04/28/93
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





struct Model0_udphdr {
 Model0___be16 Model0_source;
 Model0___be16 Model0_dest;
 Model0___be16 Model0_len;
 Model0___sum16 Model0_check;
};

/* UDP socket options */





/* UDP encapsulation types */

static inline __attribute__((no_instrument_function)) struct Model0_udphdr *Model0_udp_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_udphdr *)Model0_skb_transport_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_udphdr *Model0_inner_udp_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_udphdr *)Model0_skb_inner_transport_header(Model0_skb);
}



static inline __attribute__((no_instrument_function)) Model0_u32 Model0_udp_hashfn(const struct Model0_net *Model0_net, Model0_u32 Model0_num, Model0_u32 Model0_mask)
{
 return (Model0_num + Model0_net_hash_mix(Model0_net)) & Model0_mask;
}

struct Model0_udp_sock {
 /* inet_sock has to be the first member */
 struct Model0_inet_sock Model0_inet;



 int Model0_pending; /* Any pending frames ? */
 unsigned int Model0_corkflag; /* Cork is required */
 __u8 Model0_encap_type; /* Is this an Encapsulation socket? */
 unsigned char Model0_no_check6_tx:1,/* Send zero UDP6 checksums on TX? */
    Model0_no_check6_rx:1;/* Allow zero UDP6 checksums on RX? */
 /*
	 * Following member retains the information to create a UDP header
	 * when the socket is uncorked.
	 */
 Model0___u16 Model0_len; /* total length of pending frames */
 /*
	 * Fields specific to UDP-Lite.
	 */
 Model0___u16 Model0_pcslen;
 Model0___u16 Model0_pcrlen;
/* indicator bits used by pcflag: */



 __u8 Model0_pcflag; /* marks socket as UDP-Lite if > 0    */
 __u8 unused[3];
 /*
	 * For encapsulation sockets.
	 */
 int (*Model0_encap_rcv)(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
 void (*Model0_encap_destroy)(struct Model0_sock *Model0_sk);

 /* GRO functions for UDP socket */
 struct Model0_sk_buff ** (*Model0_gro_receive)(struct Model0_sock *Model0_sk,
            struct Model0_sk_buff **Model0_head,
            struct Model0_sk_buff *Model0_skb);
 int (*Model0_gro_complete)(struct Model0_sock *Model0_sk,
      struct Model0_sk_buff *Model0_skb,
      int Model0_nhoff);
};

static inline __attribute__((no_instrument_function)) struct Model0_udp_sock *Model0_udp_sk(const struct Model0_sock *Model0_sk)
{
 return (struct Model0_udp_sock *)Model0_sk;
}

static inline __attribute__((no_instrument_function)) void Model0_udp_set_no_check6_tx(struct Model0_sock *Model0_sk, bool Model0_val)
{
 Model0_udp_sk(Model0_sk)->Model0_no_check6_tx = Model0_val;
}

static inline __attribute__((no_instrument_function)) void Model0_udp_set_no_check6_rx(struct Model0_sock *Model0_sk, bool Model0_val)
{
 Model0_udp_sk(Model0_sk)->Model0_no_check6_rx = Model0_val;
}

static inline __attribute__((no_instrument_function)) bool Model0_udp_get_no_check6_tx(struct Model0_sock *Model0_sk)
{
 return Model0_udp_sk(Model0_sk)->Model0_no_check6_tx;
}

static inline __attribute__((no_instrument_function)) bool Model0_udp_get_no_check6_rx(struct Model0_sock *Model0_sk)
{
 return Model0_udp_sk(Model0_sk)->Model0_no_check6_rx;
}



static inline __attribute__((no_instrument_function)) struct Model0_ipv6hdr *Model0_ipv6_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_ipv6hdr *)Model0_skb_network_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_ipv6hdr *Model0_inner_ipv6_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_ipv6hdr *)Model0_skb_inner_network_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_ipv6hdr *Model0_ipipv6_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_ipv6hdr *)Model0_skb_transport_header(Model0_skb);
}

/* 
   This structure contains results of exthdrs parsing
   as offsets from skb->nh.
 */

struct Model0_inet6_skb_parm {
 int Model0_iif;
 Model0___be16 Model0_ra;
 Model0___u16 Model0_dst0;
 Model0___u16 Model0_srcrt;
 Model0___u16 Model0_dst1;
 Model0___u16 Model0_lastopt;
 Model0___u16 Model0_nhoff;
 Model0___u16 Model0_flags;



 Model0___u16 Model0_frag_max_size;
};







static inline __attribute__((no_instrument_function)) bool Model0_skb_l3mdev_slave(Model0___u16 Model0_flags)
{
 return false;
}





static inline __attribute__((no_instrument_function)) int Model0_inet6_iif(const struct Model0_sk_buff *Model0_skb)
{
 bool Model0_l3_slave = Model0_skb_l3mdev_slave(((struct Model0_inet6_skb_parm*)((Model0_skb)->Model0_cb))->Model0_flags);

 return Model0_l3_slave ? Model0_skb->Model0_skb_iif : ((struct Model0_inet6_skb_parm*)((Model0_skb)->Model0_cb))->Model0_iif;
}

struct Model0_tcp6_request_sock {
 struct Model0_tcp_request_sock Model0_tcp6rsk_tcp;
};

struct Model0_ipv6_mc_socklist;
struct Model0_ipv6_ac_socklist;
struct Model0_ipv6_fl_socklist;

struct Model0_inet6_cork {
 struct Model0_ipv6_txoptions *Model0_opt;
 Model0_u8 Model0_hop_limit;
 Model0_u8 Model0_tclass;
};

/**
 * struct ipv6_pinfo - ipv6 private area
 *
 * In the struct sock hierarchy (tcp6_sock, upd6_sock, etc)
 * this _must_ be the last member, so that inet6_sk_generic
 * is able to calculate its offset from the base struct sock
 * by using the struct proto->slab_obj_size member. -acme
 */
struct Model0_ipv6_pinfo {
 struct Model0_in6_addr Model0_saddr;
 struct Model0_in6_pktinfo Model0_sticky_pktinfo;
 const struct Model0_in6_addr *Model0_daddr_cache;




 Model0___be32 Model0_flow_label;
 __u32 Model0_frag_size;

 /*
	 * Packed in 16bits.
	 * Omit one shift by by putting the signed field at MSB.
	 */




 Model0___u16 Model0___unused_1:7;
 Model0___s16 Model0_hop_limit:9;
 Model0___u16 Model0_mc_loop:1,
    Model0___unused_2:6;
 Model0___s16 Model0_mcast_hops:9;

 int Model0_ucast_oif;
 int Model0_mcast_oif;

 /* pktoption flags */
 union {
  struct {
   Model0___u16 Model0_srcrt:1,
    Model0_osrcrt:1,
           Model0_rxinfo:1,
           Model0_rxoinfo:1,
    Model0_rxhlim:1,
    Model0_rxohlim:1,
    Model0_hopopts:1,
    Model0_ohopopts:1,
    Model0_dstopts:1,
    Model0_odstopts:1,
                                Model0_rxflow:1,
    Model0_rxtclass:1,
    Model0_rxpmtu:1,
    Model0_rxorigdstaddr:1;
    /* 2 bits hole */
  } Model0_bits;
  Model0___u16 Model0_all;
 } Model0_rxopt;

 /* sockopt flags */
 Model0___u16 Model0_recverr:1,
                         Model0_sndflow:1,
    Model0_repflow:1,
    Model0_pmtudisc:3,
    Model0_padding:1, /* 1 bit hole */
    Model0_srcprefs:3, /* 001: prefer temporary address
						 * 010: prefer public address
						 * 100: prefer care-of address
						 */
    Model0_dontfrag:1,
    Model0_autoflowlabel:1;
 __u8 Model0_min_hopcount;
 __u8 Model0_tclass;
 Model0___be32 Model0_rcv_flowinfo;

 __u32 Model0_dst_cookie;
 __u32 Model0_rx_dst_cookie;

 struct Model0_ipv6_mc_socklist *Model0_ipv6_mc_list;
 struct Model0_ipv6_ac_socklist *Model0_ipv6_ac_list;
 struct Model0_ipv6_fl_socklist *Model0_ipv6_fl_list;

 struct Model0_ipv6_txoptions *Model0_opt;
 struct Model0_sk_buff *Model0_pktoptions;
 struct Model0_sk_buff *Model0_rxpmtu;
 struct Model0_inet6_cork Model0_cork;
};

/* WARNING: don't change the layout of the members in {raw,udp,tcp}6_sock! */
struct Model0_raw6_sock {
 /* inet_sock has to be the first member of raw6_sock */
 struct Model0_inet_sock Model0_inet;
 __u32 Model0_checksum; /* perform checksum */
 __u32 Model0_offset; /* checksum offset  */
 struct Model0_icmp6_filter Model0_filter;
 __u32 Model0_ip6mr_table;
 /* ipv6_pinfo has to be the last member of raw6_sock, see inet6_sk_generic */
 struct Model0_ipv6_pinfo Model0_inet6;
};

struct Model0_udp6_sock {
 struct Model0_udp_sock Model0_udp;
 /* ipv6_pinfo has to be the last member of udp6_sock, see inet6_sk_generic */
 struct Model0_ipv6_pinfo Model0_inet6;
};

struct Model0_tcp6_sock {
 struct Model0_tcp_sock Model0_tcp;
 /* ipv6_pinfo has to be the last member of tcp6_sock, see inet6_sk_generic */
 struct Model0_ipv6_pinfo Model0_inet6;
};

extern int Model0_inet6_sk_rebuild_header(struct Model0_sock *Model0_sk);

struct Model0_tcp6_timewait_sock {
 struct Model0_tcp_timewait_sock Model0_tcp6tw_tcp;
};


bool Model0_ipv6_mod_enabled(void);

static inline __attribute__((no_instrument_function)) struct Model0_ipv6_pinfo *Model0_inet6_sk(const struct Model0_sock *Model0___sk)
{
 return Model0_sk_fullsock(Model0___sk) ? Model0_inet_sk(Model0___sk)->Model0_pinet6 : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model0_raw6_sock *Model0_raw6_sk(const struct Model0_sock *Model0_sk)
{
 return (struct Model0_raw6_sock *)Model0_sk;
}

static inline __attribute__((no_instrument_function)) void Model0_inet_sk_copy_descendant(struct Model0_sock *Model0_sk_to,
        const struct Model0_sock *Model0_sk_from)
{
 int Model0_ancestor_size = sizeof(struct Model0_inet_sock);

 if (Model0_sk_from->Model0___sk_common.Model0_skc_family == 10)
  Model0_ancestor_size += sizeof(struct Model0_ipv6_pinfo);

 Model0___inet_sk_copy_descendant(Model0_sk_to, Model0_sk_from, Model0_ancestor_size);
}






static inline __attribute__((no_instrument_function)) const struct Model0_in6_addr *Model0_inet6_rcv_saddr(const struct Model0_sock *Model0_sk)
{
 if (Model0_sk->Model0___sk_common.Model0_skc_family == 10)
  return &Model0_sk->Model0___sk_common.Model0_skc_v6_rcv_saddr;
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model0_inet_v6_ipv6only(const struct Model0_sock *Model0_sk)
{
 /* ipv6only field is at same position for timewait and other sockets */
 return ((Model0_sk->Model0___sk_common.Model0_skc_ipv6only));
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET  is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the IP router.
 *
 * Version:	@(#)route.h	1.0.4	05/27/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 * Fixes:
 *		Alan Cox	:	Reformatted. Added ip_rt_local()
 *		Alan Cox	:	Support for TCP parameters.
 *		Alexey Kuznetsov:	Major changes for new routing code.
 *		Mike McLagan    :	Routing by source
 *		Robert Olsson   :	Added rt_cache statistics
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 *		INETPEER - A storage for permanent information about peers
 *
 *  Authors:	Andrey V. Savochkin <saw@msu.ru>
 */
/*
 *	Linux INET6 implementation
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */








/*
 *	inet6 interface/address list definitions
 *	Linux INET6 implementation 
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>	
 *
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */







/* inet6_dev.if_flags */







/* prefix flags */



enum {
 Model0_INET6_IFADDR_STATE_PREDAD,
 Model0_INET6_IFADDR_STATE_DAD,
 Model0_INET6_IFADDR_STATE_POSTDAD,
 Model0_INET6_IFADDR_STATE_ERRDAD,
 Model0_INET6_IFADDR_STATE_DEAD,
};

struct Model0_inet6_ifaddr {
 struct Model0_in6_addr Model0_addr;
 __u32 Model0_prefix_len;

 /* In seconds, relative to tstamp. Expiry is at tstamp + HZ * lft. */
 __u32 Model0_valid_lft;
 __u32 Model0_prefered_lft;
 Model0_atomic_t Model0_refcnt;
 Model0_spinlock_t Model0_lock;

 int Model0_state;

 __u32 Model0_flags;
 __u8 Model0_dad_probes;
 __u8 Model0_stable_privacy_retry;

 Model0___u16 Model0_scope;

 unsigned long Model0_cstamp; /* created timestamp */
 unsigned long Model0_tstamp; /* updated timestamp */

 struct Model0_delayed_work Model0_dad_work;

 struct Model0_inet6_dev *Model0_idev;
 struct Model0_rt6_info *Model0_rt;

 struct Model0_hlist_node Model0_addr_lst;
 struct Model0_list_head Model0_if_list;

 struct Model0_list_head Model0_tmp_list;
 struct Model0_inet6_ifaddr *Model0_ifpub;
 int Model0_regen_count;

 bool Model0_tokenized;

 struct Model0_callback_head Model0_rcu;
 struct Model0_in6_addr Model0_peer_addr;
};

struct Model0_ip6_sf_socklist {
 unsigned int Model0_sl_max;
 unsigned int Model0_sl_count;
 struct Model0_in6_addr Model0_sl_addr[0];
};






struct Model0_ipv6_mc_socklist {
 struct Model0_in6_addr Model0_addr;
 int Model0_ifindex;
 struct Model0_ipv6_mc_socklist *Model0_next;
 Model0_rwlock_t Model0_sflock;
 unsigned int Model0_sfmode; /* MCAST_{INCLUDE,EXCLUDE} */
 struct Model0_ip6_sf_socklist *Model0_sflist;
 struct Model0_callback_head Model0_rcu;
};

struct Model0_ip6_sf_list {
 struct Model0_ip6_sf_list *Model0_sf_next;
 struct Model0_in6_addr Model0_sf_addr;
 unsigned long Model0_sf_count[2]; /* include/exclude counts */
 unsigned char Model0_sf_gsresp; /* include in g & s response? */
 unsigned char Model0_sf_oldin; /* change state */
 unsigned char Model0_sf_crcount; /* retrans. left to send */
};







struct Model0_ifmcaddr6 {
 struct Model0_in6_addr Model0_mca_addr;
 struct Model0_inet6_dev *Model0_idev;
 struct Model0_ifmcaddr6 *Model0_next;
 struct Model0_ip6_sf_list *Model0_mca_sources;
 struct Model0_ip6_sf_list *Model0_mca_tomb;
 unsigned int Model0_mca_sfmode;
 unsigned char Model0_mca_crcount;
 unsigned long Model0_mca_sfcount[2];
 struct Model0_timer_list Model0_mca_timer;
 unsigned int Model0_mca_flags;
 int Model0_mca_users;
 Model0_atomic_t Model0_mca_refcnt;
 Model0_spinlock_t Model0_mca_lock;
 unsigned long Model0_mca_cstamp;
 unsigned long Model0_mca_tstamp;
};

/* Anycast stuff */

struct Model0_ipv6_ac_socklist {
 struct Model0_in6_addr Model0_acl_addr;
 int Model0_acl_ifindex;
 struct Model0_ipv6_ac_socklist *Model0_acl_next;
};

struct Model0_ifacaddr6 {
 struct Model0_in6_addr Model0_aca_addr;
 struct Model0_inet6_dev *Model0_aca_idev;
 struct Model0_rt6_info *Model0_aca_rt;
 struct Model0_ifacaddr6 *Model0_aca_next;
 int Model0_aca_users;
 Model0_atomic_t Model0_aca_refcnt;
 unsigned long Model0_aca_cstamp;
 unsigned long Model0_aca_tstamp;
};





struct Model0_ipv6_devstat {
 struct Model0_proc_dir_entry *Model0_proc_dir_entry;
 __typeof__(struct Model0_ipstats_mib) *Model0_ipv6;
 __typeof__(struct Model0_icmpv6_mib_device) *Model0_icmpv6dev;
 __typeof__(struct Model0_icmpv6msg_mib_device) *Model0_icmpv6msgdev;
};

struct Model0_inet6_dev {
 struct Model0_net_device *Model0_dev;

 struct Model0_list_head Model0_addr_list;

 struct Model0_ifmcaddr6 *Model0_mc_list;
 struct Model0_ifmcaddr6 *Model0_mc_tomb;
 Model0_spinlock_t Model0_mc_lock;

 unsigned char Model0_mc_qrv; /* Query Robustness Variable */
 unsigned char Model0_mc_gq_running;
 unsigned char Model0_mc_ifc_count;
 unsigned char Model0_mc_dad_count;

 unsigned long Model0_mc_v1_seen; /* Max time we stay in MLDv1 mode */
 unsigned long Model0_mc_qi; /* Query Interval */
 unsigned long Model0_mc_qri; /* Query Response Interval */
 unsigned long Model0_mc_maxdelay;

 struct Model0_timer_list Model0_mc_gq_timer; /* general query timer */
 struct Model0_timer_list Model0_mc_ifc_timer; /* interface change timer */
 struct Model0_timer_list Model0_mc_dad_timer; /* dad complete mc timer */

 struct Model0_ifacaddr6 *Model0_ac_list;
 Model0_rwlock_t Model0_lock;
 Model0_atomic_t Model0_refcnt;
 __u32 Model0_if_flags;
 int Model0_dead;

 Model0_u8 Model0_rndid[8];
 struct Model0_timer_list Model0_regen_timer;
 struct Model0_list_head Model0_tempaddr_list;

 struct Model0_in6_addr Model0_token;

 struct Model0_neigh_parms *Model0_nd_parms;
 struct Model0_ipv6_devconf Model0_cnf;
 struct Model0_ipv6_devstat Model0_stats;

 struct Model0_timer_list Model0_rs_timer;
 __u8 Model0_rs_probes;

 __u8 Model0_addr_gen_mode;
 unsigned long Model0_tstamp; /* ipv6InterfaceTable update timestamp */
 struct Model0_callback_head Model0_rcu;
};

static inline __attribute__((no_instrument_function)) void Model0_ipv6_eth_mc_map(const struct Model0_in6_addr *Model0_addr, char *Model0_buf)
{
 /*
	 *	+-------+-------+-------+-------+-------+-------+
	 *      |   33  |   33  | DST13 | DST14 | DST15 | DST16 |
	 *      +-------+-------+-------+-------+-------+-------+
	 */

 Model0_buf[0]= 0x33;
 Model0_buf[1]= 0x33;

 ({ Model0_size_t Model0___len = (sizeof(__u32)); void *Model0___ret; if (__builtin_constant_p(sizeof(__u32)) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_buf + 2), (&Model0_addr->Model0_in6_u.Model0_u6_addr32[3]), Model0___len); else Model0___ret = __builtin_memcpy((Model0_buf + 2), (&Model0_addr->Model0_in6_u.Model0_u6_addr32[3]), Model0___len); Model0___ret; });
}

static inline __attribute__((no_instrument_function)) void Model0_ipv6_arcnet_mc_map(const struct Model0_in6_addr *Model0_addr, char *Model0_buf)
{
 Model0_buf[0] = 0x00;
}

static inline __attribute__((no_instrument_function)) void Model0_ipv6_ib_mc_map(const struct Model0_in6_addr *Model0_addr,
      const unsigned char *Model0_broadcast, char *Model0_buf)
{
 unsigned char Model0_scope = Model0_broadcast[5] & 0xF;

 Model0_buf[0] = 0; /* Reserved */
 Model0_buf[1] = 0xff; /* Multicast QPN */
 Model0_buf[2] = 0xff;
 Model0_buf[3] = 0xff;
 Model0_buf[4] = 0xff;
 Model0_buf[5] = 0x10 | Model0_scope; /* scope from broadcast address */
 Model0_buf[6] = 0x60; /* IPv6 signature */
 Model0_buf[7] = 0x1b;
 Model0_buf[8] = Model0_broadcast[8]; /* P_Key */
 Model0_buf[9] = Model0_broadcast[9];
 ({ Model0_size_t Model0___len = (10); void *Model0___ret; if (__builtin_constant_p(10) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_buf + 10), (Model0_addr->Model0_in6_u.Model0_u6_addr8 + 6), Model0___len); else Model0___ret = __builtin_memcpy((Model0_buf + 10), (Model0_addr->Model0_in6_u.Model0_u6_addr8 + 6), Model0___len); Model0___ret; });
}

static inline __attribute__((no_instrument_function)) int Model0_ipv6_ipgre_mc_map(const struct Model0_in6_addr *Model0_addr,
        const unsigned char *Model0_broadcast, char *Model0_buf)
{
 if ((Model0_broadcast[0] | Model0_broadcast[1] | Model0_broadcast[2] | Model0_broadcast[3]) != 0) {
  ({ Model0_size_t Model0___len = (4); void *Model0___ret; if (__builtin_constant_p(4) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_buf), (Model0_broadcast), Model0___len); else Model0___ret = __builtin_memcpy((Model0_buf), (Model0_broadcast), Model0___len); Model0___ret; });
 } else {
  /* v4mapped? */
  if ((Model0_addr->Model0_in6_u.Model0_u6_addr32[0] | Model0_addr->Model0_in6_u.Model0_u6_addr32[1] |
       (Model0_addr->Model0_in6_u.Model0_u6_addr32[2] ^ (( Model0___be32)(__builtin_constant_p((__u32)((0x0000ffff))) ? ((__u32)( (((__u32)((0x0000ffff)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0000ffff)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0000ffff)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0000ffff)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0000ffff)))))) != 0)
   return -22;
  ({ Model0_size_t Model0___len = (4); void *Model0___ret; if (__builtin_constant_p(4) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_buf), (&Model0_addr->Model0_in6_u.Model0_u6_addr32[3]), Model0___len); else Model0___ret = __builtin_memcpy((Model0_buf), (&Model0_addr->Model0_in6_u.Model0_u6_addr32[3]), Model0___len); Model0___ret; });
 }
 return 0;
}



/*
 *	ICMP codes for neighbour discovery messages
 */







/*
 * Router type: cross-layer information from link-layer to
 * IPv6 layer reported by certain link types (e.g., RFC4214).
 */





/*
 *	ndisc options
 */

enum {
 Model0___ND_OPT_PREFIX_INFO_END = 0,
 Model0_ND_OPT_SOURCE_LL_ADDR = 1, /* RFC2461 */
 Model0_ND_OPT_TARGET_LL_ADDR = 2, /* RFC2461 */
 Model0_ND_OPT_PREFIX_INFO = 3, /* RFC2461 */
 Model0_ND_OPT_REDIRECT_HDR = 4, /* RFC2461 */
 Model0_ND_OPT_MTU = 5, /* RFC2461 */
 Model0___ND_OPT_ARRAY_MAX,
 Model0_ND_OPT_ROUTE_INFO = 24, /* RFC4191 */
 Model0_ND_OPT_RDNSS = 25, /* RFC5006 */
 Model0_ND_OPT_DNSSL = 31, /* RFC6106 */
 Model0_ND_OPT_6CO = 34, /* RFC6775 */
 Model0___ND_OPT_MAX
};
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the ARP (RFC 826) protocol.
 *
 * Version:	@(#)if_arp.h	1.0.1	04/16/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1986-1988
 *		Portions taken from the KA9Q/NOS (v2.00m PA0GRI) source.
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Florian La Roche,
 *		Jonathan Layes <layes@loran.com>
 *		Arnaldo Carvalho de Melo <acme@conectiva.com.br> ARPHRD_HWX25
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the ARP (RFC 826) protocol.
 *
 * Version:	@(#)if_arp.h	1.0.1	04/16/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1986-1988
 *		Portions taken from the KA9Q/NOS (v2.00m PA0GRI) source.
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Florian La Roche,
 *		Jonathan Layes <layes@loran.com>
 *		Arnaldo Carvalho de Melo <acme@conectiva.com.br> ARPHRD_HWX25
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/* ARP protocol HARDWARE identifiers. */
/* Dummy types for non ARP hardware */
/* ARP works differently on different FC media .. so  */




 /* 787->799 reserved for fibrechannel media types */
/* ARP protocol opcodes. */
/* ARP ioctl request. */
struct Model0_arpreq {
  struct Model0_sockaddr Model0_arp_pa; /* protocol address		*/
  struct Model0_sockaddr Model0_arp_ha; /* hardware address		*/
  int Model0_arp_flags; /* flags			*/
  struct Model0_sockaddr Model0_arp_netmask; /* netmask (only for proxy arps) */
  char Model0_arp_dev[16];
};

struct Model0_arpreq_old {
  struct Model0_sockaddr Model0_arp_pa; /* protocol address		*/
  struct Model0_sockaddr Model0_arp_ha; /* hardware address		*/
  int Model0_arp_flags; /* flags			*/
  struct Model0_sockaddr Model0_arp_netmask; /* netmask (only for proxy arps) */
};

/* ARP Flag values. */
/*
 *	This structure defines an ethernet arp header.
 */

struct Model0_arphdr {
 Model0___be16 Model0_ar_hrd; /* format of hardware address	*/
 Model0___be16 Model0_ar_pro; /* format of protocol address	*/
 unsigned char Model0_ar_hln; /* length of hardware address	*/
 unsigned char Model0_ar_pln; /* length of protocol address	*/
 Model0___be16 Model0_ar_op; /* ARP opcode (command)		*/
};

static inline __attribute__((no_instrument_function)) struct Model0_arphdr *Model0_arp_hdr(const struct Model0_sk_buff *Model0_skb)
{
 return (struct Model0_arphdr *)Model0_skb_network_header(Model0_skb);
}

static inline __attribute__((no_instrument_function)) int Model0_arp_hdr_len(struct Model0_net_device *Model0_dev)
{
 switch (Model0_dev->Model0_type) {





 default:
  /* ARP header, plus 2 device addresses, plus 2 IP addresses. */
  return sizeof(struct Model0_arphdr) + (Model0_dev->Model0_addr_len + sizeof(Model0_u32)) * 2;
 }
}





/* Set to 3 to get tracing... */
struct Model0_ctl_table;
struct Model0_inet6_dev;
struct Model0_net_device;
struct Model0_net_proto_family;
struct Model0_sk_buff;
struct Model0_prefix_info;

extern struct Model0_neigh_table Model0_nd_tbl;

struct Model0_nd_msg {
        struct Model0_icmp6hdr Model0_icmph;
        struct Model0_in6_addr Model0_target;
 __u8 Model0_opt[0];
};

struct Model0_rs_msg {
 struct Model0_icmp6hdr Model0_icmph;
 __u8 Model0_opt[0];
};

struct Model0_ra_msg {
        struct Model0_icmp6hdr Model0_icmph;
 Model0___be32 Model0_reachable_time;
 Model0___be32 Model0_retrans_timer;
};

struct Model0_rd_msg {
 struct Model0_icmp6hdr Model0_icmph;
 struct Model0_in6_addr Model0_target;
 struct Model0_in6_addr Model0_dest;
 __u8 Model0_opt[0];
};

struct Model0_nd_opt_hdr {
 __u8 Model0_nd_opt_type;
 __u8 Model0_nd_opt_len;
} __attribute__((packed));

/* ND options */
struct Model0_ndisc_options {
 struct Model0_nd_opt_hdr *Model0_nd_opt_array[Model0___ND_OPT_ARRAY_MAX];




 struct Model0_nd_opt_hdr *Model0_nd_useropts;
 struct Model0_nd_opt_hdr *Model0_nd_useropts_end;



};
struct Model0_ndisc_options *Model0_ndisc_parse_options(const struct Model0_net_device *Model0_dev,
       Model0_u8 *Model0_opt, int Model0_opt_len,
       struct Model0_ndisc_options *Model0_ndopts);

void Model0___ndisc_fill_addr_option(struct Model0_sk_buff *Model0_skb, int Model0_type, void *Model0_data,
         int Model0_data_len, int Model0_pad);



/*
 * This structure defines the hooks for IPv6 neighbour discovery.
 * The following hooks can be defined; unless noted otherwise, they are
 * optional and can be filled with a null pointer.
 *
 * int (*is_useropt)(u8 nd_opt_type):
 *     This function is called when IPv6 decide RA userspace options. if
 *     this function returns 1 then the option given by nd_opt_type will
 *     be handled as userspace option additional to the IPv6 options.
 *
 * int (*parse_options)(const struct net_device *dev,
 *			struct nd_opt_hdr *nd_opt,
 *			struct ndisc_options *ndopts):
 *     This function is called while parsing ndisc ops and put each position
 *     as pointer into ndopts. If this function return unequal 0, then this
 *     function took care about the ndisc option, if 0 then the IPv6 ndisc
 *     option parser will take care about that option.
 *
 * void (*update)(const struct net_device *dev, struct neighbour *n,
 *		  u32 flags, u8 icmp6_type,
 *		  const struct ndisc_options *ndopts):
 *     This function is called when IPv6 ndisc updates the neighbour cache
 *     entry. Additional options which can be updated may be previously
 *     parsed by parse_opts callback and accessible over ndopts parameter.
 *
 * int (*opt_addr_space)(const struct net_device *dev, u8 icmp6_type,
 *			 struct neighbour *neigh, u8 *ha_buf,
 *			 u8 **ha):
 *     This function is called when the necessary option space will be
 *     calculated before allocating a skb. The parameters neigh, ha_buf
 *     abd ha are available on NDISC_REDIRECT messages only.
 *
 * void (*fill_addr_option)(const struct net_device *dev,
 *			    struct sk_buff *skb, u8 icmp6_type,
 *			    const u8 *ha):
 *     This function is called when the skb will finally fill the option
 *     fields inside skb. NOTE: this callback should fill the option
 *     fields to the skb which are previously indicated by opt_space
 *     parameter. That means the decision to add such option should
 *     not lost between these two callbacks, e.g. protected by interface
 *     up state.
 *
 * void (*prefix_rcv_add_addr)(struct net *net, struct net_device *dev,
 *			       const struct prefix_info *pinfo,
 *			       struct inet6_dev *in6_dev,
 *			       struct in6_addr *addr,
 *			       int addr_type, u32 addr_flags,
 *			       bool sllao, bool tokenized,
 *			       __u32 valid_lft, u32 prefered_lft,
 *			       bool dev_addr_generated):
 *     This function is called when a RA messages is received with valid
 *     PIO option fields and an IPv6 address will be added to the interface
 *     for autoconfiguration. The parameter dev_addr_generated reports about
 *     if the address was based on dev->dev_addr or not. This can be used
 *     to add a second address if link-layer operates with two link layer
 *     addresses. E.g. 802.15.4 6LoWPAN.
 */
struct Model0_ndisc_ops {
 int (*Model0_is_useropt)(Model0_u8 Model0_nd_opt_type);
 int (*Model0_parse_options)(const struct Model0_net_device *Model0_dev,
     struct Model0_nd_opt_hdr *Model0_nd_opt,
     struct Model0_ndisc_options *Model0_ndopts);
 void (*Model0_update)(const struct Model0_net_device *Model0_dev, struct Model0_neighbour *Model0_n,
     Model0_u32 Model0_flags, Model0_u8 Model0_icmp6_type,
     const struct Model0_ndisc_options *Model0_ndopts);
 int (*Model0_opt_addr_space)(const struct Model0_net_device *Model0_dev, Model0_u8 Model0_icmp6_type,
      struct Model0_neighbour *Model0_neigh, Model0_u8 *Model0_ha_buf,
      Model0_u8 **Model0_ha);
 void (*Model0_fill_addr_option)(const struct Model0_net_device *Model0_dev,
        struct Model0_sk_buff *Model0_skb, Model0_u8 Model0_icmp6_type,
        const Model0_u8 *Model0_ha);
 void (*Model0_prefix_rcv_add_addr)(struct Model0_net *Model0_net, struct Model0_net_device *Model0_dev,
           const struct Model0_prefix_info *Model0_pinfo,
           struct Model0_inet6_dev *Model0_in6_dev,
           struct Model0_in6_addr *Model0_addr,
           int Model0_addr_type, Model0_u32 Model0_addr_flags,
           bool Model0_sllao, bool Model0_tokenized,
           __u32 Model0_valid_lft, Model0_u32 Model0_prefered_lft,
           bool Model0_dev_addr_generated);
};


static inline __attribute__((no_instrument_function)) int Model0_ndisc_ops_is_useropt(const struct Model0_net_device *Model0_dev,
           Model0_u8 Model0_nd_opt_type)
{
 if (Model0_dev->Model0_ndisc_ops && Model0_dev->Model0_ndisc_ops->Model0_is_useropt)
  return Model0_dev->Model0_ndisc_ops->Model0_is_useropt(Model0_nd_opt_type);
 else
  return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_ndisc_ops_parse_options(const struct Model0_net_device *Model0_dev,
       struct Model0_nd_opt_hdr *Model0_nd_opt,
       struct Model0_ndisc_options *Model0_ndopts)
{
 if (Model0_dev->Model0_ndisc_ops && Model0_dev->Model0_ndisc_ops->Model0_parse_options)
  return Model0_dev->Model0_ndisc_ops->Model0_parse_options(Model0_dev, Model0_nd_opt, Model0_ndopts);
 else
  return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_ndisc_ops_update(const struct Model0_net_device *Model0_dev,
       struct Model0_neighbour *Model0_n, Model0_u32 Model0_flags,
       Model0_u8 Model0_icmp6_type,
       const struct Model0_ndisc_options *Model0_ndopts)
{
 if (Model0_dev->Model0_ndisc_ops && Model0_dev->Model0_ndisc_ops->Model0_update)
  Model0_dev->Model0_ndisc_ops->Model0_update(Model0_dev, Model0_n, Model0_flags, Model0_icmp6_type, Model0_ndopts);
}

static inline __attribute__((no_instrument_function)) int Model0_ndisc_ops_opt_addr_space(const struct Model0_net_device *Model0_dev,
        Model0_u8 Model0_icmp6_type)
{
 if (Model0_dev->Model0_ndisc_ops && Model0_dev->Model0_ndisc_ops->Model0_opt_addr_space &&
     Model0_icmp6_type != 137)
  return Model0_dev->Model0_ndisc_ops->Model0_opt_addr_space(Model0_dev, Model0_icmp6_type, ((void *)0),
            ((void *)0), ((void *)0));
 else
  return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_ndisc_ops_redirect_opt_addr_space(const struct Model0_net_device *Model0_dev,
          struct Model0_neighbour *Model0_neigh,
          Model0_u8 *Model0_ha_buf, Model0_u8 **Model0_ha)
{
 if (Model0_dev->Model0_ndisc_ops && Model0_dev->Model0_ndisc_ops->Model0_opt_addr_space)
  return Model0_dev->Model0_ndisc_ops->Model0_opt_addr_space(Model0_dev, 137,
            Model0_neigh, Model0_ha_buf, Model0_ha);
 else
  return 0;
}

static inline __attribute__((no_instrument_function)) void Model0_ndisc_ops_fill_addr_option(const struct Model0_net_device *Model0_dev,
           struct Model0_sk_buff *Model0_skb,
           Model0_u8 Model0_icmp6_type)
{
 if (Model0_dev->Model0_ndisc_ops && Model0_dev->Model0_ndisc_ops->Model0_fill_addr_option &&
     Model0_icmp6_type != 137)
  Model0_dev->Model0_ndisc_ops->Model0_fill_addr_option(Model0_dev, Model0_skb, Model0_icmp6_type, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model0_ndisc_ops_fill_redirect_addr_option(const struct Model0_net_device *Model0_dev,
             struct Model0_sk_buff *Model0_skb,
             const Model0_u8 *Model0_ha)
{
 if (Model0_dev->Model0_ndisc_ops && Model0_dev->Model0_ndisc_ops->Model0_fill_addr_option)
  Model0_dev->Model0_ndisc_ops->Model0_fill_addr_option(Model0_dev, Model0_skb, 137, Model0_ha);
}

static inline __attribute__((no_instrument_function)) void Model0_ndisc_ops_prefix_rcv_add_addr(struct Model0_net *Model0_net,
       struct Model0_net_device *Model0_dev,
       const struct Model0_prefix_info *Model0_pinfo,
       struct Model0_inet6_dev *Model0_in6_dev,
       struct Model0_in6_addr *Model0_addr,
       int Model0_addr_type, Model0_u32 Model0_addr_flags,
       bool Model0_sllao, bool Model0_tokenized,
       __u32 Model0_valid_lft,
       Model0_u32 Model0_prefered_lft,
       bool Model0_dev_addr_generated)
{
 if (Model0_dev->Model0_ndisc_ops && Model0_dev->Model0_ndisc_ops->Model0_prefix_rcv_add_addr)
  Model0_dev->Model0_ndisc_ops->Model0_prefix_rcv_add_addr(Model0_net, Model0_dev, Model0_pinfo, Model0_in6_dev,
          Model0_addr, Model0_addr_type,
          Model0_addr_flags, Model0_sllao,
          Model0_tokenized, Model0_valid_lft,
          Model0_prefered_lft,
          Model0_dev_addr_generated);
}


/*
 * Return the padding between the option length and the start of the
 * link addr.  Currently only IP-over-InfiniBand needs this, although
 * if RFC 3831 IPv6-over-Fibre Channel is ever implemented it may
 * also need a pad of 2.
 */
static inline __attribute__((no_instrument_function)) int Model0_ndisc_addr_option_pad(unsigned short Model0_type)
{
 switch (Model0_type) {
 case 32: return 2;
 default: return 0;
 }
}

static inline __attribute__((no_instrument_function)) int Model0___ndisc_opt_addr_space(unsigned char Model0_addr_len, int Model0_pad)
{
 return (((Model0_addr_len + Model0_pad)+2+7)&~7);
}


static inline __attribute__((no_instrument_function)) int Model0_ndisc_opt_addr_space(struct Model0_net_device *Model0_dev, Model0_u8 Model0_icmp6_type)
{
 return Model0___ndisc_opt_addr_space(Model0_dev->Model0_addr_len,
          Model0_ndisc_addr_option_pad(Model0_dev->Model0_type)) +
  Model0_ndisc_ops_opt_addr_space(Model0_dev, Model0_icmp6_type);
}

static inline __attribute__((no_instrument_function)) int Model0_ndisc_redirect_opt_addr_space(struct Model0_net_device *Model0_dev,
      struct Model0_neighbour *Model0_neigh,
      Model0_u8 *Model0_ops_data_buf,
      Model0_u8 **Model0_ops_data)
{
 return Model0___ndisc_opt_addr_space(Model0_dev->Model0_addr_len,
          Model0_ndisc_addr_option_pad(Model0_dev->Model0_type)) +
  Model0_ndisc_ops_redirect_opt_addr_space(Model0_dev, Model0_neigh, Model0_ops_data_buf,
        Model0_ops_data);
}


static inline __attribute__((no_instrument_function)) Model0_u8 *Model0___ndisc_opt_addr_data(struct Model0_nd_opt_hdr *Model0_p,
     unsigned char Model0_addr_len, int Model0_prepad)
{
 Model0_u8 *Model0_lladdr = (Model0_u8 *)(Model0_p + 1);
 int Model0_lladdrlen = Model0_p->Model0_nd_opt_len << 3;
 if (Model0_lladdrlen != Model0___ndisc_opt_addr_space(Model0_addr_len, Model0_prepad))
  return ((void *)0);
 return Model0_lladdr + Model0_prepad;
}

static inline __attribute__((no_instrument_function)) Model0_u8 *Model0_ndisc_opt_addr_data(struct Model0_nd_opt_hdr *Model0_p,
          struct Model0_net_device *Model0_dev)
{
 return Model0___ndisc_opt_addr_data(Model0_p, Model0_dev->Model0_addr_len,
         Model0_ndisc_addr_option_pad(Model0_dev->Model0_type));
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_ndisc_hashfn(const void *Model0_pkey, const struct Model0_net_device *Model0_dev, __u32 *Model0_hash_rnd)
{
 const Model0_u32 *Model0_p32 = Model0_pkey;

 return (((Model0_p32[0] ^ Model0_hash32_ptr(Model0_dev)) * Model0_hash_rnd[0]) +
  (Model0_p32[1] * Model0_hash_rnd[1]) +
  (Model0_p32[2] * Model0_hash_rnd[2]) +
  (Model0_p32[3] * Model0_hash_rnd[3]));
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour *Model0___ipv6_neigh_lookup_noref(struct Model0_net_device *Model0_dev, const void *Model0_pkey)
{
 return Model0____neigh_lookup_noref(&Model0_nd_tbl, Model0_neigh_key_eq128, Model0_ndisc_hashfn, Model0_pkey, Model0_dev);
}

static inline __attribute__((no_instrument_function)) struct Model0_neighbour *Model0___ipv6_neigh_lookup(struct Model0_net_device *Model0_dev, const void *Model0_pkey)
{
 struct Model0_neighbour *Model0_n;

 Model0_rcu_read_lock_bh();
 Model0_n = Model0___ipv6_neigh_lookup_noref(Model0_dev, Model0_pkey);
 if (Model0_n && !Model0_atomic_add_unless((&Model0_n->Model0_refcnt), 1, 0))
  Model0_n = ((void *)0);
 Model0_rcu_read_unlock_bh();

 return Model0_n;
}

int Model0_ndisc_init(void);
int Model0_ndisc_late_init(void);

void Model0_ndisc_late_cleanup(void);
void Model0_ndisc_cleanup(void);

int Model0_ndisc_rcv(struct Model0_sk_buff *Model0_skb);

void Model0_ndisc_send_ns(struct Model0_net_device *Model0_dev, const struct Model0_in6_addr *Model0_solicit,
     const struct Model0_in6_addr *Model0_daddr, const struct Model0_in6_addr *Model0_saddr);

void Model0_ndisc_send_rs(struct Model0_net_device *Model0_dev,
     const struct Model0_in6_addr *Model0_saddr, const struct Model0_in6_addr *Model0_daddr);
void Model0_ndisc_send_na(struct Model0_net_device *Model0_dev, const struct Model0_in6_addr *Model0_daddr,
     const struct Model0_in6_addr *Model0_solicited_addr,
     bool Model0_router, bool Model0_solicited, bool Model0_override, bool Model0_inc_opt);

void Model0_ndisc_send_redirect(struct Model0_sk_buff *Model0_skb, const struct Model0_in6_addr *Model0_target);

int Model0_ndisc_mc_map(const struct Model0_in6_addr *Model0_addr, char *Model0_buf, struct Model0_net_device *Model0_dev,
   int Model0_dir);

void Model0_ndisc_update(const struct Model0_net_device *Model0_dev, struct Model0_neighbour *Model0_neigh,
    const Model0_u8 *Model0_lladdr, Model0_u8 Model0_new, Model0_u32 Model0_flags, Model0_u8 Model0_icmp6_type,
    struct Model0_ndisc_options *Model0_ndopts);

/*
 *	IGMP
 */
int Model0_igmp6_init(void);

void Model0_igmp6_cleanup(void);

int Model0_igmp6_event_query(struct Model0_sk_buff *Model0_skb);

int Model0_igmp6_event_report(struct Model0_sk_buff *Model0_skb);



int Model0_ndisc_ifinfo_sysctl_change(struct Model0_ctl_table *Model0_ctl, int Model0_write,
          void *Model0_buffer, Model0_size_t *Model0_lenp, Model0_loff_t *Model0_ppos);
int Model0_ndisc_ifinfo_sysctl_strategy(struct Model0_ctl_table *Model0_ctl,
     void *Model0_oldval, Model0_size_t *Model0_oldlenp,
     void *Model0_newval, Model0_size_t Model0_newlen);


void Model0_inet6_ifinfo_notify(int Model0_event, struct Model0_inet6_dev *Model0_idev);








/*
 *	NextHeader field of IPv6 header
 */
/*
 *	Addr type
 *	
 *	type	-	unicast | multicast
 *	scope	-	local	| site	    | global
 *	v4	-	compat
 *	v4mapped
 *	any
 *	loopback
 */
/*
 *	Addr scopes
 */
/*
 *	Addr flags
 */







/*
 *	fragmentation header
 */

struct Model0_frag_hdr {
 __u8 Model0_nexthdr;
 __u8 Model0_reserved;
 Model0___be16 Model0_frag_off;
 Model0___be32 Model0_identification;
};
/* sysctls */
extern int Model0_sysctl_mld_max_msf;
extern int Model0_sysctl_mld_qrv;
/* per device counters are atomic_long_t */
/* per device and per net counters are atomic_long_t */
/* MIBs */
struct Model0_ip6_ra_chain {
 struct Model0_ip6_ra_chain *Model0_next;
 struct Model0_sock *Model0_sk;
 int Model0_sel;
 void (*Model0_destructor)(struct Model0_sock *);
};

extern struct Model0_ip6_ra_chain *Model0_ip6_ra_chain;
extern Model0_rwlock_t Model0_ip6_ra_lock;

/*
   This structure is prepared by protocol, when parsing
   ancillary data and passed to IPv6.
 */

struct Model0_ipv6_txoptions {
 Model0_atomic_t Model0_refcnt;
 /* Length of this structure */
 int Model0_tot_len;

 /* length of extension headers   */

 Model0___u16 Model0_opt_flen; /* after fragment hdr */
 Model0___u16 Model0_opt_nflen; /* before fragment hdr */

 struct Model0_ipv6_opt_hdr *Model0_hopopt;
 struct Model0_ipv6_opt_hdr *Model0_dst0opt;
 struct Model0_ipv6_rt_hdr *Model0_srcrt; /* Routing Header */
 struct Model0_ipv6_opt_hdr *Model0_dst1opt;
 struct Model0_callback_head Model0_rcu;
 /* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */
};

struct Model0_ip6_flowlabel {
 struct Model0_ip6_flowlabel *Model0_next;
 Model0___be32 Model0_label;
 Model0_atomic_t Model0_users;
 struct Model0_in6_addr Model0_dst;
 struct Model0_ipv6_txoptions *Model0_opt;
 unsigned long Model0_linger;
 struct Model0_callback_head Model0_rcu;
 Model0_u8 Model0_share;
 union {
  struct Model0_pid *Model0_pid;
  Model0_kuid_t Model0_uid;
 } Model0_owner;
 unsigned long Model0_lastuse;
 unsigned long Model0_expires;
 struct Model0_net *Model0_fl_net;
};
struct Model0_ipv6_fl_socklist {
 struct Model0_ipv6_fl_socklist *Model0_next;
 struct Model0_ip6_flowlabel *Model0_fl;
 struct Model0_callback_head Model0_rcu;
};

struct Model0_ipcm6_cookie {
 Model0___s16 Model0_hlimit;
 Model0___s16 Model0_tclass;
 Model0___s8 Model0_dontfrag;
 struct Model0_ipv6_txoptions *Model0_opt;
};

static inline __attribute__((no_instrument_function)) struct Model0_ipv6_txoptions *Model0_txopt_get(const struct Model0_ipv6_pinfo *Model0_np)
{
 struct Model0_ipv6_txoptions *Model0_opt;

 Model0_rcu_read_lock();
 Model0_opt = ({ typeof(*(Model0_np->Model0_opt)) *Model0_________p1 = (typeof(*(Model0_np->Model0_opt)) *)({ typeof((Model0_np->Model0_opt)) Model0__________p1 = ({ union { typeof((Model0_np->Model0_opt)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_np->Model0_opt)), Model0___u.Model0___c, sizeof((Model0_np->Model0_opt))); else Model0___read_once_size_nocheck(&((Model0_np->Model0_opt)), Model0___u.Model0___c, sizeof((Model0_np->Model0_opt))); Model0___u.Model0___val; }); typeof(*((Model0_np->Model0_opt))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_np->Model0_opt)) *)(Model0_________p1)); });
 if (Model0_opt) {
  if (!Model0_atomic_add_unless((&Model0_opt->Model0_refcnt), 1, 0))
   Model0_opt = ((void *)0);
  else
   Model0_opt = (Model0_opt);
 }
 Model0_rcu_read_unlock();
 return Model0_opt;
}

static inline __attribute__((no_instrument_function)) void Model0_txopt_put(struct Model0_ipv6_txoptions *Model0_opt)
{
 if (Model0_opt && Model0_atomic_dec_and_test(&Model0_opt->Model0_refcnt))
  do { do { bool Model0___cond = !(!(!((__builtin_offsetof(typeof(*(Model0_opt)), Model0_rcu)) < 4096))); extern void Model0___compiletime_assert_280(void) ; if (Model0___cond) Model0___compiletime_assert_280(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0); Model0_kfree_call_rcu(&((Model0_opt)->Model0_rcu), (Model0_rcu_callback_t)(unsigned long)(__builtin_offsetof(typeof(*(Model0_opt)), Model0_rcu))); } while (0);
}

struct Model0_ip6_flowlabel *Model0_fl6_sock_lookup(struct Model0_sock *Model0_sk, Model0___be32 Model0_label);
struct Model0_ipv6_txoptions *Model0_fl6_merge_options(struct Model0_ipv6_txoptions *Model0_opt_space,
      struct Model0_ip6_flowlabel *Model0_fl,
      struct Model0_ipv6_txoptions *Model0_fopt);
void Model0_fl6_free_socklist(struct Model0_sock *Model0_sk);
int Model0_ipv6_flowlabel_opt(struct Model0_sock *Model0_sk, char *Model0_optval, int Model0_optlen);
int Model0_ipv6_flowlabel_opt_get(struct Model0_sock *Model0_sk, struct Model0_in6_flowlabel_req *Model0_freq,
      int Model0_flags);
int Model0_ip6_flowlabel_init(void);
void Model0_ip6_flowlabel_cleanup(void);

static inline __attribute__((no_instrument_function)) void Model0_fl6_sock_release(struct Model0_ip6_flowlabel *Model0_fl)
{
 if (Model0_fl)
  Model0_atomic_dec(&Model0_fl->Model0_users);
}

void Model0_icmpv6_notify(struct Model0_sk_buff *Model0_skb, Model0_u8 Model0_type, Model0_u8 Model0_code, Model0___be32 Model0_info);

int Model0_icmpv6_push_pending_frames(struct Model0_sock *Model0_sk, struct Model0_flowi6 *Model0_fl6,
          struct Model0_icmp6hdr *Model0_thdr, int Model0_len);

int Model0_ip6_ra_control(struct Model0_sock *Model0_sk, int Model0_sel);

int Model0_ipv6_parse_hopopts(struct Model0_sk_buff *Model0_skb);

struct Model0_ipv6_txoptions *Model0_ipv6_dup_options(struct Model0_sock *Model0_sk,
     struct Model0_ipv6_txoptions *Model0_opt);
struct Model0_ipv6_txoptions *Model0_ipv6_renew_options(struct Model0_sock *Model0_sk,
       struct Model0_ipv6_txoptions *Model0_opt,
       int Model0_newtype,
       struct Model0_ipv6_opt_hdr *Model0_newopt,
       int Model0_newoptlen);
struct Model0_ipv6_txoptions *
Model0_ipv6_renew_options_kern(struct Model0_sock *Model0_sk,
   struct Model0_ipv6_txoptions *Model0_opt,
   int Model0_newtype,
   struct Model0_ipv6_opt_hdr *Model0_newopt,
   int Model0_newoptlen);
struct Model0_ipv6_txoptions *Model0_ipv6_fixup_options(struct Model0_ipv6_txoptions *Model0_opt_space,
       struct Model0_ipv6_txoptions *Model0_opt);

bool Model0_ipv6_opt_accepted(const struct Model0_sock *Model0_sk, const struct Model0_sk_buff *Model0_skb,
         const struct Model0_inet6_skb_parm *Model0_opt);
struct Model0_ipv6_txoptions *Model0_ipv6_update_options(struct Model0_sock *Model0_sk,
        struct Model0_ipv6_txoptions *Model0_opt);

static inline __attribute__((no_instrument_function)) bool Model0_ipv6_accept_ra(struct Model0_inet6_dev *Model0_idev)
{
 /* If forwarding is enabled, RA are not accepted unless the special
	 * hybrid mode (accept_ra=2) is enabled.
	 */
 return Model0_idev->Model0_cnf.Model0_forwarding ? Model0_idev->Model0_cnf.Model0_accept_ra == 2 :
     Model0_idev->Model0_cnf.Model0_accept_ra;
}


static inline __attribute__((no_instrument_function)) int Model0_ip6_frag_mem(struct Model0_net *Model0_net)
{
 return Model0_sum_frag_mem_limit(&Model0_net->Model0_ipv6.Model0_frags);
}






int Model0___ipv6_addr_type(const struct Model0_in6_addr *Model0_addr);
static inline __attribute__((no_instrument_function)) int Model0_ipv6_addr_type(const struct Model0_in6_addr *Model0_addr)
{
 return Model0___ipv6_addr_type(Model0_addr) & 0xffff;
}

static inline __attribute__((no_instrument_function)) int Model0_ipv6_addr_scope(const struct Model0_in6_addr *Model0_addr)
{
 return Model0___ipv6_addr_type(Model0_addr) & 0x00f0U;
}

static inline __attribute__((no_instrument_function)) int Model0___ipv6_addr_src_scope(int Model0_type)
{
 return (Model0_type == 0x0000U) ? -1 : (Model0_type >> 16);
}

static inline __attribute__((no_instrument_function)) int Model0_ipv6_addr_src_scope(const struct Model0_in6_addr *Model0_addr)
{
 return Model0___ipv6_addr_src_scope(Model0___ipv6_addr_type(Model0_addr));
}

static inline __attribute__((no_instrument_function)) bool Model0___ipv6_addr_needs_scope_id(int Model0_type)
{
 return Model0_type & 0x0020U ||
        (Model0_type & 0x0002U &&
  (Model0_type & (0x0010U|0x0020U)));
}

static inline __attribute__((no_instrument_function)) __u32 Model0_ipv6_iface_scope_id(const struct Model0_in6_addr *Model0_addr, int Model0_iface)
{
 return Model0___ipv6_addr_needs_scope_id(Model0___ipv6_addr_type(Model0_addr)) ? Model0_iface : 0;
}

static inline __attribute__((no_instrument_function)) int Model0_ipv6_addr_cmp(const struct Model0_in6_addr *Model0_a1, const struct Model0_in6_addr *Model0_a2)
{
 return Model0_memcmp(Model0_a1, Model0_a2, sizeof(struct Model0_in6_addr));
}

static inline __attribute__((no_instrument_function)) bool
Model0_ipv6_masked_addr_cmp(const struct Model0_in6_addr *Model0_a1, const struct Model0_in6_addr *Model0_m,
       const struct Model0_in6_addr *Model0_a2)
{

 const unsigned long *Model0_ul1 = (const unsigned long *)Model0_a1;
 const unsigned long *Model0_ulm = (const unsigned long *)Model0_m;
 const unsigned long *Model0_ul2 = (const unsigned long *)Model0_a2;

 return !!(((Model0_ul1[0] ^ Model0_ul2[0]) & Model0_ulm[0]) |
    ((Model0_ul1[1] ^ Model0_ul2[1]) & Model0_ulm[1]));






}

static inline __attribute__((no_instrument_function)) void Model0_ipv6_addr_prefix(struct Model0_in6_addr *Model0_pfx,
        const struct Model0_in6_addr *Model0_addr,
        int Model0_plen)
{
 /* caller must guarantee 0 <= plen <= 128 */
 int Model0_o = Model0_plen >> 3,
     Model0_b = Model0_plen & 0x7;

 memset(Model0_pfx->Model0_in6_u.Model0_u6_addr8, 0, sizeof(Model0_pfx->Model0_in6_u.Model0_u6_addr8));
 ({ Model0_size_t Model0___len = (Model0_o); void *Model0___ret; if (__builtin_constant_p(Model0_o) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_pfx->Model0_in6_u.Model0_u6_addr8), (Model0_addr), Model0___len); else Model0___ret = __builtin_memcpy((Model0_pfx->Model0_in6_u.Model0_u6_addr8), (Model0_addr), Model0___len); Model0___ret; });
 if (Model0_b != 0)
  Model0_pfx->Model0_in6_u.Model0_u6_addr8[Model0_o] = Model0_addr->Model0_in6_u.Model0_u6_addr8[Model0_o] & (0xff00 >> Model0_b);
}

static inline __attribute__((no_instrument_function)) void Model0_ipv6_addr_prefix_copy(struct Model0_in6_addr *Model0_addr,
      const struct Model0_in6_addr *Model0_pfx,
      int Model0_plen)
{
 /* caller must guarantee 0 <= plen <= 128 */
 int Model0_o = Model0_plen >> 3,
     Model0_b = Model0_plen & 0x7;

 ({ Model0_size_t Model0___len = (Model0_o); void *Model0___ret; if (__builtin_constant_p(Model0_o) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_addr->Model0_in6_u.Model0_u6_addr8), (Model0_pfx), Model0___len); else Model0___ret = __builtin_memcpy((Model0_addr->Model0_in6_u.Model0_u6_addr8), (Model0_pfx), Model0___len); Model0___ret; });
 if (Model0_b != 0) {
  Model0_addr->Model0_in6_u.Model0_u6_addr8[Model0_o] &= ~(0xff00 >> Model0_b);
  Model0_addr->Model0_in6_u.Model0_u6_addr8[Model0_o] |= (Model0_pfx->Model0_in6_u.Model0_u6_addr8[Model0_o] & (0xff00 >> Model0_b));
 }
}

static inline __attribute__((no_instrument_function)) void Model0___ipv6_addr_set_half(Model0___be32 *Model0_addr,
     Model0___be32 Model0_wh, Model0___be32 Model0_wl)
{







 if (__builtin_constant_p(Model0_wl) && __builtin_constant_p(Model0_wh)) {
  *( Model0_u64 *)Model0_addr = (( Model0_u64)(Model0_wl) << 32 | ( Model0_u64)(Model0_wh));
  return;
 }


 Model0_addr[0] = Model0_wh;
 Model0_addr[1] = Model0_wl;
}

static inline __attribute__((no_instrument_function)) void Model0_ipv6_addr_set(struct Model0_in6_addr *Model0_addr,
         Model0___be32 Model0_w1, Model0___be32 Model0_w2,
         Model0___be32 Model0_w3, Model0___be32 Model0_w4)
{
 Model0___ipv6_addr_set_half(&Model0_addr->Model0_in6_u.Model0_u6_addr32[0], Model0_w1, Model0_w2);
 Model0___ipv6_addr_set_half(&Model0_addr->Model0_in6_u.Model0_u6_addr32[2], Model0_w3, Model0_w4);
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv6_addr_equal(const struct Model0_in6_addr *Model0_a1,
       const struct Model0_in6_addr *Model0_a2)
{

 const unsigned long *Model0_ul1 = (const unsigned long *)Model0_a1;
 const unsigned long *Model0_ul2 = (const unsigned long *)Model0_a2;

 return ((Model0_ul1[0] ^ Model0_ul2[0]) | (Model0_ul1[1] ^ Model0_ul2[1])) == 0UL;






}


static inline __attribute__((no_instrument_function)) bool Model0___ipv6_prefix_equal64_half(const Model0___be64 *Model0_a1,
           const Model0___be64 *Model0_a2,
           unsigned int Model0_len)
{
 if (Model0_len && ((*Model0_a1 ^ *Model0_a2) & (( Model0___be64)(__builtin_constant_p((__u64)(((~0UL) << (64 - Model0_len)))) ? ((__u64)( (((__u64)(((~0UL) << (64 - Model0_len))) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(((~0UL) << (64 - Model0_len))) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(((~0UL) << (64 - Model0_len))) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(((~0UL) << (64 - Model0_len))) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(((~0UL) << (64 - Model0_len))) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(((~0UL) << (64 - Model0_len))) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(((~0UL) << (64 - Model0_len))) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(((~0UL) << (64 - Model0_len))) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(((~0UL) << (64 - Model0_len)))))))
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv6_prefix_equal(const struct Model0_in6_addr *Model0_addr1,
         const struct Model0_in6_addr *Model0_addr2,
         unsigned int Model0_prefixlen)
{
 const Model0___be64 *Model0_a1 = (const Model0___be64 *)Model0_addr1;
 const Model0___be64 *Model0_a2 = (const Model0___be64 *)Model0_addr2;

 if (Model0_prefixlen >= 64) {
  if (Model0_a1[0] ^ Model0_a2[0])
   return false;
  return Model0___ipv6_prefix_equal64_half(Model0_a1 + 1, Model0_a2 + 1, Model0_prefixlen - 64);
 }
 return Model0___ipv6_prefix_equal64_half(Model0_a1, Model0_a2, Model0_prefixlen);
}
struct Model0_inet_frag_queue;

enum Model0_ip6_defrag_users {
 Model0_IP6_DEFRAG_LOCAL_DELIVER,
 Model0_IP6_DEFRAG_CONNTRACK_IN,
 Model0___IP6_DEFRAG_CONNTRACK_IN = Model0_IP6_DEFRAG_CONNTRACK_IN + ((Model0_u16)(~0U)),
 Model0_IP6_DEFRAG_CONNTRACK_OUT,
 Model0___IP6_DEFRAG_CONNTRACK_OUT = Model0_IP6_DEFRAG_CONNTRACK_OUT + ((Model0_u16)(~0U)),
 Model0_IP6_DEFRAG_CONNTRACK_BRIDGE_IN,
 Model0___IP6_DEFRAG_CONNTRACK_BRIDGE_IN = Model0_IP6_DEFRAG_CONNTRACK_BRIDGE_IN + ((Model0_u16)(~0U)),
};

struct Model0_ip6_create_arg {
 Model0___be32 Model0_id;
 Model0_u32 Model0_user;
 const struct Model0_in6_addr *Model0_src;
 const struct Model0_in6_addr *Model0_dst;
 int Model0_iif;
 Model0_u8 Model0_ecn;
};

void Model0_ip6_frag_init(struct Model0_inet_frag_queue *Model0_q, const void *Model0_a);
bool Model0_ip6_frag_match(const struct Model0_inet_frag_queue *Model0_q, const void *Model0_a);

/*
 *	Equivalent of ipv4 struct ip
 */
struct Model0_frag_queue {
 struct Model0_inet_frag_queue Model0_q;

 Model0___be32 Model0_id; /* fragment id		*/
 Model0_u32 Model0_user;
 struct Model0_in6_addr Model0_saddr;
 struct Model0_in6_addr Model0_daddr;

 int Model0_iif;
 unsigned int Model0_csum;
 Model0___u16 Model0_nhoffset;
 Model0_u8 Model0_ecn;
};

void Model0_ip6_expire_frag_queue(struct Model0_net *Model0_net, struct Model0_frag_queue *Model0_fq,
      struct Model0_inet_frags *Model0_frags);

static inline __attribute__((no_instrument_function)) bool Model0_ipv6_addr_any(const struct Model0_in6_addr *Model0_a)
{

 const unsigned long *Model0_ul = (const unsigned long *)Model0_a;

 return (Model0_ul[0] | Model0_ul[1]) == 0UL;




}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_ipv6_addr_hash(const struct Model0_in6_addr *Model0_a)
{

 const unsigned long *Model0_ul = (const unsigned long *)Model0_a;
 unsigned long Model0_x = Model0_ul[0] ^ Model0_ul[1];

 return (Model0_u32)(Model0_x ^ (Model0_x >> 32));




}

/* more secured version of ipv6_addr_hash() */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0___ipv6_addr_jhash(const struct Model0_in6_addr *Model0_a, const Model0_u32 Model0_initval)
{
 Model0_u32 Model0_v = ( Model0_u32)Model0_a->Model0_in6_u.Model0_u6_addr32[0] ^ ( Model0_u32)Model0_a->Model0_in6_u.Model0_u6_addr32[1];

 return Model0_jhash_3words(Model0_v,
       ( Model0_u32)Model0_a->Model0_in6_u.Model0_u6_addr32[2],
       ( Model0_u32)Model0_a->Model0_in6_u.Model0_u6_addr32[3],
       Model0_initval);
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv6_addr_loopback(const struct Model0_in6_addr *Model0_a)
{

 const Model0___be64 *Model0_be = (const Model0___be64 *)Model0_a;

 return (Model0_be[0] | (Model0_be[1] ^ (( Model0___be64)(__builtin_constant_p((__u64)((1))) ? ((__u64)( (((__u64)((1)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)((1)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)((1)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)((1)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)((1)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)((1)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)((1)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)((1)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64((1)))))) == 0UL;




}

/*
 * Note that we must __force cast these to unsigned long to make sparse happy,
 * since all of the endian-annotated types are fixed size regardless of arch.
 */
static inline __attribute__((no_instrument_function)) bool Model0_ipv6_addr_v4mapped(const struct Model0_in6_addr *Model0_a)
{
 return (

  *(unsigned long *)Model0_a |



  ( unsigned long)(Model0_a->Model0_in6_u.Model0_u6_addr32[2] ^
     (( Model0___be32)(__builtin_constant_p((__u32)((0x0000ffff))) ? ((__u32)( (((__u32)((0x0000ffff)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0000ffff)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0000ffff)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0000ffff)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0000ffff)))))) == 0UL;
}

/*
 * Check for a RFC 4843 ORCHID address
 * (Overlay Routable Cryptographic Hash Identifiers)
 */
static inline __attribute__((no_instrument_function)) bool Model0_ipv6_addr_orchid(const struct Model0_in6_addr *Model0_a)
{
 return (Model0_a->Model0_in6_u.Model0_u6_addr32[0] & (( Model0___be32)(__builtin_constant_p((__u32)((0xfffffff0))) ? ((__u32)( (((__u32)((0xfffffff0)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xfffffff0)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xfffffff0)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xfffffff0)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xfffffff0))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0x20010010))) ? ((__u32)( (((__u32)((0x20010010)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x20010010)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x20010010)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x20010010)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x20010010))));
}

static inline __attribute__((no_instrument_function)) bool Model0_ipv6_addr_is_multicast(const struct Model0_in6_addr *Model0_addr)
{
 return (Model0_addr->Model0_in6_u.Model0_u6_addr32[0] & (( Model0___be32)(__builtin_constant_p((__u32)((0xFF000000))) ? ((__u32)( (((__u32)((0xFF000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xFF000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xFF000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xFF000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xFF000000))))) == (( Model0___be32)(__builtin_constant_p((__u32)((0xFF000000))) ? ((__u32)( (((__u32)((0xFF000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xFF000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xFF000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xFF000000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0xFF000000))));
}

static inline __attribute__((no_instrument_function)) void Model0_ipv6_addr_set_v4mapped(const Model0___be32 Model0_addr,
       struct Model0_in6_addr *Model0_v4mapped)
{
 Model0_ipv6_addr_set(Model0_v4mapped,
   0, 0,
   (( Model0___be32)(__builtin_constant_p((__u32)((0x0000FFFF))) ? ((__u32)( (((__u32)((0x0000FFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0000FFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0000FFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0000FFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0000FFFF)))),
   Model0_addr);
}

/*
 * find the first different bit between two addresses
 * length of address must be a multiple of 32bits
 */
static inline __attribute__((no_instrument_function)) int Model0___ipv6_addr_diff32(const void *Model0_token1, const void *Model0_token2, int Model0_addrlen)
{
 const Model0___be32 *Model0_a1 = Model0_token1, *Model0_a2 = Model0_token2;
 int Model0_i;

 Model0_addrlen >>= 2;

 for (Model0_i = 0; Model0_i < Model0_addrlen; Model0_i++) {
  Model0___be32 Model0_xb = Model0_a1[Model0_i] ^ Model0_a2[Model0_i];
  if (Model0_xb)
   return Model0_i * 32 + 31 - Model0___fls((__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_xb))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_xb)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_xb)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_xb)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_xb)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_xb))));
 }

 /*
	 *	we should *never* get to this point since that 
	 *	would mean the addrs are equal
	 *
	 *	However, we do get to it 8) And exacly, when
	 *	addresses are equal 8)
	 *
	 *	ip route add 1111::/128 via ...
	 *	ip route add 1111::/64 via ...
	 *	and we are here.
	 *
	 *	Ideally, this function should stop comparison
	 *	at prefix length. It does not, but it is still OK,
	 *	if returned value is greater than prefix length.
	 *					--ANK (980803)
	 */
 return Model0_addrlen << 5;
}


static inline __attribute__((no_instrument_function)) int Model0___ipv6_addr_diff64(const void *Model0_token1, const void *Model0_token2, int Model0_addrlen)
{
 const Model0___be64 *Model0_a1 = Model0_token1, *Model0_a2 = Model0_token2;
 int Model0_i;

 Model0_addrlen >>= 3;

 for (Model0_i = 0; Model0_i < Model0_addrlen; Model0_i++) {
  Model0___be64 Model0_xb = Model0_a1[Model0_i] ^ Model0_a2[Model0_i];
  if (Model0_xb)
   return Model0_i * 64 + 63 - Model0___fls((__builtin_constant_p((__u64)(( __u64)(Model0___be64)(Model0_xb))) ? ((__u64)( (((__u64)(( __u64)(Model0___be64)(Model0_xb)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model0___be64)(Model0_xb)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model0___be64)(Model0_xb)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model0___be64)(Model0_xb)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model0___be64)(Model0_xb)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model0___be64)(Model0_xb)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model0___be64)(Model0_xb)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model0___be64)(Model0_xb)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64(( __u64)(Model0___be64)(Model0_xb))));
 }

 return Model0_addrlen << 6;
}


static inline __attribute__((no_instrument_function)) int Model0___ipv6_addr_diff(const void *Model0_token1, const void *Model0_token2, int Model0_addrlen)
{

 if (__builtin_constant_p(Model0_addrlen) && !(Model0_addrlen & 7))
  return Model0___ipv6_addr_diff64(Model0_token1, Model0_token2, Model0_addrlen);

 return Model0___ipv6_addr_diff32(Model0_token1, Model0_token2, Model0_addrlen);
}

static inline __attribute__((no_instrument_function)) int Model0_ipv6_addr_diff(const struct Model0_in6_addr *Model0_a1, const struct Model0_in6_addr *Model0_a2)
{
 return Model0___ipv6_addr_diff(Model0_a1, Model0_a2, sizeof(struct Model0_in6_addr));
}

Model0___be32 Model0_ipv6_select_ident(struct Model0_net *Model0_net,
    const struct Model0_in6_addr *Model0_daddr,
    const struct Model0_in6_addr *Model0_saddr);
void Model0_ipv6_proxy_select_ident(struct Model0_net *Model0_net, struct Model0_sk_buff *Model0_skb);

int Model0_ip6_dst_hoplimit(struct Model0_dst_entry *Model0_dst);

static inline __attribute__((no_instrument_function)) int Model0_ip6_sk_dst_hoplimit(struct Model0_ipv6_pinfo *Model0_np, struct Model0_flowi6 *Model0_fl6,
          struct Model0_dst_entry *Model0_dst)
{
 int Model0_hlimit;

 if (Model0_ipv6_addr_is_multicast(&Model0_fl6->Model0_daddr))
  Model0_hlimit = Model0_np->Model0_mcast_hops;
 else
  Model0_hlimit = Model0_np->Model0_hop_limit;
 if (Model0_hlimit < 0)
  Model0_hlimit = Model0_ip6_dst_hoplimit(Model0_dst);
 return Model0_hlimit;
}

/* copy IPv6 saddr & daddr to flow_keys, possibly using 64bit load/store
 * Equivalent to :	flow->v6addrs.src = iph->saddr;
 *			flow->v6addrs.dst = iph->daddr;
 */
static inline __attribute__((no_instrument_function)) void Model0_iph_to_flow_copy_v6addrs(struct Model0_flow_keys *Model0_flow,
         const struct Model0_ipv6hdr *Model0_iph)
{
 do { bool Model0___cond = !(!(__builtin_offsetof(typeof(Model0_flow->Model0_addrs), Model0_v6addrs.Model0_dst) != __builtin_offsetof(typeof(Model0_flow->Model0_addrs), Model0_v6addrs.Model0_src) + sizeof(Model0_flow->Model0_addrs.Model0_v6addrs.Model0_src))); extern void Model0___compiletime_assert_756(void) ; if (Model0___cond) Model0___compiletime_assert_756(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);


 ({ Model0_size_t Model0___len = (sizeof(Model0_flow->Model0_addrs.Model0_v6addrs)); void *Model0___ret; if (__builtin_constant_p(sizeof(Model0_flow->Model0_addrs.Model0_v6addrs)) && Model0___len >= 64) Model0___ret = Model0___memcpy((&Model0_flow->Model0_addrs.Model0_v6addrs), (&Model0_iph->Model0_saddr), Model0___len); else Model0___ret = __builtin_memcpy((&Model0_flow->Model0_addrs.Model0_v6addrs), (&Model0_iph->Model0_saddr), Model0___len); Model0___ret; });
 Model0_flow->Model0_control.Model0_addr_type = Model0_FLOW_DISSECTOR_KEY_IPV6_ADDRS;
}



/* Sysctl settings for net ipv6.auto_flowlabels */
static inline __attribute__((no_instrument_function)) Model0___be32 Model0_ip6_make_flowlabel(struct Model0_net *Model0_net, struct Model0_sk_buff *Model0_skb,
     Model0___be32 Model0_flowlabel, bool Model0_autolabel,
     struct Model0_flowi6 *Model0_fl6)
{
 Model0_u32 Model0_hash;

 if (Model0_flowlabel ||
     Model0_net->Model0_ipv6.Model0_sysctl.Model0_auto_flowlabels == 0 ||
     (!Model0_autolabel &&
      Model0_net->Model0_ipv6.Model0_sysctl.Model0_auto_flowlabels != 3))
  return Model0_flowlabel;

 Model0_hash = Model0_skb_get_hash_flowi6(Model0_skb, Model0_fl6);

 /* Since this is being sent on the wire obfuscate hash a bit
	 * to minimize possbility that any useful information to an
	 * attacker is leaked. Only lower 20 bits are relevant.
	 */
 Model0_rol32(Model0_hash, 16);

 Model0_flowlabel = ( Model0___be32)Model0_hash & (( Model0___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x000FFFFF))));

 if (Model0_net->Model0_ipv6.Model0_sysctl.Model0_flowlabel_state_ranges)
  Model0_flowlabel |= (( Model0___be32)(__builtin_constant_p((__u32)((0x00080000))) ? ((__u32)( (((__u32)((0x00080000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00080000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00080000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00080000)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x00080000))));

 return Model0_flowlabel;
}

static inline __attribute__((no_instrument_function)) int Model0_ip6_default_np_autolabel(struct Model0_net *Model0_net)
{
 switch (Model0_net->Model0_ipv6.Model0_sysctl.Model0_auto_flowlabels) {
 case 0:
 case 2:
 default:
  return 0;
 case 1:
 case 3:
  return 1;
 }
}
/*
 *	Header manipulation
 */
static inline __attribute__((no_instrument_function)) void Model0_ip6_flow_hdr(struct Model0_ipv6hdr *Model0_hdr, unsigned int Model0_tclass,
    Model0___be32 Model0_flowlabel)
{
 *(Model0___be32 *)Model0_hdr = (( Model0___be32)(__builtin_constant_p((__u32)((0x60000000 | (Model0_tclass << 20)))) ? ((__u32)( (((__u32)((0x60000000 | (Model0_tclass << 20))) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x60000000 | (Model0_tclass << 20))) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x60000000 | (Model0_tclass << 20))) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x60000000 | (Model0_tclass << 20))) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x60000000 | (Model0_tclass << 20))))) | Model0_flowlabel;
}

static inline __attribute__((no_instrument_function)) Model0___be32 Model0_ip6_flowinfo(const struct Model0_ipv6hdr *Model0_hdr)
{
 return *(Model0___be32 *)Model0_hdr & (( Model0___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0FFFFFFF))));
}

static inline __attribute__((no_instrument_function)) Model0___be32 Model0_ip6_flowlabel(const struct Model0_ipv6hdr *Model0_hdr)
{
 return *(Model0___be32 *)Model0_hdr & (( Model0___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x000FFFFF))));
}

static inline __attribute__((no_instrument_function)) Model0_u8 Model0_ip6_tclass(Model0___be32 Model0_flowinfo)
{
 return (__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_flowinfo & ((( Model0___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0FFFFFFF)))) & ~(( Model0___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x000FFFFF)))))))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_flowinfo & ((( Model0___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0FFFFFFF)))) & ~(( Model0___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x000FFFFF))))))) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_flowinfo & ((( Model0___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0FFFFFFF)))) & ~(( Model0___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x000FFFFF))))))) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_flowinfo & ((( Model0___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0FFFFFFF)))) & ~(( Model0___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x000FFFFF))))))) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_flowinfo & ((( Model0___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0FFFFFFF)))) & ~(( Model0___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x000FFFFF))))))) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_flowinfo & ((( Model0___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x0FFFFFFF)))) & ~(( Model0___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((0x000FFFFF)))))))) >> 20;
}

static inline __attribute__((no_instrument_function)) Model0___be32 Model0_ip6_make_flowinfo(unsigned int Model0_tclass, Model0___be32 Model0_flowlabel)
{
 return (( Model0___be32)(__builtin_constant_p((__u32)((Model0_tclass << 20))) ? ((__u32)( (((__u32)((Model0_tclass << 20)) & (__u32)0x000000ffUL) << 24) | (((__u32)((Model0_tclass << 20)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((Model0_tclass << 20)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((Model0_tclass << 20)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((Model0_tclass << 20)))) | Model0_flowlabel;
}

/*
 *	Prototypes exported by ipv6
 */

/*
 *	rcv function (called from netdevice level)
 */

int Model0_ipv6_rcv(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev,
      struct Model0_packet_type *Model0_pt, struct Model0_net_device *Model0_orig_dev);

int Model0_ip6_rcv_finish(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

/*
 *	upper-layer output functions
 */
int Model0_ip6_xmit(const struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, struct Model0_flowi6 *Model0_fl6,
      struct Model0_ipv6_txoptions *Model0_opt, int Model0_tclass);

int Model0_ip6_find_1stfragopt(struct Model0_sk_buff *Model0_skb, Model0_u8 **Model0_nexthdr);

int Model0_ip6_append_data(struct Model0_sock *Model0_sk,
      int Model0_getfrag(void *Model0_from, char *Model0_to, int Model0_offset, int Model0_len,
    int Model0_odd, struct Model0_sk_buff *Model0_skb),
      void *Model0_from, int Model0_length, int Model0_transhdrlen,
      struct Model0_ipcm6_cookie *Model0_ipc6, struct Model0_flowi6 *Model0_fl6,
      struct Model0_rt6_info *Model0_rt, unsigned int Model0_flags,
      const struct Model0_sockcm_cookie *Model0_sockc);

int Model0_ip6_push_pending_frames(struct Model0_sock *Model0_sk);

void Model0_ip6_flush_pending_frames(struct Model0_sock *Model0_sk);

int Model0_ip6_send_skb(struct Model0_sk_buff *Model0_skb);

struct Model0_sk_buff *Model0___ip6_make_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff_head *Model0_queue,
          struct Model0_inet_cork_full *Model0_cork,
          struct Model0_inet6_cork *Model0_v6_cork);
struct Model0_sk_buff *Model0_ip6_make_skb(struct Model0_sock *Model0_sk,
        int Model0_getfrag(void *Model0_from, char *Model0_to, int Model0_offset,
      int Model0_len, int Model0_odd, struct Model0_sk_buff *Model0_skb),
        void *Model0_from, int Model0_length, int Model0_transhdrlen,
        struct Model0_ipcm6_cookie *Model0_ipc6, struct Model0_flowi6 *Model0_fl6,
        struct Model0_rt6_info *Model0_rt, unsigned int Model0_flags,
        const struct Model0_sockcm_cookie *Model0_sockc);

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_ip6_finish_skb(struct Model0_sock *Model0_sk)
{
 return Model0___ip6_make_skb(Model0_sk, &Model0_sk->Model0_sk_write_queue, &Model0_inet_sk(Model0_sk)->Model0_cork,
         &Model0_inet6_sk(Model0_sk)->Model0_cork);
}

int Model0_ip6_dst_lookup(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_dst_entry **Model0_dst,
     struct Model0_flowi6 *Model0_fl6);
struct Model0_dst_entry *Model0_ip6_dst_lookup_flow(const struct Model0_sock *Model0_sk, struct Model0_flowi6 *Model0_fl6,
          const struct Model0_in6_addr *Model0_final_dst);
struct Model0_dst_entry *Model0_ip6_sk_dst_lookup_flow(struct Model0_sock *Model0_sk, struct Model0_flowi6 *Model0_fl6,
      const struct Model0_in6_addr *Model0_final_dst);
struct Model0_dst_entry *Model0_ip6_blackhole_route(struct Model0_net *Model0_net,
          struct Model0_dst_entry *Model0_orig_dst);

/*
 *	skb processing functions
 */

int Model0_ip6_output(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_ip6_forward(struct Model0_sk_buff *Model0_skb);
int Model0_ip6_input(struct Model0_sk_buff *Model0_skb);
int Model0_ip6_mc_input(struct Model0_sk_buff *Model0_skb);

int Model0___ip6_local_out(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_ip6_local_out(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

/*
 *	Extension header (options) processing
 */

void Model0_ipv6_push_nfrag_opts(struct Model0_sk_buff *Model0_skb, struct Model0_ipv6_txoptions *Model0_opt,
     Model0_u8 *Model0_proto, struct Model0_in6_addr **Model0_daddr_p);
void Model0_ipv6_push_frag_opts(struct Model0_sk_buff *Model0_skb, struct Model0_ipv6_txoptions *Model0_opt,
    Model0_u8 *Model0_proto);

int Model0_ipv6_skip_exthdr(const struct Model0_sk_buff *, int Model0_start, Model0_u8 *Model0_nexthdrp,
       Model0___be16 *Model0_frag_offp);

bool Model0_ipv6_ext_hdr(Model0_u8 Model0_nexthdr);

enum {
 Model0_IP6_FH_F_FRAG = (1 << 0),
 Model0_IP6_FH_F_AUTH = (1 << 1),
 Model0_IP6_FH_F_SKIP_RH = (1 << 2),
};

/* find specified header and get offset to it */
int Model0_ipv6_find_hdr(const struct Model0_sk_buff *Model0_skb, unsigned int *Model0_offset, int Model0_target,
    unsigned short *Model0_fragoff, int *Model0_fragflg);

int Model0_ipv6_find_tlv(const struct Model0_sk_buff *Model0_skb, int Model0_offset, int Model0_type);

struct Model0_in6_addr *Model0_fl6_update_dst(struct Model0_flowi6 *Model0_fl6,
    const struct Model0_ipv6_txoptions *Model0_opt,
    struct Model0_in6_addr *Model0_orig);

/*
 *	socket options (ipv6_sockglue.c)
 */

int Model0_ipv6_setsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
      char *Model0_optval, unsigned int Model0_optlen);
int Model0_ipv6_getsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
      char *Model0_optval, int *Model0_optlen);
int Model0_compat_ipv6_setsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
      char *Model0_optval, unsigned int Model0_optlen);
int Model0_compat_ipv6_getsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
      char *Model0_optval, int *Model0_optlen);

int Model0_ip6_datagram_connect(struct Model0_sock *Model0_sk, struct Model0_sockaddr *Model0_addr, int Model0_addr_len);
int Model0_ip6_datagram_connect_v6_only(struct Model0_sock *Model0_sk, struct Model0_sockaddr *Model0_addr,
     int Model0_addr_len);
int Model0_ip6_datagram_dst_update(struct Model0_sock *Model0_sk, bool Model0_fix_sk_saddr);
void Model0_ip6_datagram_release_cb(struct Model0_sock *Model0_sk);

int Model0_ipv6_recv_error(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, int Model0_len,
      int *Model0_addr_len);
int Model0_ipv6_recv_rxpmtu(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, int Model0_len,
       int *Model0_addr_len);
void Model0_ipv6_icmp_error(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, int err, Model0___be16 Model0_port,
       Model0_u32 Model0_info, Model0_u8 *Model0_payload);
void Model0_ipv6_local_error(struct Model0_sock *Model0_sk, int err, struct Model0_flowi6 *Model0_fl6, Model0_u32 Model0_info);
void Model0_ipv6_local_rxpmtu(struct Model0_sock *Model0_sk, struct Model0_flowi6 *Model0_fl6, Model0_u32 Model0_mtu);

int Model0_inet6_release(struct Model0_socket *Model0_sock);
int Model0_inet6_bind(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_uaddr, int Model0_addr_len);
int Model0_inet6_getname(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_uaddr, int *Model0_uaddr_len,
    int Model0_peer);
int Model0_inet6_ioctl(struct Model0_socket *Model0_sock, unsigned int Model0_cmd, unsigned long Model0_arg);

int Model0_inet6_hash_connect(struct Model0_inet_timewait_death_row *Model0_death_row,
         struct Model0_sock *Model0_sk);

/*
 * reassembly.c
 */
extern const struct Model0_proto_ops Model0_inet6_stream_ops;
extern const struct Model0_proto_ops Model0_inet6_dgram_ops;

struct Model0_group_source_req;
struct Model0_group_filter;

int Model0_ip6_mc_source(int Model0_add, int Model0_omode, struct Model0_sock *Model0_sk,
    struct Model0_group_source_req *Model0_pgsr);
int Model0_ip6_mc_msfilter(struct Model0_sock *Model0_sk, struct Model0_group_filter *Model0_gsf);
int Model0_ip6_mc_msfget(struct Model0_sock *Model0_sk, struct Model0_group_filter *Model0_gsf,
    struct Model0_group_filter *Model0_optval, int *Model0_optlen);


int Model0_ac6_proc_init(struct Model0_net *Model0_net);
void Model0_ac6_proc_exit(struct Model0_net *Model0_net);
int Model0_raw6_proc_init(void);
void Model0_raw6_proc_exit(void);
int Model0_tcp6_proc_init(struct Model0_net *Model0_net);
void Model0_tcp6_proc_exit(struct Model0_net *Model0_net);
int Model0_udp6_proc_init(struct Model0_net *Model0_net);
void Model0_udp6_proc_exit(struct Model0_net *Model0_net);
int Model0_udplite6_proc_init(void);
void Model0_udplite6_proc_exit(void);
int Model0_ipv6_misc_proc_init(void);
void Model0_ipv6_misc_proc_exit(void);
int Model0_snmp6_register_dev(struct Model0_inet6_dev *Model0_idev);
int Model0_snmp6_unregister_dev(struct Model0_inet6_dev *Model0_idev);
extern struct Model0_ctl_table Model0_ipv6_route_table_template[];

struct Model0_ctl_table *Model0_ipv6_icmp_sysctl_init(struct Model0_net *Model0_net);
struct Model0_ctl_table *Model0_ipv6_route_sysctl_init(struct Model0_net *Model0_net);
int Model0_ipv6_sysctl_register(void);
void Model0_ipv6_sysctl_unregister(void);


int Model0_ipv6_sock_mc_join(struct Model0_sock *Model0_sk, int Model0_ifindex,
        const struct Model0_in6_addr *Model0_addr);
int Model0_ipv6_sock_mc_drop(struct Model0_sock *Model0_sk, int Model0_ifindex,
        const struct Model0_in6_addr *Model0_addr);


/* IPv4 address key for cache lookups */
struct Model0_ipv4_addr_key {
 Model0___be32 Model0_addr;
 int Model0_vif;
};



struct Model0_inetpeer_addr {
 union {
  struct Model0_ipv4_addr_key Model0_a4;
  struct Model0_in6_addr Model0_a6;
  Model0_u32 Model0_key[(sizeof(struct Model0_in6_addr) / sizeof(Model0_u32))];
 };
 Model0___u16 Model0_family;
};

struct Model0_inet_peer {
 /* group together avl_left,avl_right,v4daddr to speedup lookups */
 struct Model0_inet_peer *Model0_avl_left, *Model0_avl_right;
 struct Model0_inetpeer_addr Model0_daddr;
 __u32 Model0_avl_height;

 Model0_u32 Model0_metrics[(Model0___RTAX_MAX - 1)];
 Model0_u32 Model0_rate_tokens; /* rate limiting for ICMP */
 unsigned long Model0_rate_last;
 union {
  struct Model0_list_head Model0_gc_list;
  struct Model0_callback_head Model0_gc_rcu;
 };
 /*
	 * Once inet_peer is queued for deletion (refcnt == -1), following field
	 * is not available: rid
	 * We can share memory with rcu_head to help keep inet_peer small.
	 */
 union {
  struct {
   Model0_atomic_t Model0_rid; /* Frag reception counter */
  };
  struct Model0_callback_head Model0_rcu;
  struct Model0_inet_peer *Model0_gc_next;
 };

 /* following fields might be frequently dirtied */
 __u32 Model0_dtime; /* the time of last use of not referenced entries */
 Model0_atomic_t Model0_refcnt;
};

struct Model0_inet_peer_base {
 struct Model0_inet_peer *Model0_root;
 Model0_seqlock_t Model0_lock;
 int Model0_total;
};

void Model0_inet_peer_base_init(struct Model0_inet_peer_base *);

void Model0_inet_initpeers(void) __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function));



static inline __attribute__((no_instrument_function)) void Model0_inetpeer_set_addr_v4(struct Model0_inetpeer_addr *Model0_iaddr, Model0___be32 Model0_ip)
{
 Model0_iaddr->Model0_a4.Model0_addr = Model0_ip;
 Model0_iaddr->Model0_a4.Model0_vif = 0;
 Model0_iaddr->Model0_family = 2;
}

static inline __attribute__((no_instrument_function)) Model0___be32 Model0_inetpeer_get_addr_v4(struct Model0_inetpeer_addr *Model0_iaddr)
{
 return Model0_iaddr->Model0_a4.Model0_addr;
}

static inline __attribute__((no_instrument_function)) void Model0_inetpeer_set_addr_v6(struct Model0_inetpeer_addr *Model0_iaddr,
     struct Model0_in6_addr *Model0_in6)
{
 Model0_iaddr->Model0_a6 = *Model0_in6;
 Model0_iaddr->Model0_family = 10;
}

static inline __attribute__((no_instrument_function)) struct Model0_in6_addr *Model0_inetpeer_get_addr_v6(struct Model0_inetpeer_addr *Model0_iaddr)
{
 return &Model0_iaddr->Model0_a6;
}

/* can be called with or without local BH being disabled */
struct Model0_inet_peer *Model0_inet_getpeer(struct Model0_inet_peer_base *Model0_base,
          const struct Model0_inetpeer_addr *Model0_daddr,
          int Model0_create);

static inline __attribute__((no_instrument_function)) struct Model0_inet_peer *Model0_inet_getpeer_v4(struct Model0_inet_peer_base *Model0_base,
      Model0___be32 Model0_v4daddr,
      int Model0_vif, int Model0_create)
{
 struct Model0_inetpeer_addr Model0_daddr;

 Model0_daddr.Model0_a4.Model0_addr = Model0_v4daddr;
 Model0_daddr.Model0_a4.Model0_vif = Model0_vif;
 Model0_daddr.Model0_family = 2;
 return Model0_inet_getpeer(Model0_base, &Model0_daddr, Model0_create);
}

static inline __attribute__((no_instrument_function)) struct Model0_inet_peer *Model0_inet_getpeer_v6(struct Model0_inet_peer_base *Model0_base,
      const struct Model0_in6_addr *Model0_v6daddr,
      int Model0_create)
{
 struct Model0_inetpeer_addr Model0_daddr;

 Model0_daddr.Model0_a6 = *Model0_v6daddr;
 Model0_daddr.Model0_family = 10;
 return Model0_inet_getpeer(Model0_base, &Model0_daddr, Model0_create);
}

static inline __attribute__((no_instrument_function)) int Model0_inetpeer_addr_cmp(const struct Model0_inetpeer_addr *Model0_a,
        const struct Model0_inetpeer_addr *Model0_b)
{
 int Model0_i, Model0_n;

 if (Model0_a->Model0_family == 2)
  Model0_n = sizeof(Model0_a->Model0_a4) / sizeof(Model0_u32);
 else
  Model0_n = sizeof(Model0_a->Model0_a6) / sizeof(Model0_u32);

 for (Model0_i = 0; Model0_i < Model0_n; Model0_i++) {
  if (Model0_a->Model0_key[Model0_i] == Model0_b->Model0_key[Model0_i])
   continue;
  if (Model0_a->Model0_key[Model0_i] < Model0_b->Model0_key[Model0_i])
   return -1;
  return 1;
 }

 return 0;
}

/* can be called from BH context or outside */
void Model0_inet_putpeer(struct Model0_inet_peer *Model0_p);
bool Model0_inet_peer_xrlim_allow(struct Model0_inet_peer *Model0_peer, int Model0_timeout);

void Model0_inetpeer_invalidate_tree(struct Model0_inet_peer_base *);


/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET  is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the Forwarding Information Base.
 *
 * Authors:	A.N.Kuznetsov, <kuznet@ms2.inr.ac.ru>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
struct Model0_fib_config {
 Model0_u8 Model0_fc_dst_len;
 Model0_u8 Model0_fc_tos;
 Model0_u8 Model0_fc_protocol;
 Model0_u8 Model0_fc_scope;
 Model0_u8 Model0_fc_type;
 /* 3 bytes unused */
 Model0_u32 Model0_fc_table;
 Model0___be32 Model0_fc_dst;
 Model0___be32 Model0_fc_gw;
 int Model0_fc_oif;
 Model0_u32 Model0_fc_flags;
 Model0_u32 Model0_fc_priority;
 Model0___be32 Model0_fc_prefsrc;
 struct Model0_nlattr *Model0_fc_mx;
 struct Model0_rtnexthop *Model0_fc_mp;
 int Model0_fc_mx_len;
 int Model0_fc_mp_len;
 Model0_u32 Model0_fc_flow;
 Model0_u32 Model0_fc_nlflags;
 struct Model0_nl_info Model0_fc_nlinfo;
 struct Model0_nlattr *Model0_fc_encap;
 Model0_u16 Model0_fc_encap_type;
};

struct Model0_fib_info;
struct Model0_rtable;

struct Model0_fib_nh_exception {
 struct Model0_fib_nh_exception *Model0_fnhe_next;
 int Model0_fnhe_genid;
 Model0___be32 Model0_fnhe_daddr;
 Model0_u32 Model0_fnhe_pmtu;
 Model0___be32 Model0_fnhe_gw;
 unsigned long Model0_fnhe_expires;
 struct Model0_rtable *Model0_fnhe_rth_input;
 struct Model0_rtable *Model0_fnhe_rth_output;
 unsigned long Model0_fnhe_stamp;
 struct Model0_callback_head Model0_rcu;
};

struct Model0_fnhe_hash_bucket {
 struct Model0_fib_nh_exception *Model0_chain;
};





struct Model0_fib_nh {
 struct Model0_net_device *Model0_nh_dev;
 struct Model0_hlist_node Model0_nh_hash;
 struct Model0_fib_info *Model0_nh_parent;
 unsigned int Model0_nh_flags;
 unsigned char Model0_nh_scope;

 int Model0_nh_weight;
 Model0_atomic_t Model0_nh_upper_bound;




 int Model0_nh_oif;
 Model0___be32 Model0_nh_gw;
 Model0___be32 Model0_nh_saddr;
 int Model0_nh_saddr_genid;
 struct Model0_rtable * *Model0_nh_pcpu_rth_output;
 struct Model0_rtable *Model0_nh_rth_input;
 struct Model0_fnhe_hash_bucket *Model0_nh_exceptions;
 struct Model0_lwtunnel_state *Model0_nh_lwtstate;
};

/*
 * This structure contains data shared by many of routes.
 */

struct Model0_fib_info {
 struct Model0_hlist_node Model0_fib_hash;
 struct Model0_hlist_node Model0_fib_lhash;
 struct Model0_net *Model0_fib_net;
 int Model0_fib_treeref;
 Model0_atomic_t Model0_fib_clntref;
 unsigned int Model0_fib_flags;
 unsigned char Model0_fib_dead;
 unsigned char Model0_fib_protocol;
 unsigned char Model0_fib_scope;
 unsigned char Model0_fib_type;
 Model0___be32 Model0_fib_prefsrc;
 Model0_u32 Model0_fib_tb_id;
 Model0_u32 Model0_fib_priority;
 Model0_u32 *Model0_fib_metrics;




 int Model0_fib_nhs;

 int Model0_fib_weight;

 struct Model0_callback_head Model0_rcu;
 struct Model0_fib_nh Model0_fib_nh[0];

};



struct Model0_fib_rule;


struct Model0_fib_table;
struct Model0_fib_result {
 unsigned char Model0_prefixlen;
 unsigned char Model0_nh_sel;
 unsigned char Model0_type;
 unsigned char Model0_scope;
 Model0_u32 Model0_tclassid;
 struct Model0_fib_info *Model0_fi;
 struct Model0_fib_table *Model0_table;
 struct Model0_hlist_head *Model0_fa_head;
};

struct Model0_fib_result_nl {
 Model0___be32 Model0_fl_addr; /* To be looked up*/
 Model0_u32 Model0_fl_mark;
 unsigned char Model0_fl_tos;
 unsigned char Model0_fl_scope;
 unsigned char Model0_tb_id_in;

 unsigned char Model0_tb_id; /* Results */
 unsigned char Model0_prefixlen;
 unsigned char Model0_nh_sel;
 unsigned char Model0_type;
 unsigned char Model0_scope;
 int err;
};
Model0___be32 Model0_fib_info_update_nh_saddr(struct Model0_net *Model0_net, struct Model0_fib_nh *Model0_nh);
struct Model0_fib_table {
 struct Model0_hlist_node Model0_tb_hlist;
 Model0_u32 Model0_tb_id;
 int Model0_tb_num_default;
 struct Model0_callback_head Model0_rcu;
 unsigned long *Model0_tb_data;
 unsigned long Model0___data[0];
};

int Model0_fib_table_lookup(struct Model0_fib_table *Model0_tb, const struct Model0_flowi4 *Model0_flp,
       struct Model0_fib_result *Model0_res, int Model0_fib_flags);
int Model0_fib_table_insert(struct Model0_fib_table *, struct Model0_fib_config *);
int Model0_fib_table_delete(struct Model0_fib_table *, struct Model0_fib_config *);
int Model0_fib_table_dump(struct Model0_fib_table *Model0_table, struct Model0_sk_buff *Model0_skb,
     struct Model0_netlink_callback *Model0_cb);
int Model0_fib_table_flush(struct Model0_fib_table *Model0_table);
struct Model0_fib_table *Model0_fib_trie_unmerge(struct Model0_fib_table *Model0_main_tb);
void Model0_fib_table_flush_external(struct Model0_fib_table *Model0_table);
void Model0_fib_free_table(struct Model0_fib_table *Model0_tb);
int Model0_fib4_rules_init(struct Model0_net *Model0_net);
void Model0_fib4_rules_exit(struct Model0_net *Model0_net);

struct Model0_fib_table *Model0_fib_new_table(struct Model0_net *Model0_net, Model0_u32 Model0_id);
struct Model0_fib_table *Model0_fib_get_table(struct Model0_net *Model0_net, Model0_u32 Model0_id);

int Model0___fib_lookup(struct Model0_net *Model0_net, struct Model0_flowi4 *Model0_flp,
   struct Model0_fib_result *Model0_res, unsigned int Model0_flags);

static inline __attribute__((no_instrument_function)) int Model0_fib_lookup(struct Model0_net *Model0_net, struct Model0_flowi4 *Model0_flp,
        struct Model0_fib_result *Model0_res, unsigned int Model0_flags)
{
 struct Model0_fib_table *Model0_tb;
 int err = -101;

 Model0_flags |= 1;
 if (Model0_net->Model0_ipv4.Model0_fib_has_custom_rules)
  return Model0___fib_lookup(Model0_net, Model0_flp, Model0_res, Model0_flags);

 Model0_rcu_read_lock();

 Model0_res->Model0_tclassid = 0;

 Model0_tb = ({ typeof(*(Model0_net->Model0_ipv4.Model0_fib_main)) *Model0_________p1 = (typeof(*(Model0_net->Model0_ipv4.Model0_fib_main)) *)({ typeof((Model0_net->Model0_ipv4.Model0_fib_main)) Model0__________p1 = ({ union { typeof((Model0_net->Model0_ipv4.Model0_fib_main)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_net->Model0_ipv4.Model0_fib_main)), Model0___u.Model0___c, sizeof((Model0_net->Model0_ipv4.Model0_fib_main))); else Model0___read_once_size_nocheck(&((Model0_net->Model0_ipv4.Model0_fib_main)), Model0___u.Model0___c, sizeof((Model0_net->Model0_ipv4.Model0_fib_main))); Model0___u.Model0___val; }); typeof(*((Model0_net->Model0_ipv4.Model0_fib_main))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_net->Model0_ipv4.Model0_fib_main)) *)(Model0_________p1)); });
 if (Model0_tb)
  err = Model0_fib_table_lookup(Model0_tb, Model0_flp, Model0_res, Model0_flags);

 if (!err)
  goto Model0_out;

 Model0_tb = ({ typeof(*(Model0_net->Model0_ipv4.Model0_fib_default)) *Model0_________p1 = (typeof(*(Model0_net->Model0_ipv4.Model0_fib_default)) *)({ typeof((Model0_net->Model0_ipv4.Model0_fib_default)) Model0__________p1 = ({ union { typeof((Model0_net->Model0_ipv4.Model0_fib_default)) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&((Model0_net->Model0_ipv4.Model0_fib_default)), Model0___u.Model0___c, sizeof((Model0_net->Model0_ipv4.Model0_fib_default))); else Model0___read_once_size_nocheck(&((Model0_net->Model0_ipv4.Model0_fib_default)), Model0___u.Model0___c, sizeof((Model0_net->Model0_ipv4.Model0_fib_default))); Model0___u.Model0___val; }); typeof(*((Model0_net->Model0_ipv4.Model0_fib_default))) *Model0____typecheck_p __attribute__((unused)); do { } while (0); (Model0__________p1); }); do { } while (0); ; ((typeof(*(Model0_net->Model0_ipv4.Model0_fib_default)) *)(Model0_________p1)); });
 if (Model0_tb)
  err = Model0_fib_table_lookup(Model0_tb, Model0_flp, Model0_res, Model0_flags);

Model0_out:
 if (err == -11)
  err = -101;

 Model0_rcu_read_unlock();

 return err;
}



/* Exported by fib_frontend.c */
extern const struct Model0_nla_policy Model0_rtm_ipv4_policy[];
void Model0_ip_fib_init(void);
Model0___be32 Model0_fib_compute_spec_dst(struct Model0_sk_buff *Model0_skb);
int Model0_fib_validate_source(struct Model0_sk_buff *Model0_skb, Model0___be32 Model0_src, Model0___be32 Model0_dst,
   Model0_u8 Model0_tos, int Model0_oif, struct Model0_net_device *Model0_dev,
   struct Model0_in_device *Model0_idev, Model0_u32 *Model0_itag);
void Model0_fib_select_default(const struct Model0_flowi4 *Model0_flp, struct Model0_fib_result *Model0_res);






static inline __attribute__((no_instrument_function)) int Model0_fib_num_tclassid_users(struct Model0_net *Model0_net)
{
 return 0;
}

int Model0_fib_unmerge(struct Model0_net *Model0_net);
void Model0_fib_flush_external(struct Model0_net *Model0_net);

/* Exported by fib_semantics.c */
int Model0_ip_fib_check_default(Model0___be32 Model0_gw, struct Model0_net_device *Model0_dev);
int Model0_fib_sync_down_dev(struct Model0_net_device *Model0_dev, unsigned long Model0_event, bool Model0_force);
int Model0_fib_sync_down_addr(struct Model0_net_device *Model0_dev, Model0___be32 Model0_local);
int Model0_fib_sync_up(struct Model0_net_device *Model0_dev, unsigned int Model0_nh_flags);

extern Model0_u32 Model0_fib_multipath_secret __attribute__((__section__(".data..read_mostly")));

static inline __attribute__((no_instrument_function)) int Model0_fib_multipath_hash(Model0___be32 Model0_saddr, Model0___be32 Model0_daddr)
{
 return Model0_jhash_2words(( Model0_u32)Model0_saddr, ( Model0_u32)Model0_daddr,
       Model0_fib_multipath_secret) >> 1;
}

void Model0_fib_select_multipath(struct Model0_fib_result *Model0_res, int Model0_hash);
void Model0_fib_select_path(struct Model0_net *Model0_net, struct Model0_fib_result *Model0_res,
       struct Model0_flowi4 *Model0_fl4, int Model0_mp_hash);

/* Exported by fib_trie.c */
void Model0_fib_trie_init(void);
struct Model0_fib_table *Model0_fib_trie_table(Model0_u32 Model0_id, struct Model0_fib_table *Model0_alias);

static inline __attribute__((no_instrument_function)) void Model0_fib_combine_itag(Model0_u32 *Model0_itag, const struct Model0_fib_result *Model0_res)
{
}

void Model0_free_fib_info(struct Model0_fib_info *Model0_fi);

static inline __attribute__((no_instrument_function)) void Model0_fib_info_put(struct Model0_fib_info *Model0_fi)
{
 if (Model0_atomic_dec_and_test(&Model0_fi->Model0_fib_clntref))
  Model0_free_fib_info(Model0_fi);
}


int Model0_fib_proc_init(struct Model0_net *Model0_net);
void Model0_fib_proc_exit(struct Model0_net *Model0_net);




/* IPv4 routing cache flags */




/* Obsolete flag. About to be deleted */


/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the IP router interface.
 *
 * Version:	@(#)route.h	1.0.3	05/27/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1986-1988
 *		for the purposes of compatibility only.
 *
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 * Changes:
 *              Mike McLagan    :       Routing by source
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






/* This structure gets passed by the SIOCADDRT and SIOCDELRT calls. */
struct Model0_rtentry {
 unsigned long Model0_rt_pad1;
 struct Model0_sockaddr Model0_rt_dst; /* target address		*/
 struct Model0_sockaddr Model0_rt_gateway; /* gateway addr (RTF_GATEWAY)	*/
 struct Model0_sockaddr Model0_rt_genmask; /* target network mask (IP)	*/
 unsigned short Model0_rt_flags;
 short Model0_rt_pad2;
 unsigned long Model0_rt_pad3;
 void *Model0_rt_pad4;
 short Model0_rt_metric; /* +1 for binary compatibility!	*/
 char *Model0_rt_dev; /* forcing the device at add	*/
 unsigned long Model0_rt_mtu; /* per route MTU/Window 	*/



 unsigned long Model0_rt_window; /* Window clamping 		*/
 unsigned short Model0_rt_irtt; /* Initial RTT			*/
};
/*
 *	<linux/ipv6_route.h> uses RTF values >= 64k
 */




/* IPv4 datagram length is stored into 16bit field (tot_len) */







struct Model0_fib_nh;
struct Model0_fib_info;
struct Model0_uncached_list;
struct Model0_rtable {
 struct Model0_dst_entry Model0_dst;

 int Model0_rt_genid;
 unsigned int Model0_rt_flags;
 Model0___u16 Model0_rt_type;
 __u8 Model0_rt_is_input;
 __u8 Model0_rt_uses_gateway;

 int Model0_rt_iif;

 /* Info on neighbour */
 Model0___be32 Model0_rt_gateway;

 /* Miscellaneous cached information */
 Model0_u32 Model0_rt_pmtu;

 Model0_u32 Model0_rt_table_id;

 struct Model0_list_head Model0_rt_uncached;
 struct Model0_uncached_list *Model0_rt_uncached_list;
};

static inline __attribute__((no_instrument_function)) bool Model0_rt_is_input_route(const struct Model0_rtable *Model0_rt)
{
 return Model0_rt->Model0_rt_is_input != 0;
}

static inline __attribute__((no_instrument_function)) bool Model0_rt_is_output_route(const struct Model0_rtable *Model0_rt)
{
 return Model0_rt->Model0_rt_is_input == 0;
}

static inline __attribute__((no_instrument_function)) Model0___be32 Model0_rt_nexthop(const struct Model0_rtable *Model0_rt, Model0___be32 Model0_daddr)
{
 if (Model0_rt->Model0_rt_gateway)
  return Model0_rt->Model0_rt_gateway;
 return Model0_daddr;
}

struct Model0_ip_rt_acct {
 __u32 Model0_o_bytes;
 __u32 Model0_o_packets;
 __u32 Model0_i_bytes;
 __u32 Model0_i_packets;
};

struct Model0_rt_cache_stat {
        unsigned int Model0_in_slow_tot;
        unsigned int Model0_in_slow_mc;
        unsigned int Model0_in_no_route;
        unsigned int Model0_in_brd;
        unsigned int Model0_in_martian_dst;
        unsigned int Model0_in_martian_src;
        unsigned int Model0_out_slow_tot;
        unsigned int Model0_out_slow_mc;
};

extern struct Model0_ip_rt_acct *Model0_ip_rt_acct;

struct Model0_in_device;

int Model0_ip_rt_init(void);
void Model0_rt_cache_flush(struct Model0_net *Model0_net);
void Model0_rt_flush_dev(struct Model0_net_device *Model0_dev);
struct Model0_rtable *Model0___ip_route_output_key_hash(struct Model0_net *, struct Model0_flowi4 *Model0_flp,
       int Model0_mp_hash);

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0___ip_route_output_key(struct Model0_net *Model0_net,
         struct Model0_flowi4 *Model0_flp)
{
 return Model0___ip_route_output_key_hash(Model0_net, Model0_flp, -1);
}

struct Model0_rtable *Model0_ip_route_output_flow(struct Model0_net *, struct Model0_flowi4 *Model0_flp,
        const struct Model0_sock *Model0_sk);
struct Model0_dst_entry *Model0_ipv4_blackhole_route(struct Model0_net *Model0_net,
           struct Model0_dst_entry *Model0_dst_orig);

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0_ip_route_output_key(struct Model0_net *Model0_net, struct Model0_flowi4 *Model0_flp)
{
 return Model0_ip_route_output_flow(Model0_net, Model0_flp, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0_ip_route_output(struct Model0_net *Model0_net, Model0___be32 Model0_daddr,
          Model0___be32 Model0_saddr, Model0_u8 Model0_tos, int Model0_oif)
{
 struct Model0_flowi4 Model0_fl4 = {
  .Model0___fl_common.Model0_flowic_oif = Model0_oif,
  .Model0___fl_common.Model0_flowic_tos = Model0_tos,
  .Model0_daddr = Model0_daddr,
  .Model0_saddr = Model0_saddr,
 };
 return Model0_ip_route_output_key(Model0_net, &Model0_fl4);
}

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0_ip_route_output_ports(struct Model0_net *Model0_net, struct Model0_flowi4 *Model0_fl4,
         struct Model0_sock *Model0_sk,
         Model0___be32 Model0_daddr, Model0___be32 Model0_saddr,
         Model0___be16 Model0_dport, Model0___be16 Model0_sport,
         __u8 Model0_proto, __u8 Model0_tos, int Model0_oif)
{
 Model0_flowi4_init_output(Model0_fl4, Model0_oif, Model0_sk ? Model0_sk->Model0_sk_mark : 0, Model0_tos,
      Model0_RT_SCOPE_UNIVERSE, Model0_proto,
      Model0_sk ? Model0_inet_sk_flowi_flags(Model0_sk) : 0,
      Model0_daddr, Model0_saddr, Model0_dport, Model0_sport);
 if (Model0_sk)
  Model0_security_sk_classify_flow(Model0_sk, Model0_flowi4_to_flowi(Model0_fl4));
 return Model0_ip_route_output_flow(Model0_net, Model0_fl4, Model0_sk);
}

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0_ip_route_output_gre(struct Model0_net *Model0_net, struct Model0_flowi4 *Model0_fl4,
       Model0___be32 Model0_daddr, Model0___be32 Model0_saddr,
       Model0___be32 Model0_gre_key, __u8 Model0_tos, int Model0_oif)
{
 memset(Model0_fl4, 0, sizeof(*Model0_fl4));
 Model0_fl4->Model0___fl_common.Model0_flowic_oif = Model0_oif;
 Model0_fl4->Model0_daddr = Model0_daddr;
 Model0_fl4->Model0_saddr = Model0_saddr;
 Model0_fl4->Model0___fl_common.Model0_flowic_tos = Model0_tos;
 Model0_fl4->Model0___fl_common.Model0_flowic_proto = Model0_IPPROTO_GRE;
 Model0_fl4->Model0_uli.Model0_gre_key = Model0_gre_key;
 return Model0_ip_route_output_key(Model0_net, Model0_fl4);
}

int Model0_ip_route_input_noref(struct Model0_sk_buff *Model0_skb, Model0___be32 Model0_dst, Model0___be32 Model0_src,
    Model0_u8 Model0_tos, struct Model0_net_device *Model0_devin);

static inline __attribute__((no_instrument_function)) int Model0_ip_route_input(struct Model0_sk_buff *Model0_skb, Model0___be32 Model0_dst, Model0___be32 Model0_src,
     Model0_u8 Model0_tos, struct Model0_net_device *Model0_devin)
{
 int err;

 Model0_rcu_read_lock();
 err = Model0_ip_route_input_noref(Model0_skb, Model0_dst, Model0_src, Model0_tos, Model0_devin);
 if (!err)
  Model0_skb_dst_force(Model0_skb);
 Model0_rcu_read_unlock();

 return err;
}

void Model0_ipv4_update_pmtu(struct Model0_sk_buff *Model0_skb, struct Model0_net *Model0_net, Model0_u32 Model0_mtu, int Model0_oif,
        Model0_u32 Model0_mark, Model0_u8 Model0_protocol, int Model0_flow_flags);
void Model0_ipv4_sk_update_pmtu(struct Model0_sk_buff *Model0_skb, struct Model0_sock *Model0_sk, Model0_u32 Model0_mtu);
void Model0_ipv4_redirect(struct Model0_sk_buff *Model0_skb, struct Model0_net *Model0_net, int Model0_oif, Model0_u32 Model0_mark,
     Model0_u8 Model0_protocol, int Model0_flow_flags);
void Model0_ipv4_sk_redirect(struct Model0_sk_buff *Model0_skb, struct Model0_sock *Model0_sk);
void Model0_ip_rt_send_redirect(struct Model0_sk_buff *Model0_skb);

unsigned int Model0_inet_addr_type(struct Model0_net *Model0_net, Model0___be32 Model0_addr);
unsigned int Model0_inet_addr_type_table(struct Model0_net *Model0_net, Model0___be32 Model0_addr, Model0_u32 Model0_tb_id);
unsigned int Model0_inet_dev_addr_type(struct Model0_net *Model0_net, const struct Model0_net_device *Model0_dev,
    Model0___be32 Model0_addr);
unsigned int Model0_inet_addr_type_dev_table(struct Model0_net *Model0_net,
          const struct Model0_net_device *Model0_dev,
          Model0___be32 Model0_addr);
void Model0_ip_rt_multicast_event(struct Model0_in_device *);
int Model0_ip_rt_ioctl(struct Model0_net *, unsigned int Model0_cmd, void *Model0_arg);
void Model0_ip_rt_get_source(Model0_u8 *Model0_src, struct Model0_sk_buff *Model0_skb, struct Model0_rtable *Model0_rt);
struct Model0_rtable *Model0_rt_dst_alloc(struct Model0_net_device *Model0_dev,
        unsigned int Model0_flags, Model0_u16 Model0_type,
        bool Model0_nopolicy, bool Model0_noxfrm, bool Model0_will_cache);

struct Model0_in_ifaddr;
void Model0_fib_add_ifaddr(struct Model0_in_ifaddr *);
void Model0_fib_del_ifaddr(struct Model0_in_ifaddr *, struct Model0_in_ifaddr *);

static inline __attribute__((no_instrument_function)) void Model0_ip_rt_put(struct Model0_rtable *Model0_rt)
{
 /* dst_release() accepts a NULL parameter.
	 * We rely on dst being first structure in struct rtable
	 */
 do { bool Model0___cond = !(!(__builtin_offsetof(struct Model0_rtable, Model0_dst) != 0)); extern void Model0___compiletime_assert_225(void) ; if (Model0___cond) Model0___compiletime_assert_225(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);
 Model0_dst_release(&Model0_rt->Model0_dst);
}



extern const __u8 Model0_ip_tos2prio[16];

static inline __attribute__((no_instrument_function)) char Model0_rt_tos2priority(Model0_u8 Model0_tos)
{
 return Model0_ip_tos2prio[((Model0_tos)&0x1E)>>1];
}

/* ip_route_connect() and ip_route_newports() work in tandem whilst
 * binding a socket for a new outgoing connection.
 *
 * In order to use IPSEC properly, we must, in the end, have a
 * route that was looked up using all available keys including source
 * and destination ports.
 *
 * However, if a source port needs to be allocated (the user specified
 * a wildcard source port) we need to obtain addressing information
 * in order to perform that allocation.
 *
 * So ip_route_connect() looks up a route using wildcarded source and
 * destination ports in the key, simply so that we can get a pair of
 * addresses to use for port allocation.
 *
 * Later, once the ports are allocated, ip_route_newports() will make
 * another route lookup if needed to make sure we catch any IPSEC
 * rules keyed on the port information.
 *
 * The callers allocate the flow key on their stack, and must pass in
 * the same flowi4 object to both the ip_route_connect() and the
 * ip_route_newports() calls.
 */

static inline __attribute__((no_instrument_function)) void Model0_ip_route_connect_init(struct Model0_flowi4 *Model0_fl4, Model0___be32 Model0_dst, Model0___be32 Model0_src,
      Model0_u32 Model0_tos, int Model0_oif, Model0_u8 Model0_protocol,
      Model0___be16 Model0_sport, Model0___be16 Model0_dport,
      struct Model0_sock *Model0_sk)
{
 __u8 Model0_flow_flags = 0;

 if (Model0_inet_sk(Model0_sk)->Model0_transparent)
  Model0_flow_flags |= 0x01;

 Model0_flowi4_init_output(Model0_fl4, Model0_oif, Model0_sk->Model0_sk_mark, Model0_tos, Model0_RT_SCOPE_UNIVERSE,
      Model0_protocol, Model0_flow_flags, Model0_dst, Model0_src, Model0_dport, Model0_sport);
}

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0_ip_route_connect(struct Model0_flowi4 *Model0_fl4,
           Model0___be32 Model0_dst, Model0___be32 Model0_src, Model0_u32 Model0_tos,
           int Model0_oif, Model0_u8 Model0_protocol,
           Model0___be16 Model0_sport, Model0___be16 Model0_dport,
           struct Model0_sock *Model0_sk)
{
 struct Model0_net *Model0_net = Model0_sock_net(Model0_sk);
 struct Model0_rtable *Model0_rt;

 Model0_ip_route_connect_init(Model0_fl4, Model0_dst, Model0_src, Model0_tos, Model0_oif, Model0_protocol,
         Model0_sport, Model0_dport, Model0_sk);

 if (!Model0_src && Model0_oif) {
  int Model0_rc;

  Model0_rc = Model0_l3mdev_get_saddr(Model0_net, Model0_oif, Model0_fl4);
  if (Model0_rc < 0)
   return Model0_ERR_PTR(Model0_rc);

  Model0_src = Model0_fl4->Model0_saddr;
 }
 if (!Model0_dst || !Model0_src) {
  Model0_rt = Model0___ip_route_output_key(Model0_net, Model0_fl4);
  if (Model0_IS_ERR(Model0_rt))
   return Model0_rt;
  Model0_ip_rt_put(Model0_rt);
  Model0_flowi4_update_output(Model0_fl4, Model0_oif, Model0_tos, Model0_fl4->Model0_daddr, Model0_fl4->Model0_saddr);
 }
 Model0_security_sk_classify_flow(Model0_sk, Model0_flowi4_to_flowi(Model0_fl4));
 return Model0_ip_route_output_flow(Model0_net, Model0_fl4, Model0_sk);
}

static inline __attribute__((no_instrument_function)) struct Model0_rtable *Model0_ip_route_newports(struct Model0_flowi4 *Model0_fl4, struct Model0_rtable *Model0_rt,
            Model0___be16 Model0_orig_sport, Model0___be16 Model0_orig_dport,
            Model0___be16 Model0_sport, Model0___be16 Model0_dport,
            struct Model0_sock *Model0_sk)
{
 if (Model0_sport != Model0_orig_sport || Model0_dport != Model0_orig_dport) {
  Model0_fl4->Model0_uli.Model0_ports.Model0_dport = Model0_dport;
  Model0_fl4->Model0_uli.Model0_ports.Model0_sport = Model0_sport;
  Model0_ip_rt_put(Model0_rt);
  Model0_flowi4_update_output(Model0_fl4, Model0_sk->Model0___sk_common.Model0_skc_bound_dev_if,
         (((Model0_inet_sk(Model0_sk)->Model0_tos)&0x1E) | Model0_sock_flag(Model0_sk, Model0_SOCK_LOCALROUTE)), Model0_fl4->Model0_daddr,
         Model0_fl4->Model0_saddr);
  Model0_security_sk_classify_flow(Model0_sk, Model0_flowi4_to_flowi(Model0_fl4));
  return Model0_ip_route_output_flow(Model0_sock_net(Model0_sk), Model0_fl4, Model0_sk);
 }
 return Model0_rt;
}

static inline __attribute__((no_instrument_function)) int Model0_inet_iif(const struct Model0_sk_buff *Model0_skb)
{
 struct Model0_rtable *Model0_rt = Model0_skb_rtable(Model0_skb);

 if (Model0_rt && Model0_rt->Model0_rt_iif)
  return Model0_rt->Model0_rt_iif;

 return Model0_skb->Model0_skb_iif;
}

static inline __attribute__((no_instrument_function)) int Model0_ip4_dst_hoplimit(const struct Model0_dst_entry *Model0_dst)
{
 int Model0_hoplimit = Model0_dst_metric_raw(Model0_dst, Model0_RTAX_HOPLIMIT);
 struct Model0_net *Model0_net = Model0_dev_net(Model0_dst->Model0_dev);

 if (Model0_hoplimit == 0)
  Model0_hoplimit = Model0_net->Model0_ipv4.Model0_sysctl_ip_default_ttl;
 return Model0_hoplimit;
}






/* This is for all connections with a full identity, no wildcards.
 * The 'e' prefix stands for Establish, but we really put all sockets
 * but LISTEN ones.
 */
struct Model0_inet_ehash_bucket {
 struct Model0_hlist_nulls_head Model0_chain;
};

/* There are a few simple rules, which allow for local port reuse by
 * an application.  In essence:
 *
 *	1) Sockets bound to different interfaces may share a local port.
 *	   Failing that, goto test 2.
 *	2) If all sockets have sk->sk_reuse set, and none of them are in
 *	   TCP_LISTEN state, the port may be shared.
 *	   Failing that, goto test 3.
 *	3) If all sockets are bound to a specific inet_sk(sk)->rcv_saddr local
 *	   address, and none of them are the same, the port may be
 *	   shared.
 *	   Failing this, the port cannot be shared.
 *
 * The interesting point, is test #2.  This is what an FTP server does
 * all day.  To optimize this case we use a specific flag bit defined
 * below.  As we add sockets to a bind bucket list, we perform a
 * check of: (newsk->sk_reuse && (newsk->sk_state != TCP_LISTEN))
 * As long as all sockets added to a bind bucket pass this test,
 * the flag bit will be set.
 * The resulting situation is that tcp_v[46]_verify_bind() can just check
 * for this flag bit, if it is set and the socket trying to bind has
 * sk->sk_reuse set, we don't even have to walk the owners list at all,
 * we return that it is ok to bind this socket to the requested local port.
 *
 * Sounds like a lot of work, but it is worth it.  In a more naive
 * implementation (ie. current FreeBSD etc.) the entire list of ports
 * must be walked for each data port opened by an ftp server.  Needless
 * to say, this does not scale at all.  With a couple thousand FTP
 * users logged onto your box, isn't it nice to know that new data
 * ports are created in O(1) time?  I thought so. ;-)	-DaveM
 */
struct Model0_inet_bind_bucket {
 Model0_possible_net_t Model0_ib_net;
 unsigned short Model0_port;
 signed char Model0_fastreuse;
 signed char Model0_fastreuseport;
 Model0_kuid_t Model0_fastuid;
 int Model0_num_owners;
 struct Model0_hlist_node Model0_node;
 struct Model0_hlist_head Model0_owners;
};

static inline __attribute__((no_instrument_function)) struct Model0_net *Model0_ib_net(struct Model0_inet_bind_bucket *Model0_ib)
{
 return Model0_read_pnet(&Model0_ib->Model0_ib_net);
}




struct Model0_inet_bind_hashbucket {
 Model0_spinlock_t Model0_lock;
 struct Model0_hlist_head Model0_chain;
};

/*
 * Sockets can be hashed in established or listening table
 */
struct Model0_inet_listen_hashbucket {
 Model0_spinlock_t Model0_lock;
 struct Model0_hlist_head Model0_head;
};

/* This is for listening sockets, thus all sockets which possess wildcards. */


struct Model0_inet_hashinfo {
 /* This is for sockets with full identity only.  Sockets here will
	 * always be without wildcards and will have the following invariant:
	 *
	 *          TCP_ESTABLISHED <= sk->sk_state < TCP_CLOSE
	 *
	 */
 struct Model0_inet_ehash_bucket *Model0_ehash;
 Model0_spinlock_t *Model0_ehash_locks;
 unsigned int Model0_ehash_mask;
 unsigned int Model0_ehash_locks_mask;

 /* Ok, let's try this, I give up, we do need a local binding
	 * TCP hash as well as the others for fast bind/connect.
	 */
 struct Model0_inet_bind_hashbucket *Model0_bhash;

 unsigned int Model0_bhash_size;
 /* 4 bytes hole on 64 bit */

 struct Model0_kmem_cache *Model0_bind_bucket_cachep;

 /* All the above members are written once at bootup and
	 * never written again _or_ are predominantly read-access.
	 *
	 * Now align to a new cache line as all the following members
	 * might be often dirty.
	 */
 /* All sockets in TCP_LISTEN state will be in here.  This is the only
	 * table where wildcard'd TCP sockets can exist.  Hash function here
	 * is just local port number.
	 */
 struct Model0_inet_listen_hashbucket Model0_listening_hash[32]
     __attribute__((__aligned__((1 << (6)))));
};

static inline __attribute__((no_instrument_function)) struct Model0_inet_ehash_bucket *Model0_inet_ehash_bucket(
 struct Model0_inet_hashinfo *Model0_hashinfo,
 unsigned int Model0_hash)
{
 return &Model0_hashinfo->Model0_ehash[Model0_hash & Model0_hashinfo->Model0_ehash_mask];
}

static inline __attribute__((no_instrument_function)) Model0_spinlock_t *Model0_inet_ehash_lockp(
 struct Model0_inet_hashinfo *Model0_hashinfo,
 unsigned int Model0_hash)
{
 return &Model0_hashinfo->Model0_ehash_locks[Model0_hash & Model0_hashinfo->Model0_ehash_locks_mask];
}

int Model0_inet_ehash_locks_alloc(struct Model0_inet_hashinfo *Model0_hashinfo);

static inline __attribute__((no_instrument_function)) void Model0_inet_ehash_locks_free(struct Model0_inet_hashinfo *Model0_hashinfo)
{
 Model0_kvfree(Model0_hashinfo->Model0_ehash_locks);
 Model0_hashinfo->Model0_ehash_locks = ((void *)0);
}

struct Model0_inet_bind_bucket *
Model0_inet_bind_bucket_create(struct Model0_kmem_cache *Model0_cachep, struct Model0_net *Model0_net,
   struct Model0_inet_bind_hashbucket *Model0_head,
   const unsigned short Model0_snum);
void Model0_inet_bind_bucket_destroy(struct Model0_kmem_cache *Model0_cachep,
         struct Model0_inet_bind_bucket *Model0_tb);

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_inet_bhashfn(const struct Model0_net *Model0_net, const Model0___u16 Model0_lport,
          const Model0_u32 Model0_bhash_size)
{
 return (Model0_lport + Model0_net_hash_mix(Model0_net)) & (Model0_bhash_size - 1);
}

void Model0_inet_bind_hash(struct Model0_sock *Model0_sk, struct Model0_inet_bind_bucket *Model0_tb,
      const unsigned short Model0_snum);

/* These can have wildcards, don't try too hard. */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_inet_lhashfn(const struct Model0_net *Model0_net, const unsigned short Model0_num)
{
 return (Model0_num + Model0_net_hash_mix(Model0_net)) & (32 - 1);
}

static inline __attribute__((no_instrument_function)) int Model0_inet_sk_listen_hashfn(const struct Model0_sock *Model0_sk)
{
 return Model0_inet_lhashfn(Model0_sock_net(Model0_sk), Model0_inet_sk(Model0_sk)->Model0_sk.Model0___sk_common.Model0_skc_num);
}

/* Caller must disable local BH processing. */
int Model0___inet_inherit_port(const struct Model0_sock *Model0_sk, struct Model0_sock *Model0_child);

void Model0_inet_put_port(struct Model0_sock *Model0_sk);

void Model0_inet_hashinfo_init(struct Model0_inet_hashinfo *Model0_h);

bool Model0_inet_ehash_insert(struct Model0_sock *Model0_sk, struct Model0_sock *Model0_osk);
bool Model0_inet_ehash_nolisten(struct Model0_sock *Model0_sk, struct Model0_sock *Model0_osk);
int Model0___inet_hash(struct Model0_sock *Model0_sk, struct Model0_sock *Model0_osk,
  int (*Model0_saddr_same)(const struct Model0_sock *Model0_sk1,
      const struct Model0_sock *Model0_sk2,
      bool Model0_match_wildcard));
int Model0_inet_hash(struct Model0_sock *Model0_sk);
void Model0_inet_unhash(struct Model0_sock *Model0_sk);

struct Model0_sock *Model0___inet_lookup_listener(struct Model0_net *Model0_net,
        struct Model0_inet_hashinfo *Model0_hashinfo,
        struct Model0_sk_buff *Model0_skb, int Model0_doff,
        const Model0___be32 Model0_saddr, const Model0___be16 Model0_sport,
        const Model0___be32 Model0_daddr,
        const unsigned short Model0_hnum,
        const int Model0_dif);

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_inet_lookup_listener(struct Model0_net *Model0_net,
  struct Model0_inet_hashinfo *Model0_hashinfo,
  struct Model0_sk_buff *Model0_skb, int Model0_doff,
  Model0___be32 Model0_saddr, Model0___be16 Model0_sport,
  Model0___be32 Model0_daddr, Model0___be16 Model0_dport, int Model0_dif)
{
 return Model0___inet_lookup_listener(Model0_net, Model0_hashinfo, Model0_skb, Model0_doff, Model0_saddr, Model0_sport,
          Model0_daddr, (__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(Model0_dport))), Model0_dif);
}

/* Socket demux engine toys. */
/* What happens here is ugly; there's a pair of adjacent fields in
   struct inet_sock; __be16 dport followed by __u16 num.  We want to
   search by pair, so we combine the keys into a single 32bit value
   and compare with 32bit value read from &...->dport.  Let's at least
   make sure that it's not mixed with anything else...
   On 64bit targets we combine comparisons with pair of adjacent __be32
   fields in the same way.
*/
/* Sockets in TCP_CLOSE state are _always_ taken out of the hash, so we need
 * not check it for lookups anymore, thanks Alexey. -DaveM
 */
struct Model0_sock *Model0___inet_lookup_established(struct Model0_net *Model0_net,
           struct Model0_inet_hashinfo *Model0_hashinfo,
           const Model0___be32 Model0_saddr, const Model0___be16 Model0_sport,
           const Model0___be32 Model0_daddr, const Model0_u16 Model0_hnum,
           const int Model0_dif);

static inline __attribute__((no_instrument_function)) struct Model0_sock *
 Model0_inet_lookup_established(struct Model0_net *Model0_net, struct Model0_inet_hashinfo *Model0_hashinfo,
    const Model0___be32 Model0_saddr, const Model0___be16 Model0_sport,
    const Model0___be32 Model0_daddr, const Model0___be16 Model0_dport,
    const int Model0_dif)
{
 return Model0___inet_lookup_established(Model0_net, Model0_hashinfo, Model0_saddr, Model0_sport, Model0_daddr,
      (__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(Model0_dport))), Model0_dif);
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0___inet_lookup(struct Model0_net *Model0_net,
      struct Model0_inet_hashinfo *Model0_hashinfo,
      struct Model0_sk_buff *Model0_skb, int Model0_doff,
      const Model0___be32 Model0_saddr, const Model0___be16 Model0_sport,
      const Model0___be32 Model0_daddr, const Model0___be16 Model0_dport,
      const int Model0_dif,
      bool *Model0_refcounted)
{
 Model0_u16 Model0_hnum = (__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(Model0_dport)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(Model0_dport)));
 struct Model0_sock *Model0_sk;

 Model0_sk = Model0___inet_lookup_established(Model0_net, Model0_hashinfo, Model0_saddr, Model0_sport,
           Model0_daddr, Model0_hnum, Model0_dif);
 *Model0_refcounted = true;
 if (Model0_sk)
  return Model0_sk;
 *Model0_refcounted = false;
 return Model0___inet_lookup_listener(Model0_net, Model0_hashinfo, Model0_skb, Model0_doff, Model0_saddr,
          Model0_sport, Model0_daddr, Model0_hnum, Model0_dif);
}

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0_inet_lookup(struct Model0_net *Model0_net,
           struct Model0_inet_hashinfo *Model0_hashinfo,
           struct Model0_sk_buff *Model0_skb, int Model0_doff,
           const Model0___be32 Model0_saddr, const Model0___be16 Model0_sport,
           const Model0___be32 Model0_daddr, const Model0___be16 Model0_dport,
           const int Model0_dif)
{
 struct Model0_sock *Model0_sk;
 bool Model0_refcounted;

 Model0_sk = Model0___inet_lookup(Model0_net, Model0_hashinfo, Model0_skb, Model0_doff, Model0_saddr, Model0_sport, Model0_daddr,
      Model0_dport, Model0_dif, &Model0_refcounted);

 if (Model0_sk && !Model0_refcounted && !Model0_atomic_add_unless((&Model0_sk->Model0___sk_common.Model0_skc_refcnt), 1, 0))
  Model0_sk = ((void *)0);
 return Model0_sk;
}

#if CY_ABSTRACT1
extern struct Model0_sock *Model0_Dst_sk;
extern struct Model0_tcp_sock Model0_Server_L, Model0_Server, Model0_Server_A;
extern struct Model0_tcp_request_sock Model0_Req, Model0_Req_A;
extern bool Model0_Port_guessed, Model0_Src_sk;
#endif

static inline __attribute__((no_instrument_function)) struct Model0_sock *Model0___inet_lookup_skb(struct Model0_inet_hashinfo *Model0_hashinfo,
          struct Model0_sk_buff *Model0_skb,
          int Model0_doff,
          const Model0___be16 Model0_sport,
          const Model0___be16 Model0_dport,
          bool *Model0_refcounted)
{
 struct Model0_sock *Model0_sk = Model0_skb_steal_sock(Model0_skb);
 const struct Model0_iphdr *Model0_iph = Model0_ip_hdr(Model0_skb);

 *Model0_refcounted = true;
 if (Model0_sk)
  return Model0_sk;

 return Model0___inet_lookup(Model0_dev_net(Model0_skb_dst(Model0_skb)->Model0_dev), Model0_hashinfo, Model0_skb,
        Model0_doff, Model0_iph->Model0_saddr, Model0_sport,
        Model0_iph->Model0_daddr, Model0_dport, Model0_inet_iif(Model0_skb),
        Model0_refcounted);
}

Model0_u32 Model0_sk_ehashfn(const struct Model0_sock *Model0_sk);
Model0_u32 Model0_inet6_ehashfn(const struct Model0_net *Model0_net,
    const struct Model0_in6_addr *Model0_laddr, const Model0_u16 Model0_lport,
    const struct Model0_in6_addr *Model0_faddr, const Model0___be16 Model0_fport);

static inline __attribute__((no_instrument_function)) void Model0_sk_daddr_set(struct Model0_sock *Model0_sk, Model0___be32 Model0_addr)
{
 Model0_sk->Model0___sk_common.Model0_skc_daddr = Model0_addr; /* alias of inet_daddr */

 Model0_ipv6_addr_set_v4mapped(Model0_addr, &Model0_sk->Model0___sk_common.Model0_skc_v6_daddr);

}

static inline __attribute__((no_instrument_function)) void Model0_sk_rcv_saddr_set(struct Model0_sock *Model0_sk, Model0___be32 Model0_addr)
{
 Model0_sk->Model0___sk_common.Model0_skc_rcv_saddr = Model0_addr; /* alias of inet_rcv_saddr */

 Model0_ipv6_addr_set_v4mapped(Model0_addr, &Model0_sk->Model0___sk_common.Model0_skc_v6_rcv_saddr);

}

int Model0___inet_hash_connect(struct Model0_inet_timewait_death_row *Model0_death_row,
   struct Model0_sock *Model0_sk, Model0_u32 Model0_port_offset,
   int (*Model0_check_established)(struct Model0_inet_timewait_death_row *,
       struct Model0_sock *, Model0___u16,
       struct Model0_inet_timewait_sock **));

int Model0_inet_hash_connect(struct Model0_inet_timewait_death_row *Model0_death_row,
        struct Model0_sock *Model0_sk);




/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the IP module.
 *
 * Version:	@(#)ip.h	1.0.2	05/07/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Alan Cox, <gw4pts@gw4pts.ampr.org>
 *
 * Changes:
 *		Mike McLagan    :       Routing by source
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
struct Model0_sock;

struct Model0_inet_skb_parm {
 int Model0_iif;
 struct Model0_ip_options Model0_opt; /* Compiled IP options		*/
 unsigned char Model0_flags;
 Model0_u16 Model0_frag_max_size;
};

static inline __attribute__((no_instrument_function)) unsigned int Model0_ip_hdrlen(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_ip_hdr(Model0_skb)->Model0_ihl * 4;
}

struct Model0_ipcm_cookie {
 struct Model0_sockcm_cookie Model0_sockc;
 Model0___be32 Model0_addr;
 int Model0_oif;
 struct Model0_ip_options_rcu *Model0_opt;
 __u8 Model0_tx_flags;
 __u8 Model0_ttl;
 Model0___s16 Model0_tos;
 char Model0_priority;
};




struct Model0_ip_ra_chain {
 struct Model0_ip_ra_chain *Model0_next;
 struct Model0_sock *Model0_sk;
 union {
  void (*Model0_destructor)(struct Model0_sock *);
  struct Model0_sock *Model0_saved_sk;
 };
 struct Model0_callback_head Model0_rcu;
};

extern struct Model0_ip_ra_chain *Model0_ip_ra_chain;

/* IP flags. */







struct Model0_msghdr;
struct Model0_net_device;
struct Model0_packet_type;
struct Model0_rtable;
struct Model0_sockaddr;

int Model0_igmp_mc_init(void);

/*
 *	Functions provided by ip.c
 */

int Model0_ip_build_and_send_pkt(struct Model0_sk_buff *Model0_skb, const struct Model0_sock *Model0_sk,
     Model0___be32 Model0_saddr, Model0___be32 Model0_daddr,
     struct Model0_ip_options_rcu *Model0_opt);
int Model0_ip_rcv(struct Model0_sk_buff *Model0_skb, struct Model0_net_device *Model0_dev, struct Model0_packet_type *Model0_pt,
    struct Model0_net_device *Model0_orig_dev);
int Model0_ip_local_deliver(struct Model0_sk_buff *Model0_skb);
int Model0_ip_mr_input(struct Model0_sk_buff *Model0_skb);
int Model0_ip_output(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_ip_mc_output(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_ip_do_fragment(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
     int (*Model0_output)(struct Model0_net *, struct Model0_sock *, struct Model0_sk_buff *));
void Model0_ip_send_check(struct Model0_iphdr *Model0_ip);
int Model0___ip_local_out(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_ip_local_out(struct Model0_net *Model0_net, struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

int Model0_ip_queue_xmit(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, struct Model0_flowi *Model0_fl);
void Model0_ip_init(void);
int Model0_ip_append_data(struct Model0_sock *Model0_sk, struct Model0_flowi4 *Model0_fl4,
     int Model0_getfrag(void *Model0_from, char *Model0_to, int Model0_offset, int Model0_len,
          int Model0_odd, struct Model0_sk_buff *Model0_skb),
     void *Model0_from, int Model0_len, int Model0_protolen,
     struct Model0_ipcm_cookie *Model0_ipc,
     struct Model0_rtable **Model0_rt,
     unsigned int Model0_flags);
int Model0_ip_generic_getfrag(void *Model0_from, char *Model0_to, int Model0_offset, int Model0_len, int Model0_odd,
         struct Model0_sk_buff *Model0_skb);
Model0_ssize_t Model0_ip_append_page(struct Model0_sock *Model0_sk, struct Model0_flowi4 *Model0_fl4, struct Model0_page *Model0_page,
         int Model0_offset, Model0_size_t Model0_size, int Model0_flags);
struct Model0_sk_buff *Model0___ip_make_skb(struct Model0_sock *Model0_sk, struct Model0_flowi4 *Model0_fl4,
         struct Model0_sk_buff_head *Model0_queue,
         struct Model0_inet_cork *Model0_cork);
int Model0_ip_send_skb(struct Model0_net *Model0_net, struct Model0_sk_buff *Model0_skb);
int Model0_ip_push_pending_frames(struct Model0_sock *Model0_sk, struct Model0_flowi4 *Model0_fl4);
void Model0_ip_flush_pending_frames(struct Model0_sock *Model0_sk);
struct Model0_sk_buff *Model0_ip_make_skb(struct Model0_sock *Model0_sk, struct Model0_flowi4 *Model0_fl4,
       int Model0_getfrag(void *Model0_from, char *Model0_to, int Model0_offset,
     int Model0_len, int Model0_odd, struct Model0_sk_buff *Model0_skb),
       void *Model0_from, int Model0_length, int Model0_transhdrlen,
       struct Model0_ipcm_cookie *Model0_ipc, struct Model0_rtable **Model0_rtp,
       unsigned int Model0_flags);

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_ip_finish_skb(struct Model0_sock *Model0_sk, struct Model0_flowi4 *Model0_fl4)
{
 return Model0___ip_make_skb(Model0_sk, Model0_fl4, &Model0_sk->Model0_sk_write_queue, &Model0_inet_sk(Model0_sk)->Model0_cork.Model0_base);
}

static inline __attribute__((no_instrument_function)) __u8 Model0_get_rttos(struct Model0_ipcm_cookie* Model0_ipc, struct Model0_inet_sock *Model0_inet)
{
 return (Model0_ipc->Model0_tos != -1) ? ((Model0_ipc->Model0_tos)&0x1E) : ((Model0_inet->Model0_tos)&0x1E);
}

static inline __attribute__((no_instrument_function)) __u8 Model0_get_rtconn_flags(struct Model0_ipcm_cookie* Model0_ipc, struct Model0_sock* Model0_sk)
{
 return (Model0_ipc->Model0_tos != -1) ? (((Model0_ipc->Model0_tos)&0x1E) | Model0_sock_flag(Model0_sk, Model0_SOCK_LOCALROUTE)) : (((Model0_inet_sk(Model0_sk)->Model0_tos)&0x1E) | Model0_sock_flag(Model0_sk, Model0_SOCK_LOCALROUTE));
}

/* datagram.c */
int Model0___ip4_datagram_connect(struct Model0_sock *Model0_sk, struct Model0_sockaddr *Model0_uaddr, int Model0_addr_len);
int Model0_ip4_datagram_connect(struct Model0_sock *Model0_sk, struct Model0_sockaddr *Model0_uaddr, int Model0_addr_len);

void Model0_ip4_datagram_release_cb(struct Model0_sock *Model0_sk);

struct Model0_ip_reply_arg {
 struct Model0_kvec Model0_iov[1];
 int Model0_flags;
 Model0___wsum Model0_csum;
 int Model0_csumoffset; /* u16 offset of csum in iov[0].iov_base */
    /* -1 if not needed */
 int Model0_bound_dev_if;
 Model0_u8 Model0_tos;
};



static inline __attribute__((no_instrument_function)) __u8 Model0_ip_reply_arg_flowi_flags(const struct Model0_ip_reply_arg *Model0_arg)
{
 return (Model0_arg->Model0_flags & 1) ? 0x01 : 0;
}

void Model0_ip_send_unicast_reply(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
      const struct Model0_ip_options *Model0_sopt,
      Model0___be32 Model0_daddr, Model0___be32 Model0_saddr,
      const struct Model0_ip_reply_arg *Model0_arg,
      unsigned int Model0_len);

extern struct Model0_linux_mib Model0_cy_linux_mib;






//#define NET_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.net_statistics, field)
//#define __NET_INC_STATS(net, field)	__SNMP_INC_STATS((net)->mib.net_statistics, field)
//#define NET_ADD_STATS(net, field, adnd)	SNMP_ADD_STATS((net)->mib.net_statistics, field, adnd)





Model0_u64 Model0_snmp_get_cpu_field(void *Model0_mib, int Model0_cpu, int Model0_offct);
unsigned long Model0_snmp_fold_field(void *Model0_mib, int Model0_offt);





static inline __attribute__((no_instrument_function)) Model0_u64 Model0_snmp_get_cpu_field64(void *Model0_mib, int Model0_cpu, int Model0_offct,
     Model0_size_t Model0_syncp_offset)
{
 return Model0_snmp_get_cpu_field(Model0_mib, Model0_cpu, Model0_offct);

}

static inline __attribute__((no_instrument_function)) Model0_u64 Model0_snmp_fold_field64(void *Model0_mib, int Model0_offt, Model0_size_t Model0_syncp_off)
{
 return Model0_snmp_fold_field(Model0_mib, Model0_offt);
}


void Model0_inet_get_local_port_range(struct Model0_net *Model0_net, int *Model0_low, int *Model0_high);


static inline __attribute__((no_instrument_function)) int Model0_inet_is_local_reserved_port(struct Model0_net *Model0_net, int Model0_port)
{
 if (!Model0_net->Model0_ipv4.Model0_sysctl_local_reserved_ports)
  return 0;
 return (__builtin_constant_p((Model0_port)) ? Model0_constant_test_bit((Model0_port), (Model0_net->Model0_ipv4.Model0_sysctl_local_reserved_ports)) : Model0_variable_test_bit((Model0_port), (Model0_net->Model0_ipv4.Model0_sysctl_local_reserved_ports)));
}

static inline __attribute__((no_instrument_function)) bool Model0_sysctl_dev_name_is_allowed(const char *Model0_name)
{
 return Model0_strcmp(Model0_name, "default") != 0 && Model0_strcmp(Model0_name, "all") != 0;
}
Model0___be32 Model0_inet_current_timestamp(void);

/* From inetpeer.c */
extern int Model0_inet_peer_threshold;
extern int Model0_inet_peer_minttl;
extern int Model0_inet_peer_maxttl;

void Model0_ipfrag_init(void);

void Model0_ip_static_sysctl_init(void);




static inline __attribute__((no_instrument_function)) bool Model0_ip_is_fragment(const struct Model0_iphdr *Model0_iph)
{
 return (Model0_iph->Model0_frag_off & (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x2000 | 0x1FFF))) ? ((Model0___u16)( (((Model0___u16)((0x2000 | 0x1FFF)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x2000 | 0x1FFF)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x2000 | 0x1FFF))))) != 0;
}




/* The function in 2.2 was invalid, producing wrong result for
 * check=0xFEFF. It was noticed by Arthur Skawina _year_ ago. --ANK(000625) */
static inline __attribute__((no_instrument_function))
int Model0_ip_decrease_ttl(struct Model0_iphdr *Model0_iph)
{
 Model0_u32 Model0_check = ( Model0_u32)Model0_iph->Model0_check;
 Model0_check += ( Model0_u32)(( Model0___be16)(__builtin_constant_p((Model0___u16)((0x0100))) ? ((Model0___u16)( (((Model0___u16)((0x0100)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x0100)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x0100))));
 Model0_iph->Model0_check = ( Model0___sum16)(Model0_check + (Model0_check>=0xFFFF));
 return --Model0_iph->Model0_ttl;
}

static inline __attribute__((no_instrument_function))
int Model0_ip_dont_fragment(const struct Model0_sock *Model0_sk, const struct Model0_dst_entry *Model0_dst)
{
 Model0_u8 Model0_pmtudisc = ({ union { typeof(Model0_inet_sk(Model0_sk)->Model0_pmtudisc) Model0___val; char Model0___c[1]; } Model0___u; if (1) Model0___read_once_size(&(Model0_inet_sk(Model0_sk)->Model0_pmtudisc), Model0___u.Model0___c, sizeof(Model0_inet_sk(Model0_sk)->Model0_pmtudisc)); else Model0___read_once_size_nocheck(&(Model0_inet_sk(Model0_sk)->Model0_pmtudisc), Model0___u.Model0___c, sizeof(Model0_inet_sk(Model0_sk)->Model0_pmtudisc)); Model0___u.Model0___val; });

 return Model0_pmtudisc == 2 ||
  (Model0_pmtudisc == 1 &&
   !(Model0_dst_metric_locked(Model0_dst, Model0_RTAX_MTU)));
}

static inline __attribute__((no_instrument_function)) bool Model0_ip_sk_accept_pmtu(const struct Model0_sock *Model0_sk)
{
 return Model0_inet_sk(Model0_sk)->Model0_pmtudisc != 4 &&
        Model0_inet_sk(Model0_sk)->Model0_pmtudisc != 5;
}

static inline __attribute__((no_instrument_function)) bool Model0_ip_sk_use_pmtu(const struct Model0_sock *Model0_sk)
{
 return Model0_inet_sk(Model0_sk)->Model0_pmtudisc < 3;
}

static inline __attribute__((no_instrument_function)) bool Model0_ip_sk_ignore_df(const struct Model0_sock *Model0_sk)
{
 return Model0_inet_sk(Model0_sk)->Model0_pmtudisc < 2 ||
        Model0_inet_sk(Model0_sk)->Model0_pmtudisc == 5;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_ip_dst_mtu_maybe_forward(const struct Model0_dst_entry *Model0_dst,
          bool Model0_forwarding)
{
 struct Model0_net *Model0_net = Model0_dev_net(Model0_dst->Model0_dev);

 if (Model0_net->Model0_ipv4.Model0_sysctl_ip_fwd_use_pmtu ||
     Model0_dst_metric_locked(Model0_dst, Model0_RTAX_MTU) ||
     !Model0_forwarding)
  return Model0_dst_mtu(Model0_dst);

 return ({ typeof(Model0_dst->Model0_dev->Model0_mtu) Model0__min1 = (Model0_dst->Model0_dev->Model0_mtu); typeof(0xFFFFU) Model0__min2 = (0xFFFFU); (void) (&Model0__min1 == &Model0__min2); Model0__min1 < Model0__min2 ? Model0__min1 : Model0__min2; });
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_ip_skb_dst_mtu(struct Model0_sock *Model0_sk,
       const struct Model0_sk_buff *Model0_skb)
{
 if (!Model0_sk || !Model0_sk_fullsock(Model0_sk) || Model0_ip_sk_use_pmtu(Model0_sk)) {
  bool Model0_forwarding = ((struct Model0_inet_skb_parm*)((Model0_skb)->Model0_cb))->Model0_flags & (1UL << (0));

  return Model0_ip_dst_mtu_maybe_forward(Model0_skb_dst(Model0_skb), Model0_forwarding);
 }

 return ({ typeof(Model0_skb_dst(Model0_skb)->Model0_dev->Model0_mtu) Model0__min1 = (Model0_skb_dst(Model0_skb)->Model0_dev->Model0_mtu); typeof(0xFFFFU) Model0__min2 = (0xFFFFU); (void) (&Model0__min1 == &Model0__min2); Model0__min1 < Model0__min2 ? Model0__min1 : Model0__min2; });
}

Model0_u32 Model0_ip_idents_reserve(Model0_u32 Model0_hash, int Model0_segs);
void Model0___ip_select_ident(struct Model0_net *Model0_net, struct Model0_iphdr *Model0_iph, int Model0_segs);

static inline __attribute__((no_instrument_function)) void Model0_ip_select_ident_segs(struct Model0_net *Model0_net, struct Model0_sk_buff *Model0_skb,
     struct Model0_sock *Model0_sk, int Model0_segs)
{
 struct Model0_iphdr *Model0_iph = Model0_ip_hdr(Model0_skb);

 if ((Model0_iph->Model0_frag_off & (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x4000))) ? ((Model0___u16)( (((Model0___u16)((0x4000)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x4000)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x4000))))) && !Model0_skb->Model0_ignore_df) {
  /* This is only to work around buggy Windows95/2000
		 * VJ compression implementations.  If the ID field
		 * does not change, they drop every other packet in
		 * a TCP stream using header compression.
		 */
  if (Model0_sk && Model0_inet_sk(Model0_sk)->Model0_sk.Model0___sk_common.Model0_skc_daddr) {
   Model0_iph->Model0_id = (( Model0___be16)(__builtin_constant_p((Model0___u16)((Model0_inet_sk(Model0_sk)->Model0_inet_id))) ? ((Model0___u16)( (((Model0___u16)((Model0_inet_sk(Model0_sk)->Model0_inet_id)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((Model0_inet_sk(Model0_sk)->Model0_inet_id)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((Model0_inet_sk(Model0_sk)->Model0_inet_id))));
   Model0_inet_sk(Model0_sk)->Model0_inet_id += Model0_segs;
  } else {
   Model0_iph->Model0_id = 0;
  }
 } else {
  Model0___ip_select_ident(Model0_net, Model0_iph, Model0_segs);
 }
}

static inline __attribute__((no_instrument_function)) void Model0_ip_select_ident(struct Model0_net *Model0_net, struct Model0_sk_buff *Model0_skb,
       struct Model0_sock *Model0_sk)
{
 Model0_ip_select_ident_segs(Model0_net, Model0_skb, Model0_sk, 1);
}

static inline __attribute__((no_instrument_function)) Model0___wsum Model0_inet_compute_pseudo(struct Model0_sk_buff *Model0_skb, int Model0_proto)
{
 return Model0_csum_tcpudp_nofold(Model0_ip_hdr(Model0_skb)->Model0_saddr, Model0_ip_hdr(Model0_skb)->Model0_daddr,
      Model0_skb->Model0_len, Model0_proto, 0);
}

/* copy IPv4 saddr & daddr to flow_keys, possibly using 64bit load/store
 * Equivalent to :	flow->v4addrs.src = iph->saddr;
 *			flow->v4addrs.dst = iph->daddr;
 */
static inline __attribute__((no_instrument_function)) void Model0_iph_to_flow_copy_v4addrs(struct Model0_flow_keys *Model0_flow,
         const struct Model0_iphdr *Model0_iph)
{
 do { bool Model0___cond = !(!(__builtin_offsetof(typeof(Model0_flow->Model0_addrs), Model0_v4addrs.Model0_dst) != __builtin_offsetof(typeof(Model0_flow->Model0_addrs), Model0_v4addrs.Model0_src) + sizeof(Model0_flow->Model0_addrs.Model0_v4addrs.Model0_src))); extern void Model0___compiletime_assert_379(void) ; if (Model0___cond) Model0___compiletime_assert_379(); do { ((void)sizeof(char[1 - 2 * Model0___cond])); } while (0); } while (0);


 ({ Model0_size_t Model0___len = (sizeof(Model0_flow->Model0_addrs.Model0_v4addrs)); void *Model0___ret; if (__builtin_constant_p(sizeof(Model0_flow->Model0_addrs.Model0_v4addrs)) && Model0___len >= 64) Model0___ret = Model0___memcpy((&Model0_flow->Model0_addrs.Model0_v4addrs), (&Model0_iph->Model0_saddr), Model0___len); else Model0___ret = __builtin_memcpy((&Model0_flow->Model0_addrs.Model0_v4addrs), (&Model0_iph->Model0_saddr), Model0___len); Model0___ret; });
 Model0_flow->Model0_control.Model0_addr_type = Model0_FLOW_DISSECTOR_KEY_IPV4_ADDRS;
}

static inline __attribute__((no_instrument_function)) Model0___wsum Model0_inet_gro_compute_pseudo(struct Model0_sk_buff *Model0_skb, int Model0_proto)
{
 const struct Model0_iphdr *Model0_iph = Model0_skb_gro_network_header(Model0_skb);

 return Model0_csum_tcpudp_nofold(Model0_iph->Model0_saddr, Model0_iph->Model0_daddr,
      Model0_skb_gro_len(Model0_skb), Model0_proto, 0);
}

/*
 *	Map a multicast IP onto multicast MAC for type ethernet.
 */

static inline __attribute__((no_instrument_function)) void Model0_ip_eth_mc_map(Model0___be32 Model0_naddr, char *Model0_buf)
{
 __u32 Model0_addr=(__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_naddr))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_naddr)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_naddr)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_naddr)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_naddr)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_naddr)));
 Model0_buf[0]=0x01;
 Model0_buf[1]=0x00;
 Model0_buf[2]=0x5e;
 Model0_buf[5]=Model0_addr&0xFF;
 Model0_addr>>=8;
 Model0_buf[4]=Model0_addr&0xFF;
 Model0_addr>>=8;
 Model0_buf[3]=Model0_addr&0x7F;
}

/*
 *	Map a multicast IP onto multicast MAC for type IP-over-InfiniBand.
 *	Leave P_Key as 0 to be filled in by driver.
 */

static inline __attribute__((no_instrument_function)) void Model0_ip_ib_mc_map(Model0___be32 Model0_naddr, const unsigned char *Model0_broadcast, char *Model0_buf)
{
 __u32 Model0_addr;
 unsigned char Model0_scope = Model0_broadcast[5] & 0xF;

 Model0_buf[0] = 0; /* Reserved */
 Model0_buf[1] = 0xff; /* Multicast QPN */
 Model0_buf[2] = 0xff;
 Model0_buf[3] = 0xff;
 Model0_addr = (__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_naddr))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_naddr)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_naddr)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_naddr)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_naddr)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_naddr)));
 Model0_buf[4] = 0xff;
 Model0_buf[5] = 0x10 | Model0_scope; /* scope from broadcast address */
 Model0_buf[6] = 0x40; /* IPv4 signature */
 Model0_buf[7] = 0x1b;
 Model0_buf[8] = Model0_broadcast[8]; /* P_Key */
 Model0_buf[9] = Model0_broadcast[9];
 Model0_buf[10] = 0;
 Model0_buf[11] = 0;
 Model0_buf[12] = 0;
 Model0_buf[13] = 0;
 Model0_buf[14] = 0;
 Model0_buf[15] = 0;
 Model0_buf[19] = Model0_addr & 0xff;
 Model0_addr >>= 8;
 Model0_buf[18] = Model0_addr & 0xff;
 Model0_addr >>= 8;
 Model0_buf[17] = Model0_addr & 0xff;
 Model0_addr >>= 8;
 Model0_buf[16] = Model0_addr & 0x0f;
}

static inline __attribute__((no_instrument_function)) void Model0_ip_ipgre_mc_map(Model0___be32 Model0_naddr, const unsigned char *Model0_broadcast, char *Model0_buf)
{
 if ((Model0_broadcast[0] | Model0_broadcast[1] | Model0_broadcast[2] | Model0_broadcast[3]) != 0)
  ({ Model0_size_t Model0___len = (4); void *Model0___ret; if (__builtin_constant_p(4) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_buf), (Model0_broadcast), Model0___len); else Model0___ret = __builtin_memcpy((Model0_buf), (Model0_broadcast), Model0___len); Model0___ret; });
 else
  ({ Model0_size_t Model0___len = (sizeof(Model0_naddr)); void *Model0___ret; if (__builtin_constant_p(sizeof(Model0_naddr)) && Model0___len >= 64) Model0___ret = Model0___memcpy((Model0_buf), (&Model0_naddr), Model0___len); else Model0___ret = __builtin_memcpy((Model0_buf), (&Model0_naddr), Model0___len); Model0___ret; });
}





static __inline__ __attribute__((no_instrument_function)) void Model0_inet_reset_saddr(struct Model0_sock *Model0_sk)
{
 Model0_inet_sk(Model0_sk)->Model0_sk.Model0___sk_common.Model0_skc_rcv_saddr = Model0_inet_sk(Model0_sk)->Model0_inet_saddr = 0;

 if (Model0_sk->Model0___sk_common.Model0_skc_family == 10) {
  struct Model0_ipv6_pinfo *Model0_np = Model0_inet6_sk(Model0_sk);

  memset(&Model0_np->Model0_saddr, 0, sizeof(Model0_np->Model0_saddr));
  memset(&Model0_sk->Model0___sk_common.Model0_skc_v6_rcv_saddr, 0, sizeof(Model0_sk->Model0___sk_common.Model0_skc_v6_rcv_saddr));
 }

}



static inline __attribute__((no_instrument_function)) unsigned int Model0_ipv4_addr_hash(Model0___be32 Model0_ip)
{
 return ( unsigned int) Model0_ip;
}

bool Model0_ip_call_ra_chain(struct Model0_sk_buff *Model0_skb);

/*
 *	Functions provided by ip_fragment.c
 */

enum Model0_ip_defrag_users {
 Model0_IP_DEFRAG_LOCAL_DELIVER,
 Model0_IP_DEFRAG_CALL_RA_CHAIN,
 Model0_IP_DEFRAG_CONNTRACK_IN,
 Model0___IP_DEFRAG_CONNTRACK_IN_END = Model0_IP_DEFRAG_CONNTRACK_IN + ((Model0_u16)(~0U)),
 Model0_IP_DEFRAG_CONNTRACK_OUT,
 Model0___IP_DEFRAG_CONNTRACK_OUT_END = Model0_IP_DEFRAG_CONNTRACK_OUT + ((Model0_u16)(~0U)),
 Model0_IP_DEFRAG_CONNTRACK_BRIDGE_IN,
 Model0___IP_DEFRAG_CONNTRACK_BRIDGE_IN = Model0_IP_DEFRAG_CONNTRACK_BRIDGE_IN + ((Model0_u16)(~0U)),
 Model0_IP_DEFRAG_VS_IN,
 Model0_IP_DEFRAG_VS_OUT,
 Model0_IP_DEFRAG_VS_FWD,
 Model0_IP_DEFRAG_AF_PACKET,
 Model0_IP_DEFRAG_MACVLAN,
};

/* Return true if the value of 'user' is between 'lower_bond'
 * and 'upper_bond' inclusively.
 */
static inline __attribute__((no_instrument_function)) bool Model0_ip_defrag_user_in_between(Model0_u32 Model0_user,
          enum Model0_ip_defrag_users Model0_lower_bond,
          enum Model0_ip_defrag_users Model0_upper_bond)
{
 return Model0_user >= Model0_lower_bond && Model0_user <= Model0_upper_bond;
}

int Model0_ip_defrag(struct Model0_net *Model0_net, struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_user);

struct Model0_sk_buff *Model0_ip_check_defrag(struct Model0_net *Model0_net, struct Model0_sk_buff *Model0_skb, Model0_u32 Model0_user);






int Model0_ip_frag_mem(struct Model0_net *Model0_net);

/*
 *	Functions provided by ip_forward.c
 */

int Model0_ip_forward(struct Model0_sk_buff *Model0_skb);

/*
 *	Functions provided by ip_options.c
 */

void Model0_ip_options_build(struct Model0_sk_buff *Model0_skb, struct Model0_ip_options *Model0_opt,
        Model0___be32 Model0_daddr, struct Model0_rtable *Model0_rt, int Model0_is_frag);

int Model0___ip_options_echo(struct Model0_ip_options *Model0_dopt, struct Model0_sk_buff *Model0_skb,
        const struct Model0_ip_options *Model0_sopt);
static inline __attribute__((no_instrument_function)) int Model0_ip_options_echo(struct Model0_ip_options *Model0_dopt, struct Model0_sk_buff *Model0_skb)
{
 return Model0___ip_options_echo(Model0_dopt, Model0_skb, &((struct Model0_inet_skb_parm*)((Model0_skb)->Model0_cb))->Model0_opt);
}

void Model0_ip_options_fragment(struct Model0_sk_buff *Model0_skb);
int Model0_ip_options_compile(struct Model0_net *Model0_net, struct Model0_ip_options *Model0_opt,
         struct Model0_sk_buff *Model0_skb);
int Model0_ip_options_get(struct Model0_net *Model0_net, struct Model0_ip_options_rcu **Model0_optp,
     unsigned char *Model0_data, int Model0_optlen);
int Model0_ip_options_get_from_user(struct Model0_net *Model0_net, struct Model0_ip_options_rcu **Model0_optp,
        unsigned char *Model0_data, int Model0_optlen);
void Model0_ip_options_undo(struct Model0_ip_options *Model0_opt);
void Model0_ip_forward_options(struct Model0_sk_buff *Model0_skb);
int Model0_ip_options_rcv_srr(struct Model0_sk_buff *Model0_skb);

/*
 *	Functions provided by ip_sockglue.c
 */

void Model0_ipv4_pktinfo_prepare(const struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
void Model0_ip_cmsg_recv_offset(struct Model0_msghdr *Model0_msg, struct Model0_sk_buff *Model0_skb, int Model0_offset);
int Model0_ip_cmsg_send(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg,
   struct Model0_ipcm_cookie *Model0_ipc, bool Model0_allow_ipv6);
int Model0_ip_setsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname, char *Model0_optval,
    unsigned int Model0_optlen);
int Model0_ip_getsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname, char *Model0_optval,
    int *Model0_optlen);
int Model0_compat_ip_setsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
    char *Model0_optval, unsigned int Model0_optlen);
int Model0_compat_ip_getsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
    char *Model0_optval, int *Model0_optlen);
int Model0_ip_ra_control(struct Model0_sock *Model0_sk, unsigned char Model0_on,
    void (*Model0_destructor)(struct Model0_sock *));

int Model0_ip_recv_error(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, int Model0_len, int *Model0_addr_len);
void Model0_ip_icmp_error(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, int err, Model0___be16 Model0_port,
     Model0_u32 Model0_info, Model0_u8 *Model0_payload);
void Model0_ip_local_error(struct Model0_sock *Model0_sk, int err, Model0___be32 Model0_daddr, Model0___be16 Model0_dport,
      Model0_u32 Model0_info);

static inline __attribute__((no_instrument_function)) void Model0_ip_cmsg_recv(struct Model0_msghdr *Model0_msg, struct Model0_sk_buff *Model0_skb)
{
 Model0_ip_cmsg_recv_offset(Model0_msg, Model0_skb, 0);
}

bool Model0_icmp_global_allow(void);
extern int Model0_sysctl_icmp_msgs_per_sec;
extern int Model0_sysctl_icmp_msgs_burst;


int Model0_ip_misc_proc_init(void);








/* include/net/dsfield.h - Manipulation of the Differentiated Services field */

/* Written 1998-2000 by Werner Almesberger, EPFL ICA */
static inline __attribute__((no_instrument_function)) __u8 Model0_ipv4_get_dsfield(const struct Model0_iphdr *Model0_iph)
{
 return Model0_iph->Model0_tos;
}


static inline __attribute__((no_instrument_function)) __u8 Model0_ipv6_get_dsfield(const struct Model0_ipv6hdr *Model0_ipv6h)
{
 return (__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(*(const Model0___be16 *)Model0_ipv6h))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(*(const Model0___be16 *)Model0_ipv6h)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(*(const Model0___be16 *)Model0_ipv6h)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(*(const Model0___be16 *)Model0_ipv6h))) >> 4;
}


static inline __attribute__((no_instrument_function)) void Model0_ipv4_change_dsfield(struct Model0_iphdr *Model0_iph,__u8 Model0_mask,
    __u8 Model0_value)
{
        __u32 Model0_check = (__builtin_constant_p((Model0___u16)(( Model0___u16)(Model0___be16)(( Model0___be16)Model0_iph->Model0_check))) ? ((Model0___u16)( (((Model0___u16)(( Model0___u16)(Model0___be16)(( Model0___be16)Model0_iph->Model0_check)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(( Model0___u16)(Model0___be16)(( Model0___be16)Model0_iph->Model0_check)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(( Model0___u16)(Model0___be16)(( Model0___be16)Model0_iph->Model0_check)));
 __u8 Model0_dsfield;

 Model0_dsfield = (Model0_iph->Model0_tos & Model0_mask) | Model0_value;
 Model0_check += Model0_iph->Model0_tos;
 if ((Model0_check+1) >> 16) Model0_check = (Model0_check+1) & 0xffff;
 Model0_check -= Model0_dsfield;
 Model0_check += Model0_check >> 16; /* adjust carry */
 Model0_iph->Model0_check = ( Model0___sum16)(( Model0___be16)(__builtin_constant_p((Model0___u16)((Model0_check))) ? ((Model0___u16)( (((Model0___u16)((Model0_check)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((Model0_check)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((Model0_check))));
 Model0_iph->Model0_tos = Model0_dsfield;
}


static inline __attribute__((no_instrument_function)) void Model0_ipv6_change_dsfield(struct Model0_ipv6hdr *Model0_ipv6h,__u8 Model0_mask,
    __u8 Model0_value)
{
 Model0___be16 *Model0_p = ( Model0___be16 *)Model0_ipv6h;

 *Model0_p = (*Model0_p & (( Model0___be16)(__builtin_constant_p((Model0___u16)(((((Model0_u16)Model0_mask << 4) | 0xf00f)))) ? ((Model0___u16)( (((Model0___u16)(((((Model0_u16)Model0_mask << 4) | 0xf00f))) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(((((Model0_u16)Model0_mask << 4) | 0xf00f))) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(((((Model0_u16)Model0_mask << 4) | 0xf00f)))))) | (( Model0___be16)(__builtin_constant_p((Model0___u16)(((Model0_u16)Model0_value << 4))) ? ((Model0___u16)( (((Model0___u16)(((Model0_u16)Model0_value << 4)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)(((Model0_u16)Model0_value << 4)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16(((Model0_u16)Model0_value << 4))));
}

enum {
 Model0_INET_ECN_NOT_ECT = 0,
 Model0_INET_ECN_ECT_1 = 1,
 Model0_INET_ECN_ECT_0 = 2,
 Model0_INET_ECN_CE = 3,
 Model0_INET_ECN_MASK = 3,
};

extern int Model0_sysctl_tunnel_ecn_log;

static inline __attribute__((no_instrument_function)) int Model0_INET_ECN_is_ce(__u8 Model0_dsfield)
{
 return (Model0_dsfield & Model0_INET_ECN_MASK) == Model0_INET_ECN_CE;
}

static inline __attribute__((no_instrument_function)) int Model0_INET_ECN_is_not_ect(__u8 Model0_dsfield)
{
 return (Model0_dsfield & Model0_INET_ECN_MASK) == Model0_INET_ECN_NOT_ECT;
}

static inline __attribute__((no_instrument_function)) int Model0_INET_ECN_is_capable(__u8 Model0_dsfield)
{
 return Model0_dsfield & Model0_INET_ECN_ECT_0;
}

/*
 * RFC 3168 9.1.1
 *  The full-functionality option for ECN encapsulation is to copy the
 *  ECN codepoint of the inside header to the outside header on
 *  encapsulation if the inside header is not-ECT or ECT, and to set the
 *  ECN codepoint of the outside header to ECT(0) if the ECN codepoint of
 *  the inside header is CE.
 */
static inline __attribute__((no_instrument_function)) __u8 Model0_INET_ECN_encapsulate(__u8 Model0_outer, __u8 Model0_inner)
{
 Model0_outer &= ~Model0_INET_ECN_MASK;
 Model0_outer |= !Model0_INET_ECN_is_ce(Model0_inner) ? (Model0_inner & Model0_INET_ECN_MASK) :
       Model0_INET_ECN_ECT_0;
 return Model0_outer;
}

static inline __attribute__((no_instrument_function)) void Model0_INET_ECN_xmit(struct Model0_sock *Model0_sk)
{
 Model0_inet_sk(Model0_sk)->Model0_tos |= Model0_INET_ECN_ECT_0;
 if (Model0_inet6_sk(Model0_sk) != ((void *)0))
  Model0_inet6_sk(Model0_sk)->Model0_tclass |= Model0_INET_ECN_ECT_0;
}

static inline __attribute__((no_instrument_function)) void Model0_INET_ECN_dontxmit(struct Model0_sock *Model0_sk)
{
 Model0_inet_sk(Model0_sk)->Model0_tos &= ~Model0_INET_ECN_MASK;
 if (Model0_inet6_sk(Model0_sk) != ((void *)0))
  Model0_inet6_sk(Model0_sk)->Model0_tclass &= ~Model0_INET_ECN_MASK;
}
static inline __attribute__((no_instrument_function)) int Model0_IP_ECN_set_ce(struct Model0_iphdr *Model0_iph)
{
 Model0_u32 Model0_check = ( Model0_u32)Model0_iph->Model0_check;
 Model0_u32 Model0_ecn = (Model0_iph->Model0_tos + 1) & Model0_INET_ECN_MASK;

 /*
	 * After the last operation we have (in binary):
	 * INET_ECN_NOT_ECT => 01
	 * INET_ECN_ECT_1   => 10
	 * INET_ECN_ECT_0   => 11
	 * INET_ECN_CE      => 00
	 */
 if (!(Model0_ecn & 2))
  return !Model0_ecn;

 /*
	 * The following gives us:
	 * INET_ECN_ECT_1 => check += htons(0xFFFD)
	 * INET_ECN_ECT_0 => check += htons(0xFFFE)
	 */
 Model0_check += ( Model0_u16)(( Model0___be16)(__builtin_constant_p((Model0___u16)((0xFFFB))) ? ((Model0___u16)( (((Model0___u16)((0xFFFB)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0xFFFB)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0xFFFB)))) + ( Model0_u16)(( Model0___be16)(__builtin_constant_p((Model0___u16)((Model0_ecn))) ? ((Model0___u16)( (((Model0___u16)((Model0_ecn)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((Model0_ecn)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((Model0_ecn))));

 Model0_iph->Model0_check = ( Model0___sum16)(Model0_check + (Model0_check>=0xFFFF));
 Model0_iph->Model0_tos |= Model0_INET_ECN_CE;
 return 1;
}

static inline __attribute__((no_instrument_function)) void Model0_IP_ECN_clear(struct Model0_iphdr *Model0_iph)
{
 Model0_iph->Model0_tos &= ~Model0_INET_ECN_MASK;
}

static inline __attribute__((no_instrument_function)) void Model0_ipv4_copy_dscp(unsigned int Model0_dscp, struct Model0_iphdr *Model0_inner)
{
 Model0_dscp &= ~Model0_INET_ECN_MASK;
 Model0_ipv4_change_dsfield(Model0_inner, Model0_INET_ECN_MASK, Model0_dscp);
}

struct Model0_ipv6hdr;

/* Note:
 * IP_ECN_set_ce() has to tweak IPV4 checksum when setting CE,
 * meaning both changes have no effect on skb->csum if/when CHECKSUM_COMPLETE
 * In IPv6 case, no checksum compensates the change in IPv6 header,
 * so we have to update skb->csum.
 */
static inline __attribute__((no_instrument_function)) int Model0_IP6_ECN_set_ce(struct Model0_sk_buff *Model0_skb, struct Model0_ipv6hdr *Model0_iph)
{
 Model0___be32 Model0_from, Model0_to;

 if (Model0_INET_ECN_is_not_ect(Model0_ipv6_get_dsfield(Model0_iph)))
  return 0;

 Model0_from = *(Model0___be32 *)Model0_iph;
 Model0_to = Model0_from | (( Model0___be32)(__builtin_constant_p((__u32)((Model0_INET_ECN_CE << 20))) ? ((__u32)( (((__u32)((Model0_INET_ECN_CE << 20)) & (__u32)0x000000ffUL) << 24) | (((__u32)((Model0_INET_ECN_CE << 20)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((Model0_INET_ECN_CE << 20)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((Model0_INET_ECN_CE << 20)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((Model0_INET_ECN_CE << 20))));
 *(Model0___be32 *)Model0_iph = Model0_to;
 if (Model0_skb->Model0_ip_summed == 2)
  Model0_skb->Model0_csum = Model0_csum_add(Model0_csum_sub(Model0_skb->Model0_csum, ( Model0___wsum)Model0_from),
         ( Model0___wsum)Model0_to);
 return 1;
}

static inline __attribute__((no_instrument_function)) void Model0_IP6_ECN_clear(struct Model0_ipv6hdr *Model0_iph)
{
 *(Model0___be32*)Model0_iph &= ~(( Model0___be32)(__builtin_constant_p((__u32)((Model0_INET_ECN_MASK << 20))) ? ((__u32)( (((__u32)((Model0_INET_ECN_MASK << 20)) & (__u32)0x000000ffUL) << 24) | (((__u32)((Model0_INET_ECN_MASK << 20)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((Model0_INET_ECN_MASK << 20)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((Model0_INET_ECN_MASK << 20)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((Model0_INET_ECN_MASK << 20))));
}

static inline __attribute__((no_instrument_function)) void Model0_ipv6_copy_dscp(unsigned int Model0_dscp, struct Model0_ipv6hdr *Model0_inner)
{
 Model0_dscp &= ~Model0_INET_ECN_MASK;
 Model0_ipv6_change_dsfield(Model0_inner, Model0_INET_ECN_MASK, Model0_dscp);
}

static inline __attribute__((no_instrument_function)) int Model0_INET_ECN_set_ce(struct Model0_sk_buff *Model0_skb)
{
 switch (Model0_skb->Model0_protocol) {
 case (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x0800))) ? ((Model0___u16)( (((Model0___u16)((0x0800)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x0800)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x0800)))):
  if (Model0_skb_network_header(Model0_skb) + sizeof(struct Model0_iphdr) <=
      Model0_skb_tail_pointer(Model0_skb))
   return Model0_IP_ECN_set_ce(Model0_ip_hdr(Model0_skb));
  break;

 case (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x86DD))) ? ((Model0___u16)( (((Model0___u16)((0x86DD)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x86DD)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x86DD)))):
  if (Model0_skb_network_header(Model0_skb) + sizeof(struct Model0_ipv6hdr) <=
      Model0_skb_tail_pointer(Model0_skb))
   return Model0_IP6_ECN_set_ce(Model0_skb, Model0_ipv6_hdr(Model0_skb));
  break;
 }

 return 0;
}

/*
 * RFC 6040 4.2
 *  To decapsulate the inner header at the tunnel egress, a compliant
 *  tunnel egress MUST set the outgoing ECN field to the codepoint at the
 *  intersection of the appropriate arriving inner header (row) and outer
 *  header (column) in Figure 4
 *
 *      +---------+------------------------------------------------+
 *      |Arriving |            Arriving Outer Header               |
 *      |   Inner +---------+------------+------------+------------+
 *      |  Header | Not-ECT | ECT(0)     | ECT(1)     |     CE     |
 *      +---------+---------+------------+------------+------------+
 *      | Not-ECT | Not-ECT |Not-ECT(!!!)|Not-ECT(!!!)| <drop>(!!!)|
 *      |  ECT(0) |  ECT(0) | ECT(0)     | ECT(1)     |     CE     |
 *      |  ECT(1) |  ECT(1) | ECT(1) (!) | ECT(1)     |     CE     |
 *      |    CE   |      CE |     CE     |     CE(!!!)|     CE     |
 *      +---------+---------+------------+------------+------------+
 *
 *             Figure 4: New IP in IP Decapsulation Behaviour
 *
 *  returns 0 on success
 *          1 if something is broken and should be logged (!!! above)
 *          2 if packet should be dropped
 */
static inline __attribute__((no_instrument_function)) int Model0_INET_ECN_decapsulate(struct Model0_sk_buff *Model0_skb,
           __u8 Model0_outer, __u8 Model0_inner)
{
 if (Model0_INET_ECN_is_not_ect(Model0_inner)) {
  switch (Model0_outer & Model0_INET_ECN_MASK) {
  case Model0_INET_ECN_NOT_ECT:
   return 0;
  case Model0_INET_ECN_ECT_0:
  case Model0_INET_ECN_ECT_1:
   return 1;
  case Model0_INET_ECN_CE:
   return 2;
  }
 }

 if (Model0_INET_ECN_is_ce(Model0_outer))
  Model0_INET_ECN_set_ce(Model0_skb);

 return 0;
}

static inline __attribute__((no_instrument_function)) int Model0_IP_ECN_decapsulate(const struct Model0_iphdr *Model0_oiph,
         struct Model0_sk_buff *Model0_skb)
{
 __u8 Model0_inner;

 if (Model0_skb->Model0_protocol == (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x0800))) ? ((Model0___u16)( (((Model0___u16)((0x0800)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x0800)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x0800)))))
  Model0_inner = Model0_ip_hdr(Model0_skb)->Model0_tos;
 else if (Model0_skb->Model0_protocol == (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x86DD))) ? ((Model0___u16)( (((Model0___u16)((0x86DD)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x86DD)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x86DD)))))
  Model0_inner = Model0_ipv6_get_dsfield(Model0_ipv6_hdr(Model0_skb));
 else
  return 0;

 return Model0_INET_ECN_decapsulate(Model0_skb, Model0_oiph->Model0_tos, Model0_inner);
}

static inline __attribute__((no_instrument_function)) int Model0_IP6_ECN_decapsulate(const struct Model0_ipv6hdr *Model0_oipv6h,
          struct Model0_sk_buff *Model0_skb)
{
 __u8 Model0_inner;

 if (Model0_skb->Model0_protocol == (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x0800))) ? ((Model0___u16)( (((Model0___u16)((0x0800)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x0800)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x0800)))))
  Model0_inner = Model0_ip_hdr(Model0_skb)->Model0_tos;
 else if (Model0_skb->Model0_protocol == (( Model0___be16)(__builtin_constant_p((Model0___u16)((0x86DD))) ? ((Model0___u16)( (((Model0___u16)((0x86DD)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((0x86DD)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((0x86DD)))))
  Model0_inner = Model0_ipv6_get_dsfield(Model0_ipv6_hdr(Model0_skb));
 else
  return 0;

 return Model0_INET_ECN_decapsulate(Model0_skb, Model0_ipv6_get_dsfield(Model0_oipv6h), Model0_inner);
}





extern struct Model0_inet_hashinfo Model0_tcp_hashinfo;

extern struct Model0_percpu_counter Model0_tcp_orphan_count;
void Model0_tcp_time_wait(struct Model0_sock *Model0_sk, int Model0_state, int Model0_timeo);




/*
 * Never offer a window over 32767 without using window scaling. Some
 * poor stacks do signed 16bit maths!
 */


/* Minimal accepted MSS. It is (60+60+8) - (20+20). */


/* The least MTU to use for probing */


/* probing interval, default to 10 minutes as per RFC4821 */


/* Specify interval when tcp mtu probing will stop */


/* After receiving this amount of duplicate ACKs fast retransmit starts. */


/* Maximal number of ACKs sent quickly to accelerate slow-start. */


/* urg_data states */
                                 /* BSD style FIN_WAIT2 deadlock breaker.
				  * It used to be 3min, new value is 60sec,
				  * to combine FIN-WAIT-2 timeout with
				  * TIME-WAIT timer.
				  */
/*
 *	TCP option
 */
/* Magic number to be after the option value for sharing TCP
 * experimental options. See draft-ietf-tcpm-experimental-options-00.txt
 */


/*
 *     TCP option lengths
 */
/* But this is what stacks really send out. */
/* Flags in tp->nonagle */




/* TCP thin-stream limits */


/* TCP initial congestion window as per rfc6928 */


/* Bit Flags for sysctl_tcp_fastopen */




/* Accept SYN data w/o any cookie option */


/* Force enable TFO on all listeners, i.e., not requiring the
 * TCP_FASTOPEN socket option. SOCKOPT1/2 determine how to set max_qlen.
 */



extern struct Model0_inet_timewait_death_row Model0_tcp_death_row;

/* sysctl variables for tcp */
extern int Model0_sysctl_tcp_timestamps;
extern int Model0_sysctl_tcp_window_scaling;
extern int Model0_sysctl_tcp_sack;
extern int Model0_sysctl_tcp_fastopen;
extern int Model0_sysctl_tcp_retrans_collapse;
extern int Model0_sysctl_tcp_stdurg;
extern int Model0_sysctl_tcp_rfc1337;
extern int Model0_sysctl_tcp_abort_on_overflow;
extern int Model0_sysctl_tcp_max_orphans;
extern int Model0_sysctl_tcp_fack;
extern int Model0_sysctl_tcp_reordering;
extern int Model0_sysctl_tcp_max_reordering;
extern int Model0_sysctl_tcp_dsack;
extern long Model0_sysctl_tcp_mem[3];
extern int Model0_sysctl_tcp_wmem[3];
extern int Model0_sysctl_tcp_rmem[3];
extern int Model0_sysctl_tcp_app_win;
extern int Model0_sysctl_tcp_adv_win_scale;
extern int Model0_sysctl_tcp_tw_reuse;
extern int Model0_sysctl_tcp_frto;
extern int Model0_sysctl_tcp_low_latency;
extern int Model0_sysctl_tcp_nometrics_save;
extern int Model0_sysctl_tcp_moderate_rcvbuf;
extern int Model0_sysctl_tcp_tso_win_divisor;
extern int Model0_sysctl_tcp_workaround_signed_windows;
extern int Model0_sysctl_tcp_slow_start_after_idle;
extern int Model0_sysctl_tcp_thin_linear_timeouts;
extern int Model0_sysctl_tcp_thin_dupack;
extern int Model0_sysctl_tcp_early_retrans;
extern int Model0_sysctl_tcp_limit_output_bytes;
extern int Model0_sysctl_tcp_challenge_ack_limit;
extern int Model0_sysctl_tcp_min_tso_segs;
extern int Model0_sysctl_tcp_min_rtt_wlen;
extern int Model0_sysctl_tcp_autocorking;
extern int Model0_sysctl_tcp_invalid_ratelimit;
extern int Model0_sysctl_tcp_pacing_ss_ratio;
extern int Model0_sysctl_tcp_pacing_ca_ratio;

extern Model0_atomic_long_t Model0_tcp_memory_allocated;
extern struct Model0_percpu_counter Model0_tcp_sockets_allocated;
extern int Model0_tcp_memory_pressure;

/* optimized version of sk_under_memory_pressure() for TCP sockets */
static inline __attribute__((no_instrument_function)) bool Model0_tcp_under_memory_pressure(const struct Model0_sock *Model0_sk)
{
 if (0 && Model0_sk->Model0_sk_memcg &&
     Model0_mem_cgroup_under_socket_pressure(Model0_sk->Model0_sk_memcg))
  return true;

 return Model0_tcp_memory_pressure;
}
/*
 * The next routines deal with comparing 32 bit unsigned ints
 * and worry about wraparound (automatic with unsigned arithmetic).
 */

static inline __attribute__((no_instrument_function)) bool Model0_before(__u32 Model0_seq1, __u32 Model0_seq2)
{
        return (Model0___s32)(Model0_seq1-Model0_seq2) < 0;
}


/* is s2<=s1<=s3 ? */
static inline __attribute__((no_instrument_function)) bool Model0_between(__u32 Model0_seq1, __u32 Model0_seq2, __u32 Model0_seq3)
{
 return Model0_seq3 - Model0_seq2 >= Model0_seq1 - Model0_seq2;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_out_of_memory(struct Model0_sock *Model0_sk)
{
 if (Model0_sk->Model0_sk_wmem_queued > ((2048 + ((((sizeof(struct Model0_sk_buff))) + ((typeof((sizeof(struct Model0_sk_buff))))(((1 << (6)))) - 1)) & ~((typeof((sizeof(struct Model0_sk_buff))))(((1 << (6)))) - 1))) * 2) &&
     Model0_sk_memory_allocated(Model0_sk) > Model0_sk_prot_mem_limits(Model0_sk, 2))
  return true;
 return false;
}

void Model0_sk_forced_mem_schedule(struct Model0_sock *Model0_sk, int Model0_size);

static inline __attribute__((no_instrument_function)) bool Model0_tcp_too_many_orphans(struct Model0_sock *Model0_sk, int Model0_shift)
{
 struct Model0_percpu_counter *Model0_ocp = Model0_sk->Model0___sk_common.Model0_skc_prot->Model0_orphan_count;
 int Model0_orphans = Model0_percpu_counter_read_positive(Model0_ocp);

 if (Model0_orphans << Model0_shift > Model0_sysctl_tcp_max_orphans) {
  Model0_orphans = Model0_percpu_counter_sum_positive(Model0_ocp);
  if (Model0_orphans << Model0_shift > Model0_sysctl_tcp_max_orphans)
   return true;
 }
 return false;
}

bool Model0_tcp_check_oom(struct Model0_sock *Model0_sk, int Model0_shift);


extern struct Model0_proto Model0_tcp_prot;
extern struct Model0_tcp_mib Model0_cy_tcp_mib;
//#define TCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.tcp_statistics, field)
//#define __TCP_INC_STATS(net, field)	__SNMP_INC_STATS((net)->mib.tcp_statistics, field)

//#define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)





void Model0_tcp_tasklet_init(void);

void Model0_tcp_v4_err(struct Model0_sk_buff *Model0_skb, Model0_u32);

void Model0_tcp_shutdown(struct Model0_sock *Model0_sk, int Model0_how);

void Model0_tcp_v4_early_demux(struct Model0_sk_buff *Model0_skb);
int Model0_tcp_v4_rcv(struct Model0_sk_buff *Model0_skb);

int Model0_tcp_v4_tw_remember_stamp(struct Model0_inet_timewait_sock *Model0_tw);
int Model0_tcp_sendmsg(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, Model0_size_t Model0_size);
int Model0_tcp_sendpage(struct Model0_sock *Model0_sk, struct Model0_page *Model0_page, int Model0_offset, Model0_size_t Model0_size,
   int Model0_flags);
void Model0_tcp_release_cb(struct Model0_sock *Model0_sk);
void Model0_tcp_wfree(struct Model0_sk_buff *Model0_skb);
void Model0_tcp_write_timer_handler(struct Model0_sock *Model0_sk);
void Model0_tcp_delack_timer_handler(struct Model0_sock *Model0_sk);
int Model0_tcp_ioctl(struct Model0_sock *Model0_sk, int Model0_cmd, unsigned long Model0_arg);
int Model0_tcp_rcv_state_process(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
void Model0_tcp_rcv_established(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
    const struct Model0_tcphdr *Model0_th, unsigned int Model0_len);
void Model0_tcp_rcv_space_adjust(struct Model0_sock *Model0_sk);
int Model0_tcp_twsk_unique(struct Model0_sock *Model0_sk, struct Model0_sock *Model0_sktw, void *Model0_twp);
void Model0_tcp_twsk_destructor(struct Model0_sock *Model0_sk);
Model0_ssize_t Model0_tcp_splice_read(struct Model0_socket *Model0_sk, Model0_loff_t *Model0_ppos,
   struct Model0_pipe_inode_info *Model0_pipe, Model0_size_t Model0_len,
   unsigned int Model0_flags);

static inline __attribute__((no_instrument_function)) void Model0_tcp_dec_quickack_mode(struct Model0_sock *Model0_sk,
      const unsigned int Model0_pkts)
{
 struct Model0_inet_connection_sock *Model0_icsk = Model0_inet_csk(Model0_sk);

 if (Model0_icsk->Model0_icsk_ack.Model0_quick) {
  if (Model0_pkts >= Model0_icsk->Model0_icsk_ack.Model0_quick) {
   Model0_icsk->Model0_icsk_ack.Model0_quick = 0;
   /* Leaving quickack mode we deflate ATO. */
   Model0_icsk->Model0_icsk_ack.Model0_ato = ((unsigned)(1000/25));
  } else
   Model0_icsk->Model0_icsk_ack.Model0_quick -= Model0_pkts;
 }
}






enum Model0_tcp_tw_status {
 Model0_TCP_TW_SUCCESS = 0,
 Model0_TCP_TW_RST = 1,
 Model0_TCP_TW_ACK = 2,
 Model0_TCP_TW_SYN = 3
};


enum Model0_tcp_tw_status Model0_tcp_timewait_state_process(struct Model0_inet_timewait_sock *Model0_tw,
           struct Model0_sk_buff *Model0_skb,
           const struct Model0_tcphdr *Model0_th);
struct Model0_sock *Model0_tcp_check_req(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
      struct Model0_request_sock *Model0_req, bool Model0_fastopen);
int Model0_tcp_child_process(struct Model0_sock *Model0_parent, struct Model0_sock *Model0_child,
        struct Model0_sk_buff *Model0_skb);
void Model0_tcp_enter_loss(struct Model0_sock *Model0_sk);
void Model0_tcp_clear_retrans(struct Model0_tcp_sock *Model0_tp);
void Model0_tcp_update_metrics(struct Model0_sock *Model0_sk);
void Model0_tcp_init_metrics(struct Model0_sock *Model0_sk);
void Model0_tcp_metrics_init(void);
bool Model0_tcp_peer_is_proven(struct Model0_request_sock *Model0_req, struct Model0_dst_entry *Model0_dst,
   bool Model0_paws_check, bool Model0_timestamps);
bool Model0_tcp_remember_stamp(struct Model0_sock *Model0_sk);
bool Model0_tcp_tw_remember_stamp(struct Model0_inet_timewait_sock *Model0_tw);
void Model0_tcp_fetch_timewait_stamp(struct Model0_sock *Model0_sk, struct Model0_dst_entry *Model0_dst);
void Model0_tcp_disable_fack(struct Model0_tcp_sock *Model0_tp);
void Model0_tcp_close(struct Model0_sock *Model0_sk, long Model0_timeout);
void Model0_tcp_init_sock(struct Model0_sock *Model0_sk);
unsigned int Model0_tcp_poll(struct Model0_file *Model0_file, struct Model0_socket *Model0_sock,
        struct Model0_poll_table_struct *Model0_wait);
int Model0_tcp_getsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
     char *Model0_optval, int *Model0_optlen);
int Model0_tcp_setsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
     char *Model0_optval, unsigned int Model0_optlen);
int Model0_compat_tcp_getsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
     char *Model0_optval, int *Model0_optlen);
int Model0_compat_tcp_setsockopt(struct Model0_sock *Model0_sk, int Model0_level, int Model0_optname,
     char *Model0_optval, unsigned int Model0_optlen);
void Model0_tcp_set_keepalive(struct Model0_sock *Model0_sk, int Model0_val);
void Model0_tcp_syn_ack_timeout(const struct Model0_request_sock *Model0_req);
int Model0_tcp_recvmsg(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, Model0_size_t Model0_len, int Model0_nonblock,
  int Model0_flags, int *Model0_addr_len);
void Model0_tcp_parse_options(const struct Model0_sk_buff *Model0_skb,
         struct Model0_tcp_options_received *Model0_opt_rx,
         int Model0_estab, struct Model0_tcp_fastopen_cookie *Model0_foc);
const Model0_u8 *Model0_tcp_parse_md5sig_option(const struct Model0_tcphdr *Model0_th);

/*
 *	TCP v4 functions exported for the inet6 API
 */

void Model0_tcp_v4_send_check(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
void Model0_tcp_v4_mtu_reduced(struct Model0_sock *Model0_sk);
void Model0_tcp_req_err(struct Model0_sock *Model0_sk, Model0_u32 Model0_seq, bool Model0_abort);
int Model0_tcp_v4_conn_request(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
struct Model0_sock *Model0_tcp_create_openreq_child(const struct Model0_sock *Model0_sk,
          struct Model0_request_sock *Model0_req,
          struct Model0_sk_buff *Model0_skb);
void Model0_tcp_ca_openreq_child(struct Model0_sock *Model0_sk, const struct Model0_dst_entry *Model0_dst);
struct Model0_sock *Model0_tcp_v4_syn_recv_sock(const struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
      struct Model0_request_sock *Model0_req,
      struct Model0_dst_entry *Model0_dst,
      struct Model0_request_sock *Model0_req_unhash,
      bool *Model0_own_req);
int Model0_tcp_v4_do_rcv(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_tcp_v4_connect(struct Model0_sock *Model0_sk, struct Model0_sockaddr *Model0_uaddr, int Model0_addr_len);
int Model0_tcp_connect(struct Model0_sock *Model0_sk);
enum Model0_tcp_synack_type {
 Model0_TCP_SYNACK_NORMAL,
 Model0_TCP_SYNACK_FASTOPEN,
 Model0_TCP_SYNACK_COOKIE,
};
struct Model0_sk_buff *Model0_tcp_make_synack(const struct Model0_sock *Model0_sk, struct Model0_dst_entry *Model0_dst,
    struct Model0_request_sock *Model0_req,
    struct Model0_tcp_fastopen_cookie *Model0_foc,
    enum Model0_tcp_synack_type Model0_synack_type);
int Model0_tcp_disconnect(struct Model0_sock *Model0_sk, int Model0_flags);

void Model0_tcp_finish_connect(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
int Model0_tcp_send_rcvq(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, Model0_size_t Model0_size);
void Model0_inet_sk_rx_dst_set(struct Model0_sock *Model0_sk, const struct Model0_sk_buff *Model0_skb);

/* From syncookies.c */
struct Model0_sock *Model0_tcp_get_cookie_sock(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
     struct Model0_request_sock *Model0_req,
     struct Model0_dst_entry *Model0_dst);
int Model0___cookie_v4_check(const struct Model0_iphdr *Model0_iph, const struct Model0_tcphdr *Model0_th,
        Model0_u32 Model0_cookie);
struct Model0_sock *Model0_cookie_v4_check(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);


/* Syncookies use a monotonic timer which increments every 60 seconds.
 * This counter is used both as a hash input and partially encoded into
 * the cookie value.  A cookie is only validated further if the delta
 * between the current counter value and the encoded one is less than this,
 * i.e. a sent cookie is valid only at most for 2*60 seconds (or less if
 * the counter advances immediately after a cookie is generated).
 */




/* syncookies: remember time of last synqueue overflow
 * But do not dirty this field too often (once per second is enough)
 * It is racy as we do not hold a lock, but race is very minor.
 */
static inline __attribute__((no_instrument_function)) void Model0_tcp_synq_overflow(const struct Model0_sock *Model0_sk)
{
 unsigned long Model0_last_overflow = Model0_tcp_sk(Model0_sk)->Model0_rx_opt.Model0_ts_recent_stamp;
 unsigned long Model0_now = Model0_jiffies;

 if ((({ unsigned long Model0___dummy; typeof(Model0_now) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }) && ({ unsigned long Model0___dummy; typeof(Model0_last_overflow + 1000) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }) && ((long)((Model0_last_overflow + 1000) - (Model0_now)) < 0)))
  Model0_tcp_sk(Model0_sk)->Model0_rx_opt.Model0_ts_recent_stamp = Model0_now;
}

/* syncookies: no recent synqueue overflow on this listening socket? */
static inline __attribute__((no_instrument_function)) bool Model0_tcp_synq_no_recent_overflow(const struct Model0_sock *Model0_sk)
{
 unsigned long Model0_last_overflow = Model0_tcp_sk(Model0_sk)->Model0_rx_opt.Model0_ts_recent_stamp;

 return (({ unsigned long Model0___dummy; typeof(Model0_jiffies) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }) && ({ unsigned long Model0___dummy; typeof(Model0_last_overflow + (2 * (60 * 1000))) Model0___dummy2; (void)(&Model0___dummy == &Model0___dummy2); 1; }) && ((long)((Model0_last_overflow + (2 * (60 * 1000))) - (Model0_jiffies)) < 0));
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_cookie_time(void)
{
 Model0_u64 Model0_val = Model0_get_jiffies_64();

 ({ Model0_uint32_t Model0___base = ((60 * 1000)); Model0_uint32_t Model0___rem; Model0___rem = ((Model0_uint64_t)(Model0_val)) % Model0___base; (Model0_val) = ((Model0_uint64_t)(Model0_val)) / Model0___base; Model0___rem; });
 return Model0_val;
}

Model0_u32 Model0___cookie_v4_init_sequence(const struct Model0_iphdr *Model0_iph, const struct Model0_tcphdr *Model0_th,
         Model0_u16 *Model0_mssp);
__u32 Model0_cookie_v4_init_sequence(const struct Model0_sk_buff *Model0_skb, Model0___u16 *Model0_mss);
__u32 Model0_cookie_init_timestamp(struct Model0_request_sock *Model0_req);
bool Model0_cookie_timestamp_decode(struct Model0_tcp_options_received *Model0_opt);
bool Model0_cookie_ecn_ok(const struct Model0_tcp_options_received *Model0_opt,
     const struct Model0_net *Model0_net, const struct Model0_dst_entry *Model0_dst);

/* From net/ipv6/syncookies.c */
int Model0___cookie_v6_check(const struct Model0_ipv6hdr *Model0_iph, const struct Model0_tcphdr *Model0_th,
        Model0_u32 Model0_cookie);
struct Model0_sock *Model0_cookie_v6_check(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

Model0_u32 Model0___cookie_v6_init_sequence(const struct Model0_ipv6hdr *Model0_iph,
         const struct Model0_tcphdr *Model0_th, Model0_u16 *Model0_mssp);
__u32 Model0_cookie_v6_init_sequence(const struct Model0_sk_buff *Model0_skb, Model0___u16 *Model0_mss);

/* tcp_output.c */

void Model0___tcp_push_pending_frames(struct Model0_sock *Model0_sk, unsigned int Model0_cur_mss,
          int Model0_nonagle);
bool Model0_tcp_may_send_now(struct Model0_sock *Model0_sk);
int Model0___tcp_retransmit_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, int Model0_segs);
int Model0_tcp_retransmit_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb, int Model0_segs);
void Model0_tcp_retransmit_timer(struct Model0_sock *Model0_sk);
void Model0_tcp_xmit_retransmit_queue(struct Model0_sock *);
void Model0_tcp_simple_retransmit(struct Model0_sock *);
int Model0_tcp_trim_head(struct Model0_sock *, struct Model0_sk_buff *, Model0_u32);
int Model0_tcp_fragment(struct Model0_sock *, struct Model0_sk_buff *, Model0_u32, unsigned int, Model0_gfp_t);

void Model0_tcp_send_probe0(struct Model0_sock *);
void Model0_tcp_send_partial(struct Model0_sock *);
int Model0_tcp_write_wakeup(struct Model0_sock *, int Model0_mib);
void Model0_tcp_send_fin(struct Model0_sock *Model0_sk);
void Model0_tcp_send_active_reset(struct Model0_sock *Model0_sk, Model0_gfp_t Model0_priority);
int Model0_tcp_send_synack(struct Model0_sock *);
void Model0_tcp_push_one(struct Model0_sock *, unsigned int Model0_mss_now);
void Model0_tcp_send_ack(struct Model0_sock *Model0_sk);
void Model0_tcp_send_delayed_ack(struct Model0_sock *Model0_sk);
void Model0_tcp_send_loss_probe(struct Model0_sock *Model0_sk);
bool Model0_tcp_schedule_loss_probe(struct Model0_sock *Model0_sk);
void Model0_tcp_skb_collapse_tstamp(struct Model0_sk_buff *Model0_skb,
        const struct Model0_sk_buff *Model0_next_skb);

/* tcp_input.c */
void Model0_tcp_resume_early_retransmit(struct Model0_sock *Model0_sk);
void Model0_tcp_rearm_rto(struct Model0_sock *Model0_sk);
void Model0_tcp_synack_rtt_meas(struct Model0_sock *Model0_sk, struct Model0_request_sock *Model0_req);
void Model0_tcp_reset(struct Model0_sock *Model0_sk);
void Model0_tcp_skb_mark_lost_uncond_verify(struct Model0_tcp_sock *Model0_tp, struct Model0_sk_buff *Model0_skb);
void Model0_tcp_fin(struct Model0_sock *Model0_sk);

/* tcp_timer.c */
void Model0_tcp_init_xmit_timers(struct Model0_sock *);
static inline __attribute__((no_instrument_function)) void Model0_tcp_clear_xmit_timers(struct Model0_sock *Model0_sk)
{
 Model0_inet_csk_clear_xmit_timers(Model0_sk);
}

unsigned int Model0_tcp_sync_mss(struct Model0_sock *Model0_sk, Model0_u32 Model0_pmtu);
unsigned int Model0_tcp_current_mss(struct Model0_sock *Model0_sk);

/* Bound MSS / TSO packet size with the half of the window */
static inline __attribute__((no_instrument_function)) int Model0_tcp_bound_to_half_wnd(struct Model0_tcp_sock *Model0_tp, int Model0_pktsize)
{
 int Model0_cutoff;

 /* When peer uses tiny windows, there is no use in packetizing
	 * to sub-MSS pieces for the sake of SWS or making sure there
	 * are enough packets in the pipe for fast recovery.
	 *
	 * On the other hand, for extremely large MSS devices, handling
	 * smaller than MSS windows in this way does make sense.
	 */
 if (Model0_tp->Model0_max_window > 536U)
  Model0_cutoff = (Model0_tp->Model0_max_window >> 1);
 else
  Model0_cutoff = Model0_tp->Model0_max_window;

 if (Model0_cutoff && Model0_pktsize > Model0_cutoff)
  return ({ int Model0___max1 = (Model0_cutoff); int Model0___max2 = (68U - Model0_tp->Model0_tcp_header_len); Model0___max1 > Model0___max2 ? Model0___max1: Model0___max2; });
 else
  return Model0_pktsize;
}

/* tcp.c */
void Model0_tcp_get_info(struct Model0_sock *, struct Model0_tcp_info *);

/* Read 'sendfile()'-style from a TCP socket */
typedef int (*Model0_sk_read_actor_t)(Model0_read_descriptor_t *, struct Model0_sk_buff *,
    unsigned int, Model0_size_t);
int Model0_tcp_read_sock(struct Model0_sock *Model0_sk, Model0_read_descriptor_t *Model0_desc,
    Model0_sk_read_actor_t Model0_recv_actor);

void Model0_tcp_initialize_rcv_mss(struct Model0_sock *Model0_sk);

int Model0_tcp_mtu_to_mss(struct Model0_sock *Model0_sk, int Model0_pmtu);
int Model0_tcp_mss_to_mtu(struct Model0_sock *Model0_sk, int Model0_mss);
void Model0_tcp_mtup_init(struct Model0_sock *Model0_sk);
void Model0_tcp_init_buffer_space(struct Model0_sock *Model0_sk);

static inline __attribute__((no_instrument_function)) void Model0_tcp_bound_rto(const struct Model0_sock *Model0_sk)
{
 if (Model0_inet_csk(Model0_sk)->Model0_icsk_rto > ((unsigned)(120*1000)))
  Model0_inet_csk(Model0_sk)->Model0_icsk_rto = ((unsigned)(120*1000));
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0___tcp_set_rto(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_usecs_to_jiffies((Model0_tp->Model0_srtt_us >> 3) + Model0_tp->Model0_rttvar_us);
}

static inline __attribute__((no_instrument_function)) void Model0___tcp_fast_path_on(struct Model0_tcp_sock *Model0_tp, Model0_u32 Model0_snd_wnd)
{
 Model0_tp->Model0_pred_flags = (( Model0___be32)(__builtin_constant_p((__u32)(((Model0_tp->Model0_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) | Model0_snd_wnd))) ? ((__u32)( (((__u32)(((Model0_tp->Model0_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) | Model0_snd_wnd)) & (__u32)0x000000ffUL) << 24) | (((__u32)(((Model0_tp->Model0_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) | Model0_snd_wnd)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(((Model0_tp->Model0_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) | Model0_snd_wnd)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(((Model0_tp->Model0_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) | Model0_snd_wnd)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(((Model0_tp->Model0_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32(( __u32)(Model0___be32)(Model0_TCP_FLAG_ACK))) | Model0_snd_wnd))));


}

static inline __attribute__((no_instrument_function)) void Model0_tcp_fast_path_on(struct Model0_tcp_sock *Model0_tp)
{
 Model0___tcp_fast_path_on(Model0_tp, Model0_tp->Model0_snd_wnd >> Model0_tp->Model0_rx_opt.Model0_snd_wscale);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_fast_path_check(struct Model0_sock *Model0_sk)
{
 struct Model0_tcp_sock *Model0_tp = Model0_tcp_sk(Model0_sk);

 if (Model0_skb_queue_empty(&Model0_tp->Model0_out_of_order_queue) &&
     Model0_tp->Model0_rcv_wnd &&
     Model0_atomic_read(&Model0_sk->Model0_sk_backlog.Model0_rmem_alloc) < Model0_sk->Model0_sk_rcvbuf &&
     !Model0_tp->Model0_urg_data)
  Model0_tcp_fast_path_on(Model0_tp);
}

/* Compute the actual rto_min value */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_rto_min(struct Model0_sock *Model0_sk)
{
 const struct Model0_dst_entry *Model0_dst = Model0___sk_dst_get(Model0_sk);
 Model0_u32 Model0_rto_min = ((unsigned)(1000/5));

 if (Model0_dst && Model0_dst_metric_locked(Model0_dst, Model0_RTAX_RTO_MIN))
  Model0_rto_min = Model0_dst_metric_rtt(Model0_dst, Model0_RTAX_RTO_MIN);
 return Model0_rto_min;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_rto_min_us(struct Model0_sock *Model0_sk)
{
 return Model0_jiffies_to_usecs(Model0_tcp_rto_min(Model0_sk));
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_ca_dst_locked(const struct Model0_dst_entry *Model0_dst)
{
 return Model0_dst_metric_locked(Model0_dst, Model0_RTAX_CC_ALGO);
}

/* Minimum RTT in usec. ~0 means not available. */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_min_rtt(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_rtt_min[0].Model0_rtt;
}

/* Compute the actual receive window we are currently advertising.
 * Rcv_nxt can be after the window if our peer push more data
 * than the offered window.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_receive_window(const struct Model0_tcp_sock *Model0_tp)
{
 Model0_s32 Model0_win = Model0_tp->Model0_rcv_wup + Model0_tp->Model0_rcv_wnd - Model0_tp->Model0_rcv_nxt;

 if (Model0_win < 0)
  Model0_win = 0;
 return (Model0_u32) Model0_win;
}

/* Choose a new window, without checks for shrinking, and without
 * scaling applied to the result.  The caller does these things
 * if necessary.  This is a "raw" window selection.
 */
Model0_u32 Model0___tcp_select_window(struct Model0_sock *Model0_sk);

void Model0_tcp_send_window_probe(struct Model0_sock *Model0_sk);

/* TCP timestamps are only 32-bits, this causes a slight
 * complication on 64-bit systems since we store a snapshot
 * of jiffies in the buffer control blocks below.  We decided
 * to use only the low 32-bits of jiffies and hide the ugly
 * casts with the following macro.
 */


static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_skb_timestamp(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_skb_mstamp.Model0_stamp_jiffies;
}
/* This is what the send packet queuing engine uses to pass
 * TCP per-packet control information to the transmission code.
 * We also store the host-order sequence numbers in here too.
 * This is 44 bytes if IPV6 is enabled.
 * If this grows please adjust skbuff.h:skbuff->cb[xxx] size appropriately.
 */
struct Model0_tcp_skb_cb {
 __u32 Model0_seq; /* Starting sequence number	*/
 __u32 Model0_end_seq; /* SEQ + FIN + SYN + datalen	*/
 union {
  /* Note : tcp_tw_isn is used in input path only
		 *	  (isn chosen by tcp_timewait_state_process())
		 *
		 * 	  tcp_gso_segs/size are used in write queue only,
		 *	  cf tcp_skb_pcount()/tcp_skb_mss()
		 */
  __u32 Model0_tcp_tw_isn;
  struct {
   Model0_u16 Model0_tcp_gso_segs;
   Model0_u16 Model0_tcp_gso_size;
  };
 };
 __u8 Model0_tcp_flags; /* TCP header flags. (tcp[13])	*/

 __u8 Model0_sacked; /* State flags for SACK/FACK.	*/
 __u8 Model0_ip_dsfield; /* IPv4 tos or IPv6 dsfield	*/
 __u8 Model0_txstamp_ack:1, /* Record TX timestamp for ack? */
   Model0_eor:1, /* Is skb MSG_EOR marked? */
   unused:6;
 __u32 Model0_ack_seq; /* Sequence number ACK'd	*/
 union {
  struct {
   /* There is space for up to 20 bytes */
   __u32 Model0_in_flight;/* Bytes in flight when packet sent */
  } Model0_tx; /* only used for outgoing skbs */
  union {
   struct Model0_inet_skb_parm Model0_h4;

   struct Model0_inet6_skb_parm Model0_h6;

  } Model0_header; /* For incoming skbs */
 };
};





/* This is the variant of inet6_iif() that must be used by TCP,
 * as TCP moves IP6CB into a different location in skb->cb[]
 */
static inline __attribute__((no_instrument_function)) int Model0_tcp_v6_iif(const struct Model0_sk_buff *Model0_skb)
{
 bool Model0_l3_slave = Model0_skb_l3mdev_slave(((struct Model0_tcp_skb_cb *)&((Model0_skb)->Model0_cb[0]))->Model0_header.Model0_h6.Model0_flags);

 return Model0_l3_slave ? Model0_skb->Model0_skb_iif : ((struct Model0_tcp_skb_cb *)&((Model0_skb)->Model0_cb[0]))->Model0_header.Model0_h6.Model0_iif;
}


/* Due to TSO, an SKB can be composed of multiple actual
 * packets.  To keep these tracked properly, we use this.
 */
static inline __attribute__((no_instrument_function)) int Model0_tcp_skb_pcount(const struct Model0_sk_buff *Model0_skb)
{
 return ((struct Model0_tcp_skb_cb *)&((Model0_skb)->Model0_cb[0]))->Model0_tcp_gso_segs;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_skb_pcount_set(struct Model0_sk_buff *Model0_skb, int Model0_segs)
{
 ((struct Model0_tcp_skb_cb *)&((Model0_skb)->Model0_cb[0]))->Model0_tcp_gso_segs = Model0_segs;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_skb_pcount_add(struct Model0_sk_buff *Model0_skb, int Model0_segs)
{
 ((struct Model0_tcp_skb_cb *)&((Model0_skb)->Model0_cb[0]))->Model0_tcp_gso_segs += Model0_segs;
}

/* This is valid iff skb is in write queue and tcp_skb_pcount() > 1. */
static inline __attribute__((no_instrument_function)) int Model0_tcp_skb_mss(const struct Model0_sk_buff *Model0_skb)
{
 return ((struct Model0_tcp_skb_cb *)&((Model0_skb)->Model0_cb[0]))->Model0_tcp_gso_size;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_skb_can_collapse_to(const struct Model0_sk_buff *Model0_skb)
{
 return __builtin_expect(!!(!((struct Model0_tcp_skb_cb *)&((Model0_skb)->Model0_cb[0]))->Model0_eor), 1);
}

/* Events passed to congestion control interface */
enum Model0_tcp_ca_event {
 Model0_CA_EVENT_TX_START, /* first transmit when no packets in flight */
 Model0_CA_EVENT_CWND_RESTART, /* congestion window restart */
 Model0_CA_EVENT_COMPLETE_CWR, /* end of congestion recovery */
 Model0_CA_EVENT_LOSS, /* loss timeout */
 Model0_CA_EVENT_ECN_NO_CE, /* ECT set, but not CE marked */
 Model0_CA_EVENT_ECN_IS_CE, /* received CE marked IP packet */
 Model0_CA_EVENT_DELAYED_ACK, /* Delayed ack is sent */
 Model0_CA_EVENT_NON_DELAYED_ACK,
};

/* Information about inbound ACK, passed to cong_ops->in_ack_event() */
enum Model0_tcp_ca_ack_event_flags {
 Model0_CA_ACK_SLOWPATH = (1 << 0), /* In slow path processing */
 Model0_CA_ACK_WIN_UPDATE = (1 << 1), /* ACK updated window */
 Model0_CA_ACK_ECE = (1 << 2), /* ECE bit is set on ack */
};

/*
 * Interface for adding new TCP congestion control handlers
 */






/* Algorithm can be set on socket without CAP_NET_ADMIN privileges */

/* Requires ECN/ECT set on all packets */


union Model0_tcp_cc_info;

struct Model0_ack_sample {
 Model0_u32 Model0_pkts_acked;
 Model0_s32 Model0_rtt_us;
 Model0_u32 Model0_in_flight;
};

struct Model0_tcp_congestion_ops {
 struct Model0_list_head Model0_list;
 Model0_u32 Model0_key;
 Model0_u32 Model0_flags;

 /* initialize private data (optional) */
 void (*Model0_init)(struct Model0_sock *Model0_sk);
 /* cleanup private data  (optional) */
 void (*Model0_release)(struct Model0_sock *Model0_sk);

 /* return slow start threshold (required) */
 Model0_u32 (*Model0_ssthresh)(struct Model0_sock *Model0_sk);
 /* do new cwnd calculation (required) */
 void (*Model0_cong_avoid)(struct Model0_sock *Model0_sk, Model0_u32 Model0_ack, Model0_u32 Model0_acked);
 /* call before changing ca_state (optional) */
 void (*Model0_set_state)(struct Model0_sock *Model0_sk, Model0_u8 Model0_new_state);
 /* call when cwnd event occurs (optional) */
 void (*Model0_cwnd_event)(struct Model0_sock *Model0_sk, enum Model0_tcp_ca_event Model0_ev);
 /* call when ack arrives (optional) */
 void (*Model0_in_ack_event)(struct Model0_sock *Model0_sk, Model0_u32 Model0_flags);
 /* new value of cwnd after loss (optional) */
 Model0_u32 (*Model0_undo_cwnd)(struct Model0_sock *Model0_sk);
 /* hook for packet ack accounting (optional) */
 void (*Model0_pkts_acked)(struct Model0_sock *Model0_sk, const struct Model0_ack_sample *Model0_sample);
 /* get info for inet_diag (optional) */
 Model0_size_t (*Model0_get_info)(struct Model0_sock *Model0_sk, Model0_u32 Model0_ext, int *Model0_attr,
      union Model0_tcp_cc_info *Model0_info);

 char Model0_name[16];
 struct Model0_module *Model0_owner;
};

int Model0_tcp_register_congestion_control(struct Model0_tcp_congestion_ops *Model0_type);
void Model0_tcp_unregister_congestion_control(struct Model0_tcp_congestion_ops *Model0_type);

void Model0_tcp_assign_congestion_control(struct Model0_sock *Model0_sk);
void Model0_tcp_init_congestion_control(struct Model0_sock *Model0_sk);
void Model0_tcp_cleanup_congestion_control(struct Model0_sock *Model0_sk);
int Model0_tcp_set_default_congestion_control(const char *Model0_name);
void Model0_tcp_get_default_congestion_control(char *Model0_name);
void Model0_tcp_get_available_congestion_control(char *Model0_buf, Model0_size_t Model0_len);
void Model0_tcp_get_allowed_congestion_control(char *Model0_buf, Model0_size_t Model0_len);
int Model0_tcp_set_allowed_congestion_control(char *Model0_allowed);
int Model0_tcp_set_congestion_control(struct Model0_sock *Model0_sk, const char *Model0_name);
Model0_u32 Model0_tcp_slow_start(struct Model0_tcp_sock *Model0_tp, Model0_u32 Model0_acked);
void Model0_tcp_cong_avoid_ai(struct Model0_tcp_sock *Model0_tp, Model0_u32 Model0_w, Model0_u32 Model0_acked);

Model0_u32 Model0_tcp_reno_ssthresh(struct Model0_sock *Model0_sk);
void Model0_tcp_reno_cong_avoid(struct Model0_sock *Model0_sk, Model0_u32 Model0_ack, Model0_u32 Model0_acked);
extern struct Model0_tcp_congestion_ops Model0_tcp_reno;

struct Model0_tcp_congestion_ops *Model0_tcp_ca_find_key(Model0_u32 Model0_key);
Model0_u32 Model0_tcp_ca_get_key_by_name(const char *Model0_name, bool *Model0_ecn_ca);

char *Model0_tcp_ca_get_name_by_key(Model0_u32 Model0_key, char *Model0_buffer);







static inline __attribute__((no_instrument_function)) bool Model0_tcp_ca_needs_ecn(const struct Model0_sock *Model0_sk)
{
 const struct Model0_inet_connection_sock *Model0_icsk = Model0_inet_csk(Model0_sk);

 return Model0_icsk->Model0_icsk_ca_ops->Model0_flags & 0x2;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_set_ca_state(struct Model0_sock *Model0_sk, const Model0_u8 Model0_ca_state)
{
 struct Model0_inet_connection_sock *Model0_icsk = Model0_inet_csk(Model0_sk);

#if CY_ABSTRACT7 //the function pointer is empty
#else
 if (Model0_icsk->Model0_icsk_ca_ops->Model0_set_state)
  Model0_icsk->Model0_icsk_ca_ops->Model0_set_state(Model0_sk, Model0_ca_state);
#endif
 Model0_icsk->Model0_icsk_ca_state = Model0_ca_state;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_ca_event(struct Model0_sock *Model0_sk, const enum Model0_tcp_ca_event Model0_event)
{
 const struct Model0_inet_connection_sock *Model0_icsk = Model0_inet_csk(Model0_sk);

#if !CY_ABSTRACT1
 if (Model0_icsk->Model0_icsk_ca_ops->Model0_cwnd_event)
  Model0_icsk->Model0_icsk_ca_ops->Model0_cwnd_event(Model0_sk, Model0_event);
#endif
}

/* These functions determine how the current flow behaves in respect of SACK
 * handling. SACK is negotiated with the peer, and therefore it can vary
 * between different flows.
 *
 * tcp_is_sack - SACK enabled
 * tcp_is_reno - No SACK
 * tcp_is_fack - FACK enabled, implies SACK enabled
 */
static inline __attribute__((no_instrument_function)) int Model0_tcp_is_sack(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_rx_opt.Model0_sack_ok;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_is_reno(const struct Model0_tcp_sock *Model0_tp)
{
 return !Model0_tcp_is_sack(Model0_tp);
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_is_fack(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_rx_opt.Model0_sack_ok & (1 << 1);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_enable_fack(struct Model0_tcp_sock *Model0_tp)
{
 Model0_tp->Model0_rx_opt.Model0_sack_ok |= (1 << 1);
}

/* TCP early-retransmit (ER) is similar to but more conservative than
 * the thin-dupack feature.  Enable ER only if thin-dupack is disabled.
 */
static inline __attribute__((no_instrument_function)) void Model0_tcp_enable_early_retrans(struct Model0_tcp_sock *Model0_tp)
{
 struct Model0_net *Model0_net = Model0_sock_net((struct Model0_sock *)Model0_tp);

 Model0_tp->Model0_do_early_retrans = Model0_sysctl_tcp_early_retrans &&
  Model0_sysctl_tcp_early_retrans < 4 && !Model0_sysctl_tcp_thin_dupack &&
  Model0_net->Model0_ipv4.Model0_sysctl_tcp_reordering == 3;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_disable_early_retrans(struct Model0_tcp_sock *Model0_tp)
{
 Model0_tp->Model0_do_early_retrans = 0;
}

static inline __attribute__((no_instrument_function)) unsigned int Model0_tcp_left_out(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_sacked_out + Model0_tp->Model0_lost_out;
}

/* This determines how many packets are "in the network" to the best
 * of our knowledge.  In many cases it is conservative, but where
 * detailed information is available from the receiver (via SACK
 * blocks etc.) we can make more aggressive calculations.
 *
 * Use this for decisions involving congestion control, use just
 * tp->packets_out to determine if the send queue is empty or not.
 *
 * Read this equation as:
 *
 *	"Packets sent once on transmission queue" MINUS
 *	"Packets left network, but not honestly ACKed yet" PLUS
 *	"Packets fast retransmitted"
 */
static inline __attribute__((no_instrument_function)) unsigned int Model0_tcp_packets_in_flight(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_packets_out - Model0_tcp_left_out(Model0_tp) + Model0_tp->Model0_retrans_out;
}



static inline __attribute__((no_instrument_function)) bool Model0_tcp_in_slow_start(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_snd_cwnd < Model0_tp->Model0_snd_ssthresh;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_in_initial_slowstart(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_snd_ssthresh >= 0x7fffffff;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_in_cwnd_reduction(const struct Model0_sock *Model0_sk)
{
 return ((1<<Model0_TCP_CA_CWR) | (1<<Model0_TCP_CA_Recovery)) &
        (1 << Model0_inet_csk(Model0_sk)->Model0_icsk_ca_state);
}

/* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
 * The exception is cwnd reduction phase, when cwnd is decreasing towards
 * ssthresh.
 */
static inline __attribute__((no_instrument_function)) __u32 Model0_tcp_current_ssthresh(const struct Model0_sock *Model0_sk)
{
 const struct Model0_tcp_sock *Model0_tp = Model0_tcp_sk(Model0_sk);

 if (Model0_tcp_in_cwnd_reduction(Model0_sk))
  return Model0_tp->Model0_snd_ssthresh;
 else
  return ({ typeof(Model0_tp->Model0_snd_ssthresh) Model0__max1 = (Model0_tp->Model0_snd_ssthresh); typeof(((Model0_tp->Model0_snd_cwnd >> 1) + (Model0_tp->Model0_snd_cwnd >> 2))) Model0__max2 = (((Model0_tp->Model0_snd_cwnd >> 1) + (Model0_tp->Model0_snd_cwnd >> 2))); (void) (&Model0__max1 == &Model0__max2); Model0__max1 > Model0__max2 ? Model0__max1 : Model0__max2; });


}

/* Use define here intentionally to get WARN_ON location shown at the caller */


void Model0_tcp_enter_cwr(struct Model0_sock *Model0_sk);
__u32 Model0_tcp_init_cwnd(const struct Model0_tcp_sock *Model0_tp, const struct Model0_dst_entry *Model0_dst);

/* The maximum number of MSS of available cwnd for which TSO defers
 * sending if not using sysctl_tcp_tso_win_divisor.
 */
static inline __attribute__((no_instrument_function)) __u32 Model0_tcp_max_tso_deferred_mss(const struct Model0_tcp_sock *Model0_tp)
{
 return 3;
}

/* Returns end sequence number of the receiver's advertised window */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_wnd_end(const struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_snd_una + Model0_tp->Model0_snd_wnd;
}

/* We follow the spirit of RFC2861 to validate cwnd but implement a more
 * flexible approach. The RFC suggests cwnd should not be raised unless
 * it was fully used previously. And that's exactly what we do in
 * congestion avoidance mode. But in slow start we allow cwnd to grow
 * as long as the application has used half the cwnd.
 * Example :
 *    cwnd is 10 (IW10), but application sends 9 frames.
 *    We allow cwnd to reach 18 when all frames are ACKed.
 * This check is safe because it's as aggressive as slow start which already
 * risks 100% overshoot. The advantage is that we discourage application to
 * either send more filler packets or data to artificially blow up the cwnd
 * usage, and allow application-limited process to probe bw more aggressively.
 */
static inline __attribute__((no_instrument_function)) bool Model0_tcp_is_cwnd_limited(const struct Model0_sock *Model0_sk)
{
 const struct Model0_tcp_sock *Model0_tp = Model0_tcp_sk(Model0_sk);

 /* If in slow start, ensure cwnd grows to twice what was ACKed. */
 if (Model0_tcp_in_slow_start(Model0_tp))
  return Model0_tp->Model0_snd_cwnd < 2 * Model0_tp->Model0_max_packets_out;

 return Model0_tp->Model0_is_cwnd_limited;
}

/* Something is really bad, we could not queue an additional packet,
 * because qdisc is full or receiver sent a 0 window.
 * We do not want to add fuel to the fire, or abort too early,
 * so make sure the timer we arm now is at least 200ms in the future,
 * regardless of current icsk_rto value (as it could be ~2ms)
 */
static inline __attribute__((no_instrument_function)) unsigned long Model0_tcp_probe0_base(const struct Model0_sock *Model0_sk)
{
 return ({ unsigned long Model0___max1 = (Model0_inet_csk(Model0_sk)->Model0_icsk_rto); unsigned long Model0___max2 = (((unsigned)(1000/5))); Model0___max1 > Model0___max2 ? Model0___max1: Model0___max2; });
}

/* Variant of inet_csk_rto_backoff() used for zero window probes */
static inline __attribute__((no_instrument_function)) unsigned long Model0_tcp_probe0_when(const struct Model0_sock *Model0_sk,
         unsigned long Model0_max_when)
{
 Model0_u64 Model0_when = (Model0_u64)Model0_tcp_probe0_base(Model0_sk) << Model0_inet_csk(Model0_sk)->Model0_icsk_backoff;

 return (unsigned long)({ Model0_u64 Model0___min1 = (Model0_when); Model0_u64 Model0___min2 = (Model0_max_when); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; });
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_check_probe_timer(struct Model0_sock *Model0_sk)
{
 if (!Model0_tcp_sk(Model0_sk)->Model0_packets_out && !Model0_inet_csk(Model0_sk)->Model0_icsk_pending)
  Model0_inet_csk_reset_xmit_timer(Model0_sk, 3,
       Model0_tcp_probe0_base(Model0_sk), ((unsigned)(120*1000)));
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_init_wl(struct Model0_tcp_sock *Model0_tp, Model0_u32 Model0_seq)
{
 Model0_tp->Model0_snd_wl1 = Model0_seq;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_update_wl(struct Model0_tcp_sock *Model0_tp, Model0_u32 Model0_seq)
{
 Model0_tp->Model0_snd_wl1 = Model0_seq;
}

/*
 * Calculate(/check) TCP checksum
 */
static inline __attribute__((no_instrument_function)) Model0___sum16 Model0_tcp_v4_check(int Model0_len, Model0___be32 Model0_saddr,
       Model0___be32 Model0_daddr, Model0___wsum Model0_base)
{
 return Model0_csum_tcpudp_magic(Model0_saddr,Model0_daddr,Model0_len,Model0_IPPROTO_TCP,Model0_base);
}

static inline __attribute__((no_instrument_function)) Model0___sum16 Model0___tcp_checksum_complete(struct Model0_sk_buff *Model0_skb)
{
 return Model0___skb_checksum_complete(Model0_skb);
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_checksum_complete(struct Model0_sk_buff *Model0_skb)
{
 return !Model0_skb_csum_unnecessary(Model0_skb) &&
  Model0___tcp_checksum_complete(Model0_skb);
}

/* Prequeue for VJ style copy to user, combined with checksumming. */

static inline __attribute__((no_instrument_function)) void Model0_tcp_prequeue_init(struct Model0_tcp_sock *Model0_tp)
{
 Model0_tp->Model0_ucopy.Model0_task = ((void *)0);
 Model0_tp->Model0_ucopy.Model0_len = 0;
 Model0_tp->Model0_ucopy.Model0_memory = 0;
 Model0_skb_queue_head_init(&Model0_tp->Model0_ucopy.Model0_prequeue);
}

bool Model0_tcp_prequeue(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
void Model0_tcp_set_state(struct Model0_sock *Model0_sk, int Model0_state);

void Model0_tcp_done(struct Model0_sock *Model0_sk);

int Model0_tcp_abort(struct Model0_sock *Model0_sk, int err);

static inline __attribute__((no_instrument_function)) void Model0_tcp_sack_reset(struct Model0_tcp_options_received *Model0_rx_opt)
{
 Model0_rx_opt->Model0_dsack = 0;
 Model0_rx_opt->Model0_num_sacks = 0;
}

Model0_u32 Model0_tcp_default_init_rwnd(Model0_u32 Model0_mss);
void Model0_tcp_cwnd_restart(struct Model0_sock *Model0_sk, Model0_s32 Model0_delta);

static inline __attribute__((no_instrument_function)) void Model0_tcp_slow_start_after_idle_check(struct Model0_sock *Model0_sk)
{
 struct Model0_tcp_sock *Model0_tp = Model0_tcp_sk(Model0_sk);
 Model0_s32 Model0_delta;

 if (!Model0_sysctl_tcp_slow_start_after_idle || Model0_tp->Model0_packets_out)
  return;
 Model0_delta = ((__u32)(Model0_jiffies)) - Model0_tp->Model0_lsndtime;
 if (Model0_delta > Model0_inet_csk(Model0_sk)->Model0_icsk_rto)
  Model0_tcp_cwnd_restart(Model0_sk, Model0_delta);
}

/* Determine a window scaling and initial window to offer. */
void Model0_tcp_select_initial_window(int Model0___space, __u32 Model0_mss, __u32 *Model0_rcv_wnd,
          __u32 *Model0_window_clamp, int Model0_wscale_ok,
          __u8 *Model0_rcv_wscale, __u32 Model0_init_rcv_wnd);

static inline __attribute__((no_instrument_function)) int Model0_tcp_win_from_space(int Model0_space)
{
 return Model0_sysctl_tcp_adv_win_scale<=0 ?
  (Model0_space>>(-Model0_sysctl_tcp_adv_win_scale)) :
  Model0_space - (Model0_space>>Model0_sysctl_tcp_adv_win_scale);
}

/* Note: caller must be prepared to deal with negative returns */
static inline __attribute__((no_instrument_function)) int Model0_tcp_space(const struct Model0_sock *Model0_sk)
{
 return Model0_tcp_win_from_space(Model0_sk->Model0_sk_rcvbuf -
      Model0_atomic_read(&Model0_sk->Model0_sk_backlog.Model0_rmem_alloc));
}

static inline __attribute__((no_instrument_function)) int Model0_tcp_full_space(const struct Model0_sock *Model0_sk)
{
 return Model0_tcp_win_from_space(Model0_sk->Model0_sk_rcvbuf);
}

extern void Model0_tcp_openreq_init_rwin(struct Model0_request_sock *Model0_req,
      const struct Model0_sock *Model0_sk_listener,
      const struct Model0_dst_entry *Model0_dst);

void Model0_tcp_enter_memory_pressure(struct Model0_sock *Model0_sk);

static inline __attribute__((no_instrument_function)) int Model0_keepalive_intvl_when(const struct Model0_tcp_sock *Model0_tp)
{
 struct Model0_net *Model0_net = Model0_sock_net((struct Model0_sock *)Model0_tp);

 return Model0_tp->Model0_keepalive_intvl ? : Model0_net->Model0_ipv4.Model0_sysctl_tcp_keepalive_intvl;
}

static inline __attribute__((no_instrument_function)) int Model0_keepalive_time_when(const struct Model0_tcp_sock *Model0_tp)
{
 struct Model0_net *Model0_net = Model0_sock_net((struct Model0_sock *)Model0_tp);

 return Model0_tp->Model0_keepalive_time ? : Model0_net->Model0_ipv4.Model0_sysctl_tcp_keepalive_time;
}

static inline __attribute__((no_instrument_function)) int Model0_keepalive_probes(const struct Model0_tcp_sock *Model0_tp)
{
 struct Model0_net *Model0_net = Model0_sock_net((struct Model0_sock *)Model0_tp);

 return Model0_tp->Model0_keepalive_probes ? : Model0_net->Model0_ipv4.Model0_sysctl_tcp_keepalive_probes;
}

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_keepalive_time_elapsed(const struct Model0_tcp_sock *Model0_tp)
{
 const struct Model0_inet_connection_sock *Model0_icsk = &Model0_tp->Model0_inet_conn;

 return ({ Model0_u32 Model0___min1 = (((__u32)(Model0_jiffies)) - Model0_icsk->Model0_icsk_ack.Model0_lrcvtime); Model0_u32 Model0___min2 = (((__u32)(Model0_jiffies)) - Model0_tp->Model0_rcv_tstamp); Model0___min1 < Model0___min2 ? Model0___min1: Model0___min2; });

}

static inline __attribute__((no_instrument_function)) int Model0_tcp_fin_time(const struct Model0_sock *Model0_sk)
{
 int Model0_fin_timeout = Model0_tcp_sk(Model0_sk)->Model0_linger2 ? : Model0_sock_net(Model0_sk)->Model0_ipv4.Model0_sysctl_tcp_fin_timeout;
 const int Model0_rto = Model0_inet_csk(Model0_sk)->Model0_icsk_rto;

 if (Model0_fin_timeout < (Model0_rto << 2) - (Model0_rto >> 1))
  Model0_fin_timeout = (Model0_rto << 2) - (Model0_rto >> 1);

 return Model0_fin_timeout;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_paws_check(const struct Model0_tcp_options_received *Model0_rx_opt,
      int Model0_paws_win)
{
 if ((Model0_s32)(Model0_rx_opt->Model0_ts_recent - Model0_rx_opt->Model0_rcv_tsval) <= Model0_paws_win)
  return true;
 if (__builtin_expect(!!(Model0_get_seconds() >= Model0_rx_opt->Model0_ts_recent_stamp + (60 * 60 * 24 * 24)), 0))
  return true;
 /*
	 * Some OSes send SYN and SYNACK messages with tsval=0 tsecr=0,
	 * then following tcp messages have valid values. Ignore 0 value,
	 * or else 'negative' tsval might forbid us to accept their packets.
	 */
 if (!Model0_rx_opt->Model0_ts_recent)
  return true;
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_paws_reject(const struct Model0_tcp_options_received *Model0_rx_opt,
       int Model0_rst)
{
 if (Model0_tcp_paws_check(Model0_rx_opt, 0))
  return false;

 /* RST segments are not recommended to carry timestamp,
	   and, if they do, it is recommended to ignore PAWS because
	   "their cleanup function should take precedence over timestamps."
	   Certainly, it is mistake. It is necessary to understand the reasons
	   of this constraint to relax it: if peer reboots, clock may go
	   out-of-sync and half-open connections will not be reset.
	   Actually, the problem would be not existing if all
	   the implementations followed draft about maintaining clock
	   via reboots. Linux-2.2 DOES NOT!

	   However, we can relax time bounds for RST segments to MSL.
	 */
 if (Model0_rst && Model0_get_seconds() >= Model0_rx_opt->Model0_ts_recent_stamp + 60)
  return false;
 return true;
}

bool Model0_tcp_oow_rate_limited(struct Model0_net *Model0_net, const struct Model0_sk_buff *Model0_skb,
     int Model0_mib_idx, Model0_u32 *Model0_last_oow_ack_time);

#if CY_ABSTRACT0
extern struct Model0_tcp_mib Model0_cy_tcp_mib;
extern struct Model0_linux_mib Model0_cy_linux_mib;
#endif

static inline __attribute__((no_instrument_function)) void Model0_tcp_mib_init(struct Model0_net *Model0_net)
{
 /* See RFC 2012 */
 (Model0_cy_tcp_mib.Model0_mibs[Model0_TCP_MIB_RTOALGORITHM] += 1);
 (Model0_cy_tcp_mib.Model0_mibs[Model0_TCP_MIB_RTOMIN] += ((unsigned)(1000/5))*1000/1000);
 (Model0_cy_tcp_mib.Model0_mibs[Model0_TCP_MIB_RTOMAX] += ((unsigned)(120*1000))*1000/1000);
 (Model0_cy_tcp_mib.Model0_mibs[Model0_TCP_MIB_MAXCONN] += -1);
}

/* from STCP */
static inline __attribute__((no_instrument_function)) void Model0_tcp_clear_retrans_hints_partial(struct Model0_tcp_sock *Model0_tp)
{
 Model0_tp->Model0_lost_skb_hint = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_clear_all_retrans_hints(struct Model0_tcp_sock *Model0_tp)
{
 Model0_tcp_clear_retrans_hints_partial(Model0_tp);
 Model0_tp->Model0_retransmit_skb_hint = ((void *)0);
}

union Model0_tcp_md5_addr {
 struct Model0_in_addr Model0_a4;

 struct Model0_in6_addr Model0_a6;

};

/* - key database */
struct Model0_tcp_md5sig_key {
 struct Model0_hlist_node Model0_node;
 Model0_u8 Model0_keylen;
 Model0_u8 Model0_family; /* AF_INET or AF_INET6 */
 union Model0_tcp_md5_addr Model0_addr;
 Model0_u8 Model0_key[80];
 struct Model0_callback_head Model0_rcu;
};

/* - sock block */
struct Model0_tcp_md5sig_info {
 struct Model0_hlist_head Model0_head;
 struct Model0_callback_head Model0_rcu;
};

/* - pseudo header */
struct Model0_tcp4_pseudohdr {
 Model0___be32 Model0_saddr;
 Model0___be32 Model0_daddr;
 __u8 Model0_pad;
 __u8 Model0_protocol;
 Model0___be16 Model0_len;
};

struct Model0_tcp6_pseudohdr {
 struct Model0_in6_addr Model0_saddr;
 struct Model0_in6_addr Model0_daddr;
 Model0___be32 Model0_len;
 Model0___be32 Model0_protocol; /* including padding */
};

union Model0_tcp_md5sum_block {
 struct Model0_tcp4_pseudohdr Model0_ip4;

 struct Model0_tcp6_pseudohdr Model0_ip6;

};

/* - pool: digest algorithm, hash description and scratch buffer */
struct Model0_tcp_md5sig_pool {
 struct Model0_ahash_request *Model0_md5_req;
 void *Model0_scratch;
};

/* - functions */
int Model0_tcp_v4_md5_hash_skb(char *Model0_md5_hash, const struct Model0_tcp_md5sig_key *Model0_key,
   const struct Model0_sock *Model0_sk, const struct Model0_sk_buff *Model0_skb);
int Model0_tcp_md5_do_add(struct Model0_sock *Model0_sk, const union Model0_tcp_md5_addr *Model0_addr,
     int Model0_family, const Model0_u8 *Model0_newkey, Model0_u8 Model0_newkeylen, Model0_gfp_t Model0_gfp);
int Model0_tcp_md5_do_del(struct Model0_sock *Model0_sk, const union Model0_tcp_md5_addr *Model0_addr,
     int Model0_family);
struct Model0_tcp_md5sig_key *Model0_tcp_v4_md5_lookup(const struct Model0_sock *Model0_sk,
      const struct Model0_sock *Model0_addr_sk);


static struct Model0_tcp_md5sig_key *Model0_tcp_md5_do_lookup(const struct Model0_sock *Model0_sk,
      const union Model0_tcp_md5_addr *Model0_addr,
      int Model0_family);
bool Model0_tcp_alloc_md5sig_pool(void);

struct Model0_tcp_md5sig_pool *Model0_tcp_get_md5sig_pool(void);
static inline __attribute__((no_instrument_function)) void Model0_tcp_put_md5sig_pool(void)
{
 Model0_local_bh_enable();
}

int Model0_tcp_md5_hash_skb_data(struct Model0_tcp_md5sig_pool *, const struct Model0_sk_buff *,
     unsigned int Model0_header_len);
int Model0_tcp_md5_hash_key(struct Model0_tcp_md5sig_pool *Model0_hp,
       const struct Model0_tcp_md5sig_key *Model0_key);

/* From tcp_fastopen.c */
void Model0_tcp_fastopen_cache_get(struct Model0_sock *Model0_sk, Model0_u16 *Model0_mss,
       struct Model0_tcp_fastopen_cookie *Model0_cookie, int *Model0_syn_loss,
       unsigned long *Model0_last_syn_loss);
void Model0_tcp_fastopen_cache_set(struct Model0_sock *Model0_sk, Model0_u16 Model0_mss,
       struct Model0_tcp_fastopen_cookie *Model0_cookie, bool Model0_syn_lost,
       Model0_u16 Model0_try_exp);
struct Model0_tcp_fastopen_request {
 /* Fast Open cookie. Size 0 means a cookie request */
 struct Model0_tcp_fastopen_cookie Model0_cookie;
 struct Model0_msghdr *Model0_data; /* data in MSG_FASTOPEN */
 Model0_size_t Model0_size;
 int Model0_copied; /* queued in tcp_connect() */
};
void Model0_tcp_free_fastopen_req(struct Model0_tcp_sock *Model0_tp);

extern struct Model0_tcp_fastopen_context *Model0_tcp_fastopen_ctx;
int Model0_tcp_fastopen_reset_cipher(void *Model0_key, unsigned int Model0_len);
void Model0_tcp_fastopen_add_skb(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);
struct Model0_sock *Model0_tcp_try_fastopen(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
         struct Model0_request_sock *Model0_req,
         struct Model0_tcp_fastopen_cookie *Model0_foc,
         struct Model0_dst_entry *Model0_dst);
void Model0_tcp_fastopen_init_key_once(bool Model0_publish);


/* Fastopen key context */
struct Model0_tcp_fastopen_context {
 struct Model0_crypto_cipher *Model0_tfm;
 __u8 Model0_key[16];
 struct Model0_callback_head Model0_rcu;
};

/* write queue abstraction */
static inline __attribute__((no_instrument_function)) void Model0_tcp_write_queue_purge(struct Model0_sock *Model0_sk)
{
 struct Model0_sk_buff *Model0_skb;

 while ((Model0_skb = Model0___skb_dequeue(&Model0_sk->Model0_sk_write_queue)) != ((void *)0))
  Model0_sk_wmem_free_skb(Model0_sk, Model0_skb);
 Model0_sk_mem_reclaim(Model0_sk);
 Model0_tcp_clear_all_retrans_hints(Model0_tcp_sk(Model0_sk));
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_tcp_write_queue_head(const struct Model0_sock *Model0_sk)
{
 return Model0_skb_peek(&Model0_sk->Model0_sk_write_queue);
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_tcp_write_queue_tail(const struct Model0_sock *Model0_sk)
{
 return Model0_skb_peek_tail(&Model0_sk->Model0_sk_write_queue);
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_tcp_write_queue_next(const struct Model0_sock *Model0_sk,
         const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_queue_next(&Model0_sk->Model0_sk_write_queue, Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_tcp_write_queue_prev(const struct Model0_sock *Model0_sk,
         const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_queue_prev(&Model0_sk->Model0_sk_write_queue, Model0_skb);
}
static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_tcp_send_head(const struct Model0_sock *Model0_sk)
{
 return Model0_sk->Model0_sk_send_head;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_skb_is_last(const struct Model0_sock *Model0_sk,
       const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb_queue_is_last(&Model0_sk->Model0_sk_write_queue, Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_advance_send_head(struct Model0_sock *Model0_sk, const struct Model0_sk_buff *Model0_skb)
{
 if (Model0_tcp_skb_is_last(Model0_sk, Model0_skb))
  Model0_sk->Model0_sk_send_head = ((void *)0);
 else
  Model0_sk->Model0_sk_send_head = Model0_tcp_write_queue_next(Model0_sk, Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_check_send_head(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb_unlinked)
{
 if (Model0_sk->Model0_sk_send_head == Model0_skb_unlinked)
  Model0_sk->Model0_sk_send_head = ((void *)0);
 if (Model0_tcp_sk(Model0_sk)->Model0_highest_sack == Model0_skb_unlinked)
  Model0_tcp_sk(Model0_sk)->Model0_highest_sack = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_init_send_head(struct Model0_sock *Model0_sk)
{
 Model0_sk->Model0_sk_send_head = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model0___tcp_add_write_queue_tail(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 Model0___skb_queue_tail(&Model0_sk->Model0_sk_write_queue, Model0_skb);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_add_write_queue_tail(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 Model0___tcp_add_write_queue_tail(Model0_sk, Model0_skb);

 /* Queue it, remembering where we must start sending. */
 if (Model0_sk->Model0_sk_send_head == ((void *)0)) {
  Model0_sk->Model0_sk_send_head = Model0_skb;

  if (Model0_tcp_sk(Model0_sk)->Model0_highest_sack == ((void *)0))
   Model0_tcp_sk(Model0_sk)->Model0_highest_sack = Model0_skb;
 }
}

static inline __attribute__((no_instrument_function)) void Model0___tcp_add_write_queue_head(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 Model0___skb_queue_head(&Model0_sk->Model0_sk_write_queue, Model0_skb);
}

/* Insert buff after skb on the write queue of sk.  */
static inline __attribute__((no_instrument_function)) void Model0_tcp_insert_write_queue_after(struct Model0_sk_buff *Model0_skb,
      struct Model0_sk_buff *Model0_buff,
      struct Model0_sock *Model0_sk)
{
 Model0___skb_queue_after(&Model0_sk->Model0_sk_write_queue, Model0_skb, Model0_buff);
}

/* Insert new before skb on the write queue of sk.  */
static inline __attribute__((no_instrument_function)) void Model0_tcp_insert_write_queue_before(struct Model0_sk_buff *Model0_new,
        struct Model0_sk_buff *Model0_skb,
        struct Model0_sock *Model0_sk)
{
 Model0___skb_queue_before(&Model0_sk->Model0_sk_write_queue, Model0_skb, Model0_new);

 if (Model0_sk->Model0_sk_send_head == Model0_skb)
  Model0_sk->Model0_sk_send_head = Model0_new;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_unlink_write_queue(struct Model0_sk_buff *Model0_skb, struct Model0_sock *Model0_sk)
{
 Model0___skb_unlink(Model0_skb, &Model0_sk->Model0_sk_write_queue);
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_write_queue_empty(struct Model0_sock *Model0_sk)
{
 return Model0_skb_queue_empty(&Model0_sk->Model0_sk_write_queue);
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_push_pending_frames(struct Model0_sock *Model0_sk)
{
#if !CY_ABSTRACT1
 if (Model0_tcp_send_head(Model0_sk)) {
  struct Model0_tcp_sock *Model0_tp = Model0_tcp_sk(Model0_sk);

  Model0___tcp_push_pending_frames(Model0_sk, Model0_tcp_current_mss(Model0_sk), Model0_tp->Model0_nonagle);
 }
#endif
}

/* Start sequence of the skb just after the highest skb with SACKed
 * bit, valid only if sacked_out > 0 or when the caller has ensured
 * validity by itself.
 */
static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_highest_sack_seq(struct Model0_tcp_sock *Model0_tp)
{
 if (!Model0_tp->Model0_sacked_out)
  return Model0_tp->Model0_snd_una;

 if (Model0_tp->Model0_highest_sack == ((void *)0))
  return Model0_tp->Model0_snd_nxt;

 return ((struct Model0_tcp_skb_cb *)&((Model0_tp->Model0_highest_sack)->Model0_cb[0]))->Model0_seq;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_advance_highest_sack(struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb)
{
 Model0_tcp_sk(Model0_sk)->Model0_highest_sack = Model0_tcp_skb_is_last(Model0_sk, Model0_skb) ? ((void *)0) :
      Model0_tcp_write_queue_next(Model0_sk, Model0_skb);
}

static inline __attribute__((no_instrument_function)) struct Model0_sk_buff *Model0_tcp_highest_sack(struct Model0_sock *Model0_sk)
{
 return Model0_tcp_sk(Model0_sk)->Model0_highest_sack;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_highest_sack_reset(struct Model0_sock *Model0_sk)
{
 Model0_tcp_sk(Model0_sk)->Model0_highest_sack = Model0_tcp_write_queue_head(Model0_sk);
}

/* Called when old skb is about to be deleted (to be combined with new skb) */
static inline __attribute__((no_instrument_function)) void Model0_tcp_highest_sack_combine(struct Model0_sock *Model0_sk,
         struct Model0_sk_buff *old,
         struct Model0_sk_buff *Model0_new)
{
 if (Model0_tcp_sk(Model0_sk)->Model0_sacked_out && (old == Model0_tcp_sk(Model0_sk)->Model0_highest_sack))
  Model0_tcp_sk(Model0_sk)->Model0_highest_sack = Model0_new;
}

/* This helper checks if socket has IP_TRANSPARENT set */
static inline __attribute__((no_instrument_function)) bool Model0_inet_sk_transparent(const struct Model0_sock *Model0_sk)
{
 switch (Model0_sk->Model0___sk_common.Model0_skc_state) {
 case Model0_TCP_TIME_WAIT:
  return Model0_inet_twsk(Model0_sk)->Model0_tw_transparent;
 case Model0_TCP_NEW_SYN_RECV:
  return Model0_inet_rsk(Model0_inet_reqsk(Model0_sk))->Model0_no_srccheck;
 }
 return Model0_inet_sk(Model0_sk)->Model0_transparent;
}

/* Determines whether this is a thin stream (which may suffer from
 * increased latency). Used to trigger latency-reducing mechanisms.
 */
static inline __attribute__((no_instrument_function)) bool Model0_tcp_stream_is_thin(struct Model0_tcp_sock *Model0_tp)
{
 return Model0_tp->Model0_packets_out < 4 && !Model0_tcp_in_initial_slowstart(Model0_tp);
}

/* /proc */
enum Model0_tcp_seq_states {
 Model0_TCP_SEQ_STATE_LISTENING,
 Model0_TCP_SEQ_STATE_ESTABLISHED,
};

int Model0_tcp_seq_open(struct Model0_inode *Model0_inode, struct Model0_file *Model0_file);

struct Model0_tcp_seq_afinfo {
 char *Model0_name;
 Model0_sa_family_t Model0_family;
 const struct Model0_file_operations *Model0_seq_fops;
 struct Model0_seq_operations Model0_seq_ops;
};

struct Model0_tcp_iter_state {
 struct Model0_seq_net_private Model0_p;
 Model0_sa_family_t Model0_family;
 enum Model0_tcp_seq_states Model0_state;
 struct Model0_sock *Model0_syn_wait_sk;
 int Model0_bucket, Model0_offset, Model0_sbucket, Model0_num;
 Model0_loff_t Model0_last_pos;
};

int Model0_tcp_proc_register(struct Model0_net *Model0_net, struct Model0_tcp_seq_afinfo *Model0_afinfo);
void Model0_tcp_proc_unregister(struct Model0_net *Model0_net, struct Model0_tcp_seq_afinfo *Model0_afinfo);

extern struct Model0_request_sock_ops Model0_tcp_request_sock_ops;
extern struct Model0_request_sock_ops Model0_tcp6_request_sock_ops;

void Model0_tcp_v4_destroy_sock(struct Model0_sock *Model0_sk);

struct Model0_sk_buff *Model0_tcp_gso_segment(struct Model0_sk_buff *Model0_skb,
    Model0_netdev_features_t Model0_features);
struct Model0_sk_buff **Model0_tcp_gro_receive(struct Model0_sk_buff **Model0_head, struct Model0_sk_buff *Model0_skb);
int Model0_tcp_gro_complete(struct Model0_sk_buff *Model0_skb);

void Model0___tcp_v4_send_check(struct Model0_sk_buff *Model0_skb, Model0___be32 Model0_saddr, Model0___be32 Model0_daddr);

static inline __attribute__((no_instrument_function)) Model0_u32 Model0_tcp_notsent_lowat(const struct Model0_tcp_sock *Model0_tp)
{
 struct Model0_net *Model0_net = Model0_sock_net((struct Model0_sock *)Model0_tp);
 return Model0_tp->Model0_notsent_lowat ?: Model0_net->Model0_ipv4.Model0_sysctl_tcp_notsent_lowat;
}

static inline __attribute__((no_instrument_function)) bool Model0_tcp_stream_memory_free(const struct Model0_sock *Model0_sk)
{
 const struct Model0_tcp_sock *Model0_tp = Model0_tcp_sk(Model0_sk);
 Model0_u32 Model0_notsent_bytes = Model0_tp->Model0_write_seq - Model0_tp->Model0_snd_nxt;

 return Model0_notsent_bytes < Model0_tcp_notsent_lowat(Model0_tp);
}


int Model0_tcp4_proc_init(void);
void Model0_tcp4_proc_exit(void);


int Model0_tcp_rtx_synack(const struct Model0_sock *Model0_sk, struct Model0_request_sock *Model0_req);
int Model0_tcp_conn_request(struct Model0_request_sock_ops *Model0_rsk_ops,
       const struct Model0_tcp_request_sock_ops *Model0_af_ops,
       struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb);

/* TCP af-specific functions */
struct Model0_tcp_sock_af_ops {

 struct Model0_tcp_md5sig_key *(*Model0_md5_lookup) (const struct Model0_sock *Model0_sk,
      const struct Model0_sock *Model0_addr_sk);
 int (*Model0_calc_md5_hash)(char *Model0_location,
      const struct Model0_tcp_md5sig_key *Model0_md5,
      const struct Model0_sock *Model0_sk,
      const struct Model0_sk_buff *Model0_skb);
 int (*Model0_md5_parse)(struct Model0_sock *Model0_sk,
         char *Model0_optval,
         int Model0_optlen);

};

struct Model0_tcp_request_sock_ops {
 Model0_u16 Model0_mss_clamp;

 struct Model0_tcp_md5sig_key *(*Model0_req_md5_lookup)(const struct Model0_sock *Model0_sk,
       const struct Model0_sock *Model0_addr_sk);
 int (*Model0_calc_md5_hash) (char *Model0_location,
       const struct Model0_tcp_md5sig_key *Model0_md5,
       const struct Model0_sock *Model0_sk,
       const struct Model0_sk_buff *Model0_skb);

 void (*Model0_init_req)(struct Model0_request_sock *Model0_req,
    const struct Model0_sock *Model0_sk_listener,
    struct Model0_sk_buff *Model0_skb);

 __u32 (*Model0_cookie_init_seq)(const struct Model0_sk_buff *Model0_skb,
     Model0___u16 *Model0_mss);

 struct Model0_dst_entry *(*Model0_route_req)(const struct Model0_sock *Model0_sk, struct Model0_flowi *Model0_fl,
           const struct Model0_request_sock *Model0_req,
           bool *Model0_strict);
 __u32 (*Model0_init_seq)(const struct Model0_sk_buff *Model0_skb);
 int (*Model0_send_synack)(const struct Model0_sock *Model0_sk, struct Model0_dst_entry *Model0_dst,
      struct Model0_flowi *Model0_fl, struct Model0_request_sock *Model0_req,
      struct Model0_tcp_fastopen_cookie *Model0_foc,
      enum Model0_tcp_synack_type Model0_synack_type);
};


static inline __attribute__((no_instrument_function)) __u32 Model0_cookie_init_sequence(const struct Model0_tcp_request_sock_ops *Model0_ops,
      const struct Model0_sock *Model0_sk, struct Model0_sk_buff *Model0_skb,
      Model0___u16 *Model0_mss)
{
 Model0_tcp_synq_overflow(Model0_sk);
 (Model0_cy_linux_mib.Model0_mibs[Model0_LINUX_MIB_SYNCOOKIESSENT] += 1);
 return Model0_ops->Model0_cookie_init_seq(Model0_skb, Model0_mss);
}
int Model0_tcpv4_offload_init(void);

void Model0_tcp_v4_init(void);
void Model0_tcp_init(void);

/* tcp_recovery.c */

/* Flags to enable various loss recovery features. See below */
extern int Model0_sysctl_tcp_recovery;

/* Use TCP RACK to detect (some) tail and retransmit losses */


extern int Model0_tcp_rack_mark_lost(struct Model0_sock *Model0_sk);

extern void Model0_tcp_rack_advance(struct Model0_tcp_sock *Model0_tp,
        const struct Model0_skb_mstamp *Model0_xmit_time, Model0_u8 Model0_sacked);

/*
 * Save and compile IPv4 options, return a pointer to it
 */
static inline __attribute__((no_instrument_function)) struct Model0_ip_options_rcu *Model0_tcp_v4_save_options(struct Model0_sk_buff *Model0_skb)
{
 const struct Model0_ip_options *Model0_opt = &((struct Model0_tcp_skb_cb *)&((Model0_skb)->Model0_cb[0]))->Model0_header.Model0_h4.Model0_opt;
 struct Model0_ip_options_rcu *Model0_dopt = ((void *)0);

 if (Model0_opt->Model0_optlen) {
  int Model0_opt_size = sizeof(*Model0_dopt) + Model0_opt->Model0_optlen;

  Model0_dopt = Model0_kmalloc(Model0_opt_size, ((( Model0_gfp_t)0x20u)|(( Model0_gfp_t)0x80000u)|(( Model0_gfp_t)0x2000000u)));
  if (Model0_dopt && Model0___ip_options_echo(&Model0_dopt->Model0_opt, Model0_skb, Model0_opt)) {
   Model0_kfree(Model0_dopt);
   Model0_dopt = ((void *)0);
  }
 }
 return Model0_dopt;
}

/* locally generated TCP pure ACKs have skb->truesize == 2
 * (check tcp_send_ack() in net/ipv4/tcp_output.c )
 * This is much faster than dissecting the packet to find out.
 * (Think of GRE encapsulations, IPv4, IPv6, ...)
 */
static inline __attribute__((no_instrument_function)) bool Model0_skb_is_tcp_pure_ack(const struct Model0_sk_buff *Model0_skb)
{
 return Model0_skb->Model0_truesize == 2;
}

static inline __attribute__((no_instrument_function)) void Model0_skb_set_tcp_pure_ack(struct Model0_sk_buff *Model0_skb)
{
 Model0_skb->Model0_truesize = 2;
}

static inline __attribute__((no_instrument_function)) int Model0_tcp_inq(struct Model0_sock *Model0_sk)
{
 struct Model0_tcp_sock *Model0_tp = Model0_tcp_sk(Model0_sk);
 int Model0_answ;

 if ((1 << Model0_sk->Model0___sk_common.Model0_skc_state) & (Model0_TCPF_SYN_SENT | Model0_TCPF_SYN_RECV)) {
  Model0_answ = 0;
 } else if (Model0_sock_flag(Model0_sk, Model0_SOCK_URGINLINE) ||
     !Model0_tp->Model0_urg_data ||
     Model0_before(Model0_tp->Model0_urg_seq, Model0_tp->Model0_copied_seq) ||
     !Model0_before(Model0_tp->Model0_urg_seq, Model0_tp->Model0_rcv_nxt)) {

  Model0_answ = Model0_tp->Model0_rcv_nxt - Model0_tp->Model0_copied_seq;

  /* Subtract 1, if FIN was received */
  if (Model0_answ && Model0_sock_flag(Model0_sk, Model0_SOCK_DONE))
   Model0_answ--;
 } else {
  Model0_answ = Model0_tp->Model0_urg_seq - Model0_tp->Model0_copied_seq;
 }

 return Model0_answ;
}

static inline __attribute__((no_instrument_function)) void Model0_tcp_segs_in(struct Model0_tcp_sock *Model0_tp, const struct Model0_sk_buff *Model0_skb)
{
 Model0_u16 Model0_segs_in;

 Model0_segs_in = ({ Model0_u16 Model0___max1 = (1); Model0_u16 Model0___max2 = (((struct Model0_skb_shared_info *)(Model0_skb_end_pointer(Model0_skb)))->Model0_gso_segs); Model0___max1 > Model0___max2 ? Model0___max1: Model0___max2; });
 Model0_tp->Model0_segs_in += Model0_segs_in;
 if (Model0_skb->Model0_len > Model0_tcp_hdrlen(Model0_skb))
  Model0_tp->Model0_data_segs_in += Model0_segs_in;
}

/*
 * TCP listen path runs lockless.
 * We forced "struct sock" to be const qualified to make sure
 * we don't modify one of its field by mistake.
 * Here, we increment sk_drops which is an atomic_t, so we can safely
 * make sock writable again.
 */
static inline __attribute__((no_instrument_function)) void Model0_tcp_listendrop(const struct Model0_sock *Model0_sk)
{
 Model0_atomic_inc(&((struct Model0_sock *)Model0_sk)->Model0_sk_drops);
 (Model0_cy_linux_mib.Model0_mibs[Model0_LINUX_MIB_LISTENDROPS] += 1);
}



extern const struct Model0_proto_ops Model0_inet_stream_ops;
extern const struct Model0_proto_ops Model0_inet_dgram_ops;

/*
 *	INET4 prototypes used by INET6
 */

struct Model0_msghdr;
struct Model0_sock;
struct Model0_sockaddr;
struct Model0_socket;

int Model0_inet_release(struct Model0_socket *Model0_sock);
int Model0_inet_stream_connect(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_uaddr,
   int Model0_addr_len, int Model0_flags);
int Model0___inet_stream_connect(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_uaddr,
     int Model0_addr_len, int Model0_flags);
int Model0_inet_dgram_connect(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_uaddr,
         int Model0_addr_len, int Model0_flags);
int Model0_inet_accept(struct Model0_socket *Model0_sock, struct Model0_socket *Model0_newsock, int Model0_flags);
int Model0_inet_sendmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, Model0_size_t Model0_size);
Model0_ssize_t Model0_inet_sendpage(struct Model0_socket *Model0_sock, struct Model0_page *Model0_page, int Model0_offset,
        Model0_size_t Model0_size, int Model0_flags);
int Model0_inet_recvmsg(struct Model0_socket *Model0_sock, struct Model0_msghdr *Model0_msg, Model0_size_t Model0_size,
   int Model0_flags);
int Model0_inet_shutdown(struct Model0_socket *Model0_sock, int Model0_how);
int Model0_inet_listen(struct Model0_socket *Model0_sock, int Model0_backlog);
void Model0_inet_sock_destruct(struct Model0_sock *Model0_sk);
int Model0_inet_bind(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_uaddr, int Model0_addr_len);
int Model0_inet_getname(struct Model0_socket *Model0_sock, struct Model0_sockaddr *Model0_uaddr, int *Model0_uaddr_len,
   int Model0_peer);
int Model0_inet_ioctl(struct Model0_socket *Model0_sock, unsigned int Model0_cmd, unsigned long Model0_arg);
int Model0_inet_ctl_sock_create(struct Model0_sock **Model0_sk, unsigned short Model0_family,
    unsigned short Model0_type, unsigned char Model0_protocol,
    struct Model0_net *Model0_net);
int Model0_inet_recv_error(struct Model0_sock *Model0_sk, struct Model0_msghdr *Model0_msg, int Model0_len,
      int *Model0_addr_len);

struct Model0_sk_buff **Model0_inet_gro_receive(struct Model0_sk_buff **Model0_head, struct Model0_sk_buff *Model0_skb);
int Model0_inet_gro_complete(struct Model0_sk_buff *Model0_skb, int Model0_nhoff);
struct Model0_sk_buff *Model0_inet_gso_segment(struct Model0_sk_buff *Model0_skb,
     Model0_netdev_features_t Model0_features);

static inline __attribute__((no_instrument_function)) void Model0_inet_ctl_sock_destroy(struct Model0_sock *Model0_sk)
{
 if (Model0_sk)
  Model0_sock_release(Model0_sk->Model0_sk_socket);
}



/* The definitions, required to talk to KAME racoon IKE. */


/* PF_KEY user interface, this is defined by rfc2367 so
 * do not make arbitrary modifications or else this header
 * file will not be compliant.
 */
struct Model0_sadb_msg {
 __u8 Model0_sadb_msg_version;
 __u8 Model0_sadb_msg_type;
 __u8 Model0_sadb_msg_errno;
 __u8 Model0_sadb_msg_satype;
 Model0___u16 Model0_sadb_msg_len;
 Model0___u16 Model0_sadb_msg_reserved;
 __u32 Model0_sadb_msg_seq;
 __u32 Model0_sadb_msg_pid;
} __attribute__((packed));
/* sizeof(struct sadb_msg) == 16 */

struct Model0_sadb_ext {
 Model0___u16 Model0_sadb_ext_len;
 Model0___u16 Model0_sadb_ext_type;
} __attribute__((packed));
/* sizeof(struct sadb_ext) == 4 */

struct Model0_sadb_sa {
 Model0___u16 Model0_sadb_sa_len;
 Model0___u16 Model0_sadb_sa_exttype;
 Model0___be32 Model0_sadb_sa_spi;
 __u8 Model0_sadb_sa_replay;
 __u8 Model0_sadb_sa_state;
 __u8 Model0_sadb_sa_auth;
 __u8 Model0_sadb_sa_encrypt;
 __u32 Model0_sadb_sa_flags;
} __attribute__((packed));
/* sizeof(struct sadb_sa) == 16 */

struct Model0_sadb_lifetime {
 Model0___u16 Model0_sadb_lifetime_len;
 Model0___u16 Model0_sadb_lifetime_exttype;
 __u32 Model0_sadb_lifetime_allocations;
 __u64 Model0_sadb_lifetime_bytes;
 __u64 Model0_sadb_lifetime_addtime;
 __u64 Model0_sadb_lifetime_usetime;
} __attribute__((packed));
/* sizeof(struct sadb_lifetime) == 32 */

struct Model0_sadb_address {
 Model0___u16 Model0_sadb_address_len;
 Model0___u16 Model0_sadb_address_exttype;
 __u8 Model0_sadb_address_proto;
 __u8 Model0_sadb_address_prefixlen;
 Model0___u16 Model0_sadb_address_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_address) == 8 */

struct Model0_sadb_key {
 Model0___u16 Model0_sadb_key_len;
 Model0___u16 Model0_sadb_key_exttype;
 Model0___u16 Model0_sadb_key_bits;
 Model0___u16 Model0_sadb_key_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_key) == 8 */

struct Model0_sadb_ident {
 Model0___u16 Model0_sadb_ident_len;
 Model0___u16 Model0_sadb_ident_exttype;
 Model0___u16 Model0_sadb_ident_type;
 Model0___u16 Model0_sadb_ident_reserved;
 __u64 Model0_sadb_ident_id;
} __attribute__((packed));
/* sizeof(struct sadb_ident) == 16 */

struct Model0_sadb_sens {
 Model0___u16 Model0_sadb_sens_len;
 Model0___u16 Model0_sadb_sens_exttype;
 __u32 Model0_sadb_sens_dpd;
 __u8 Model0_sadb_sens_sens_level;
 __u8 Model0_sadb_sens_sens_len;
 __u8 Model0_sadb_sens_integ_level;
 __u8 Model0_sadb_sens_integ_len;
 __u32 Model0_sadb_sens_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_sens) == 16 */

/* followed by:
	__u64	sadb_sens_bitmap[sens_len];
	__u64	sadb_integ_bitmap[integ_len];  */

struct Model0_sadb_prop {
 Model0___u16 Model0_sadb_prop_len;
 Model0___u16 Model0_sadb_prop_exttype;
 __u8 Model0_sadb_prop_replay;
 __u8 Model0_sadb_prop_reserved[3];
} __attribute__((packed));
/* sizeof(struct sadb_prop) == 8 */

/* followed by:
	struct sadb_comb sadb_combs[(sadb_prop_len +
		sizeof(__u64) - sizeof(struct sadb_prop)) /
		sizeof(struct sadb_comb)]; */

struct Model0_sadb_comb {
 __u8 Model0_sadb_comb_auth;
 __u8 Model0_sadb_comb_encrypt;
 Model0___u16 Model0_sadb_comb_flags;
 Model0___u16 Model0_sadb_comb_auth_minbits;
 Model0___u16 Model0_sadb_comb_auth_maxbits;
 Model0___u16 Model0_sadb_comb_encrypt_minbits;
 Model0___u16 Model0_sadb_comb_encrypt_maxbits;
 __u32 Model0_sadb_comb_reserved;
 __u32 Model0_sadb_comb_soft_allocations;
 __u32 Model0_sadb_comb_hard_allocations;
 __u64 Model0_sadb_comb_soft_bytes;
 __u64 Model0_sadb_comb_hard_bytes;
 __u64 Model0_sadb_comb_soft_addtime;
 __u64 Model0_sadb_comb_hard_addtime;
 __u64 Model0_sadb_comb_soft_usetime;
 __u64 Model0_sadb_comb_hard_usetime;
} __attribute__((packed));
/* sizeof(struct sadb_comb) == 72 */

struct Model0_sadb_supported {
 Model0___u16 Model0_sadb_supported_len;
 Model0___u16 Model0_sadb_supported_exttype;
 __u32 Model0_sadb_supported_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_supported) == 8 */

/* followed by:
	struct sadb_alg sadb_algs[(sadb_supported_len +
		sizeof(__u64) - sizeof(struct sadb_supported)) /
		sizeof(struct sadb_alg)]; */

struct Model0_sadb_alg {
 __u8 Model0_sadb_alg_id;
 __u8 Model0_sadb_alg_ivlen;
 Model0___u16 Model0_sadb_alg_minbits;
 Model0___u16 Model0_sadb_alg_maxbits;
 Model0___u16 Model0_sadb_alg_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_alg) == 8 */

struct Model0_sadb_spirange {
 Model0___u16 Model0_sadb_spirange_len;
 Model0___u16 Model0_sadb_spirange_exttype;
 __u32 Model0_sadb_spirange_min;
 __u32 Model0_sadb_spirange_max;
 __u32 Model0_sadb_spirange_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_spirange) == 16 */

struct Model0_sadb_x_kmprivate {
 Model0___u16 Model0_sadb_x_kmprivate_len;
 Model0___u16 Model0_sadb_x_kmprivate_exttype;
 __u32 Model0_sadb_x_kmprivate_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_x_kmprivate) == 8 */

struct Model0_sadb_x_sa2 {
 Model0___u16 Model0_sadb_x_sa2_len;
 Model0___u16 Model0_sadb_x_sa2_exttype;
 __u8 Model0_sadb_x_sa2_mode;
 __u8 Model0_sadb_x_sa2_reserved1;
 Model0___u16 Model0_sadb_x_sa2_reserved2;
 __u32 Model0_sadb_x_sa2_sequence;
 __u32 Model0_sadb_x_sa2_reqid;
} __attribute__((packed));
/* sizeof(struct sadb_x_sa2) == 16 */

struct Model0_sadb_x_policy {
 Model0___u16 Model0_sadb_x_policy_len;
 Model0___u16 Model0_sadb_x_policy_exttype;
 Model0___u16 Model0_sadb_x_policy_type;
 __u8 Model0_sadb_x_policy_dir;
 __u8 Model0_sadb_x_policy_reserved;
 __u32 Model0_sadb_x_policy_id;
 __u32 Model0_sadb_x_policy_priority;
} __attribute__((packed));
/* sizeof(struct sadb_x_policy) == 16 */

struct Model0_sadb_x_ipsecrequest {
 Model0___u16 Model0_sadb_x_ipsecrequest_len;
 Model0___u16 Model0_sadb_x_ipsecrequest_proto;
 __u8 Model0_sadb_x_ipsecrequest_mode;
 __u8 Model0_sadb_x_ipsecrequest_level;
 Model0___u16 Model0_sadb_x_ipsecrequest_reserved1;
 __u32 Model0_sadb_x_ipsecrequest_reqid;
 __u32 Model0_sadb_x_ipsecrequest_reserved2;
} __attribute__((packed));
/* sizeof(struct sadb_x_ipsecrequest) == 16 */

/* This defines the TYPE of Nat Traversal in use.  Currently only one
 * type of NAT-T is supported, draft-ietf-ipsec-udp-encaps-06
 */
struct Model0_sadb_x_nat_t_type {
 Model0___u16 Model0_sadb_x_nat_t_type_len;
 Model0___u16 Model0_sadb_x_nat_t_type_exttype;
 __u8 Model0_sadb_x_nat_t_type_type;
 __u8 Model0_sadb_x_nat_t_type_reserved[3];
} __attribute__((packed));
/* sizeof(struct sadb_x_nat_t_type) == 8 */

/* Pass a NAT Traversal port (Source or Dest port) */
struct Model0_sadb_x_nat_t_port {
 Model0___u16 Model0_sadb_x_nat_t_port_len;
 Model0___u16 Model0_sadb_x_nat_t_port_exttype;
 Model0___be16 Model0_sadb_x_nat_t_port_port;
 Model0___u16 Model0_sadb_x_nat_t_port_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_x_nat_t_port) == 8 */

/* Generic LSM security context */
struct Model0_sadb_x_sec_ctx {
 Model0___u16 Model0_sadb_x_sec_len;
 Model0___u16 Model0_sadb_x_sec_exttype;
 __u8 Model0_sadb_x_ctx_alg; /* LSMs: e.g., selinux == 1 */
 __u8 Model0_sadb_x_ctx_doi;
 Model0___u16 Model0_sadb_x_ctx_len;
} __attribute__((packed));
/* sizeof(struct sadb_sec_ctx) = 8 */

/* Used by MIGRATE to pass addresses IKE will use to perform
 * negotiation with the peer */
struct Model0_sadb_x_kmaddress {
 Model0___u16 Model0_sadb_x_kmaddress_len;
 Model0___u16 Model0_sadb_x_kmaddress_exttype;
 __u32 Model0_sadb_x_kmaddress_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_x_kmaddress) == 8 */

/* To specify the SA dump filter */
struct Model0_sadb_x_filter {
 Model0___u16 Model0_sadb_x_filter_len;
 Model0___u16 Model0_sadb_x_filter_exttype;
 __u32 Model0_sadb_x_filter_saddr[4];
 __u32 Model0_sadb_x_filter_daddr[4];
 Model0___u16 Model0_sadb_x_filter_family;
 __u8 Model0_sadb_x_filter_splen;
 __u8 Model0_sadb_x_filter_dplen;
} __attribute__((packed));
/* sizeof(struct sadb_x_filter) == 40 */

/* Message types */
/* Security Association flags */





/* Security Association states */






/* Security Association types */
/* Authentication algorithms */
/* Encryption algorithms */
/* private allocations should use 249-255 (RFC2407) */



/* Compression algorithms */







/* Extension Header values */
/* The next four entries are for setting up NAT Traversal */





/* Used with MIGRATE to pass @ to IKE for negotiation */




/* Identity Extension values */





enum {
 Model0_IPSEC_MODE_ANY = 0, /* We do not support this for SA */
 Model0_IPSEC_MODE_TRANSPORT = 1,
 Model0_IPSEC_MODE_TUNNEL = 2,
 Model0_IPSEC_MODE_BEET = 3
};

enum {
 Model0_IPSEC_DIR_ANY = 0,
 Model0_IPSEC_DIR_INBOUND = 1,
 Model0_IPSEC_DIR_OUTBOUND = 2,
 Model0_IPSEC_DIR_FWD = 3, /* It is our own */
 Model0_IPSEC_DIR_MAX = 4,
 Model0_IPSEC_DIR_INVALID = 5
};

enum {
 Model0_IPSEC_POLICY_DISCARD = 0,
 Model0_IPSEC_POLICY_NONE = 1,
 Model0_IPSEC_POLICY_IPSEC = 2,
 Model0_IPSEC_POLICY_ENTRUST = 3,
 Model0_IPSEC_POLICY_BYPASS = 4
};

enum {
 Model0_IPSEC_LEVEL_DEFAULT = 0,
 Model0_IPSEC_LEVEL_USE = 1,
 Model0_IPSEC_LEVEL_REQUIRE = 2,
 Model0_IPSEC_LEVEL_UNIQUE = 3
};



/*
 * The x86 can do unaligned accesses itself.
 */








static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_u16 Model0_get_unaligned_le16(const void *Model0_p)
{
 return Model0___le16_to_cpup((Model0___le16 *)Model0_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_u32 Model0_get_unaligned_le32(const void *Model0_p)
{
 return Model0___le32_to_cpup((Model0___le32 *)Model0_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_u64 Model0_get_unaligned_le64(const void *Model0_p)
{
 return Model0___le64_to_cpup((Model0___le64 *)Model0_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_u16 Model0_get_unaligned_be16(const void *Model0_p)
{
 return Model0___be16_to_cpup((Model0___be16 *)Model0_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_u32 Model0_get_unaligned_be32(const void *Model0_p)
{
 return Model0___be32_to_cpup((Model0___be32 *)Model0_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model0_u64 Model0_get_unaligned_be64(const void *Model0_p)
{
 return Model0___be64_to_cpup((Model0___be64 *)Model0_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_put_unaligned_le16(Model0_u16 Model0_val, void *Model0_p)
{
 *((Model0___le16 *)Model0_p) = (( Model0___le16)(Model0___u16)(Model0_val));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_put_unaligned_le32(Model0_u32 Model0_val, void *Model0_p)
{
 *((Model0___le32 *)Model0_p) = (( Model0___le32)(__u32)(Model0_val));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_put_unaligned_le64(Model0_u64 Model0_val, void *Model0_p)
{
 *((Model0___le64 *)Model0_p) = (( Model0___le64)(__u64)(Model0_val));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_put_unaligned_be16(Model0_u16 Model0_val, void *Model0_p)
{
 *((Model0___be16 *)Model0_p) = (( Model0___be16)(__builtin_constant_p((Model0___u16)((Model0_val))) ? ((Model0___u16)( (((Model0___u16)((Model0_val)) & (Model0___u16)0x00ffU) << 8) | (((Model0___u16)((Model0_val)) & (Model0___u16)0xff00U) >> 8))) : Model0___fswab16((Model0_val))));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_put_unaligned_be32(Model0_u32 Model0_val, void *Model0_p)
{
 *((Model0___be32 *)Model0_p) = (( Model0___be32)(__builtin_constant_p((__u32)((Model0_val))) ? ((__u32)( (((__u32)((Model0_val)) & (__u32)0x000000ffUL) << 24) | (((__u32)((Model0_val)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((Model0_val)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((Model0_val)) & (__u32)0xff000000UL) >> 24))) : Model0___fswab32((Model0_val))));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model0_put_unaligned_be64(Model0_u64 Model0_val, void *Model0_p)
{
 *((Model0___be64 *)Model0_p) = (( Model0___be64)(__builtin_constant_p((__u64)((Model0_val))) ? ((__u64)( (((__u64)((Model0_val)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)((Model0_val)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)((Model0_val)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)((Model0_val)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)((Model0_val)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)((Model0_val)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)((Model0_val)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)((Model0_val)) & (__u64)0xff00000000000000ULL) >> 56))) : Model0___fswab64((Model0_val))));
}



/*
 * Cause a link-time error if we try an unaligned access other than
 * 1,2,4 or 8 bytes long
 */
extern void Model0___bad_unaligned_access_size(void);













struct Model0_sock_extended_err {
 __u32 Model0_ee_errno;
 __u8 Model0_ee_origin;
 __u8 Model0_ee_type;
 __u8 Model0_ee_code;
 __u8 Model0_ee_pad;
 __u32 Model0_ee_info;
 __u32 Model0_ee_data;
};
/**
 *	struct scm_timestamping - timestamps exposed through cmsg
 *
 *	The timestamping interfaces SO_TIMESTAMPING, MSG_TSTAMP_*
 *	communicate network timestamps by passing this struct in a cmsg with
 *	recvmsg(). See Documentation/networking/timestamping.txt for details.
 */
struct Model0_scm_timestamping {
 struct Model0_timespec Model0_ts[3];
};

/* The type of scm_timestamping, passed in sock_extended_err ee_info.
 * This defines the type of ts[0]. For SCM_TSTAMP_SND only, if ts[0]
 * is zero, then this is a hardware timestamp and recorded in ts[2].
 */
enum {
 Model0_SCM_TSTAMP_SND, /* driver passed skb to NIC, or HW */
 Model0_SCM_TSTAMP_SCHED, /* data entered the packet scheduler */
 Model0_SCM_TSTAMP_ACK, /* data acknowledged by peer */
};



struct Model0_sock_exterr_skb {
 union {
  struct Model0_inet_skb_parm Model0_h4;

  struct Model0_inet6_skb_parm Model0_h6;

 } Model0_header;
 struct Model0_sock_extended_err Model0_ee;
 Model0_u16 Model0_addr_offset;
 Model0___be16 Model0_port;
};
#endif
